{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "import sys; sys.path.append(\"../src2\")\n",
    "from pendulum import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load optimal control data\n",
    "trajs = np.load(\"energy_walks_alpha.npy\")\n",
    "ax1 = plot_traj(trajs[-1][:,1:5], arm=True)\n",
    "ax1.set_xlabel(r\"$x$\")\n",
    "ax1.set_ylabel(r\"$y$\")\n",
    "ax2 = plot_controls(trajs[-1][:,0], trajs[-1][:,-1])\n",
    "ax2.set_xlabel(r\"$Tau$\")\n",
    "ax2.set_ylabel(r\"$u$\")\n",
    "for traj in trajs:\n",
    "    t = traj[:,0]\n",
    "    x = traj[:,1:5]\n",
    "    u = traj[:,-1]\n",
    "    plot_traj(x, ax=ax1, alpha=0.05)\n",
    "    plot_controls(t, u, ax=ax2, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([354918, 4])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format data for neural network\n",
    "db = data(np.vstack(trajs), [1,2,3,4], [-1])\n",
    "db.i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate neural network\n",
    "nn = mlp([4, 50, 50, 50, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0; Testing Loss 0.005884160985343454; Training Loss 0.004794387821959924\n",
      "Episode 1; Testing Loss 0.005884218183143108; Training Loss 0.004794384494254969\n",
      "Episode 2; Testing Loss 0.00588422679634794; Training Loss 0.004794385865203296\n",
      "Episode 3; Testing Loss 0.0058841821021413205; Training Loss 0.004794382976391512\n",
      "Episode 4; Testing Loss 0.00588416378776544; Training Loss 0.00479437980964613\n",
      "Episode 5; Testing Loss 0.005884245782768113; Training Loss 0.004794373949302251\n",
      "Episode 6; Testing Loss 0.0058842910110231475; Training Loss 0.004794370055154585\n",
      "Episode 7; Testing Loss 0.005884239225782648; Training Loss 0.004794362804256742\n",
      "Episode 8; Testing Loss 0.005884187238419011; Training Loss 0.00479435717830755\n",
      "Episode 9; Testing Loss 0.005884214475042917; Training Loss 0.004794349847953296\n",
      "Episode 10; Testing Loss 0.005884241991625032; Training Loss 0.004794344077368118\n",
      "Episode 11; Testing Loss 0.005884184737951667; Training Loss 0.0047943373863431294\n",
      "Episode 12; Testing Loss 0.005884107110728229; Training Loss 0.004794329364717483\n",
      "Episode 13; Testing Loss 0.005884143244907975; Training Loss 0.0047943211715752825\n",
      "Episode 14; Testing Loss 0.005884234260705403; Training Loss 0.0047943113034241555\n",
      "Episode 15; Testing Loss 0.0058842507232791125; Training Loss 0.004794305435492976\n",
      "Episode 16; Testing Loss 0.005884189890469513; Training Loss 0.0047942971036736\n",
      "Episode 17; Testing Loss 0.00588413065096485; Training Loss 0.004794288745537196\n",
      "Episode 18; Testing Loss 0.005884130739629621; Training Loss 0.0047942771997585105\n",
      "Episode 19; Testing Loss 0.005884146889526445; Training Loss 0.004794267167888024\n",
      "Episode 20; Testing Loss 0.005884150524617592; Training Loss 0.004794259480561746\n",
      "Episode 21; Testing Loss 0.00588411050191633; Training Loss 0.004794250030911557\n",
      "Episode 22; Testing Loss 0.005884079879111277; Training Loss 0.0047942387562817474\n",
      "Episode 23; Testing Loss 0.0058841000121033675; Training Loss 0.004794229247540625\n",
      "Episode 24; Testing Loss 0.0058841082327022835; Training Loss 0.00479422148021295\n",
      "Episode 25; Testing Loss 0.0058840473762989355; Training Loss 0.004794208802371081\n",
      "Episode 26; Testing Loss 0.005884000685893207; Training Loss 0.004794197449360875\n",
      "Episode 27; Testing Loss 0.0058840422015680295; Training Loss 0.004794189167908686\n",
      "Episode 28; Testing Loss 0.005884098908710598; Training Loss 0.004794178597982977\n",
      "Episode 29; Testing Loss 0.005884092060733398; Training Loss 0.004794168374213389\n",
      "Episode 30; Testing Loss 0.005884053383548616; Training Loss 0.004794157382204129\n",
      "Episode 31; Testing Loss 0.0058840265264748165; Training Loss 0.004794145894163158\n",
      "Episode 32; Testing Loss 0.005884015261583727; Training Loss 0.0047941363337619695\n",
      "Episode 33; Testing Loss 0.005884015928647121; Training Loss 0.004794126596493146\n",
      "Episode 34; Testing Loss 0.005884015038712042; Training Loss 0.0047941158154043585\n",
      "Episode 35; Testing Loss 0.005884062925005452; Training Loss 0.004794103980607715\n",
      "Episode 36; Testing Loss 0.005884105135988284; Training Loss 0.00479409383384772\n",
      "Episode 37; Testing Loss 0.005884077152470436; Training Loss 0.004794082568421289\n",
      "Episode 38; Testing Loss 0.005883995053669469; Training Loss 0.00479407129911452\n",
      "Episode 39; Testing Loss 0.00588395280754146; Training Loss 0.004794063062796969\n",
      "Episode 40; Testing Loss 0.005884038299563895; Training Loss 0.004794051653005601\n",
      "Episode 41; Testing Loss 0.005884105558354176; Training Loss 0.004794040845869506\n",
      "Episode 42; Testing Loss 0.005884008439696534; Training Loss 0.0047940287041377165\n",
      "Episode 43; Testing Loss 0.005883924981398883; Training Loss 0.004794021238332411\n",
      "Episode 44; Testing Loss 0.0058840012193148794; Training Loss 0.004794009121639989\n",
      "Episode 45; Testing Loss 0.005884045357478522; Training Loss 0.004793998465780272\n",
      "Episode 46; Testing Loss 0.0058839203622714775; Training Loss 0.004793987107294406\n",
      "Episode 47; Testing Loss 0.005883831920046398; Training Loss 0.004793977669900207\n",
      "Episode 48; Testing Loss 0.005883915338314384; Training Loss 0.004793964773208846\n",
      "Episode 49; Testing Loss 0.005883991297691916; Training Loss 0.004793958191277136\n",
      "Episode 50; Testing Loss 0.005883869577509668; Training Loss 0.004793945256141688\n",
      "Episode 51; Testing Loss 0.005883778102291444; Training Loss 0.004793934211232548\n",
      "Episode 52; Testing Loss 0.005883863685721251; Training Loss 0.004793924040846835\n",
      "Episode 53; Testing Loss 0.005883958688233797; Training Loss 0.004793913963620914\n",
      "Episode 54; Testing Loss 0.005883932555519017; Training Loss 0.004793901505870238\n",
      "Episode 55; Testing Loss 0.0058838504326084624; Training Loss 0.00479389106654441\n",
      "Episode 56; Testing Loss 0.00588379175553243; Training Loss 0.004793882094680812\n",
      "Episode 57; Testing Loss 0.005883819410076626; Training Loss 0.004793870247383718\n",
      "Episode 58; Testing Loss 0.005883892154862354; Training Loss 0.00479385761725238\n",
      "Episode 59; Testing Loss 0.005883911843994606; Training Loss 0.004793847099982483\n",
      "Episode 60; Testing Loss 0.0058838359013430475; Training Loss 0.004793837121634446\n",
      "Episode 61; Testing Loss 0.005883760569827737; Training Loss 0.004793825822333086\n",
      "Episode 62; Testing Loss 0.005883800104635904; Training Loss 0.00479381561615093\n",
      "Episode 63; Testing Loss 0.005883908185879921; Training Loss 0.004793805410248361\n",
      "Episode 64; Testing Loss 0.005883876157488338; Training Loss 0.004793793721483042\n",
      "Episode 65; Testing Loss 0.005883778353649743; Training Loss 0.004793783367968119\n",
      "Episode 66; Testing Loss 0.005883761065941807; Training Loss 0.004793772936381786\n",
      "Episode 67; Testing Loss 0.005883840719437206; Training Loss 0.004793761578814995\n",
      "Episode 68; Testing Loss 0.005883837877695875; Training Loss 0.004793751302213502\n",
      "Episode 69; Testing Loss 0.005883737755828032; Training Loss 0.004793740899903481\n",
      "Episode 70; Testing Loss 0.005883702502074516; Training Loss 0.004793729072923743\n",
      "Episode 71; Testing Loss 0.005883760639200951; Training Loss 0.004793720919128633\n",
      "Episode 72; Testing Loss 0.005883807456797623; Training Loss 0.004793710169541871\n",
      "Episode 73; Testing Loss 0.00588378413753907; Training Loss 0.004793697771963481\n",
      "Episode 74; Testing Loss 0.005883716188088197; Training Loss 0.0047936861840307135\n",
      "Episode 75; Testing Loss 0.00588368950021388; Training Loss 0.004793677391555237\n",
      "Episode 76; Testing Loss 0.005883695601206119; Training Loss 0.004793665199852796\n",
      "Episode 77; Testing Loss 0.0058837300264802965; Training Loss 0.004793654521062852\n",
      "Episode 78; Testing Loss 0.005883816980464426; Training Loss 0.004793644469676339\n",
      "Episode 79; Testing Loss 0.005883840933826514; Training Loss 0.004793633859984537\n",
      "Episode 80; Testing Loss 0.005883744718454798; Training Loss 0.0047936224575811206\n",
      "Episode 81; Testing Loss 0.005883697381163433; Training Loss 0.004793612859509342\n",
      "Episode 82; Testing Loss 0.005883767856560002; Training Loss 0.004793601139997121\n",
      "Episode 83; Testing Loss 0.0058838049391354485; Training Loss 0.00479359170177236\n",
      "Episode 84; Testing Loss 0.00588369469499993; Training Loss 0.004793578996368994\n",
      "Episode 85; Testing Loss 0.005883571642848393; Training Loss 0.004793570426410537\n",
      "Episode 86; Testing Loss 0.005883645081763346; Training Loss 0.0047935581773546785\n",
      "Episode 87; Testing Loss 0.0058837520780125096; Training Loss 0.004793549743401891\n",
      "Episode 88; Testing Loss 0.005883669317012165; Training Loss 0.004793538158545206\n",
      "Episode 89; Testing Loss 0.005883507268944259; Training Loss 0.004793526515191519\n",
      "Episode 90; Testing Loss 0.005883512804813753; Training Loss 0.004793516358079745\n",
      "Episode 91; Testing Loss 0.005883634320960031; Training Loss 0.004793507743298173\n",
      "Episode 92; Testing Loss 0.005883639614883013; Training Loss 0.004793495537506463\n",
      "Episode 93; Testing Loss 0.005883538168223839; Training Loss 0.0047934827162509205\n",
      "Episode 94; Testing Loss 0.005883504550569337; Training Loss 0.004793474010490431\n",
      "Episode 95; Testing Loss 0.005883620034425768; Training Loss 0.0047934624528265775\n",
      "Episode 96; Testing Loss 0.005883675925551658; Training Loss 0.0047934501642127765\n",
      "Episode 97; Testing Loss 0.005883563843047855; Training Loss 0.004793441770918294\n",
      "Episode 98; Testing Loss 0.0058835129517883136; Training Loss 0.004793433384138302\n",
      "Episode 99; Testing Loss 0.0058835673325858115; Training Loss 0.004793421186023635\n",
      "Episode 100; Testing Loss 0.005883632300862754; Training Loss 0.004793408256547402\n",
      "Episode 101; Testing Loss 0.005883632863231672; Training Loss 0.00479339864259326\n",
      "Episode 102; Testing Loss 0.005883558180305217; Training Loss 0.004793390138881552\n",
      "Episode 103; Testing Loss 0.005883533171816208; Training Loss 0.004793378879829012\n",
      "Episode 104; Testing Loss 0.005883548621550996; Training Loss 0.004793365654584886\n",
      "Episode 105; Testing Loss 0.005883513617707351; Training Loss 0.004793354369864001\n",
      "Episode 106; Testing Loss 0.005883468536821835; Training Loss 0.004793344808007314\n",
      "Episode 107; Testing Loss 0.005883449934009329; Training Loss 0.004793334682767261\n",
      "Episode 108; Testing Loss 0.00588350455474361; Training Loss 0.004793323279423862\n",
      "Episode 109; Testing Loss 0.005883527163069857; Training Loss 0.004793313277687926\n",
      "Episode 110; Testing Loss 0.005883463634606466; Training Loss 0.004793300693689258\n",
      "Episode 111; Testing Loss 0.0058834002831934795; Training Loss 0.004793292342878245\n",
      "Episode 112; Testing Loss 0.005883462096233042; Training Loss 0.0047932800617864185\n",
      "Episode 113; Testing Loss 0.005883505102729199; Training Loss 0.004793269253476086\n",
      "Episode 114; Testing Loss 0.0058834246071793155; Training Loss 0.004793259499732928\n",
      "Episode 115; Testing Loss 0.005883338575858788; Training Loss 0.00479325003079558\n",
      "Episode 116; Testing Loss 0.00588340112686277; Training Loss 0.004793238152739216\n",
      "Episode 117; Testing Loss 0.005883503409180724; Training Loss 0.004793225878016982\n",
      "Episode 118; Testing Loss 0.005883482250661855; Training Loss 0.004793216825484231\n",
      "Episode 119; Testing Loss 0.005883393695949009; Training Loss 0.004793207281202494\n",
      "Episode 120; Testing Loss 0.005883364500711441; Training Loss 0.004793195618638227\n",
      "Episode 121; Testing Loss 0.0058834204120231035; Training Loss 0.004793184457748682\n",
      "Episode 122; Testing Loss 0.005883414714222131; Training Loss 0.004793173916532628\n",
      "Episode 123; Testing Loss 0.005883314888741302; Training Loss 0.004793163575830169\n",
      "Episode 124; Testing Loss 0.0058833042531237145; Training Loss 0.0047931532876537\n",
      "Episode 125; Testing Loss 0.005883387295530518; Training Loss 0.004793140728996238\n",
      "Episode 126; Testing Loss 0.0058834087843879935; Training Loss 0.0047931295717139315\n",
      "Episode 127; Testing Loss 0.0058833538630527205; Training Loss 0.0047931192322812\n",
      "Episode 128; Testing Loss 0.005883287085299829; Training Loss 0.004793108795512518\n",
      "Episode 129; Testing Loss 0.0058832361316448496; Training Loss 0.004793097093510962\n",
      "Episode 130; Testing Loss 0.005883254593740076; Training Loss 0.004793086313539676\n",
      "Episode 131; Testing Loss 0.005883345771684813; Training Loss 0.004793075228055441\n",
      "Episode 132; Testing Loss 0.005883367057210755; Training Loss 0.004793065708892957\n",
      "Episode 133; Testing Loss 0.005883305484550192; Training Loss 0.004793055185742837\n",
      "Episode 134; Testing Loss 0.005883253420635232; Training Loss 0.004793042771209553\n",
      "Episode 135; Testing Loss 0.0058832674639986045; Training Loss 0.004793032671784837\n",
      "Episode 136; Testing Loss 0.005883258364294431; Training Loss 0.0047930241479614815\n",
      "Episode 137; Testing Loss 0.005883269189167616; Training Loss 0.004793011724950017\n",
      "Episode 138; Testing Loss 0.005883276233567723; Training Loss 0.004792999920824918\n",
      "Episode 139; Testing Loss 0.0058832035474797525; Training Loss 0.004792992419261656\n",
      "Episode 140; Testing Loss 0.005883194796612681; Training Loss 0.004792983784088136\n",
      "Episode 141; Testing Loss 0.005883235780786411; Training Loss 0.00479297203512215\n",
      "Episode 142; Testing Loss 0.005883228644352936; Training Loss 0.004792958250144158\n",
      "Episode 143; Testing Loss 0.005883143471255531; Training Loss 0.0047929491023576175\n",
      "Episode 144; Testing Loss 0.005883074771461536; Training Loss 0.004792939807698333\n",
      "Episode 145; Testing Loss 0.005883117401357914; Training Loss 0.004792928639364422\n",
      "Episode 146; Testing Loss 0.0058831798538064636; Training Loss 0.004792916936464398\n",
      "Episode 147; Testing Loss 0.005883191619057791; Training Loss 0.004792904539455034\n",
      "Episode 148; Testing Loss 0.005883182237720962; Training Loss 0.004792894790438879\n",
      "Episode 149; Testing Loss 0.0058831564475629325; Training Loss 0.004792882311911529\n",
      "Episode 150; Testing Loss 0.005883131421155604; Training Loss 0.004792872161357028\n",
      "Episode 151; Testing Loss 0.005883118441235442; Training Loss 0.004792863732499887\n",
      "Episode 152; Testing Loss 0.005883110634287886; Training Loss 0.004792851288926235\n",
      "Episode 153; Testing Loss 0.0058831272515047495; Training Loss 0.004792839291730012\n",
      "Episode 154; Testing Loss 0.00588320621493728; Training Loss 0.004792829619388406\n",
      "Episode 155; Testing Loss 0.005883258978829617; Training Loss 0.004792818493905946\n",
      "Episode 156; Testing Loss 0.005883233427972455; Training Loss 0.004792808769605919\n",
      "Episode 157; Testing Loss 0.005883120085230462; Training Loss 0.004792798850038907\n",
      "Episode 158; Testing Loss 0.005883095452683382; Training Loss 0.004792786812243871\n",
      "Episode 159; Testing Loss 0.005883155291163721; Training Loss 0.004792775042940454\n",
      "Episode 160; Testing Loss 0.005883144465427673; Training Loss 0.00479276451638332\n",
      "Episode 161; Testing Loss 0.005883074863213616; Training Loss 0.004792753758970894\n",
      "Episode 162; Testing Loss 0.005883051684223896; Training Loss 0.004792743682541016\n",
      "Episode 163; Testing Loss 0.005883100014718597; Training Loss 0.004792733000476484\n",
      "Episode 164; Testing Loss 0.00588315433838449; Training Loss 0.00479272181082448\n",
      "Episode 165; Testing Loss 0.005883104124207469; Training Loss 0.004792711944428544\n",
      "Episode 166; Testing Loss 0.005883014701965002; Training Loss 0.004792699940549074\n",
      "Episode 167; Testing Loss 0.005882998459130575; Training Loss 0.0047926893506862415\n",
      "Episode 168; Testing Loss 0.005883028082649908; Training Loss 0.004792680934102589\n",
      "Episode 169; Testing Loss 0.005883077683005417; Training Loss 0.004792669899432695\n",
      "Episode 170; Testing Loss 0.005883106422826156; Training Loss 0.004792658395364782\n",
      "Episode 171; Testing Loss 0.0058830804462808505; Training Loss 0.004792645967431852\n",
      "Episode 172; Testing Loss 0.005883030733318825; Training Loss 0.004792636529494062\n",
      "Episode 173; Testing Loss 0.005883031180046931; Training Loss 0.004792624222454799\n",
      "Episode 174; Testing Loss 0.005883035977256858; Training Loss 0.00479261352031156\n",
      "Episode 175; Testing Loss 0.00588302384073369; Training Loss 0.0047926057244200414\n",
      "Episode 176; Testing Loss 0.005882949375633903; Training Loss 0.004792593563163937\n",
      "Episode 177; Testing Loss 0.0058829238525962444; Training Loss 0.004792582409510556\n",
      "Episode 178; Testing Loss 0.005882985865081076; Training Loss 0.004792573042409434\n",
      "Episode 179; Testing Loss 0.0058830312148116844; Training Loss 0.0047925621825834945\n",
      "Episode 180; Testing Loss 0.0058829947165149905; Training Loss 0.0047925494473075505\n",
      "Episode 181; Testing Loss 0.005882949150004688; Training Loss 0.004792540009363146\n",
      "Episode 182; Testing Loss 0.005882962581022526; Training Loss 0.0047925298094711796\n",
      "Episode 183; Testing Loss 0.005882976038512588; Training Loss 0.0047925205228327265\n",
      "Episode 184; Testing Loss 0.005882926519799631; Training Loss 0.004792507047060939\n",
      "Episode 185; Testing Loss 0.005882877480720541; Training Loss 0.004792496896110012\n",
      "Episode 186; Testing Loss 0.005882919067886993; Training Loss 0.0047924889162717236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 187; Testing Loss 0.0058830031203659545; Training Loss 0.004792478475091347\n",
      "Episode 188; Testing Loss 0.005882991517148623; Training Loss 0.004792465563361731\n",
      "Episode 189; Testing Loss 0.005882917492475668; Training Loss 0.004792452194494648\n",
      "Episode 190; Testing Loss 0.005882921850815568; Training Loss 0.004792442068479462\n",
      "Episode 191; Testing Loss 0.005882969302266813; Training Loss 0.0047924328264499745\n",
      "Episode 192; Testing Loss 0.005882952303140911; Training Loss 0.0047924219440018834\n",
      "Episode 193; Testing Loss 0.005882844872045354; Training Loss 0.0047924103745041755\n",
      "Episode 194; Testing Loss 0.00588281164581718; Training Loss 0.004792400071946708\n",
      "Episode 195; Testing Loss 0.005882887566418256; Training Loss 0.004792389805551975\n",
      "Episode 196; Testing Loss 0.005882958014962906; Training Loss 0.004792379705354753\n",
      "Episode 197; Testing Loss 0.005882900983779262; Training Loss 0.004792366322346158\n",
      "Episode 198; Testing Loss 0.005882798834567954; Training Loss 0.004792357050612001\n",
      "Episode 199; Testing Loss 0.0058827729030837056; Training Loss 0.004792348134801142\n",
      "Episode 200; Testing Loss 0.005882834763735303; Training Loss 0.004792335776874679\n",
      "Episode 201; Testing Loss 0.005882868464357579; Training Loss 0.004792323901813312\n",
      "Episode 202; Testing Loss 0.005882892037674592; Training Loss 0.0047923146171360564\n",
      "Episode 203; Testing Loss 0.005882912653149448; Training Loss 0.004792305670900738\n",
      "Episode 204; Testing Loss 0.005882923518583974; Training Loss 0.004792294256857975\n",
      "Episode 205; Testing Loss 0.005882897161156169; Training Loss 0.004792280721224364\n",
      "Episode 206; Testing Loss 0.005882868742512877; Training Loss 0.0047922714223131305\n",
      "Episode 207; Testing Loss 0.005882835294261237; Training Loss 0.004792262500252126\n",
      "Episode 208; Testing Loss 0.005882765304939062; Training Loss 0.0047922509930365745\n",
      "Episode 209; Testing Loss 0.005882719714924398; Training Loss 0.004792239205197519\n",
      "Episode 210; Testing Loss 0.005882777748639512; Training Loss 0.004792227730630644\n",
      "Episode 211; Testing Loss 0.005882830142468512; Training Loss 0.004792218983750686\n",
      "Episode 212; Testing Loss 0.005882790240789047; Training Loss 0.0047922071653182016\n",
      "Episode 213; Testing Loss 0.005882716050290575; Training Loss 0.00479219559809515\n",
      "Episode 214; Testing Loss 0.005882670197023586; Training Loss 0.004792185176405873\n",
      "Episode 215; Testing Loss 0.005882693770301344; Training Loss 0.004792173021855065\n",
      "Episode 216; Testing Loss 0.005882731190135796; Training Loss 0.004792165068032502\n",
      "Episode 217; Testing Loss 0.005882717371198063; Training Loss 0.00479215419458569\n",
      "Episode 218; Testing Loss 0.005882702980823385; Training Loss 0.004792139990549503\n",
      "Episode 219; Testing Loss 0.005882737669235863; Training Loss 0.0047921325885347515\n",
      "Episode 220; Testing Loss 0.005882748305211045; Training Loss 0.004792122798273503\n",
      "Episode 221; Testing Loss 0.0058827184592710495; Training Loss 0.004792111266226195\n",
      "Episode 222; Testing Loss 0.005882689259337283; Training Loss 0.004792098441881174\n",
      "Episode 223; Testing Loss 0.0058826986472672385; Training Loss 0.004792089430992125\n",
      "Episode 224; Testing Loss 0.005882712012687188; Training Loss 0.004792079438647834\n",
      "Episode 225; Testing Loss 0.005882727653976927; Training Loss 0.004792068678760251\n",
      "Episode 226; Testing Loss 0.005882701111605394; Training Loss 0.004792055564153152\n",
      "Episode 227; Testing Loss 0.00588261595704808; Training Loss 0.0047920463100687965\n",
      "Episode 228; Testing Loss 0.005882583875027138; Training Loss 0.004792037656379448\n",
      "Episode 229; Testing Loss 0.005882631662889569; Training Loss 0.004792026528720736\n",
      "Episode 230; Testing Loss 0.005882661291701259; Training Loss 0.004792014111991035\n",
      "Episode 231; Testing Loss 0.005882601580559131; Training Loss 0.004792001675093847\n",
      "Episode 232; Testing Loss 0.005882535085722414; Training Loss 0.0047919919650028266\n",
      "Episode 233; Testing Loss 0.005882586415052421; Training Loss 0.004791980877439185\n",
      "Episode 234; Testing Loss 0.005882684988434975; Training Loss 0.004791970789097492\n",
      "Episode 235; Testing Loss 0.0058826267417805824; Training Loss 0.00479195999308711\n",
      "Episode 236; Testing Loss 0.005882511307258145; Training Loss 0.004791948593578593\n",
      "Episode 237; Testing Loss 0.005882571372905897; Training Loss 0.004791938527551626\n",
      "Episode 238; Testing Loss 0.005882692437157604; Training Loss 0.004791928887131956\n",
      "Episode 239; Testing Loss 0.005882673998383667; Training Loss 0.004791917122111992\n",
      "Episode 240; Testing Loss 0.005882521082567776; Training Loss 0.004791907450772183\n",
      "Episode 241; Testing Loss 0.0058824691233361585; Training Loss 0.00479189714253753\n",
      "Episode 242; Testing Loss 0.005882522928314727; Training Loss 0.004791884598139606\n",
      "Episode 243; Testing Loss 0.005882520592750973; Training Loss 0.004791871593081425\n",
      "Episode 244; Testing Loss 0.0058824145809395515; Training Loss 0.004791863021031051\n",
      "Episode 245; Testing Loss 0.005882342721180216; Training Loss 0.004791852466790683\n",
      "Episode 246; Testing Loss 0.005882384030203746; Training Loss 0.004791840203400817\n",
      "Episode 247; Testing Loss 0.005882448500715044; Training Loss 0.004791826987623774\n",
      "Episode 248; Testing Loss 0.005882408231173463; Training Loss 0.004791816580875459\n",
      "Episode 249; Testing Loss 0.005882340228105826; Training Loss 0.004791807515255848\n",
      "Episode 250; Testing Loss 0.005882374450633703; Training Loss 0.004791795937192242\n",
      "Episode 251; Testing Loss 0.005882491708870933; Training Loss 0.004791782721446463\n",
      "Episode 252; Testing Loss 0.005882534934319731; Training Loss 0.004791768596451086\n",
      "Episode 253; Testing Loss 0.005882506632954008; Training Loss 0.004791760880696481\n",
      "Episode 254; Testing Loss 0.005882501380585204; Training Loss 0.004791752606405081\n",
      "Episode 255; Testing Loss 0.0058825506119313965; Training Loss 0.004791740420235682\n",
      "Episode 256; Testing Loss 0.005882627738357133; Training Loss 0.004791727922981494\n",
      "Episode 257; Testing Loss 0.005882615113553599; Training Loss 0.0047917137745688045\n",
      "Episode 258; Testing Loss 0.005882533047840916; Training Loss 0.004791704088248486\n",
      "Episode 259; Testing Loss 0.005882443817215577; Training Loss 0.004791696225833194\n",
      "Episode 260; Testing Loss 0.005882489866692881; Training Loss 0.004791685525653019\n",
      "Episode 261; Testing Loss 0.005882585675155696; Training Loss 0.0047916731532508024\n",
      "Episode 262; Testing Loss 0.005882584205873071; Training Loss 0.004791659006616747\n",
      "Episode 263; Testing Loss 0.0058825158078324065; Training Loss 0.00479164900027351\n",
      "Episode 264; Testing Loss 0.005882502381829567; Training Loss 0.0047916399205380125\n",
      "Episode 265; Testing Loss 0.005882558881081286; Training Loss 0.004791630756424937\n",
      "Episode 266; Testing Loss 0.005882576959719366; Training Loss 0.0047916173419089375\n",
      "Episode 267; Testing Loss 0.005882538861948832; Training Loss 0.004791602593233584\n",
      "Episode 268; Testing Loss 0.005882565677775172; Training Loss 0.004791593571223217\n",
      "Episode 269; Testing Loss 0.005882591975932158; Training Loss 0.004791584943677326\n",
      "Episode 270; Testing Loss 0.005882592466117824; Training Loss 0.004791573709103745\n",
      "Episode 271; Testing Loss 0.005882571644810181; Training Loss 0.004791560371823129\n",
      "Episode 272; Testing Loss 0.005882572653910312; Training Loss 0.004791546196948685\n",
      "Episode 273; Testing Loss 0.005882609756308065; Training Loss 0.004791536497483082\n",
      "Episode 274; Testing Loss 0.005882575858827085; Training Loss 0.004791525086734589\n",
      "Episode 275; Testing Loss 0.005882562217001057; Training Loss 0.004791512593161186\n",
      "Episode 276; Testing Loss 0.005882569014211319; Training Loss 0.004791503813288675\n",
      "Episode 277; Testing Loss 0.005882537508739354; Training Loss 0.004791493508285101\n",
      "Episode 278; Testing Loss 0.005882522663013452; Training Loss 0.004791481851466875\n",
      "Episode 279; Testing Loss 0.00588253431242461; Training Loss 0.00479146895642314\n",
      "Episode 280; Testing Loss 0.005882533901043583; Training Loss 0.004791461294179817\n",
      "Episode 281; Testing Loss 0.005882539848319244; Training Loss 0.004791451802357905\n",
      "Episode 282; Testing Loss 0.005882525090625028; Training Loss 0.004791440395193605\n",
      "Episode 283; Testing Loss 0.005882502436618898; Training Loss 0.004791427472064578\n",
      "Episode 284; Testing Loss 0.0058825198582093405; Training Loss 0.0047914141067863176\n",
      "Episode 285; Testing Loss 0.005882520919525687; Training Loss 0.004791404306943976\n",
      "Episode 286; Testing Loss 0.005882491059144982; Training Loss 0.0047913936124990755\n",
      "Episode 287; Testing Loss 0.0058824725243318545; Training Loss 0.004791380957918486\n",
      "Episode 288; Testing Loss 0.005882533735745801; Training Loss 0.004791370002238933\n",
      "Episode 289; Testing Loss 0.005882563300985847; Training Loss 0.004791359912180334\n",
      "Episode 290; Testing Loss 0.005882480274042528; Training Loss 0.00479134865368438\n",
      "Episode 291; Testing Loss 0.005882445045086165; Training Loss 0.0047913395954430164\n",
      "Episode 292; Testing Loss 0.005882552493000302; Training Loss 0.00479132805213022\n",
      "Episode 293; Testing Loss 0.005882603787042054; Training Loss 0.0047913164343674226\n",
      "Episode 294; Testing Loss 0.005882499365135904; Training Loss 0.004791304532240964\n",
      "Episode 295; Testing Loss 0.005882415804413713; Training Loss 0.004791294524367416\n",
      "Episode 296; Testing Loss 0.0058824223477227395; Training Loss 0.00479128251133077\n",
      "Episode 297; Testing Loss 0.005882514858701509; Training Loss 0.004791273008194752\n",
      "Episode 298; Testing Loss 0.005882560434865477; Training Loss 0.004791261994123255\n",
      "Episode 299; Testing Loss 0.005882499003816113; Training Loss 0.00479125046096666\n",
      "Episode 300; Testing Loss 0.0058824114191080275; Training Loss 0.004791241992239597\n",
      "Episode 301; Testing Loss 0.005882411762613099; Training Loss 0.004791231836034428\n",
      "Episode 302; Testing Loss 0.005882477468622629; Training Loss 0.00479122006441111\n",
      "Episode 303; Testing Loss 0.005882482086482784; Training Loss 0.004791207296764407\n",
      "Episode 304; Testing Loss 0.005882444034742743; Training Loss 0.004791196803961934\n",
      "Episode 305; Testing Loss 0.005882390541208659; Training Loss 0.004791184458370065\n",
      "Episode 306; Testing Loss 0.005882418883966968; Training Loss 0.004791173255672187\n",
      "Episode 307; Testing Loss 0.005882505171072143; Training Loss 0.0047911635408024785\n",
      "Episode 308; Testing Loss 0.005882497055593878; Training Loss 0.004791151777774516\n",
      "Episode 309; Testing Loss 0.005882479144977291; Training Loss 0.004791141173132138\n",
      "Episode 310; Testing Loss 0.00588245644315023; Training Loss 0.004791129881876523\n",
      "Episode 311; Testing Loss 0.00588247281948122; Training Loss 0.004791120974707703\n",
      "Episode 312; Testing Loss 0.005882521479884744; Training Loss 0.004791111521584073\n",
      "Episode 313; Testing Loss 0.005882486657949247; Training Loss 0.00479109777174955\n",
      "Episode 314; Testing Loss 0.005882445276758622; Training Loss 0.004791087650458378\n",
      "Episode 315; Testing Loss 0.005882426343740541; Training Loss 0.004791076859954278\n",
      "Episode 316; Testing Loss 0.005882451901552381; Training Loss 0.004791065911817676\n",
      "Episode 317; Testing Loss 0.005882463084435078; Training Loss 0.004791053584985979\n",
      "Episode 318; Testing Loss 0.005882517880930901; Training Loss 0.00479104246866854\n",
      "Episode 319; Testing Loss 0.005882514343186243; Training Loss 0.004791031606216897\n",
      "Episode 320; Testing Loss 0.005882440137059628; Training Loss 0.004791021419232299\n",
      "Episode 321; Testing Loss 0.005882428453159942; Training Loss 0.004791010338066249\n",
      "Episode 322; Testing Loss 0.005882450506487389; Training Loss 0.0047909990226170495\n",
      "Episode 323; Testing Loss 0.00588245783281349; Training Loss 0.004790988145191271\n",
      "Episode 324; Testing Loss 0.00588238114480328; Training Loss 0.004790976969357061\n",
      "Episode 325; Testing Loss 0.005882325334296848; Training Loss 0.0047909688849859565\n",
      "Episode 326; Testing Loss 0.005882344523330223; Training Loss 0.004790957245941709\n",
      "Episode 327; Testing Loss 0.005882385685064636; Training Loss 0.004790944877051424\n",
      "Episode 328; Testing Loss 0.005882418122441295; Training Loss 0.004790936831383702\n",
      "Episode 329; Testing Loss 0.005882380632395534; Training Loss 0.004790926772907572\n",
      "Episode 330; Testing Loss 0.005882342914924024; Training Loss 0.004790915207216964\n",
      "Episode 331; Testing Loss 0.005882371430956376; Training Loss 0.004790901867133712\n",
      "Episode 332; Testing Loss 0.005882416992265364; Training Loss 0.004790891664763933\n",
      "Episode 333; Testing Loss 0.005882415284752168; Training Loss 0.004790882195892377\n",
      "Episode 334; Testing Loss 0.005882391343175191; Training Loss 0.004790870914913055\n",
      "Episode 335; Testing Loss 0.0058823759807373754; Training Loss 0.004790858275649501\n",
      "Episode 336; Testing Loss 0.0058823400091256505; Training Loss 0.004790848536899507\n",
      "Episode 337; Testing Loss 0.005882345696888627; Training Loss 0.004790839666613445\n",
      "Episode 338; Testing Loss 0.005882365932385284; Training Loss 0.004790827688653078\n",
      "Episode 339; Testing Loss 0.005882328186195636; Training Loss 0.004790814579670922\n",
      "Episode 340; Testing Loss 0.005882273763925059; Training Loss 0.004790805260937911\n",
      "Episode 341; Testing Loss 0.005882296354951901; Training Loss 0.00479079677937151\n",
      "Episode 342; Testing Loss 0.005882308649970986; Training Loss 0.004790785134946664\n",
      "Episode 343; Testing Loss 0.005882285846872983; Training Loss 0.004790772093078083\n",
      "Episode 344; Testing Loss 0.005882294746237875; Training Loss 0.004790762822016172\n",
      "Episode 345; Testing Loss 0.005882301933400221; Training Loss 0.00479075428891892\n",
      "Episode 346; Testing Loss 0.005882327785523042; Training Loss 0.004790742412810222\n",
      "Episode 347; Testing Loss 0.00588230507865419; Training Loss 0.004790729230124465\n",
      "Episode 348; Testing Loss 0.0058822713896381894; Training Loss 0.004790716845794412\n",
      "Episode 349; Testing Loss 0.005882282365030588; Training Loss 0.004790709194889545\n",
      "Episode 350; Testing Loss 0.005882278320110803; Training Loss 0.0047906970401963645\n",
      "Episode 351; Testing Loss 0.005882268826993769; Training Loss 0.004790683781439975\n",
      "Episode 352; Testing Loss 0.00588226995414283; Training Loss 0.004790675061769542\n",
      "Episode 353; Testing Loss 0.005882309557556869; Training Loss 0.004790664572513635\n",
      "Episode 354; Testing Loss 0.005882345811844971; Training Loss 0.004790656074964547\n",
      "Episode 355; Testing Loss 0.005882351389713076; Training Loss 0.004790645498726176\n",
      "Episode 356; Testing Loss 0.005882325220746176; Training Loss 0.004790633242488957\n",
      "Episode 357; Testing Loss 0.005882296995259763; Training Loss 0.004790619951341013\n",
      "Episode 358; Testing Loss 0.005882198191367136; Training Loss 0.004790608520540275\n",
      "Episode 359; Testing Loss 0.005882144713857297; Training Loss 0.00479059836982202\n",
      "Episode 360; Testing Loss 0.005882144062729034; Training Loss 0.004790588424633067\n",
      "Episode 361; Testing Loss 0.005882186130048071; Training Loss 0.004790576505930253\n",
      "Episode 362; Testing Loss 0.005882206835664044; Training Loss 0.004790564578481146\n",
      "Episode 363; Testing Loss 0.00588219191447488; Training Loss 0.004790555688109855\n",
      "Episode 364; Testing Loss 0.005882157017192058; Training Loss 0.004790546196122839\n",
      "Episode 365; Testing Loss 0.005882183442347116; Training Loss 0.0047905344465298495\n",
      "Episode 366; Testing Loss 0.005882235574475415; Training Loss 0.004790522279564363\n",
      "Episode 367; Testing Loss 0.005882252101639063; Training Loss 0.004790510876741316\n",
      "Episode 368; Testing Loss 0.005882203252538193; Training Loss 0.004790497758359606\n",
      "Episode 369; Testing Loss 0.0058821270454607185; Training Loss 0.00479048867327927\n",
      "Episode 370; Testing Loss 0.005882123360021265; Training Loss 0.004790479901323647\n",
      "Episode 371; Testing Loss 0.00588216391984085; Training Loss 0.004790465909590951\n",
      "Episode 372; Testing Loss 0.005882184451209523; Training Loss 0.004790456190776211\n",
      "Episode 373; Testing Loss 0.005882180038595387; Training Loss 0.004790446737450052\n",
      "Episode 374; Testing Loss 0.005882148282962559; Training Loss 0.004790434789575478\n",
      "Episode 375; Testing Loss 0.005882122949315935; Training Loss 0.004790425750680363\n",
      "Episode 376; Testing Loss 0.005882086418587083; Training Loss 0.004790415440425326\n",
      "Episode 377; Testing Loss 0.005882063819902339; Training Loss 0.004790403059138685\n",
      "Episode 378; Testing Loss 0.005882050299501328; Training Loss 0.004790388999951151\n",
      "Episode 379; Testing Loss 0.005882042361499895; Training Loss 0.004790380760363379\n",
      "Episode 380; Testing Loss 0.0058820407330162075; Training Loss 0.004790371091394307\n",
      "Episode 381; Testing Loss 0.005882035785475446; Training Loss 0.004790357270219124\n",
      "Episode 382; Testing Loss 0.005882049253139113; Training Loss 0.004790345080576299\n",
      "Episode 383; Testing Loss 0.00588209843703419; Training Loss 0.004790337809059677\n",
      "Episode 384; Testing Loss 0.005882112412664775; Training Loss 0.004790328940751741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 385; Testing Loss 0.005882099305242133; Training Loss 0.004790317369971497\n",
      "Episode 386; Testing Loss 0.005882093379041107; Training Loss 0.004790304085718229\n",
      "Episode 387; Testing Loss 0.005882098057811021; Training Loss 0.004790289873281444\n",
      "Episode 388; Testing Loss 0.005882131408489812; Training Loss 0.004790278618919645\n",
      "Episode 389; Testing Loss 0.005882130243696901; Training Loss 0.004790268808374391\n",
      "Episode 390; Testing Loss 0.00588208917479872; Training Loss 0.004790257366977871\n",
      "Episode 391; Testing Loss 0.005882018606355848; Training Loss 0.004790244548721004\n",
      "Episode 392; Testing Loss 0.005882011710058531; Training Loss 0.004790234117959056\n",
      "Episode 393; Testing Loss 0.005882129277471205; Training Loss 0.004790223223873164\n",
      "Episode 394; Testing Loss 0.005882170138452334; Training Loss 0.004790213994657566\n",
      "Episode 395; Testing Loss 0.005882107733023414; Training Loss 0.0047902021053263955\n",
      "Episode 396; Testing Loss 0.0058820469765667426; Training Loss 0.004790191862401898\n",
      "Episode 397; Testing Loss 0.005882042671117405; Training Loss 0.0047901799725397366\n",
      "Episode 398; Testing Loss 0.00588206160884064; Training Loss 0.00479016691670278\n",
      "Episode 399; Testing Loss 0.0058820403792456235; Training Loss 0.004790156772025695\n",
      "Episode 400; Testing Loss 0.005881985352875044; Training Loss 0.004790146385786172\n",
      "Episode 401; Testing Loss 0.005881968260079373; Training Loss 0.0047901344278382186\n",
      "Episode 402; Testing Loss 0.005882033897680063; Training Loss 0.004790122521571989\n",
      "Episode 403; Testing Loss 0.005882035584099263; Training Loss 0.0047901124031945255\n",
      "Episode 404; Testing Loss 0.005881953111193689; Training Loss 0.004790100162285875\n",
      "Episode 405; Testing Loss 0.005881946717092223; Training Loss 0.004790089395983812\n",
      "Episode 406; Testing Loss 0.005882029472958277; Training Loss 0.00479008030710918\n",
      "Episode 407; Testing Loss 0.005882009711731829; Training Loss 0.004790067326813065\n",
      "Episode 408; Testing Loss 0.005881918069914301; Training Loss 0.004790057226750028\n",
      "Episode 409; Testing Loss 0.005881920016327571; Training Loss 0.004790047112209019\n",
      "Episode 410; Testing Loss 0.005882000429230008; Training Loss 0.004790035269362977\n",
      "Episode 411; Testing Loss 0.005882024674700633; Training Loss 0.00479002326868244\n",
      "Episode 412; Testing Loss 0.005881975216490623; Training Loss 0.004790012532586945\n",
      "Episode 413; Testing Loss 0.005881928906949476; Training Loss 0.004790000777272304\n",
      "Episode 414; Testing Loss 0.005881898130708177; Training Loss 0.004789989864537024\n",
      "Episode 415; Testing Loss 0.005881959717515329; Training Loss 0.004789979105949022\n",
      "Episode 416; Testing Loss 0.005881970303596909; Training Loss 0.004789968344070264\n",
      "Episode 417; Testing Loss 0.0058819595840715645; Training Loss 0.004789958746365665\n",
      "Episode 418; Testing Loss 0.005881985482926942; Training Loss 0.004789947188112045\n",
      "Episode 419; Testing Loss 0.005881989713277271; Training Loss 0.004789935339095262\n",
      "Episode 420; Testing Loss 0.005881934010886142; Training Loss 0.004789923729431755\n",
      "Episode 421; Testing Loss 0.0058818779489498265; Training Loss 0.004789914980154891\n",
      "Episode 422; Testing Loss 0.005881895441363035; Training Loss 0.004789902993315567\n",
      "Episode 423; Testing Loss 0.005881904589659819; Training Loss 0.004789891111170068\n",
      "Episode 424; Testing Loss 0.005881874027527218; Training Loss 0.004789881535302459\n",
      "Episode 425; Testing Loss 0.005881841929394113; Training Loss 0.004789871700987678\n",
      "Episode 426; Testing Loss 0.005881875603212144; Training Loss 0.0047898593516094\n",
      "Episode 427; Testing Loss 0.005881882660790312; Training Loss 0.004789848787510738\n",
      "Episode 428; Testing Loss 0.0058818105253530956; Training Loss 0.00478983837096577\n",
      "Episode 429; Testing Loss 0.005881746418589174; Training Loss 0.004789826072893049\n",
      "Episode 430; Testing Loss 0.005881781332464372; Training Loss 0.004789817669778287\n",
      "Episode 431; Testing Loss 0.005881908368840269; Training Loss 0.0047898073489547706\n",
      "Episode 432; Testing Loss 0.0058819299823202635; Training Loss 0.004789796563164424\n",
      "Episode 433; Testing Loss 0.00588182919738103; Training Loss 0.004789784131240822\n",
      "Episode 434; Testing Loss 0.005881742217574084; Training Loss 0.004789773862609898\n",
      "Episode 435; Testing Loss 0.0058817729335052535; Training Loss 0.004789760294039278\n",
      "Episode 436; Testing Loss 0.005881856622196667; Training Loss 0.004789751546479586\n",
      "Episode 437; Testing Loss 0.005881834151018281; Training Loss 0.00478974104798578\n",
      "Episode 438; Testing Loss 0.005881699633587432; Training Loss 0.004789727521946336\n",
      "Episode 439; Testing Loss 0.005881621317387243; Training Loss 0.004789718175538363\n",
      "Episode 440; Testing Loss 0.005881636192402134; Training Loss 0.00478970720360936\n",
      "Episode 441; Testing Loss 0.005881657053662425; Training Loss 0.004789694740931201\n",
      "Episode 442; Testing Loss 0.00588158849502768; Training Loss 0.004789679899612953\n",
      "Episode 443; Testing Loss 0.005881489059922774; Training Loss 0.004789670726178841\n",
      "Episode 444; Testing Loss 0.005881476653899002; Training Loss 0.00478965990450369\n",
      "Episode 445; Testing Loss 0.005881558809731874; Training Loss 0.004789645945371488\n",
      "Episode 446; Testing Loss 0.005881584875902503; Training Loss 0.004789629988142598\n",
      "Episode 447; Testing Loss 0.005881555957380428; Training Loss 0.0047896168337758825\n",
      "Episode 448; Testing Loss 0.005881496547786779; Training Loss 0.004789604956593905\n",
      "Episode 449; Testing Loss 0.005881537008079978; Training Loss 0.004789591796964485\n",
      "Episode 450; Testing Loss 0.005881600159737652; Training Loss 0.004789577400171601\n",
      "Episode 451; Testing Loss 0.00588157292324299; Training Loss 0.004789563478911011\n",
      "Episode 452; Testing Loss 0.00588158142411379; Training Loss 0.0047895519281928945\n",
      "Episode 453; Testing Loss 0.005881616021924234; Training Loss 0.004789538820432304\n",
      "Episode 454; Testing Loss 0.0058816796492165285; Training Loss 0.0047895248564547065\n",
      "Episode 455; Testing Loss 0.0058817067128089495; Training Loss 0.004789512704580287\n",
      "Episode 456; Testing Loss 0.005881725268032099; Training Loss 0.004789499868813045\n",
      "Episode 457; Testing Loss 0.005881773254021624; Training Loss 0.004789486712524456\n",
      "Episode 458; Testing Loss 0.005881796575319722; Training Loss 0.004789478022350769\n",
      "Episode 459; Testing Loss 0.005881766440699163; Training Loss 0.004789464652356918\n",
      "Episode 460; Testing Loss 0.005881777136540377; Training Loss 0.004789451944238606\n",
      "Episode 461; Testing Loss 0.005881844075187774; Training Loss 0.004789440537035988\n",
      "Episode 462; Testing Loss 0.005881852756860674; Training Loss 0.004789429290316263\n",
      "Episode 463; Testing Loss 0.005881787143051964; Training Loss 0.004789417245570638\n",
      "Episode 464; Testing Loss 0.005881759303726121; Training Loss 0.004789407629419414\n",
      "Episode 465; Testing Loss 0.005881883663205382; Training Loss 0.0047893955691070906\n",
      "Episode 466; Testing Loss 0.005881984360022365; Training Loss 0.004789384342600294\n",
      "Episode 467; Testing Loss 0.005881908980918428; Training Loss 0.004789370979792116\n",
      "Episode 468; Testing Loss 0.0058817726161649; Training Loss 0.004789359840708383\n",
      "Episode 469; Testing Loss 0.0058817474585896225; Training Loss 0.0047893483773821\n",
      "Episode 470; Testing Loss 0.005881888486340064; Training Loss 0.004789336324820548\n",
      "Episode 471; Testing Loss 0.005881960651564368; Training Loss 0.004789325614318814\n",
      "Episode 472; Testing Loss 0.005881840090397174; Training Loss 0.004789312417587638\n",
      "Episode 473; Testing Loss 0.005881748491729013; Training Loss 0.004789301775934706\n",
      "Episode 474; Testing Loss 0.005881836301494795; Training Loss 0.004789289436920265\n",
      "Episode 475; Testing Loss 0.005882001387852648; Training Loss 0.004789278462928382\n",
      "Episode 476; Testing Loss 0.005881984622691103; Training Loss 0.004789266875199363\n",
      "Episode 477; Testing Loss 0.005881856313015435; Training Loss 0.004789255541770449\n",
      "Episode 478; Testing Loss 0.00588187871156033; Training Loss 0.004789244175749621\n",
      "Episode 479; Testing Loss 0.005881986212378478; Training Loss 0.004789232521581941\n",
      "Episode 480; Testing Loss 0.005881975616041042; Training Loss 0.004789221579542432\n",
      "Episode 481; Testing Loss 0.005881867380357682; Training Loss 0.004789210129812842\n",
      "Episode 482; Testing Loss 0.005881796659902238; Training Loss 0.004789197920316182\n",
      "Episode 483; Testing Loss 0.005881869608750768; Training Loss 0.0047891879132965255\n",
      "Episode 484; Testing Loss 0.005881945612458477; Training Loss 0.004789177216528562\n",
      "Episode 485; Testing Loss 0.005881906209240763; Training Loss 0.0047891651728115825\n",
      "Episode 486; Testing Loss 0.005881800427974543; Training Loss 0.004789152513880566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 487; Testing Loss 0.005881859574555774; Training Loss 0.004789141493878144\n",
      "Episode 488; Testing Loss 0.005881974466228619; Training Loss 0.004789129104360224\n",
      "Episode 489; Testing Loss 0.0058819123984319285; Training Loss 0.004789118605233432\n",
      "Episode 490; Testing Loss 0.005881812095678281; Training Loss 0.004789107009728007\n",
      "Episode 491; Testing Loss 0.005881799496525265; Training Loss 0.0047890945725412\n",
      "Episode 492; Testing Loss 0.0058818844880318116; Training Loss 0.004789083124222771\n",
      "Episode 493; Testing Loss 0.005881952001367869; Training Loss 0.004789072257732097\n",
      "Episode 494; Testing Loss 0.005881892466168258; Training Loss 0.004789058884318568\n",
      "Episode 495; Testing Loss 0.005881880491104898; Training Loss 0.004789047286184438\n",
      "Episode 496; Testing Loss 0.0058819166055736425; Training Loss 0.004789036464557489\n",
      "Episode 497; Testing Loss 0.0058819667515538; Training Loss 0.0047890240787696586\n",
      "Episode 498; Testing Loss 0.005881926884015213; Training Loss 0.004789012177386703\n",
      "Episode 499; Testing Loss 0.005881910721486088; Training Loss 0.004789001251438921\n",
      "Episode 500; Testing Loss 0.005881945021934794; Training Loss 0.0047889892456093475\n",
      "Episode 501; Testing Loss 0.0058819790147349674; Training Loss 0.004788977740225567\n",
      "Episode 502; Testing Loss 0.005881895238025577; Training Loss 0.004788966613520993\n",
      "Episode 503; Testing Loss 0.005881840238168718; Training Loss 0.004788954903185056\n",
      "Episode 504; Testing Loss 0.0058819124291300305; Training Loss 0.004788941295045198\n",
      "Episode 505; Testing Loss 0.0058819953008291855; Training Loss 0.004788930683874786\n",
      "Episode 506; Testing Loss 0.00588193887394104; Training Loss 0.004788916589044548\n",
      "Episode 507; Testing Loss 0.005881872151322357; Training Loss 0.004788906844981698\n",
      "Episode 508; Testing Loss 0.0058819348953999765; Training Loss 0.004788893597339067\n",
      "Episode 509; Testing Loss 0.0058819492408243; Training Loss 0.0047888821509120185\n",
      "Episode 510; Testing Loss 0.005881888338752865; Training Loss 0.00478887095848864\n",
      "Episode 511; Testing Loss 0.005881862845923996; Training Loss 0.004788858853221898\n",
      "Episode 512; Testing Loss 0.005881924327947855; Training Loss 0.004788848092974147\n",
      "Episode 513; Testing Loss 0.005882022307124777; Training Loss 0.004788837277502076\n",
      "Episode 514; Testing Loss 0.00588197901862199; Training Loss 0.00478882309547884\n",
      "Episode 515; Testing Loss 0.005881913572341118; Training Loss 0.004788814446510004\n",
      "Episode 516; Testing Loss 0.005881931545145027; Training Loss 0.004788803925825544\n",
      "Episode 517; Testing Loss 0.005882020923115968; Training Loss 0.004788791875611637\n",
      "Episode 518; Testing Loss 0.00588199345185497; Training Loss 0.004788776153506741\n",
      "Episode 519; Testing Loss 0.005881920737588721; Training Loss 0.004788768140318696\n",
      "Episode 520; Testing Loss 0.00588192676919834; Training Loss 0.004788757848938813\n",
      "Episode 521; Testing Loss 0.005881997783387965; Training Loss 0.004788745894367864\n",
      "Episode 522; Testing Loss 0.005881986835610206; Training Loss 0.004788730136055056\n",
      "Episode 523; Testing Loss 0.005881967912660166; Training Loss 0.004788719945771985\n",
      "Episode 524; Testing Loss 0.005881946152825909; Training Loss 0.00478871057171384\n",
      "Episode 525; Testing Loss 0.0058819723554333755; Training Loss 0.0047886997639862245\n",
      "Episode 526; Testing Loss 0.005881954711756535; Training Loss 0.0047886848161640795\n",
      "Episode 527; Testing Loss 0.005881944398381758; Training Loss 0.004788671851888901\n",
      "Episode 528; Testing Loss 0.005882019796326754; Training Loss 0.004788662317638564\n",
      "Episode 529; Testing Loss 0.005882064475948167; Training Loss 0.004788650539978667\n",
      "Episode 530; Testing Loss 0.005882011390493582; Training Loss 0.004788638293140033\n",
      "Episode 531; Testing Loss 0.0058819379356249615; Training Loss 0.004788627089813136\n",
      "Episode 532; Testing Loss 0.005881961326404152; Training Loss 0.004788613176412647\n",
      "Episode 533; Testing Loss 0.005882009262206531; Training Loss 0.0047886034110754296\n",
      "Episode 534; Testing Loss 0.005881996405808422; Training Loss 0.004788592141532341\n",
      "Episode 535; Testing Loss 0.005881960179299737; Training Loss 0.0047885793369207\n",
      "Episode 536; Testing Loss 0.0058819804691227105; Training Loss 0.004788568496797301\n",
      "Episode 537; Testing Loss 0.005881965554107934; Training Loss 0.004788558354710482\n",
      "Episode 538; Testing Loss 0.005881908030473761; Training Loss 0.004788546300154107\n",
      "Episode 539; Testing Loss 0.005881834291786047; Training Loss 0.004788533645534513\n",
      "Episode 540; Testing Loss 0.005881837303667264; Training Loss 0.004788521966393018\n",
      "Episode 541; Testing Loss 0.005881867644466925; Training Loss 0.004788509434660015\n",
      "Episode 542; Testing Loss 0.005881815207186728; Training Loss 0.004788498229498717\n",
      "Episode 543; Testing Loss 0.005881807201592623; Training Loss 0.0047884875855149596\n",
      "Episode 544; Testing Loss 0.005881862533401866; Training Loss 0.004788474857513695\n",
      "Episode 545; Testing Loss 0.005881930578138094; Training Loss 0.0047884637695129175\n",
      "Episode 546; Testing Loss 0.005881921036798157; Training Loss 0.00478845221866838\n",
      "Episode 547; Testing Loss 0.005881886580725232; Training Loss 0.004788439366870941\n",
      "Episode 548; Testing Loss 0.005881942828653244; Training Loss 0.00478842768648179\n",
      "Episode 549; Testing Loss 0.0058819576866237875; Training Loss 0.0047884162491294915\n",
      "Episode 550; Testing Loss 0.005881966586819677; Training Loss 0.004788404264325608\n",
      "Episode 551; Testing Loss 0.005881902937631025; Training Loss 0.004788392947683052\n",
      "Episode 552; Testing Loss 0.005881793988789668; Training Loss 0.004788381662158414\n",
      "Episode 553; Testing Loss 0.00588180886623745; Training Loss 0.004788370462708909\n",
      "Episode 554; Testing Loss 0.005881933663735161; Training Loss 0.00478835881104584\n",
      "Episode 555; Testing Loss 0.005881901283429733; Training Loss 0.004788348377638216\n",
      "Episode 556; Testing Loss 0.005881795772598308; Training Loss 0.004788337098734959\n",
      "Episode 557; Testing Loss 0.00588180467542753; Training Loss 0.004788324570336369\n",
      "Episode 558; Testing Loss 0.005881932700273627; Training Loss 0.00478831309534706\n",
      "Episode 559; Testing Loss 0.005881993398472348; Training Loss 0.0047883040849568\n",
      "Episode 560; Testing Loss 0.005881898876594516; Training Loss 0.004788290337491732\n",
      "Episode 561; Testing Loss 0.0058818564016757495; Training Loss 0.004788278966455503\n",
      "Episode 562; Testing Loss 0.005881881208246962; Training Loss 0.004788268995683418\n",
      "Episode 563; Testing Loss 0.005881927388633944; Training Loss 0.004788256929504407\n",
      "Episode 564; Testing Loss 0.005881902213111569; Training Loss 0.004788244276154463\n",
      "Episode 565; Testing Loss 0.005881884755661299; Training Loss 0.004788232713117518\n",
      "Episode 566; Testing Loss 0.005881859102571278; Training Loss 0.004788219805975094\n",
      "Episode 567; Testing Loss 0.005881821802624419; Training Loss 0.004788208236588456\n",
      "Episode 568; Testing Loss 0.005881800334412025; Training Loss 0.004788198880854023\n",
      "Episode 569; Testing Loss 0.005881811920062397; Training Loss 0.004788185989466116\n",
      "Episode 570; Testing Loss 0.005881857955743236; Training Loss 0.004788175466872488\n",
      "Episode 571; Testing Loss 0.005881890167191265; Training Loss 0.004788164509439523\n",
      "Episode 572; Testing Loss 0.0058818654058787985; Training Loss 0.00478815335544472\n",
      "Episode 573; Testing Loss 0.005881793325428274; Training Loss 0.00478814066919085\n",
      "Episode 574; Testing Loss 0.005881739668270843; Training Loss 0.004788127972961531\n",
      "Episode 575; Testing Loss 0.005881776761627854; Training Loss 0.0047881172312177036\n",
      "Episode 576; Testing Loss 0.00588180415836608; Training Loss 0.004788107182820129\n",
      "Episode 577; Testing Loss 0.005881743083593476; Training Loss 0.0047880951163926135\n",
      "Episode 578; Testing Loss 0.005881722524163113; Training Loss 0.00478808346713719\n",
      "Episode 579; Testing Loss 0.005881787934081721; Training Loss 0.004788074065117018\n",
      "Episode 580; Testing Loss 0.005881860144066057; Training Loss 0.0047880627905560165\n",
      "Episode 581; Testing Loss 0.00588181177963113; Training Loss 0.0047880483459089365\n",
      "Episode 582; Testing Loss 0.005881711196824639; Training Loss 0.004788040302302195\n",
      "Episode 583; Testing Loss 0.005881680333302587; Training Loss 0.004788029942407787\n",
      "Episode 584; Testing Loss 0.00588176286926685; Training Loss 0.004788015381228627\n",
      "Episode 585; Testing Loss 0.005881799224133158; Training Loss 0.004788003344358205\n",
      "Episode 586; Testing Loss 0.005881695720628645; Training Loss 0.004787994790715466\n",
      "Episode 587; Testing Loss 0.0058816413768319285; Training Loss 0.004787985020843233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 588; Testing Loss 0.005881705947654144; Training Loss 0.004787971090017154\n",
      "Episode 589; Testing Loss 0.0058817347432268815; Training Loss 0.004787957594214543\n",
      "Episode 590; Testing Loss 0.005881632589971996; Training Loss 0.004787947814110794\n",
      "Episode 591; Testing Loss 0.005881604507042207; Training Loss 0.004787937321923794\n",
      "Episode 592; Testing Loss 0.005881746246972682; Training Loss 0.0047879246801775565\n",
      "Episode 593; Testing Loss 0.005881804961263182; Training Loss 0.004787915390761213\n",
      "Episode 594; Testing Loss 0.005881694696296081; Training Loss 0.0047879020125301056\n",
      "Episode 595; Testing Loss 0.0058815727433837; Training Loss 0.0047878919525160425\n",
      "Episode 596; Testing Loss 0.005881572495872216; Training Loss 0.004787879979024209\n",
      "Episode 597; Testing Loss 0.005881640416759313; Training Loss 0.004787869372936858\n",
      "Episode 598; Testing Loss 0.005881616972455396; Training Loss 0.004787856873950625\n",
      "Episode 599; Testing Loss 0.005881553474281481; Training Loss 0.00478784753955567\n",
      "Episode 600; Testing Loss 0.005881603593169717; Training Loss 0.004787835665216554\n",
      "Episode 601; Testing Loss 0.005881686250989686; Training Loss 0.004787824449553424\n",
      "Episode 602; Testing Loss 0.005881662698618241; Training Loss 0.004787811063939415\n",
      "Episode 603; Testing Loss 0.005881575232431041; Training Loss 0.004787803271606986\n",
      "Episode 604; Testing Loss 0.005881488025794139; Training Loss 0.004787793512112843\n",
      "Episode 605; Testing Loss 0.005881506907089882; Training Loss 0.004787781582588165\n",
      "Episode 606; Testing Loss 0.005881550298404706; Training Loss 0.004787767936634711\n",
      "Episode 607; Testing Loss 0.005881513683946912; Training Loss 0.004787754283064982\n",
      "Episode 608; Testing Loss 0.0058814383331774744; Training Loss 0.004787746065772644\n",
      "Episode 609; Testing Loss 0.005881512424534129; Training Loss 0.0047877363653596825\n",
      "Episode 610; Testing Loss 0.005881603898705192; Training Loss 0.004787723444069849\n",
      "Episode 611; Testing Loss 0.0058816300873058315; Training Loss 0.004787709954615454\n",
      "Episode 612; Testing Loss 0.005881568584828145; Training Loss 0.004787698257598442\n",
      "Episode 613; Testing Loss 0.005881527044237647; Training Loss 0.004787689218522532\n",
      "Episode 614; Testing Loss 0.005881569034622341; Training Loss 0.004787677573793767\n",
      "Episode 615; Testing Loss 0.005881617585137937; Training Loss 0.004787663588772992\n",
      "Episode 616; Testing Loss 0.005881588653087167; Training Loss 0.004787650847608646\n",
      "Episode 617; Testing Loss 0.005881530017473096; Training Loss 0.004787640381487459\n",
      "Episode 618; Testing Loss 0.005881497446646341; Training Loss 0.004787631213126746\n",
      "Episode 619; Testing Loss 0.00588152070400497; Training Loss 0.004787617337713238\n",
      "Episode 620; Testing Loss 0.005881554545569244; Training Loss 0.004787607369072254\n",
      "Episode 621; Testing Loss 0.0058815620475312054; Training Loss 0.004787598266553833\n",
      "Episode 622; Testing Loss 0.005881537082974079; Training Loss 0.004787587639897615\n",
      "Episode 623; Testing Loss 0.005881534541719098; Training Loss 0.004787574683977574\n",
      "Episode 624; Testing Loss 0.005881525570734685; Training Loss 0.004787563771342543\n",
      "Episode 625; Testing Loss 0.005881508412719756; Training Loss 0.004787553255423927\n",
      "Episode 626; Testing Loss 0.00588146905182309; Training Loss 0.004787539303468425\n",
      "Episode 627; Testing Loss 0.005881411637008688; Training Loss 0.004787525217010894\n",
      "Episode 628; Testing Loss 0.00588141977731504; Training Loss 0.00478751580811944\n",
      "Episode 629; Testing Loss 0.005881470104401172; Training Loss 0.004787504722465338\n",
      "Episode 630; Testing Loss 0.005881523952486989; Training Loss 0.004787492313723307\n",
      "Episode 631; Testing Loss 0.005881541713426135; Training Loss 0.004787481789951266\n",
      "Episode 632; Testing Loss 0.005881507308007241; Training Loss 0.004787470399430819\n",
      "Episode 633; Testing Loss 0.005881439251448784; Training Loss 0.004787459970894749\n",
      "Episode 634; Testing Loss 0.0058814247929960435; Training Loss 0.0047874492803090946\n",
      "Episode 635; Testing Loss 0.0058814593662334025; Training Loss 0.004787436481303819\n",
      "Episode 636; Testing Loss 0.005881463015846002; Training Loss 0.004787424200319366\n",
      "Episode 637; Testing Loss 0.005881409467712144; Training Loss 0.004787412535422993\n",
      "Episode 638; Testing Loss 0.005881388376324605; Training Loss 0.004787401742995403\n",
      "Episode 639; Testing Loss 0.005881438904562554; Training Loss 0.004787392645111447\n",
      "Episode 640; Testing Loss 0.005881435081820004; Training Loss 0.0047873820262953745\n",
      "Episode 641; Testing Loss 0.005881355736036067; Training Loss 0.004787369416684772\n",
      "Episode 642; Testing Loss 0.00588134718139824; Training Loss 0.004787356822510287\n",
      "Episode 643; Testing Loss 0.005881453545249691; Training Loss 0.00478734660574973\n",
      "Episode 644; Testing Loss 0.00588148009288655; Training Loss 0.0047873355607775175\n",
      "Episode 645; Testing Loss 0.005881354452061696; Training Loss 0.004787322750013108\n",
      "Episode 646; Testing Loss 0.005881283090459693; Training Loss 0.004787313576618792\n",
      "Episode 647; Testing Loss 0.005881392296573713; Training Loss 0.004787299695110955\n",
      "Episode 648; Testing Loss 0.005881461569261866; Training Loss 0.004787292100263284\n",
      "Episode 649; Testing Loss 0.005881338575763828; Training Loss 0.0047872797400942526\n",
      "Episode 650; Testing Loss 0.005881261060555582; Training Loss 0.00478726587673092\n",
      "Episode 651; Testing Loss 0.005881370402107852; Training Loss 0.004787254177938568\n",
      "Episode 652; Testing Loss 0.0058814054382369165; Training Loss 0.004787244741887971\n",
      "Episode 653; Testing Loss 0.005881308679906629; Training Loss 0.004787232509610783\n",
      "Episode 654; Testing Loss 0.00588121349762667; Training Loss 0.004787222075106953\n",
      "Episode 655; Testing Loss 0.005881183950769162; Training Loss 0.004787213028116473\n",
      "Episode 656; Testing Loss 0.005881209099693613; Training Loss 0.004787200045358441\n",
      "Episode 657; Testing Loss 0.005881248397786869; Training Loss 0.004787187396667133\n",
      "Episode 658; Testing Loss 0.005881293944220454; Training Loss 0.004787176541941442\n",
      "Episode 659; Testing Loss 0.00588133367356523; Training Loss 0.004787164837924819\n",
      "Episode 660; Testing Loss 0.005881303179061572; Training Loss 0.004787153599621013\n",
      "Episode 661; Testing Loss 0.00588128286059048; Training Loss 0.004787141380078996\n",
      "Episode 662; Testing Loss 0.005881297743319819; Training Loss 0.004787131460552517\n",
      "Episode 663; Testing Loss 0.005881266232108642; Training Loss 0.0047871208532713275\n",
      "Episode 664; Testing Loss 0.005881246149231442; Training Loss 0.004787108485797811\n",
      "Episode 665; Testing Loss 0.005881306671191662; Training Loss 0.004787096967135181\n",
      "Episode 666; Testing Loss 0.005881308885326323; Training Loss 0.004787089558645157\n",
      "Episode 667; Testing Loss 0.005881234072457106; Training Loss 0.004787078889917135\n",
      "Episode 668; Testing Loss 0.005881185617663219; Training Loss 0.004787065181897852\n",
      "Episode 669; Testing Loss 0.005881201089844919; Training Loss 0.004787053841358489\n",
      "Episode 670; Testing Loss 0.005881178893280335; Training Loss 0.004787043277587449\n",
      "Episode 671; Testing Loss 0.005881183615252101; Training Loss 0.004787032636154716\n",
      "Episode 672; Testing Loss 0.005881259045538199; Training Loss 0.004787020120190771\n",
      "Episode 673; Testing Loss 0.005881297975095233; Training Loss 0.004787009355385638\n",
      "Episode 674; Testing Loss 0.005881287531330283; Training Loss 0.004786999041204025\n",
      "Episode 675; Testing Loss 0.005881222929763745; Training Loss 0.00478698593560025\n",
      "Episode 676; Testing Loss 0.005881181348462523; Training Loss 0.0047869745869363175\n",
      "Episode 677; Testing Loss 0.005881142703679987; Training Loss 0.004786963613967507\n",
      "Episode 678; Testing Loss 0.005881171915705669; Training Loss 0.004786952382285477\n",
      "Episode 679; Testing Loss 0.00588121262600112; Training Loss 0.004786940301320078\n",
      "Episode 680; Testing Loss 0.005881190019005182; Training Loss 0.0047869286767170255\n",
      "Episode 681; Testing Loss 0.005881142228771827; Training Loss 0.004786917408662072\n",
      "Episode 682; Testing Loss 0.005881120429565791; Training Loss 0.004786905984860509\n",
      "Episode 683; Testing Loss 0.005881155238492723; Training Loss 0.004786896437088887\n",
      "Episode 684; Testing Loss 0.005881186278819639; Training Loss 0.004786884353369558\n",
      "Episode 685; Testing Loss 0.005881158348607956; Training Loss 0.0047868721927692755\n",
      "Episode 686; Testing Loss 0.005881092786155366; Training Loss 0.004786861776163767\n",
      "Episode 687; Testing Loss 0.005881090811466644; Training Loss 0.004786850015639325\n",
      "Episode 688; Testing Loss 0.005881159009354776; Training Loss 0.004786838120937951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 689; Testing Loss 0.00588120616961832; Training Loss 0.004786829019501503\n",
      "Episode 690; Testing Loss 0.005881139465059967; Training Loss 0.004786818439551018\n",
      "Episode 691; Testing Loss 0.005881118153018868; Training Loss 0.004786807333018523\n",
      "Episode 692; Testing Loss 0.005881207216866005; Training Loss 0.004786796259154193\n",
      "Episode 693; Testing Loss 0.005881249209742909; Training Loss 0.004786785544990711\n",
      "Episode 694; Testing Loss 0.00588115681963029; Training Loss 0.004786771664032003\n",
      "Episode 695; Testing Loss 0.005881071580817555; Training Loss 0.004786761381133784\n",
      "Episode 696; Testing Loss 0.005881062126128397; Training Loss 0.004786750476895279\n",
      "Episode 697; Testing Loss 0.005881119361259088; Training Loss 0.004786739219178522\n",
      "Episode 698; Testing Loss 0.005881185248414543; Training Loss 0.004786726900204101\n",
      "Episode 699; Testing Loss 0.005881189793112188; Training Loss 0.004786716362406471\n",
      "Episode 700; Testing Loss 0.005881104666941307; Training Loss 0.004786704498238577\n",
      "Episode 701; Testing Loss 0.00588102289237038; Training Loss 0.004786693618568156\n",
      "Episode 702; Testing Loss 0.00588106121595682; Training Loss 0.004786681973953665\n",
      "Episode 703; Testing Loss 0.00588113660503883; Training Loss 0.004786673927848928\n",
      "Episode 704; Testing Loss 0.005881077067275284; Training Loss 0.004786662316671596\n",
      "Episode 705; Testing Loss 0.005880976077791854; Training Loss 0.004786650502293615\n",
      "Episode 706; Testing Loss 0.005881017297045043; Training Loss 0.00478663778665999\n",
      "Episode 707; Testing Loss 0.005881181407962436; Training Loss 0.004786628272605656\n",
      "Episode 708; Testing Loss 0.005881156250728033; Training Loss 0.004786618641026133\n",
      "Episode 709; Testing Loss 0.005881022087541572; Training Loss 0.004786606171809781\n",
      "Episode 710; Testing Loss 0.0058809730488361784; Training Loss 0.004786596047977428\n",
      "Episode 711; Testing Loss 0.00588103756050809; Training Loss 0.004786584259434262\n",
      "Episode 712; Testing Loss 0.005881084921680634; Training Loss 0.004786571618071937\n",
      "Episode 713; Testing Loss 0.005881043404123392; Training Loss 0.004786559618743716\n",
      "Episode 714; Testing Loss 0.005881003912227799; Training Loss 0.0047865506149429105\n",
      "Episode 715; Testing Loss 0.0058809881421670865; Training Loss 0.004786537622043646\n",
      "Episode 716; Testing Loss 0.00588100974113973; Training Loss 0.004786528725983221\n",
      "Episode 717; Testing Loss 0.00588104395686575; Training Loss 0.004786518535739282\n",
      "Episode 718; Testing Loss 0.005881063767137071; Training Loss 0.0047865069200456216\n",
      "Episode 719; Testing Loss 0.005881011267131021; Training Loss 0.0047864939954946305\n",
      "Episode 720; Testing Loss 0.005880962873856165; Training Loss 0.004786485976508043\n",
      "Episode 721; Testing Loss 0.005881009185267221; Training Loss 0.00478647657307711\n",
      "Episode 722; Testing Loss 0.005881081830747236; Training Loss 0.004786463165643108\n",
      "Episode 723; Testing Loss 0.005881035670205971; Training Loss 0.004786447412249092\n",
      "Episode 724; Testing Loss 0.005880909460152115; Training Loss 0.004786436866641621\n",
      "Episode 725; Testing Loss 0.005880900221503433; Training Loss 0.004786426915869856\n",
      "Episode 726; Testing Loss 0.005880958034699095; Training Loss 0.004786415574832253\n",
      "Episode 727; Testing Loss 0.005881003687915309; Training Loss 0.004786403352713907\n",
      "Episode 728; Testing Loss 0.005881006203290788; Training Loss 0.00478639119487359\n",
      "Episode 729; Testing Loss 0.005880974172112917; Training Loss 0.0047863808954517586\n",
      "Episode 730; Testing Loss 0.005880919013810108; Training Loss 0.004786371359403471\n",
      "Episode 731; Testing Loss 0.005880934504527897; Training Loss 0.0047863599546165\n",
      "Episode 732; Testing Loss 0.005880956485423416; Training Loss 0.004786349159812255\n",
      "Episode 733; Testing Loss 0.005880914309005183; Training Loss 0.004786337714609558\n",
      "Episode 734; Testing Loss 0.005880849421977859; Training Loss 0.0047863251422158295\n",
      "Episode 735; Testing Loss 0.00588086324898164; Training Loss 0.004786313750342147\n",
      "Episode 736; Testing Loss 0.005880915079030127; Training Loss 0.004786302816254168\n",
      "Episode 737; Testing Loss 0.005880921960366774; Training Loss 0.0047862896994917085\n",
      "Episode 738; Testing Loss 0.005880901476007941; Training Loss 0.004786279273884146\n",
      "Episode 739; Testing Loss 0.005880928336352204; Training Loss 0.004786269990328415\n",
      "Episode 740; Testing Loss 0.005881001452946092; Training Loss 0.004786257641128895\n",
      "Episode 741; Testing Loss 0.0058809664059141; Training Loss 0.004786246370110745\n",
      "Episode 742; Testing Loss 0.005880800614519563; Training Loss 0.004786236376935865\n",
      "Episode 743; Testing Loss 0.005880723106182102; Training Loss 0.0047862250507118495\n",
      "Episode 744; Testing Loss 0.005880809401910524; Training Loss 0.00478621240137136\n",
      "Episode 745; Testing Loss 0.005880905896081149; Training Loss 0.004786202172383751\n",
      "Episode 746; Testing Loss 0.005880896060061989; Training Loss 0.004786191256178551\n",
      "Episode 747; Testing Loss 0.005880761688391797; Training Loss 0.004786179347907875\n",
      "Episode 748; Testing Loss 0.005880736775236526; Training Loss 0.004786167825554422\n",
      "Episode 749; Testing Loss 0.005880834775439171; Training Loss 0.004786157132002986\n",
      "Episode 750; Testing Loss 0.005880873218528824; Training Loss 0.0047861478273068915\n",
      "Episode 751; Testing Loss 0.005880859702618108; Training Loss 0.0047861347860319505\n",
      "Episode 752; Testing Loss 0.005880831535947478; Training Loss 0.0047861229772247675\n",
      "Episode 753; Testing Loss 0.0058808115601090595; Training Loss 0.004786112321276546\n",
      "Episode 754; Testing Loss 0.005880831138847809; Training Loss 0.004786101511706689\n",
      "Episode 755; Testing Loss 0.005880877984566293; Training Loss 0.0047860899078817016\n",
      "Episode 756; Testing Loss 0.005880838868747647; Training Loss 0.004786079965732048\n",
      "Episode 757; Testing Loss 0.005880736385514416; Training Loss 0.004786070223222778\n",
      "Episode 758; Testing Loss 0.005880647628947903; Training Loss 0.004786057695220482\n",
      "Episode 759; Testing Loss 0.005880691161015285; Training Loss 0.004786046354080631\n",
      "Episode 760; Testing Loss 0.0058808152331482; Training Loss 0.004786036642719131\n",
      "Episode 761; Testing Loss 0.005880813915853447; Training Loss 0.0047860231676899396\n",
      "Episode 762; Testing Loss 0.005880702910411518; Training Loss 0.0047860135977113565\n",
      "Episode 763; Testing Loss 0.00588066391966929; Training Loss 0.004786004375716933\n",
      "Episode 764; Testing Loss 0.005880704981709358; Training Loss 0.0047859922228504015\n",
      "Episode 765; Testing Loss 0.005880783231046358; Training Loss 0.004785979080143191\n",
      "Episode 766; Testing Loss 0.005880796500885065; Training Loss 0.004785969418618775\n",
      "Episode 767; Testing Loss 0.005880746026550435; Training Loss 0.004785958961723556\n",
      "Episode 768; Testing Loss 0.005880706510560106; Training Loss 0.004785947678084835\n",
      "Episode 769; Testing Loss 0.0058807346848607864; Training Loss 0.004785933876675201\n",
      "Episode 770; Testing Loss 0.0058806988203946075; Training Loss 0.004785923441600396\n",
      "Episode 771; Testing Loss 0.005880628928447978; Training Loss 0.004785912751075266\n",
      "Episode 772; Testing Loss 0.005880585625087015; Training Loss 0.004785902185758729\n",
      "Episode 773; Testing Loss 0.00588066912383074; Training Loss 0.0047858898248090195\n",
      "Episode 774; Testing Loss 0.005880763470555793; Training Loss 0.004785879429362861\n",
      "Episode 775; Testing Loss 0.00588069914594752; Training Loss 0.004785867816318136\n",
      "Episode 776; Testing Loss 0.005880597856822207; Training Loss 0.004785858120943982\n",
      "Episode 777; Testing Loss 0.005880618374395791; Training Loss 0.00478584532738176\n",
      "Episode 778; Testing Loss 0.005880669973839453; Training Loss 0.004785835520102448\n",
      "Episode 779; Testing Loss 0.0058806007625719975; Training Loss 0.0047858233612252405\n",
      "Episode 780; Testing Loss 0.005880528907712265; Training Loss 0.004785812248370688\n",
      "Episode 781; Testing Loss 0.0058805911255580805; Training Loss 0.0047858041646038066\n",
      "Episode 782; Testing Loss 0.005880689437217805; Training Loss 0.004785793824163997\n",
      "Episode 783; Testing Loss 0.0058806640863053065; Training Loss 0.004785780254478353\n",
      "Episode 784; Testing Loss 0.005880537283898509; Training Loss 0.004785769158295063\n",
      "Episode 785; Testing Loss 0.0058804595672185146; Training Loss 0.004785760261830027\n",
      "Episode 786; Testing Loss 0.0058805342229353245; Training Loss 0.004785749202114409\n",
      "Episode 787; Testing Loss 0.005880548330829642; Training Loss 0.004785736541645016\n",
      "Episode 788; Testing Loss 0.005880455677735494; Training Loss 0.004785724922376718\n",
      "Episode 789; Testing Loss 0.005880457351811348; Training Loss 0.004785715566940125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 790; Testing Loss 0.005880558149465172; Training Loss 0.004785703110239698\n",
      "Episode 791; Testing Loss 0.005880587643953853; Training Loss 0.004785689923781585\n",
      "Episode 792; Testing Loss 0.005880487629113545; Training Loss 0.004785679102541637\n",
      "Episode 793; Testing Loss 0.0058803903488040665; Training Loss 0.004785668574111358\n",
      "Episode 794; Testing Loss 0.005880481348057823; Training Loss 0.004785656220938578\n",
      "Episode 795; Testing Loss 0.005880594492021798; Training Loss 0.0047856474615186745\n",
      "Episode 796; Testing Loss 0.005880501867344236; Training Loss 0.0047856340362889375\n",
      "Episode 797; Testing Loss 0.005880383103717549; Training Loss 0.00478562371003815\n",
      "Episode 798; Testing Loss 0.005880449942402249; Training Loss 0.004785612609922117\n",
      "Episode 799; Testing Loss 0.0058805238538677815; Training Loss 0.004785603635003164\n",
      "Episode 800; Testing Loss 0.005880453226222354; Training Loss 0.004785590425251976\n",
      "Episode 801; Testing Loss 0.0058803954020665745; Training Loss 0.004785579371712619\n",
      "Episode 802; Testing Loss 0.005880421480514741; Training Loss 0.004785568784615923\n",
      "Episode 803; Testing Loss 0.0058804353907262; Training Loss 0.004785557736477477\n",
      "Episode 804; Testing Loss 0.005880405634424096; Training Loss 0.004785545822504228\n",
      "Episode 805; Testing Loss 0.005880398355963315; Training Loss 0.004785534442689317\n",
      "Episode 806; Testing Loss 0.005880380112300028; Training Loss 0.004785523445004876\n",
      "Episode 807; Testing Loss 0.0058803514786094305; Training Loss 0.004785512300111457\n",
      "Episode 808; Testing Loss 0.005880383310167827; Training Loss 0.004785501566310963\n",
      "Episode 809; Testing Loss 0.005880415803491345; Training Loss 0.0047854913755938375\n",
      "Episode 810; Testing Loss 0.005880364104934833; Training Loss 0.004785479396338082\n",
      "Episode 811; Testing Loss 0.005880353669802901; Training Loss 0.004785469461769908\n",
      "Episode 812; Testing Loss 0.005880381723651327; Training Loss 0.00478545862379989\n",
      "Episode 813; Testing Loss 0.005880376130667885; Training Loss 0.004785445476271203\n",
      "Episode 814; Testing Loss 0.0058803015256826555; Training Loss 0.004785437761599013\n",
      "Episode 815; Testing Loss 0.005880222053585955; Training Loss 0.004785427493900125\n",
      "Episode 816; Testing Loss 0.005880248196422776; Training Loss 0.004785416502056709\n",
      "Episode 817; Testing Loss 0.005880388050023967; Training Loss 0.004785403468691263\n",
      "Episode 818; Testing Loss 0.005880412384664668; Training Loss 0.0047853935927147945\n",
      "Episode 819; Testing Loss 0.005880287382401512; Training Loss 0.00478538305641907\n",
      "Episode 820; Testing Loss 0.0058802260364130415; Training Loss 0.00478537102187656\n",
      "Episode 821; Testing Loss 0.005880300116095233; Training Loss 0.0047853590933903236\n",
      "Episode 822; Testing Loss 0.005880287448565602; Training Loss 0.004785349211425941\n",
      "Episode 823; Testing Loss 0.0058802091353376135; Training Loss 0.004785336600226455\n",
      "Episode 824; Testing Loss 0.005880222424944569; Training Loss 0.004785324379226326\n",
      "Episode 825; Testing Loss 0.005880340189221712; Training Loss 0.004785316781436181\n",
      "Episode 826; Testing Loss 0.00588034187310385; Training Loss 0.0047853071827736945\n",
      "Episode 827; Testing Loss 0.005880193931333842; Training Loss 0.004785292994476365\n",
      "Episode 828; Testing Loss 0.0058800934702927165; Training Loss 0.004785280724917526\n",
      "Episode 829; Testing Loss 0.0058801203350456605; Training Loss 0.0047852705015563966\n",
      "Episode 830; Testing Loss 0.005880213141198232; Training Loss 0.004785259711365224\n",
      "Episode 831; Testing Loss 0.005880244666405776; Training Loss 0.0047852482462478595\n",
      "Episode 832; Testing Loss 0.005880193011769418; Training Loss 0.0047852395502813085\n",
      "Episode 833; Testing Loss 0.005880151350431738; Training Loss 0.004785229397364365\n",
      "Episode 834; Testing Loss 0.005880155094033763; Training Loss 0.004785215795149306\n",
      "Episode 835; Testing Loss 0.0058801585509897164; Training Loss 0.004785203461431561\n",
      "Episode 836; Testing Loss 0.005880102261813715; Training Loss 0.004785192853961374\n",
      "Episode 837; Testing Loss 0.005880073989486364; Training Loss 0.0047851821346426284\n",
      "Episode 838; Testing Loss 0.0058801477878982515; Training Loss 0.0047851702137595925\n",
      "Episode 839; Testing Loss 0.0058802183541540745; Training Loss 0.004785159857840128\n",
      "Episode 840; Testing Loss 0.005880164625245757; Training Loss 0.004785149421110995\n",
      "Episode 841; Testing Loss 0.005880065329597985; Training Loss 0.004785137000149937\n",
      "Episode 842; Testing Loss 0.005880017159959793; Training Loss 0.004785126371648638\n",
      "Episode 843; Testing Loss 0.005880039661011183; Training Loss 0.004785115262980097\n",
      "Episode 844; Testing Loss 0.005880054561935821; Training Loss 0.004785104394511305\n",
      "Episode 845; Testing Loss 0.005880026388915534; Training Loss 0.0047850935702476866\n",
      "Episode 846; Testing Loss 0.005880015612419287; Training Loss 0.004785081561574631\n",
      "Episode 847; Testing Loss 0.005880041532908092; Training Loss 0.004785072655949534\n",
      "Episode 848; Testing Loss 0.005880079068277072; Training Loss 0.004785061349697151\n",
      "Episode 849; Testing Loss 0.005880070115241747; Training Loss 0.004785049641827942\n",
      "Episode 850; Testing Loss 0.005880002059640149; Training Loss 0.004785039652785701\n",
      "Episode 851; Testing Loss 0.005879964591291924; Training Loss 0.004785029461823588\n",
      "Episode 852; Testing Loss 0.0058800123812885585; Training Loss 0.004785019084350827\n",
      "Episode 853; Testing Loss 0.005880011259948881; Training Loss 0.00478500727868708\n",
      "Episode 854; Testing Loss 0.005879951715460924; Training Loss 0.004784999364367449\n",
      "Episode 855; Testing Loss 0.005879939511453378; Training Loss 0.0047849889038465\n",
      "Episode 856; Testing Loss 0.005879949998465336; Training Loss 0.004784974801514223\n",
      "Episode 857; Testing Loss 0.005879936148007271; Training Loss 0.00478496373741384\n",
      "Episode 858; Testing Loss 0.005879858693112713; Training Loss 0.004784953044625503\n",
      "Episode 859; Testing Loss 0.0058798635099391785; Training Loss 0.004784941532679505\n",
      "Episode 860; Testing Loss 0.005880004030151493; Training Loss 0.004784928545543938\n",
      "Episode 861; Testing Loss 0.005880025288258299; Training Loss 0.004784919887209482\n",
      "Episode 862; Testing Loss 0.0058799342413880734; Training Loss 0.004784909697921385\n",
      "Episode 863; Testing Loss 0.005879881807208992; Training Loss 0.004784898696153212\n",
      "Episode 864; Testing Loss 0.005879910544060367; Training Loss 0.0047848850408107905\n",
      "Episode 865; Testing Loss 0.005879895732998145; Training Loss 0.004784874921856518\n",
      "Episode 866; Testing Loss 0.0058798887800710865; Training Loss 0.004784864014222665\n",
      "Episode 867; Testing Loss 0.0058798980340162435; Training Loss 0.004784853320278483\n",
      "Episode 868; Testing Loss 0.005879943806208454; Training Loss 0.0047848418218199166\n",
      "Episode 869; Testing Loss 0.0058799143445668264; Training Loss 0.004784829076860497\n",
      "Episode 870; Testing Loss 0.0058798324995431115; Training Loss 0.004784818715325618\n",
      "Episode 871; Testing Loss 0.0058798165870895605; Training Loss 0.004784808695414132\n",
      "Episode 872; Testing Loss 0.005879876674234227; Training Loss 0.004784796263042418\n",
      "Episode 873; Testing Loss 0.005879919743333618; Training Loss 0.004784786463700406\n",
      "Episode 874; Testing Loss 0.0058798866642068275; Training Loss 0.004784775365882444\n",
      "Episode 875; Testing Loss 0.005879814051336489; Training Loss 0.004784763698852044\n",
      "Episode 876; Testing Loss 0.005879761529746503; Training Loss 0.004784752805229445\n",
      "Episode 877; Testing Loss 0.005879793179840934; Training Loss 0.00478474049942334\n",
      "Episode 878; Testing Loss 0.00587982679001685; Training Loss 0.004784729449674793\n",
      "Episode 879; Testing Loss 0.005879841557281093; Training Loss 0.0047847208170758515\n",
      "Episode 880; Testing Loss 0.00587975931947213; Training Loss 0.004784709666692579\n",
      "Episode 881; Testing Loss 0.005879719065250528; Training Loss 0.004784697922465153\n",
      "Episode 882; Testing Loss 0.005879761246882705; Training Loss 0.004784688386135686\n",
      "Episode 883; Testing Loss 0.005879827608692352; Training Loss 0.004784677702560702\n",
      "Episode 884; Testing Loss 0.00587976539904212; Training Loss 0.004784665685754837\n",
      "Episode 885; Testing Loss 0.005879650712695701; Training Loss 0.0047846556158331836\n",
      "Episode 886; Testing Loss 0.005879666421037227; Training Loss 0.004784646918109324\n",
      "Episode 887; Testing Loss 0.005879838840654083; Training Loss 0.004784635636239103\n",
      "Episode 888; Testing Loss 0.005879837399608295; Training Loss 0.004784623188218173\n",
      "Episode 889; Testing Loss 0.005879684170631075; Training Loss 0.004784612837059281\n",
      "Episode 890; Testing Loss 0.005879647279611891; Training Loss 0.004784602758292396\n",
      "Episode 891; Testing Loss 0.005879733172995353; Training Loss 0.004784590145514439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 892; Testing Loss 0.005879730819974267; Training Loss 0.00478457877578187\n",
      "Episode 893; Testing Loss 0.005879594229486845; Training Loss 0.004784569257098179\n",
      "Episode 894; Testing Loss 0.005879603808160927; Training Loss 0.004784557176644626\n",
      "Episode 895; Testing Loss 0.005879669984576591; Training Loss 0.004784545062135919\n",
      "Episode 896; Testing Loss 0.005879664501483019; Training Loss 0.00478453653712094\n",
      "Episode 897; Testing Loss 0.005879623332945032; Training Loss 0.004784525882357338\n",
      "Episode 898; Testing Loss 0.005879601642534924; Training Loss 0.004784514171032989\n",
      "Episode 899; Testing Loss 0.005879589445647571; Training Loss 0.004784501905451272\n",
      "Episode 900; Testing Loss 0.005879563487012807; Training Loss 0.004784493476514891\n",
      "Episode 901; Testing Loss 0.005879635450897506; Training Loss 0.004784481284179141\n",
      "Episode 902; Testing Loss 0.005879646794131977; Training Loss 0.004784467973768409\n",
      "Episode 903; Testing Loss 0.005879522889497271; Training Loss 0.004784459514662977\n",
      "Episode 904; Testing Loss 0.005879452548413406; Training Loss 0.004784450139202898\n",
      "Episode 905; Testing Loss 0.0058795361681065915; Training Loss 0.004784436991756176\n",
      "Episode 906; Testing Loss 0.0058795727185310025; Training Loss 0.004784424602644487\n",
      "Episode 907; Testing Loss 0.005879491761839952; Training Loss 0.004784414130984146\n",
      "Episode 908; Testing Loss 0.0058794274440903535; Training Loss 0.0047844029485492575\n",
      "Episode 909; Testing Loss 0.00587948468667308; Training Loss 0.0047843918063194815\n",
      "Episode 910; Testing Loss 0.005879520718680034; Training Loss 0.004784380867713469\n",
      "Episode 911; Testing Loss 0.005879461203822387; Training Loss 0.004784368522976308\n",
      "Episode 912; Testing Loss 0.005879423006302845; Training Loss 0.004784358454239368\n",
      "Episode 913; Testing Loss 0.0058794834648647105; Training Loss 0.004784347734352372\n",
      "Episode 914; Testing Loss 0.005879542484232869; Training Loss 0.004784336379015734\n",
      "Episode 915; Testing Loss 0.005879472556582472; Training Loss 0.004784325047091384\n",
      "Episode 916; Testing Loss 0.005879394266843813; Training Loss 0.004784314700690496\n",
      "Episode 917; Testing Loss 0.005879443768418329; Training Loss 0.004784301984892433\n",
      "Episode 918; Testing Loss 0.005879480483922554; Training Loss 0.00478429095652923\n",
      "Episode 919; Testing Loss 0.005879427939156384; Training Loss 0.004784280724925935\n",
      "Episode 920; Testing Loss 0.0058794055169427584; Training Loss 0.004784269797186956\n",
      "Episode 921; Testing Loss 0.005879415868613146; Training Loss 0.00478425896507298\n",
      "Episode 922; Testing Loss 0.005879436147281963; Training Loss 0.00478424918247015\n",
      "Episode 923; Testing Loss 0.005879433262665728; Training Loss 0.004784237815095881\n",
      "Episode 924; Testing Loss 0.0058794224627164845; Training Loss 0.004784227061888345\n",
      "Episode 925; Testing Loss 0.005879440782074015; Training Loss 0.004784216053071522\n",
      "Episode 926; Testing Loss 0.005879430317035521; Training Loss 0.004784202415592702\n",
      "Episode 927; Testing Loss 0.005879394989654108; Training Loss 0.004784195037085883\n",
      "Episode 928; Testing Loss 0.00587935831684659; Training Loss 0.004784184859198759\n",
      "Episode 929; Testing Loss 0.00587934130233056; Training Loss 0.004784171443956385\n",
      "Episode 930; Testing Loss 0.005879358232177535; Training Loss 0.004784159453302778\n",
      "Episode 931; Testing Loss 0.005879418140827488; Training Loss 0.004784152745770308\n",
      "Episode 932; Testing Loss 0.0058794775590737215; Training Loss 0.004784143199798789\n",
      "Episode 933; Testing Loss 0.005879456950364794; Training Loss 0.004784130218938649\n",
      "Episode 934; Testing Loss 0.005879427844108904; Training Loss 0.004784115642080998\n",
      "Episode 935; Testing Loss 0.005879401251452869; Training Loss 0.004784104399212254\n",
      "Episode 936; Testing Loss 0.005879382408823634; Training Loss 0.004784096139341437\n",
      "Episode 937; Testing Loss 0.005879307283844427; Training Loss 0.004784084400375407\n",
      "Episode 938; Testing Loss 0.005879292766198253; Training Loss 0.004784072424334536\n",
      "Episode 939; Testing Loss 0.0058793736239812116; Training Loss 0.004784061069296578\n",
      "Episode 940; Testing Loss 0.0058794185942044825; Training Loss 0.0047840516241913515\n",
      "Episode 941; Testing Loss 0.005879376704443965; Training Loss 0.004784039894077652\n",
      "Episode 942; Testing Loss 0.005879338745658702; Training Loss 0.00478403098265863\n",
      "Episode 943; Testing Loss 0.005879405476660187; Training Loss 0.004784018665446742\n",
      "Episode 944; Testing Loss 0.005879487046928722; Training Loss 0.004784005466948558\n",
      "Episode 945; Testing Loss 0.005879437127639574; Training Loss 0.004783995833876938\n",
      "Episode 946; Testing Loss 0.005879309128897675; Training Loss 0.004783984512022134\n",
      "Episode 947; Testing Loss 0.0058792955204564476; Training Loss 0.004783972978981862\n",
      "Episode 948; Testing Loss 0.005879398423380001; Training Loss 0.004783961383172192\n",
      "Episode 949; Testing Loss 0.005879450106142244; Training Loss 0.004783951870061068\n",
      "Episode 950; Testing Loss 0.005879375734974; Training Loss 0.00478393927814254\n",
      "Episode 951; Testing Loss 0.0058793286088510005; Training Loss 0.0047839279080030325\n",
      "Episode 952; Testing Loss 0.005879399841827558; Training Loss 0.004783918702343218\n",
      "Episode 953; Testing Loss 0.0058794508437908155; Training Loss 0.0047839062455067045\n",
      "Episode 954; Testing Loss 0.005879379040413651; Training Loss 0.004783895376048082\n",
      "Episode 955; Testing Loss 0.005879379387219187; Training Loss 0.004783883968024054\n",
      "Episode 956; Testing Loss 0.005879461058220212; Training Loss 0.0047838728839567635\n",
      "Episode 957; Testing Loss 0.005879445319528367; Training Loss 0.0047838614423234066\n",
      "Episode 958; Testing Loss 0.005879349707826322; Training Loss 0.004783849584672341\n",
      "Episode 959; Testing Loss 0.005879284073219714; Training Loss 0.004783840919923321\n",
      "Episode 960; Testing Loss 0.005879269209493393; Training Loss 0.0047838293720623565\n",
      "Episode 961; Testing Loss 0.005879317694757055; Training Loss 0.0047838163565892\n",
      "Episode 962; Testing Loss 0.005879372363567478; Training Loss 0.004783806156983302\n",
      "Episode 963; Testing Loss 0.0058793704842657795; Training Loss 0.004783795367538456\n",
      "Episode 964; Testing Loss 0.005879314249132014; Training Loss 0.004783782073821538\n",
      "Episode 965; Testing Loss 0.005879329303146479; Training Loss 0.004783773164522846\n",
      "Episode 966; Testing Loss 0.005879301498970686; Training Loss 0.004783763124829293\n",
      "Episode 967; Testing Loss 0.005879277228083308; Training Loss 0.004783751075473665\n",
      "Episode 968; Testing Loss 0.005879274765035704; Training Loss 0.004783737951865387\n",
      "Episode 969; Testing Loss 0.005879257680872572; Training Loss 0.0047837288074744355\n",
      "Episode 970; Testing Loss 0.0058792945471168225; Training Loss 0.004783718948803784\n",
      "Episode 971; Testing Loss 0.005879298233111324; Training Loss 0.004783706641762552\n",
      "Episode 972; Testing Loss 0.0058792434820643035; Training Loss 0.004783693300401416\n",
      "Episode 973; Testing Loss 0.005879186039045435; Training Loss 0.004783683100999153\n",
      "Episode 974; Testing Loss 0.00587917949089761; Training Loss 0.004783671577448341\n",
      "Episode 975; Testing Loss 0.005879246709511852; Training Loss 0.004783660045632727\n",
      "Episode 976; Testing Loss 0.0058792635335087875; Training Loss 0.004783648482040934\n",
      "Episode 977; Testing Loss 0.005879236110916969; Training Loss 0.004783638190185957\n",
      "Episode 978; Testing Loss 0.005879213890341553; Training Loss 0.004783627168994884\n",
      "Episode 979; Testing Loss 0.0058791945652091615; Training Loss 0.0047836152111149265\n",
      "Episode 980; Testing Loss 0.005879256366770479; Training Loss 0.0047836039911722085\n",
      "Episode 981; Testing Loss 0.005879237476884251; Training Loss 0.004783594747905362\n",
      "Episode 982; Testing Loss 0.005879167443315464; Training Loss 0.004783583577035895\n",
      "Episode 983; Testing Loss 0.0058791438442273945; Training Loss 0.004783571006145122\n",
      "Episode 984; Testing Loss 0.005879169409698754; Training Loss 0.004783562942496787\n",
      "Episode 985; Testing Loss 0.005879181216020898; Training Loss 0.004783552185226178\n",
      "Episode 986; Testing Loss 0.005879134259900573; Training Loss 0.0047835389577241885\n",
      "Episode 987; Testing Loss 0.005879177947059042; Training Loss 0.004783530725419987\n",
      "Episode 988; Testing Loss 0.00587925133361594; Training Loss 0.004783520993142745\n",
      "Episode 989; Testing Loss 0.005879195566087686; Training Loss 0.004783509383532854\n",
      "Episode 990; Testing Loss 0.005879104919417697; Training Loss 0.004783496396044641\n",
      "Episode 991; Testing Loss 0.005879110606091088; Training Loss 0.004783485379289954\n",
      "Episode 992; Testing Loss 0.005879158346599837; Training Loss 0.004783477321642175\n",
      "Episode 993; Testing Loss 0.005879126792280383; Training Loss 0.004783464255588797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 994; Testing Loss 0.005879047980292469; Training Loss 0.004783452662711391\n",
      "Episode 995; Testing Loss 0.005879039187368014; Training Loss 0.004783441171395775\n",
      "Episode 996; Testing Loss 0.005879112309138917; Training Loss 0.004783429533245004\n",
      "Episode 997; Testing Loss 0.0058791258631337604; Training Loss 0.004783417254414561\n",
      "Episode 998; Testing Loss 0.005879059355611242; Training Loss 0.0047834082707547\n",
      "Episode 999; Testing Loss 0.005879045588249225; Training Loss 0.0047833986837492876\n",
      "Episode 1000; Testing Loss 0.005879079729698189; Training Loss 0.004783385347161322\n",
      "Episode 1001; Testing Loss 0.005879097909498317; Training Loss 0.004783372625788717\n",
      "Episode 1002; Testing Loss 0.005879085801427946; Training Loss 0.004783362989651331\n",
      "Episode 1003; Testing Loss 0.0058790558884295446; Training Loss 0.004783350695093851\n",
      "Episode 1004; Testing Loss 0.005878975432481223; Training Loss 0.004783339530136883\n",
      "Episode 1005; Testing Loss 0.005878984943681528; Training Loss 0.004783328679480025\n",
      "Episode 1006; Testing Loss 0.005879033245900232; Training Loss 0.00478331903337931\n",
      "Episode 1007; Testing Loss 0.005878951876997484; Training Loss 0.004783306268101266\n",
      "Episode 1008; Testing Loss 0.005878896437053884; Training Loss 0.00478329712577449\n",
      "Episode 1009; Testing Loss 0.005879013137490251; Training Loss 0.004783284839751526\n",
      "Episode 1010; Testing Loss 0.005879134342689639; Training Loss 0.004783275291379955\n",
      "Episode 1011; Testing Loss 0.005879073426575857; Training Loss 0.004783262564067986\n",
      "Episode 1012; Testing Loss 0.005878957083070874; Training Loss 0.004783251191076287\n",
      "Episode 1013; Testing Loss 0.005878989185834426; Training Loss 0.004783241748057089\n",
      "Episode 1014; Testing Loss 0.005879068817208149; Training Loss 0.004783230424916741\n",
      "Episode 1015; Testing Loss 0.005879060437042672; Training Loss 0.004783217387351798\n",
      "Episode 1016; Testing Loss 0.0058789460165204165; Training Loss 0.004783208306717373\n",
      "Episode 1017; Testing Loss 0.0058789039320592965; Training Loss 0.004783198288302928\n",
      "Episode 1018; Testing Loss 0.005878979567589813; Training Loss 0.004783186057590264\n",
      "Episode 1019; Testing Loss 0.005879008133491489; Training Loss 0.004783174324174911\n",
      "Episode 1020; Testing Loss 0.0058789204948185994; Training Loss 0.004783164264840394\n",
      "Episode 1021; Testing Loss 0.005878902813519991; Training Loss 0.004783153463133051\n",
      "Episode 1022; Testing Loss 0.005878972392234898; Training Loss 0.004783140470812803\n",
      "Episode 1023; Testing Loss 0.005878997732045386; Training Loss 0.004783128551627012\n",
      "Episode 1024; Testing Loss 0.005878921501772094; Training Loss 0.00478311868554611\n",
      "Episode 1025; Testing Loss 0.005878859692177578; Training Loss 0.004783106937812684\n",
      "Episode 1026; Testing Loss 0.0058789376502660215; Training Loss 0.00478309478034094\n",
      "Episode 1027; Testing Loss 0.005879017281506531; Training Loss 0.004783083419209882\n",
      "Episode 1028; Testing Loss 0.005879009431166166; Training Loss 0.004783074164744386\n",
      "Episode 1029; Testing Loss 0.005878945681175935; Training Loss 0.00478306375444653\n",
      "Episode 1030; Testing Loss 0.00587894864529492; Training Loss 0.0047830522389607265\n",
      "Episode 1031; Testing Loss 0.005879016923854061; Training Loss 0.004783040817884383\n",
      "Episode 1032; Testing Loss 0.005879035054196991; Training Loss 0.004783029681719626\n",
      "Episode 1033; Testing Loss 0.0058789234889101245; Training Loss 0.004783020798305018\n",
      "Episode 1034; Testing Loss 0.0058788287898941805; Training Loss 0.004783010932922348\n",
      "Episode 1035; Testing Loss 0.005878861011988615; Training Loss 0.004782999100436622\n",
      "Episode 1036; Testing Loss 0.005878970305097154; Training Loss 0.004782985036711135\n",
      "Episode 1037; Testing Loss 0.0058789971041456335; Training Loss 0.004782974754871624\n",
      "Episode 1038; Testing Loss 0.005878933896265029; Training Loss 0.00478296614014208\n",
      "Episode 1039; Testing Loss 0.005878946582839516; Training Loss 0.004782955596740026\n",
      "Episode 1040; Testing Loss 0.005879053961061106; Training Loss 0.004782943032374399\n",
      "Episode 1041; Testing Loss 0.0058790137660310274; Training Loss 0.004782931946072763\n",
      "Episode 1042; Testing Loss 0.005878895939350557; Training Loss 0.004782918901124696\n",
      "Episode 1043; Testing Loss 0.0058788331647800915; Training Loss 0.004782906909560204\n",
      "Episode 1044; Testing Loss 0.005878860788926153; Training Loss 0.0047828964020276005\n",
      "Episode 1045; Testing Loss 0.0058788724786105915; Training Loss 0.004782884825052962\n",
      "Episode 1046; Testing Loss 0.00587886938200905; Training Loss 0.0047828745039333815\n",
      "Episode 1047; Testing Loss 0.005878877020407607; Training Loss 0.004782863804651417\n",
      "Episode 1048; Testing Loss 0.005878864223835901; Training Loss 0.004782853780793237\n",
      "Episode 1049; Testing Loss 0.005878827356725246; Training Loss 0.004782842424610723\n",
      "Episode 1050; Testing Loss 0.005878812015227705; Training Loss 0.004782829507777848\n",
      "Episode 1051; Testing Loss 0.005878841072125735; Training Loss 0.004782821133203296\n",
      "Episode 1052; Testing Loss 0.005878862643481478; Training Loss 0.004782810298561632\n",
      "Episode 1053; Testing Loss 0.005878815535755986; Training Loss 0.0047827964643951835\n",
      "Episode 1054; Testing Loss 0.0058787699982997274; Training Loss 0.004782785908349491\n",
      "Episode 1055; Testing Loss 0.005878856174765852; Training Loss 0.004782775218641095\n",
      "Episode 1056; Testing Loss 0.005878908558093554; Training Loss 0.004782764164534284\n",
      "Episode 1057; Testing Loss 0.005878863092923172; Training Loss 0.004782752554422486\n",
      "Episode 1058; Testing Loss 0.005878816347302343; Training Loss 0.004782741408313832\n",
      "Episode 1059; Testing Loss 0.005878792814500702; Training Loss 0.0047827307236993554\n",
      "Episode 1060; Testing Loss 0.0058787867046400915; Training Loss 0.004782719591718865\n",
      "Episode 1061; Testing Loss 0.005878846431057959; Training Loss 0.004782708510734763\n",
      "Episode 1062; Testing Loss 0.005878875333281064; Training Loss 0.004782698449114505\n",
      "Episode 1063; Testing Loss 0.005878851880023843; Training Loss 0.004782688925356034\n",
      "Episode 1064; Testing Loss 0.005878807911332413; Training Loss 0.004782675774088116\n",
      "Episode 1065; Testing Loss 0.005878800212289951; Training Loss 0.0047826649298292005\n",
      "Episode 1066; Testing Loss 0.005878809492165008; Training Loss 0.004782655476434423\n",
      "Episode 1067; Testing Loss 0.005878793544632958; Training Loss 0.004782643747122354\n",
      "Episode 1068; Testing Loss 0.005878756226459055; Training Loss 0.0047826311003019975\n",
      "Episode 1069; Testing Loss 0.005878709249461418; Training Loss 0.004782622243992149\n",
      "Episode 1070; Testing Loss 0.005878787679111447; Training Loss 0.004782611079628201\n",
      "Episode 1071; Testing Loss 0.005878852851565289; Training Loss 0.0047825979035163455\n",
      "Episode 1072; Testing Loss 0.0058788497070937085; Training Loss 0.004782587649292316\n",
      "Episode 1073; Testing Loss 0.005878804396004306; Training Loss 0.004782577553472386\n",
      "Episode 1074; Testing Loss 0.005878764591182002; Training Loss 0.004782566143971051\n",
      "Episode 1075; Testing Loss 0.005878784150594328; Training Loss 0.00478255330185965\n",
      "Episode 1076; Testing Loss 0.005878791326165545; Training Loss 0.0047825453137393574\n",
      "Episode 1077; Testing Loss 0.005878760118175269; Training Loss 0.0047825349114433285\n",
      "Episode 1078; Testing Loss 0.005878752725022619; Training Loss 0.004782522884109847\n",
      "Episode 1079; Testing Loss 0.005878810960635564; Training Loss 0.004782509919078378\n",
      "Episode 1080; Testing Loss 0.0058788211968471915; Training Loss 0.004782499412362575\n",
      "Episode 1081; Testing Loss 0.005878785179114717; Training Loss 0.00478248851756677\n",
      "Episode 1082; Testing Loss 0.005878778379128372; Training Loss 0.004782477870663175\n",
      "Episode 1083; Testing Loss 0.005878782942540191; Training Loss 0.004782466382877106\n",
      "Episode 1084; Testing Loss 0.00587874803018443; Training Loss 0.004782454477196236\n",
      "Episode 1085; Testing Loss 0.005878694992433033; Training Loss 0.004782445990736907\n",
      "Episode 1086; Testing Loss 0.0058787620704384945; Training Loss 0.004782434197990695\n",
      "Episode 1087; Testing Loss 0.005878846931972334; Training Loss 0.004782423733297292\n",
      "Episode 1088; Testing Loss 0.005878814855057635; Training Loss 0.004782412262044605\n",
      "Episode 1089; Testing Loss 0.005878773012868459; Training Loss 0.004782398854906797\n",
      "Episode 1090; Testing Loss 0.0058787760382121995; Training Loss 0.004782390636025426\n",
      "Episode 1091; Testing Loss 0.005878790745623009; Training Loss 0.004782380074769451\n",
      "Episode 1092; Testing Loss 0.0058787473179337515; Training Loss 0.004782366384886412\n",
      "Episode 1093; Testing Loss 0.005878696906525133; Training Loss 0.004782355157449636\n",
      "Episode 1094; Testing Loss 0.005878736030655699; Training Loss 0.004782344557803866\n",
      "Episode 1095; Testing Loss 0.005878738377869162; Training Loss 0.004782333791447233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1096; Testing Loss 0.005878684001703731; Training Loss 0.0047823226561807914\n",
      "Episode 1097; Testing Loss 0.005878658369514152; Training Loss 0.004782310307161735\n",
      "Episode 1098; Testing Loss 0.005878716330721309; Training Loss 0.004782300861899379\n",
      "Episode 1099; Testing Loss 0.005878725167384014; Training Loss 0.004782289806736716\n",
      "Episode 1100; Testing Loss 0.005878729615935904; Training Loss 0.004782277164190312\n",
      "Episode 1101; Testing Loss 0.005878690406190582; Training Loss 0.004782268162963269\n",
      "Episode 1102; Testing Loss 0.0058786937883097525; Training Loss 0.004782260302624824\n",
      "Episode 1103; Testing Loss 0.005878745011945815; Training Loss 0.004782248968492761\n",
      "Episode 1104; Testing Loss 0.0058787351446313585; Training Loss 0.004782236679975187\n",
      "Episode 1105; Testing Loss 0.005878653819056998; Training Loss 0.0047822252750032825\n",
      "Episode 1106; Testing Loss 0.005878636151733323; Training Loss 0.004782211793286953\n",
      "Episode 1107; Testing Loss 0.00587866188833727; Training Loss 0.004782205300593815\n",
      "Episode 1108; Testing Loss 0.00587866345992093; Training Loss 0.004782195713169203\n",
      "Episode 1109; Testing Loss 0.005878643497819468; Training Loss 0.00478218104511382\n",
      "Episode 1110; Testing Loss 0.0058786161822694475; Training Loss 0.0047821698841068695\n",
      "Episode 1111; Testing Loss 0.0058785937192409165; Training Loss 0.004782161523815036\n",
      "Episode 1112; Testing Loss 0.005878596472563534; Training Loss 0.004782150283092283\n",
      "Episode 1113; Testing Loss 0.005878615043326767; Training Loss 0.004782137108185856\n",
      "Episode 1114; Testing Loss 0.005878607543121412; Training Loss 0.00478212493337069\n",
      "Episode 1115; Testing Loss 0.005878606201725755; Training Loss 0.004782114651043297\n",
      "Episode 1116; Testing Loss 0.005878633379538618; Training Loss 0.00478210161405371\n",
      "Episode 1117; Testing Loss 0.005878685877105316; Training Loss 0.004782093415935437\n",
      "Episode 1118; Testing Loss 0.005878690170756912; Training Loss 0.004782082777404731\n",
      "Episode 1119; Testing Loss 0.005878611260432997; Training Loss 0.004782070136911035\n",
      "Episode 1120; Testing Loss 0.005878503276640047; Training Loss 0.004782058193659771\n",
      "Episode 1121; Testing Loss 0.005878539573131721; Training Loss 0.004782049610888332\n",
      "Episode 1122; Testing Loss 0.005878638500380771; Training Loss 0.0047820413872446035\n",
      "Episode 1123; Testing Loss 0.00587861763527422; Training Loss 0.004782028127053296\n",
      "Episode 1124; Testing Loss 0.005878549820612824; Training Loss 0.004782014037570564\n",
      "Episode 1125; Testing Loss 0.00587859177014033; Training Loss 0.004782004466227307\n",
      "Episode 1126; Testing Loss 0.005878702437025635; Training Loss 0.004781993207073786\n",
      "Episode 1127; Testing Loss 0.005878678212044692; Training Loss 0.004781980748774771\n",
      "Episode 1128; Testing Loss 0.0058785853223285445; Training Loss 0.00478197030782169\n",
      "Episode 1129; Testing Loss 0.005878530786314304; Training Loss 0.004781960521200932\n",
      "Episode 1130; Testing Loss 0.0058785885368734915; Training Loss 0.004781948269425287\n",
      "Episode 1131; Testing Loss 0.005878641073093396; Training Loss 0.00478193689899476\n",
      "Episode 1132; Testing Loss 0.005878604966692352; Training Loss 0.004781925345710398\n",
      "Episode 1133; Testing Loss 0.005878487464719273; Training Loss 0.004781916895724396\n",
      "Episode 1134; Testing Loss 0.005878481966637618; Training Loss 0.004781904442168943\n",
      "Episode 1135; Testing Loss 0.005878603612123711; Training Loss 0.0047818941336331645\n",
      "Episode 1136; Testing Loss 0.005878663181955023; Training Loss 0.004781885703862823\n",
      "Episode 1137; Testing Loss 0.005878603923421423; Training Loss 0.004781872769728926\n",
      "Episode 1138; Testing Loss 0.00587849525550866; Training Loss 0.0047818603864580676\n",
      "Episode 1139; Testing Loss 0.005878485546017084; Training Loss 0.004781851185641793\n",
      "Episode 1140; Testing Loss 0.005878493213824801; Training Loss 0.004781840688092598\n",
      "Episode 1141; Testing Loss 0.005878458958270517; Training Loss 0.004781828484470156\n",
      "Episode 1142; Testing Loss 0.005878460434827424; Training Loss 0.004781816682232455\n",
      "Episode 1143; Testing Loss 0.005878522163385492; Training Loss 0.004781807585496072\n",
      "Episode 1144; Testing Loss 0.005878551597080186; Training Loss 0.004781797405375985\n",
      "Episode 1145; Testing Loss 0.005878493709004807; Training Loss 0.004781783604735923\n",
      "Episode 1146; Testing Loss 0.005878488229920021; Training Loss 0.0047817759590059045\n",
      "Episode 1147; Testing Loss 0.005878568549883927; Training Loss 0.004781766391594753\n",
      "Episode 1148; Testing Loss 0.00587859086499907; Training Loss 0.004781754551271943\n",
      "Episode 1149; Testing Loss 0.005878477467583814; Training Loss 0.004781740483215807\n",
      "Episode 1150; Testing Loss 0.005878368993341424; Training Loss 0.004781730810792081\n",
      "Episode 1151; Testing Loss 0.005878405538733175; Training Loss 0.004781720627995571\n",
      "Episode 1152; Testing Loss 0.005878506908097283; Training Loss 0.00478170805187564\n",
      "Episode 1153; Testing Loss 0.00587851361909442; Training Loss 0.004781696914104447\n",
      "Episode 1154; Testing Loss 0.005878408705333395; Training Loss 0.004781686446989015\n",
      "Episode 1155; Testing Loss 0.005878413578376346; Training Loss 0.0047816765101555804\n",
      "Episode 1156; Testing Loss 0.00587848288848257; Training Loss 0.004781663187522222\n",
      "Episode 1157; Testing Loss 0.005878484204166887; Training Loss 0.0047816554650261965\n",
      "Episode 1158; Testing Loss 0.005878459210265663; Training Loss 0.004781646211216035\n",
      "Episode 1159; Testing Loss 0.005878423161532023; Training Loss 0.004781633492417949\n",
      "Episode 1160; Testing Loss 0.005878424765259559; Training Loss 0.004781619622100839\n",
      "Episode 1161; Testing Loss 0.005878375037603282; Training Loss 0.0047816097874719835\n",
      "Episode 1162; Testing Loss 0.005878312049528511; Training Loss 0.004781598359821214\n",
      "Episode 1163; Testing Loss 0.005878348245967444; Training Loss 0.004781586344971481\n",
      "Episode 1164; Testing Loss 0.005878491734435663; Training Loss 0.004781576173583288\n",
      "Episode 1165; Testing Loss 0.0058785197486216625; Training Loss 0.004781566279223956\n",
      "Episode 1166; Testing Loss 0.005878366640541718; Training Loss 0.004781554197205633\n",
      "Episode 1167; Testing Loss 0.005878295261088513; Training Loss 0.0047815450854012944\n",
      "Episode 1168; Testing Loss 0.005878334428179254; Training Loss 0.004781533904163545\n",
      "Episode 1169; Testing Loss 0.005878380577378025; Training Loss 0.004781522910069614\n",
      "Episode 1170; Testing Loss 0.005878340627372205; Training Loss 0.00478151253314654\n",
      "Episode 1171; Testing Loss 0.005878340049499908; Training Loss 0.004781501607634257\n",
      "Episode 1172; Testing Loss 0.005878390608057561; Training Loss 0.004781491235241478\n",
      "Episode 1173; Testing Loss 0.005878446384240026; Training Loss 0.00478147975839485\n",
      "Episode 1174; Testing Loss 0.005878438593115137; Training Loss 0.004781467273770177\n",
      "Episode 1175; Testing Loss 0.005878369063321347; Training Loss 0.004781455835245267\n",
      "Episode 1176; Testing Loss 0.005878301293159721; Training Loss 0.004781445737944311\n",
      "Episode 1177; Testing Loss 0.005878315721365787; Training Loss 0.004781433589821767\n",
      "Episode 1178; Testing Loss 0.00587835694157848; Training Loss 0.004781424214525005\n",
      "Episode 1179; Testing Loss 0.005878323357152643; Training Loss 0.004781413685375871\n",
      "Episode 1180; Testing Loss 0.0058782552445521665; Training Loss 0.004781402385873726\n",
      "Episode 1181; Testing Loss 0.005878229956810115; Training Loss 0.0047813904223924526\n",
      "Episode 1182; Testing Loss 0.00587834243157872; Training Loss 0.004781379632469381\n",
      "Episode 1183; Testing Loss 0.005878425584745902; Training Loss 0.004781369573206776\n",
      "Episode 1184; Testing Loss 0.005878370790645494; Training Loss 0.004781358011431978\n",
      "Episode 1185; Testing Loss 0.00587826876407377; Training Loss 0.004781346793908321\n",
      "Episode 1186; Testing Loss 0.005878266740695799; Training Loss 0.0047813360682790376\n",
      "Episode 1187; Testing Loss 0.005878262847397095; Training Loss 0.0047813267032755956\n",
      "Episode 1188; Testing Loss 0.005878213863603942; Training Loss 0.004781316316500318\n",
      "Episode 1189; Testing Loss 0.005878223788042266; Training Loss 0.00478130456569002\n",
      "Episode 1190; Testing Loss 0.005878288344777031; Training Loss 0.0047812929129046715\n",
      "Episode 1191; Testing Loss 0.005878326851740591; Training Loss 0.004781281940845045\n",
      "Episode 1192; Testing Loss 0.0058782818955146355; Training Loss 0.004781270161857188\n",
      "Episode 1193; Testing Loss 0.0058782591624612895; Training Loss 0.004781259104506118\n",
      "Episode 1194; Testing Loss 0.005878287097312915; Training Loss 0.004781249390531673\n",
      "Episode 1195; Testing Loss 0.005878263727496146; Training Loss 0.004781238119821612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1196; Testing Loss 0.00587824172765389; Training Loss 0.004781229259676067\n",
      "Episode 1197; Testing Loss 0.005878242177724648; Training Loss 0.004781219285330275\n",
      "Episode 1198; Testing Loss 0.0058782693350116185; Training Loss 0.004781207024003991\n",
      "Episode 1199; Testing Loss 0.005878284003175032; Training Loss 0.004781194419077418\n",
      "Episode 1200; Testing Loss 0.005878282266561864; Training Loss 0.0047811835956917785\n",
      "Episode 1201; Testing Loss 0.005878251836079826; Training Loss 0.004781173467925487\n",
      "Episode 1202; Testing Loss 0.005878246654556836; Training Loss 0.004781161940646582\n",
      "Episode 1203; Testing Loss 0.0058783040248527405; Training Loss 0.00478115042391332\n",
      "Episode 1204; Testing Loss 0.005878312738663565; Training Loss 0.004781139916689628\n",
      "Episode 1205; Testing Loss 0.005878201201609187; Training Loss 0.0047811288745068575\n",
      "Episode 1206; Testing Loss 0.0058781531453134485; Training Loss 0.0047811184577711266\n",
      "Episode 1207; Testing Loss 0.005878195936293106; Training Loss 0.004781107037480012\n",
      "Episode 1208; Testing Loss 0.00587826477099525; Training Loss 0.004781095803978901\n",
      "Episode 1209; Testing Loss 0.005878284484350549; Training Loss 0.004781085176298292\n",
      "Episode 1210; Testing Loss 0.005878266815692868; Training Loss 0.004781073048917691\n",
      "Episode 1211; Testing Loss 0.005878307842154113; Training Loss 0.004781062455812291\n",
      "Episode 1212; Testing Loss 0.00587826093806911; Training Loss 0.004781052347026749\n",
      "Episode 1213; Testing Loss 0.005878248277507794; Training Loss 0.004781040466456936\n",
      "Episode 1214; Testing Loss 0.005878286249524683; Training Loss 0.004781032049874338\n",
      "Episode 1215; Testing Loss 0.0058782415272085695; Training Loss 0.004781021953148986\n",
      "Episode 1216; Testing Loss 0.005878226983927673; Training Loss 0.004781010567878952\n",
      "Episode 1217; Testing Loss 0.005878279182679083; Training Loss 0.004780999766355629\n",
      "Episode 1218; Testing Loss 0.005878297345435096; Training Loss 0.004780989513466564\n",
      "Episode 1219; Testing Loss 0.0058782444113433365; Training Loss 0.004780978907944671\n",
      "Episode 1220; Testing Loss 0.005878170206478594; Training Loss 0.004780968134407674\n",
      "Episode 1221; Testing Loss 0.005878159421702023; Training Loss 0.004780955223519167\n",
      "Episode 1222; Testing Loss 0.005878229918571649; Training Loss 0.0047809454714650024\n",
      "Episode 1223; Testing Loss 0.005878335263947622; Training Loss 0.004780935418496373\n",
      "Episode 1224; Testing Loss 0.005878278215051969; Training Loss 0.004780922472087222\n",
      "Episode 1225; Testing Loss 0.005878147558493375; Training Loss 0.004780912314703011\n",
      "Episode 1226; Testing Loss 0.005878152357995647; Training Loss 0.0047809025392938935\n",
      "Episode 1227; Testing Loss 0.005878230037295394; Training Loss 0.0047808913638568255\n",
      "Episode 1228; Testing Loss 0.005878220235862305; Training Loss 0.004780879762221861\n",
      "Episode 1229; Testing Loss 0.005878147772037353; Training Loss 0.004780870739552735\n",
      "Episode 1230; Testing Loss 0.005878158697650409; Training Loss 0.004780859665193958\n",
      "Episode 1231; Testing Loss 0.005878249226461037; Training Loss 0.0047808492478366715\n",
      "Episode 1232; Testing Loss 0.005878266789854883; Training Loss 0.004780837679265618\n",
      "Episode 1233; Testing Loss 0.005878206157495264; Training Loss 0.004780826232483225\n",
      "Episode 1234; Testing Loss 0.005878144360232649; Training Loss 0.004780814582142564\n",
      "Episode 1235; Testing Loss 0.00587813190551243; Training Loss 0.004780801842170926\n",
      "Episode 1236; Testing Loss 0.005878105605896482; Training Loss 0.004780791665010695\n",
      "Episode 1237; Testing Loss 0.005878052869985217; Training Loss 0.004780781202251826\n",
      "Episode 1238; Testing Loss 0.0058780994816129755; Training Loss 0.004780769453902802\n",
      "Episode 1239; Testing Loss 0.005878169293611402; Training Loss 0.0047807612235377115\n",
      "Episode 1240; Testing Loss 0.00587811288804556; Training Loss 0.0047807506426749136\n",
      "Episode 1241; Testing Loss 0.005878074528648253; Training Loss 0.004780739185837112\n",
      "Episode 1242; Testing Loss 0.005878149547961272; Training Loss 0.004780725160146959\n",
      "Episode 1243; Testing Loss 0.005878219493183694; Training Loss 0.004780720163267371\n",
      "Episode 1244; Testing Loss 0.005878190975151881; Training Loss 0.004780709992403241\n",
      "Episode 1245; Testing Loss 0.005878110147356605; Training Loss 0.0047806966604006535\n",
      "Episode 1246; Testing Loss 0.005878106814914523; Training Loss 0.004780682680793469\n",
      "Episode 1247; Testing Loss 0.005878132799358259; Training Loss 0.004780675345264942\n",
      "Episode 1248; Testing Loss 0.0058780663409427345; Training Loss 0.004780666183607473\n",
      "Episode 1249; Testing Loss 0.005877982719865213; Training Loss 0.004780655777443621\n",
      "Episode 1250; Testing Loss 0.005878032170158774; Training Loss 0.0047806432303945726\n",
      "Episode 1251; Testing Loss 0.005878119149343087; Training Loss 0.004780629647392762\n",
      "Episode 1252; Testing Loss 0.005878159752502839; Training Loss 0.004780618745625129\n",
      "Episode 1253; Testing Loss 0.00587815115553248; Training Loss 0.004780609837820399\n",
      "Episode 1254; Testing Loss 0.0058780987893129345; Training Loss 0.0047805985255465104\n",
      "Episode 1255; Testing Loss 0.005878044571403666; Training Loss 0.0047805844135276\n",
      "Episode 1256; Testing Loss 0.005878111843000508; Training Loss 0.004780576504803952\n",
      "Episode 1257; Testing Loss 0.005878121604179846; Training Loss 0.0047805690399068896\n",
      "Episode 1258; Testing Loss 0.005878126293929927; Training Loss 0.00478055802259308\n",
      "Episode 1259; Testing Loss 0.005878069661452524; Training Loss 0.00478054534705258\n",
      "Episode 1260; Testing Loss 0.005878033305072288; Training Loss 0.004780532923179695\n",
      "Episode 1261; Testing Loss 0.005878014473975815; Training Loss 0.0047805229578038524\n",
      "Episode 1262; Testing Loss 0.005878062925875854; Training Loss 0.004780513681031348\n",
      "Episode 1263; Testing Loss 0.005878118405302339; Training Loss 0.00478050315696104\n",
      "Episode 1264; Testing Loss 0.005878112856955451; Training Loss 0.004780488905656898\n",
      "Episode 1265; Testing Loss 0.005878043985092177; Training Loss 0.00478047785577837\n",
      "Episode 1266; Testing Loss 0.005877962018838354; Training Loss 0.004780466363895921\n",
      "Episode 1267; Testing Loss 0.00587790550673946; Training Loss 0.004780456995263312\n",
      "Episode 1268; Testing Loss 0.005877924435017046; Training Loss 0.004780446589177154\n",
      "Episode 1269; Testing Loss 0.005877983800138326; Training Loss 0.004780436529213428\n",
      "Episode 1270; Testing Loss 0.005877945065244738; Training Loss 0.004780425365030107\n",
      "Episode 1271; Testing Loss 0.0058778679726792064; Training Loss 0.004780412941110523\n",
      "Episode 1272; Testing Loss 0.005877872724425974; Training Loss 0.004780401118105391\n",
      "Episode 1273; Testing Loss 0.005877975137860477; Training Loss 0.004780393897395199\n",
      "Episode 1274; Testing Loss 0.005878017774367335; Training Loss 0.0047803832561677554\n",
      "Episode 1275; Testing Loss 0.005877890897349142; Training Loss 0.004780369210567333\n",
      "Episode 1276; Testing Loss 0.005877793849451835; Training Loss 0.0047803603046297554\n",
      "Episode 1277; Testing Loss 0.005877899416304552; Training Loss 0.004780348000564125\n",
      "Episode 1278; Testing Loss 0.005878024999692619; Training Loss 0.004780339871781943\n",
      "Episode 1279; Testing Loss 0.005877917194888759; Training Loss 0.004780327183407762\n",
      "Episode 1280; Testing Loss 0.005877759858376924; Training Loss 0.004780315725284635\n",
      "Episode 1281; Testing Loss 0.0058777753173686964; Training Loss 0.004780304092322792\n",
      "Episode 1282; Testing Loss 0.005877898362851688; Training Loss 0.004780293005964389\n",
      "Episode 1283; Testing Loss 0.005877941379371539; Training Loss 0.004780282124580251\n",
      "Episode 1284; Testing Loss 0.005877875394997833; Training Loss 0.004780271259349745\n",
      "Episode 1285; Testing Loss 0.00587780939506141; Training Loss 0.0047802598085647015\n",
      "Episode 1286; Testing Loss 0.005877849862810258; Training Loss 0.004780247978505255\n",
      "Episode 1287; Testing Loss 0.005877834072120524; Training Loss 0.004780239442976196\n",
      "Episode 1288; Testing Loss 0.0058777604738652; Training Loss 0.004780227826573924\n",
      "Episode 1289; Testing Loss 0.005877715663870696; Training Loss 0.0047802165717407774\n",
      "Episode 1290; Testing Loss 0.0058778148248253646; Training Loss 0.004780206445619533\n",
      "Episode 1291; Testing Loss 0.005877929756394282; Training Loss 0.004780194987275683\n",
      "Episode 1292; Testing Loss 0.005877881843244524; Training Loss 0.004780184648103422\n",
      "Episode 1293; Testing Loss 0.005877726770873893; Training Loss 0.004780174802088562\n",
      "Episode 1294; Testing Loss 0.005877748235422694; Training Loss 0.004780161151516533\n",
      "Episode 1295; Testing Loss 0.005877843273882295; Training Loss 0.004780152405260256\n",
      "Episode 1296; Testing Loss 0.005877871552668754; Training Loss 0.004780142701190263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1297; Testing Loss 0.005877805513346739; Training Loss 0.0047801294518538305\n",
      "Episode 1298; Testing Loss 0.005877744458713718; Training Loss 0.004780119334116887\n",
      "Episode 1299; Testing Loss 0.005877745405601906; Training Loss 0.004780111328922737\n",
      "Episode 1300; Testing Loss 0.00587777051350089; Training Loss 0.004780100227863728\n",
      "Episode 1301; Testing Loss 0.005877810407244348; Training Loss 0.004780086566388867\n",
      "Episode 1302; Testing Loss 0.005877827765223485; Training Loss 0.0047800765967507906\n",
      "Episode 1303; Testing Loss 0.005877863083598898; Training Loss 0.00478006798295301\n",
      "Episode 1304; Testing Loss 0.0058778519761767695; Training Loss 0.004780055953526626\n",
      "Episode 1305; Testing Loss 0.005877809310366384; Training Loss 0.004780043217269012\n",
      "Episode 1306; Testing Loss 0.005877747689906252; Training Loss 0.004780033437191714\n",
      "Episode 1307; Testing Loss 0.005877734354900101; Training Loss 0.004780020824480622\n",
      "Episode 1308; Testing Loss 0.005877728833925383; Training Loss 0.004780011718527935\n",
      "Episode 1309; Testing Loss 0.005877680362787647; Training Loss 0.0047800007418027654\n",
      "Episode 1310; Testing Loss 0.005877641875534956; Training Loss 0.004779989262840206\n",
      "Episode 1311; Testing Loss 0.00587766464056052; Training Loss 0.004779978347728882\n",
      "Episode 1312; Testing Loss 0.00587772697256651; Training Loss 0.004779967904494401\n",
      "Episode 1313; Testing Loss 0.005877760894247953; Training Loss 0.004779959114621189\n",
      "Episode 1314; Testing Loss 0.005877712114309211; Training Loss 0.004779947751746042\n",
      "Episode 1315; Testing Loss 0.005877651418015706; Training Loss 0.004779933811652976\n",
      "Episode 1316; Testing Loss 0.005877650018673206; Training Loss 0.004779924538291366\n",
      "Episode 1317; Testing Loss 0.005877675375174472; Training Loss 0.004779916931959132\n",
      "Episode 1318; Testing Loss 0.005877614734818891; Training Loss 0.004779906025946246\n",
      "Episode 1319; Testing Loss 0.005877537866471405; Training Loss 0.004779892817643893\n",
      "Episode 1320; Testing Loss 0.005877593357351439; Training Loss 0.004779880456891823\n",
      "Episode 1321; Testing Loss 0.005877747147955198; Training Loss 0.004779871111284565\n",
      "Episode 1322; Testing Loss 0.005877782575878074; Training Loss 0.004779859545454283\n",
      "Episode 1323; Testing Loss 0.005877686671094553; Training Loss 0.004779849049457265\n",
      "Episode 1324; Testing Loss 0.005877571226607366; Training Loss 0.004779839130671126\n",
      "Episode 1325; Testing Loss 0.005877604422804588; Training Loss 0.004779826835470515\n",
      "Episode 1326; Testing Loss 0.005877637393646357; Training Loss 0.00477981494654221\n",
      "Episode 1327; Testing Loss 0.00587757801629453; Training Loss 0.004779806513280851\n",
      "Episode 1328; Testing Loss 0.0058775669528454595; Training Loss 0.004779797153467436\n",
      "Episode 1329; Testing Loss 0.005877626758801558; Training Loss 0.004779783682749924\n",
      "Episode 1330; Testing Loss 0.005877602833188842; Training Loss 0.004779771230644859\n",
      "Episode 1331; Testing Loss 0.005877506554996527; Training Loss 0.004779763172055921\n",
      "Episode 1332; Testing Loss 0.005877481686352952; Training Loss 0.004779753246295377\n",
      "Episode 1333; Testing Loss 0.005877589965407525; Training Loss 0.004779738130941948\n",
      "Episode 1334; Testing Loss 0.005877664795773196; Training Loss 0.004779729915583196\n",
      "Episode 1335; Testing Loss 0.005877573341648273; Training Loss 0.004779720340067221\n",
      "Episode 1336; Testing Loss 0.0058774736054810475; Training Loss 0.004779707692605211\n",
      "Episode 1337; Testing Loss 0.005877478099761049; Training Loss 0.004779696984367144\n",
      "Episode 1338; Testing Loss 0.005877479102608032; Training Loss 0.004779687066408428\n",
      "Episode 1339; Testing Loss 0.0058774703254154655; Training Loss 0.004779674644206084\n",
      "Episode 1340; Testing Loss 0.005877517419418644; Training Loss 0.00477966293693884\n",
      "Episode 1341; Testing Loss 0.0058775801017869; Training Loss 0.004779654121802003\n",
      "Episode 1342; Testing Loss 0.005877535493458335; Training Loss 0.004779644066209501\n",
      "Episode 1343; Testing Loss 0.005877447569274661; Training Loss 0.00477963193271708\n",
      "Episode 1344; Testing Loss 0.005877466482213758; Training Loss 0.004779619075525354\n",
      "Episode 1345; Testing Loss 0.00587751730416304; Training Loss 0.004779610560959499\n",
      "Episode 1346; Testing Loss 0.0058774640581181654; Training Loss 0.004779597767207246\n",
      "Episode 1347; Testing Loss 0.005877379284701913; Training Loss 0.004779587808901694\n",
      "Episode 1348; Testing Loss 0.005877459298439422; Training Loss 0.004779576574239093\n",
      "Episode 1349; Testing Loss 0.005877530206131581; Training Loss 0.004779567363838011\n",
      "Episode 1350; Testing Loss 0.005877497494516533; Training Loss 0.004779554360744122\n",
      "Episode 1351; Testing Loss 0.005877419474963602; Training Loss 0.0047795454391331696\n",
      "Episode 1352; Testing Loss 0.005877382409372243; Training Loss 0.004779535220184068\n",
      "Episode 1353; Testing Loss 0.005877379056501892; Training Loss 0.004779524062993622\n",
      "Episode 1354; Testing Loss 0.005877369914846513; Training Loss 0.004779510404694135\n",
      "Episode 1355; Testing Loss 0.0058773811568621475; Training Loss 0.004779500464310241\n",
      "Episode 1356; Testing Loss 0.005877377418141265; Training Loss 0.004779490010304933\n",
      "Episode 1357; Testing Loss 0.005877319434574218; Training Loss 0.004779479763279085\n",
      "Episode 1358; Testing Loss 0.005877285136391764; Training Loss 0.004779467385277504\n",
      "Episode 1359; Testing Loss 0.005877357066155211; Training Loss 0.00477945792370589\n",
      "Episode 1360; Testing Loss 0.005877403971169545; Training Loss 0.00477944783491192\n",
      "Episode 1361; Testing Loss 0.00587742673576766; Training Loss 0.004779435909043023\n",
      "Episode 1362; Testing Loss 0.005877398351263832; Training Loss 0.0047794278049802805\n",
      "Episode 1363; Testing Loss 0.005877327575540067; Training Loss 0.004779416981851741\n",
      "Episode 1364; Testing Loss 0.005877249670812217; Training Loss 0.004779405979319152\n",
      "Episode 1365; Testing Loss 0.005877275690254377; Training Loss 0.00477939321131845\n",
      "Episode 1366; Testing Loss 0.005877317499078387; Training Loss 0.004779381869420723\n",
      "Episode 1367; Testing Loss 0.005877289956333727; Training Loss 0.004779370990576537\n",
      "Episode 1368; Testing Loss 0.005877281894790095; Training Loss 0.004779358867714028\n",
      "Episode 1369; Testing Loss 0.005877303904088987; Training Loss 0.00477935042337415\n",
      "Episode 1370; Testing Loss 0.005877337162388681; Training Loss 0.004779340870102618\n",
      "Episode 1371; Testing Loss 0.005877309678936907; Training Loss 0.0047793293533142465\n",
      "Episode 1372; Testing Loss 0.005877257316056632; Training Loss 0.004779316757921922\n",
      "Episode 1373; Testing Loss 0.005877238681188743; Training Loss 0.004779306670005158\n",
      "Episode 1374; Testing Loss 0.005877350254959677; Training Loss 0.004779297528193958\n",
      "Episode 1375; Testing Loss 0.005877326307624418; Training Loss 0.004779285031745785\n",
      "Episode 1376; Testing Loss 0.005877218928337026; Training Loss 0.004779273772423197\n",
      "Episode 1377; Testing Loss 0.005877171895790296; Training Loss 0.004779264119803412\n",
      "Episode 1378; Testing Loss 0.005877218297942223; Training Loss 0.00477925304175778\n",
      "Episode 1379; Testing Loss 0.005877257089363735; Training Loss 0.004779239773462699\n",
      "Episode 1380; Testing Loss 0.005877249990936725; Training Loss 0.004779232712904274\n",
      "Episode 1381; Testing Loss 0.00587724763302401; Training Loss 0.004779223683798278\n",
      "Episode 1382; Testing Loss 0.005877262966048967; Training Loss 0.004779211201838571\n",
      "Episode 1383; Testing Loss 0.0058772290144572656; Training Loss 0.0047791974960850435\n",
      "Episode 1384; Testing Loss 0.005877145744029273; Training Loss 0.0047791880208990995\n",
      "Episode 1385; Testing Loss 0.005877119682203304; Training Loss 0.004779178344402468\n",
      "Episode 1386; Testing Loss 0.00587713048360305; Training Loss 0.004779166447840621\n",
      "Episode 1387; Testing Loss 0.005877072683695424; Training Loss 0.004779157764792552\n",
      "Episode 1388; Testing Loss 0.005877054525517365; Training Loss 0.004779149388971543\n",
      "Episode 1389; Testing Loss 0.00587715819719744; Training Loss 0.004779137324562027\n",
      "Episode 1390; Testing Loss 0.0058772056125101155; Training Loss 0.004779125533433542\n",
      "Episode 1391; Testing Loss 0.005877083647193889; Training Loss 0.004779115134850245\n",
      "Episode 1392; Testing Loss 0.005877017432813642; Training Loss 0.004779103836891064\n",
      "Episode 1393; Testing Loss 0.005877089353067484; Training Loss 0.004779089530712619\n",
      "Episode 1394; Testing Loss 0.0058771663488794415; Training Loss 0.004779082841055773\n",
      "Episode 1395; Testing Loss 0.005877127536996582; Training Loss 0.0047790731949044225\n",
      "Episode 1396; Testing Loss 0.005877020277150182; Training Loss 0.004779058520288366\n",
      "Episode 1397; Testing Loss 0.005876965284945921; Training Loss 0.004779047791783705\n",
      "Episode 1398; Testing Loss 0.005877011355256949; Training Loss 0.004779038963929964\n",
      "Episode 1399; Testing Loss 0.0058770543002709084; Training Loss 0.004779028384635583\n",
      "Episode 1400; Testing Loss 0.005877028982431395; Training Loss 0.004779015528613195\n",
      "Episode 1401; Testing Loss 0.005876962973515306; Training Loss 0.004779006352921013\n",
      "Episode 1402; Testing Loss 0.0058770165991898025; Training Loss 0.004778995398619132\n",
      "Episode 1403; Testing Loss 0.005877076073987572; Training Loss 0.004778983648552427\n",
      "Episode 1404; Testing Loss 0.00587706137405031; Training Loss 0.00477897234848687\n",
      "Episode 1405; Testing Loss 0.005877022194381134; Training Loss 0.004778962856756524\n",
      "Episode 1406; Testing Loss 0.00587698776117337; Training Loss 0.004778951909378105\n",
      "Episode 1407; Testing Loss 0.005877003860042553; Training Loss 0.004778938374539048\n",
      "Episode 1408; Testing Loss 0.005877004096808997; Training Loss 0.004778929119253053\n",
      "Episode 1409; Testing Loss 0.005876948618555238; Training Loss 0.004778919060501543\n",
      "Episode 1410; Testing Loss 0.005876894522444026; Training Loss 0.004778907666151443\n",
      "Episode 1411; Testing Loss 0.005876942952751602; Training Loss 0.0047788957404899515\n",
      "Episode 1412; Testing Loss 0.005877055435683127; Training Loss 0.004778885659462826\n",
      "Episode 1413; Testing Loss 0.005877009660102392; Training Loss 0.0047788738483453665\n",
      "Episode 1414; Testing Loss 0.005876871715406588; Training Loss 0.004778863189851414\n",
      "Episode 1415; Testing Loss 0.005876817335296374; Training Loss 0.004778852859996839\n",
      "Episode 1416; Testing Loss 0.005876909392686832; Training Loss 0.004778842512333642\n",
      "Episode 1417; Testing Loss 0.005876969866083038; Training Loss 0.00477883240379963\n",
      "Episode 1418; Testing Loss 0.0058768889496846; Training Loss 0.004778820527532004\n",
      "Episode 1419; Testing Loss 0.0058768306246797; Training Loss 0.004778809026490324\n",
      "Episode 1420; Testing Loss 0.005876847367483526; Training Loss 0.004778799841009225\n",
      "Episode 1421; Testing Loss 0.00587691361687564; Training Loss 0.004778789685492361\n",
      "Episode 1422; Testing Loss 0.0058769197123344375; Training Loss 0.0047787785816816196\n",
      "Episode 1423; Testing Loss 0.005876797946886962; Training Loss 0.004778766777457223\n",
      "Episode 1424; Testing Loss 0.005876792928866037; Training Loss 0.0047787562456439335\n",
      "Episode 1425; Testing Loss 0.005876880955506194; Training Loss 0.004778744982542975\n",
      "Episode 1426; Testing Loss 0.005876853379609216; Training Loss 0.004778733150612059\n",
      "Episode 1427; Testing Loss 0.005876806613869617; Training Loss 0.0047787229704694765\n",
      "Episode 1428; Testing Loss 0.005876793177589775; Training Loss 0.004778712403287804\n",
      "Episode 1429; Testing Loss 0.005876849508875731; Training Loss 0.004778703247941097\n",
      "Episode 1430; Testing Loss 0.005876844066488811; Training Loss 0.0047786927723752395\n",
      "Episode 1431; Testing Loss 0.00587680831715395; Training Loss 0.0047786811364840046\n",
      "Episode 1432; Testing Loss 0.00587680072379123; Training Loss 0.004778669931833267\n",
      "Episode 1433; Testing Loss 0.00587677060139656; Training Loss 0.004778659191777638\n",
      "Episode 1434; Testing Loss 0.005876735606575485; Training Loss 0.0047786488046853365\n",
      "Episode 1435; Testing Loss 0.005876785151380328; Training Loss 0.004778638307720262\n",
      "Episode 1436; Testing Loss 0.005876806598907297; Training Loss 0.004778626229299083\n",
      "Episode 1437; Testing Loss 0.005876770736640685; Training Loss 0.004778617143256742\n",
      "Episode 1438; Testing Loss 0.005876748927409957; Training Loss 0.004778606314357884\n",
      "Episode 1439; Testing Loss 0.005876747288748516; Training Loss 0.004778594269113792\n",
      "Episode 1440; Testing Loss 0.005876717223243926; Training Loss 0.004778585415178557\n",
      "Episode 1441; Testing Loss 0.00587669034499403; Training Loss 0.004778576031007442\n",
      "Episode 1442; Testing Loss 0.005876702230842821; Training Loss 0.0047785647236789595\n",
      "Episode 1443; Testing Loss 0.0058766770202717125; Training Loss 0.004778553271508178\n",
      "Episode 1444; Testing Loss 0.005876599209301076; Training Loss 0.0047785410493531725\n",
      "Episode 1445; Testing Loss 0.0058766393669563086; Training Loss 0.0047785320224709195\n",
      "Episode 1446; Testing Loss 0.005876753623467327; Training Loss 0.004778519919031952\n",
      "Episode 1447; Testing Loss 0.005876763253418117; Training Loss 0.00477851175173481\n",
      "Episode 1448; Testing Loss 0.005876652379763744; Training Loss 0.004778500970497658\n",
      "Episode 1449; Testing Loss 0.005876559571131586; Training Loss 0.004778489278941985\n",
      "Episode 1450; Testing Loss 0.0058765545555750625; Training Loss 0.004778478085621321\n",
      "Episode 1451; Testing Loss 0.005876560231165504; Training Loss 0.004778468178018026\n",
      "Episode 1452; Testing Loss 0.005876539871368845; Training Loss 0.0047784572032604005\n",
      "Episode 1453; Testing Loss 0.005876553821139465; Training Loss 0.004778444968453056\n",
      "Episode 1454; Testing Loss 0.005876562509521066; Training Loss 0.004778434428555822\n",
      "Episode 1455; Testing Loss 0.005876527041020035; Training Loss 0.004778425215706102\n",
      "Episode 1456; Testing Loss 0.005876506271336172; Training Loss 0.004778414951698148\n",
      "Episode 1457; Testing Loss 0.005876576357942742; Training Loss 0.004778401671049957\n",
      "Episode 1458; Testing Loss 0.005876658272758651; Training Loss 0.004778392480514972\n",
      "Episode 1459; Testing Loss 0.005876631035125908; Training Loss 0.004778383026646054\n",
      "Episode 1460; Testing Loss 0.005876558839196653; Training Loss 0.004778370191325632\n",
      "Episode 1461; Testing Loss 0.005876535218865703; Training Loss 0.004778359143031871\n",
      "Episode 1462; Testing Loss 0.005876518079277819; Training Loss 0.004778351777850842\n",
      "Episode 1463; Testing Loss 0.005876477566418299; Training Loss 0.004778340587138327\n",
      "Episode 1464; Testing Loss 0.005876466232666909; Training Loss 0.004778327576073127\n",
      "Episode 1465; Testing Loss 0.005876513131542294; Training Loss 0.004778317395731247\n",
      "Episode 1466; Testing Loss 0.00587659057094964; Training Loss 0.004778309608603622\n",
      "Episode 1467; Testing Loss 0.005876532750138438; Training Loss 0.004778296791109322\n",
      "Episode 1468; Testing Loss 0.005876413148872907; Training Loss 0.004778285659157549\n",
      "Episode 1469; Testing Loss 0.005876370234136024; Training Loss 0.0047782761946201\n",
      "Episode 1470; Testing Loss 0.005876447128949092; Training Loss 0.00477826344665212\n",
      "Episode 1471; Testing Loss 0.0058764946271683965; Training Loss 0.0047782532084841385\n",
      "Episode 1472; Testing Loss 0.005876426337449222; Training Loss 0.0047782435462042\n",
      "Episode 1473; Testing Loss 0.005876398831929267; Training Loss 0.004778233299262863\n",
      "Episode 1474; Testing Loss 0.005876477599499801; Training Loss 0.0047782199920518905\n",
      "Episode 1475; Testing Loss 0.005876523467109711; Training Loss 0.004778210430983192\n",
      "Episode 1476; Testing Loss 0.005876397478341845; Training Loss 0.004778200570971421\n",
      "Episode 1477; Testing Loss 0.005876276383210936; Training Loss 0.004778190215891122\n",
      "Episode 1478; Testing Loss 0.00587633362136282; Training Loss 0.004778175780339965\n",
      "Episode 1479; Testing Loss 0.00587643712357716; Training Loss 0.004778166620032427\n",
      "Episode 1480; Testing Loss 0.005876430256957503; Training Loss 0.004778158530507174\n",
      "Episode 1481; Testing Loss 0.005876322884598509; Training Loss 0.004778147005059835\n",
      "Episode 1482; Testing Loss 0.005876307013843754; Training Loss 0.004778133776219696\n",
      "Episode 1483; Testing Loss 0.005876361885344395; Training Loss 0.0047781227051780254\n",
      "Episode 1484; Testing Loss 0.0058764068015832215; Training Loss 0.004778113195004691\n",
      "Episode 1485; Testing Loss 0.005876358430579132; Training Loss 0.004778101134679191\n",
      "Episode 1486; Testing Loss 0.00587629960253019; Training Loss 0.004778093138402619\n",
      "Episode 1487; Testing Loss 0.0058763285786027014; Training Loss 0.004778081492704859\n",
      "Episode 1488; Testing Loss 0.0058763507990690425; Training Loss 0.0047780678637276116\n",
      "Episode 1489; Testing Loss 0.0058762743452470435; Training Loss 0.004778060551093892\n",
      "Episode 1490; Testing Loss 0.005876201775037814; Training Loss 0.004778050549604448\n",
      "Episode 1491; Testing Loss 0.005876200477556771; Training Loss 0.004778038046710053\n",
      "Episode 1492; Testing Loss 0.0058762267135378955; Training Loss 0.004778025581222789\n",
      "Episode 1493; Testing Loss 0.005876244479382429; Training Loss 0.004778017372842231\n",
      "Episode 1494; Testing Loss 0.005876244716823612; Training Loss 0.00477800805493577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1495; Testing Loss 0.005876237341569071; Training Loss 0.004777994698975438\n",
      "Episode 1496; Testing Loss 0.005876215813750822; Training Loss 0.004777983059652947\n",
      "Episode 1497; Testing Loss 0.00587616454241222; Training Loss 0.004777974511992385\n",
      "Episode 1498; Testing Loss 0.00587613359251731; Training Loss 0.004777965387036993\n",
      "Episode 1499; Testing Loss 0.005876188447660743; Training Loss 0.004777950694917652\n",
      "Episode 1500; Testing Loss 0.005876239439824808; Training Loss 0.004777939888629973\n",
      "Episode 1501; Testing Loss 0.00587620951264891; Training Loss 0.004777931347920099\n",
      "Episode 1502; Testing Loss 0.005876151200537724; Training Loss 0.004777920059989691\n",
      "Episode 1503; Testing Loss 0.005876154279106173; Training Loss 0.004777907706242929\n",
      "Episode 1504; Testing Loss 0.005876166521631987; Training Loss 0.004777896222270585\n",
      "Episode 1505; Testing Loss 0.005876142246805056; Training Loss 0.004777886502946698\n",
      "Episode 1506; Testing Loss 0.005876099868431369; Training Loss 0.004777874905147879\n",
      "Episode 1507; Testing Loss 0.005876132197073604; Training Loss 0.004777865697343485\n",
      "Episode 1508; Testing Loss 0.005876194855874761; Training Loss 0.0047778569421207395\n",
      "Episode 1509; Testing Loss 0.005876174149661386; Training Loss 0.004777845778912739\n",
      "Episode 1510; Testing Loss 0.005876088831684145; Training Loss 0.0047778328750454355\n",
      "Episode 1511; Testing Loss 0.005876021335570942; Training Loss 0.004777822809925565\n",
      "Episode 1512; Testing Loss 0.005876038774137138; Training Loss 0.004777809740435056\n",
      "Episode 1513; Testing Loss 0.005876093101170506; Training Loss 0.004777799602068288\n",
      "Episode 1514; Testing Loss 0.00587604411347396; Training Loss 0.004777791646145891\n",
      "Episode 1515; Testing Loss 0.005876021531176107; Training Loss 0.00477778165342565\n",
      "Episode 1516; Testing Loss 0.0058760701520303; Training Loss 0.004777767138179451\n",
      "Episode 1517; Testing Loss 0.00587612363805101; Training Loss 0.004777756398617961\n",
      "Episode 1518; Testing Loss 0.005876019404978175; Training Loss 0.004777746117406233\n",
      "Episode 1519; Testing Loss 0.005875921130626841; Training Loss 0.004777735111999519\n",
      "Episode 1520; Testing Loss 0.005875935244452526; Training Loss 0.004777721993806055\n",
      "Episode 1521; Testing Loss 0.005875986199133543; Training Loss 0.004777714907702628\n",
      "Episode 1522; Testing Loss 0.005876002780061855; Training Loss 0.0047777048035844635\n",
      "Episode 1523; Testing Loss 0.005875990342559981; Training Loss 0.004777692501793723\n",
      "Episode 1524; Testing Loss 0.0058759768150169565; Training Loss 0.004777679041860542\n",
      "Episode 1525; Testing Loss 0.005875909118531369; Training Loss 0.004777670351910948\n",
      "Episode 1526; Testing Loss 0.005875811642516761; Training Loss 0.004777658693211979\n",
      "Episode 1527; Testing Loss 0.005875792348719873; Training Loss 0.004777649397611625\n",
      "Episode 1528; Testing Loss 0.005875903474404232; Training Loss 0.004777638312672388\n",
      "Episode 1529; Testing Loss 0.005875956861150115; Training Loss 0.004777628427447832\n",
      "Episode 1530; Testing Loss 0.005875846638235851; Training Loss 0.004777615560942987\n",
      "Episode 1531; Testing Loss 0.005875778669443609; Training Loss 0.004777605265973418\n",
      "Episode 1532; Testing Loss 0.005875840493026074; Training Loss 0.004777597450582552\n",
      "Episode 1533; Testing Loss 0.005875919936569346; Training Loss 0.004777586343492218\n",
      "Episode 1534; Testing Loss 0.005875901190769944; Training Loss 0.004777572829617597\n",
      "Episode 1535; Testing Loss 0.005875840291792054; Training Loss 0.004777564609592968\n",
      "Episode 1536; Testing Loss 0.005875820128994607; Training Loss 0.004777555582293408\n",
      "Episode 1537; Testing Loss 0.005875812766879053; Training Loss 0.004777544586535706\n",
      "Episode 1538; Testing Loss 0.005875770463335133; Training Loss 0.004777530582363398\n",
      "Episode 1539; Testing Loss 0.005875711066100959; Training Loss 0.004777519713595067\n",
      "Episode 1540; Testing Loss 0.005875704937023433; Training Loss 0.0047775111764805445\n",
      "Episode 1541; Testing Loss 0.005875764473138064; Training Loss 0.004777498103544659\n",
      "Episode 1542; Testing Loss 0.00587580715633044; Training Loss 0.004777488256780694\n",
      "Episode 1543; Testing Loss 0.00587576764522075; Training Loss 0.00477747980418435\n",
      "Episode 1544; Testing Loss 0.005875678573093987; Training Loss 0.004777468670875115\n",
      "Episode 1545; Testing Loss 0.005875653594863677; Training Loss 0.004777455687391174\n",
      "Episode 1546; Testing Loss 0.005875695768211803; Training Loss 0.00477744818699449\n",
      "Episode 1547; Testing Loss 0.005875697540272943; Training Loss 0.004777438189517051\n",
      "Episode 1548; Testing Loss 0.005875638958878213; Training Loss 0.004777424844893637\n",
      "Episode 1549; Testing Loss 0.005875624738015397; Training Loss 0.004777412135399724\n",
      "Episode 1550; Testing Loss 0.00587563649463191; Training Loss 0.00477740280558933\n",
      "Episode 1551; Testing Loss 0.0058756562886548395; Training Loss 0.00477739196914313\n",
      "Episode 1552; Testing Loss 0.005875597241929997; Training Loss 0.0047773793371370535\n",
      "Episode 1553; Testing Loss 0.005875552690900363; Training Loss 0.004777371669441422\n",
      "Episode 1554; Testing Loss 0.005875570616771693; Training Loss 0.004777362197756623\n",
      "Episode 1555; Testing Loss 0.005875651726651856; Training Loss 0.00477735024929914\n",
      "Episode 1556; Testing Loss 0.005875641926047204; Training Loss 0.004777337290472794\n",
      "Episode 1557; Testing Loss 0.0058755102126547215; Training Loss 0.004777327776981786\n",
      "Episode 1558; Testing Loss 0.00587550197593459; Training Loss 0.004777316403684527\n",
      "Episode 1559; Testing Loss 0.005875576890452326; Training Loss 0.004777304956584748\n",
      "Episode 1560; Testing Loss 0.0058755557724555684; Training Loss 0.004777296623603635\n",
      "Episode 1561; Testing Loss 0.0058754755100693164; Training Loss 0.004777286693706976\n",
      "Episode 1562; Testing Loss 0.005875498218214066; Training Loss 0.0047772747068975585\n",
      "Episode 1563; Testing Loss 0.005875592501321782; Training Loss 0.004777262015255393\n",
      "Episode 1564; Testing Loss 0.005875516994455062; Training Loss 0.004777252785207762\n",
      "Episode 1565; Testing Loss 0.005875337283643194; Training Loss 0.004777241768567016\n",
      "Episode 1566; Testing Loss 0.005875324993271743; Training Loss 0.004777229160778944\n",
      "Episode 1567; Testing Loss 0.005875487725051379; Training Loss 0.004777218192253095\n",
      "Episode 1568; Testing Loss 0.0058754967066397875; Training Loss 0.004777208747001214\n",
      "Episode 1569; Testing Loss 0.005875327470816979; Training Loss 0.004777195546207436\n",
      "Episode 1570; Testing Loss 0.005875218444540313; Training Loss 0.004777187629669649\n",
      "Episode 1571; Testing Loss 0.005875319945905266; Training Loss 0.0047771758628571935\n",
      "Episode 1572; Testing Loss 0.005875423841632298; Training Loss 0.004777165534528867\n",
      "Episode 1573; Testing Loss 0.005875411906489183; Training Loss 0.004777152140078127\n",
      "Episode 1574; Testing Loss 0.005875300699994286; Training Loss 0.004777143152827348\n",
      "Episode 1575; Testing Loss 0.005875268525663059; Training Loss 0.004777135378938029\n",
      "Episode 1576; Testing Loss 0.005875263921978616; Training Loss 0.004777123126311733\n",
      "Episode 1577; Testing Loss 0.005875275356514525; Training Loss 0.004777107384280729\n",
      "Episode 1578; Testing Loss 0.005875242408813942; Training Loss 0.004777099223692871\n",
      "Episode 1579; Testing Loss 0.0058752421611121194; Training Loss 0.004777089478156218\n",
      "Episode 1580; Testing Loss 0.0058753024296302105; Training Loss 0.004777076891388742\n",
      "Episode 1581; Testing Loss 0.005875307250597358; Training Loss 0.004777065316284664\n",
      "Episode 1582; Testing Loss 0.005875227318742885; Training Loss 0.004777053837674495\n",
      "Episode 1583; Testing Loss 0.005875183379676109; Training Loss 0.004777042541430166\n",
      "Episode 1584; Testing Loss 0.005875128624100412; Training Loss 0.0047770330842071525\n",
      "Episode 1585; Testing Loss 0.005875131844902409; Training Loss 0.004777021101119836\n",
      "Episode 1586; Testing Loss 0.005875128938245626; Training Loss 0.004777009134077397\n",
      "Episode 1587; Testing Loss 0.005875164804459587; Training Loss 0.004776999733330699\n",
      "Episode 1588; Testing Loss 0.005875187078361; Training Loss 0.004776988014113773\n",
      "Episode 1589; Testing Loss 0.005875122503038018; Training Loss 0.004776977469980801\n",
      "Episode 1590; Testing Loss 0.005875019846550181; Training Loss 0.00477696699456495\n",
      "Episode 1591; Testing Loss 0.0058749894577527426; Training Loss 0.004776955253351944\n",
      "Episode 1592; Testing Loss 0.0058750113460123935; Training Loss 0.004776943901678242\n",
      "Episode 1593; Testing Loss 0.005875072196907051; Training Loss 0.004776933358435569\n",
      "Episode 1594; Testing Loss 0.005875049507874005; Training Loss 0.004776921620308442\n",
      "Episode 1595; Testing Loss 0.005874957711078041; Training Loss 0.004776911872935643\n",
      "Episode 1596; Testing Loss 0.00587491885248914; Training Loss 0.004776900267856337\n",
      "Episode 1597; Testing Loss 0.00587501291900189; Training Loss 0.004776890124659075\n",
      "Episode 1598; Testing Loss 0.005875118184877982; Training Loss 0.004776880912779381\n",
      "Episode 1599; Testing Loss 0.0058750469351237095; Training Loss 0.0047768668215752005\n",
      "Episode 1600; Testing Loss 0.005874921184905947; Training Loss 0.004776856705303298\n",
      "Episode 1601; Testing Loss 0.005874838605355091; Training Loss 0.004776847688301664\n",
      "Episode 1602; Testing Loss 0.00587492027708643; Training Loss 0.004776836215313001\n",
      "Episode 1603; Testing Loss 0.005874973425324098; Training Loss 0.004776823330749469\n",
      "Episode 1604; Testing Loss 0.005874914918808108; Training Loss 0.004776812823953608\n",
      "Episode 1605; Testing Loss 0.0058749339524487645; Training Loss 0.00477680287668311\n",
      "Episode 1606; Testing Loss 0.005874962263266202; Training Loss 0.004776790644890762\n",
      "Episode 1607; Testing Loss 0.00587492102181997; Training Loss 0.004776779909620062\n",
      "Episode 1608; Testing Loss 0.005874829315166535; Training Loss 0.00477676942311627\n",
      "Episode 1609; Testing Loss 0.005874749374372718; Training Loss 0.004776758849999146\n",
      "Episode 1610; Testing Loss 0.0058747666280229306; Training Loss 0.004776746003942002\n",
      "Episode 1611; Testing Loss 0.005874849612318354; Training Loss 0.004776735871915108\n",
      "Episode 1612; Testing Loss 0.0058748967971575606; Training Loss 0.004776725461596295\n",
      "Episode 1613; Testing Loss 0.005874841575996375; Training Loss 0.004776713397414\n",
      "Episode 1614; Testing Loss 0.0058747216371154865; Training Loss 0.004776703275134382\n",
      "Episode 1615; Testing Loss 0.005874700403726672; Training Loss 0.004776693894331979\n",
      "Episode 1616; Testing Loss 0.005874771493806596; Training Loss 0.004776683180885386\n",
      "Episode 1617; Testing Loss 0.00587477361164685; Training Loss 0.004776670628257978\n",
      "Episode 1618; Testing Loss 0.005874673504031049; Training Loss 0.004776659558441421\n",
      "Episode 1619; Testing Loss 0.005874601319177486; Training Loss 0.004776649258474986\n",
      "Episode 1620; Testing Loss 0.0058746924594667215; Training Loss 0.004776636625822553\n",
      "Episode 1621; Testing Loss 0.005874737271813706; Training Loss 0.00477662685210364\n",
      "Episode 1622; Testing Loss 0.0058746646644255215; Training Loss 0.0047766178129329765\n",
      "Episode 1623; Testing Loss 0.005874616536327083; Training Loss 0.004776606504870503\n",
      "Episode 1624; Testing Loss 0.005874615474111364; Training Loss 0.00477659436885038\n",
      "Episode 1625; Testing Loss 0.005874631924733775; Training Loss 0.004776584258782068\n",
      "Episode 1626; Testing Loss 0.005874629951978823; Training Loss 0.0047765729956134205\n",
      "Episode 1627; Testing Loss 0.005874582318510651; Training Loss 0.004776561810476211\n",
      "Episode 1628; Testing Loss 0.0058745084332190904; Training Loss 0.004776551107189818\n",
      "Episode 1629; Testing Loss 0.005874497991449818; Training Loss 0.004776540440294688\n",
      "Episode 1630; Testing Loss 0.005874521900852703; Training Loss 0.00477652833829131\n",
      "Episode 1631; Testing Loss 0.005874574659609072; Training Loss 0.004776516219544202\n",
      "Episode 1632; Testing Loss 0.005874566912849468; Training Loss 0.004776506846737592\n",
      "Episode 1633; Testing Loss 0.005874537844697747; Training Loss 0.004776496478969769\n",
      "Episode 1634; Testing Loss 0.005874534739522949; Training Loss 0.004776483605009549\n",
      "Episode 1635; Testing Loss 0.005874491482467365; Training Loss 0.004776472151378369\n",
      "Episode 1636; Testing Loss 0.005874430084115945; Training Loss 0.0047764622845568915\n",
      "Episode 1637; Testing Loss 0.0058744673287026055; Training Loss 0.004776450874244359\n",
      "Episode 1638; Testing Loss 0.005874501585836283; Training Loss 0.00477643812062879\n",
      "Episode 1639; Testing Loss 0.005874469584840011; Training Loss 0.0047764303374280784\n",
      "Episode 1640; Testing Loss 0.005874414926233642; Training Loss 0.00477641934228939\n",
      "Episode 1641; Testing Loss 0.005874395677437619; Training Loss 0.004776407377810489\n",
      "Episode 1642; Testing Loss 0.0058744042044437655; Training Loss 0.004776396480255714\n",
      "Episode 1643; Testing Loss 0.0058743318091261315; Training Loss 0.004776386405781216\n",
      "Episode 1644; Testing Loss 0.005874261764387841; Training Loss 0.0047763746232177525\n",
      "Episode 1645; Testing Loss 0.005874288302218458; Training Loss 0.004776363557570116\n",
      "Episode 1646; Testing Loss 0.0058743275104521184; Training Loss 0.004776354487273083\n",
      "Episode 1647; Testing Loss 0.005874338416231391; Training Loss 0.004776342651373021\n",
      "Episode 1648; Testing Loss 0.005874329716514558; Training Loss 0.004776330106977711\n",
      "Episode 1649; Testing Loss 0.005874374941856409; Training Loss 0.004776320987031979\n",
      "Episode 1650; Testing Loss 0.0058743604459104625; Training Loss 0.004776310640579132\n",
      "Episode 1651; Testing Loss 0.005874275508372631; Training Loss 0.004776298643204361\n",
      "Episode 1652; Testing Loss 0.00587420066556501; Training Loss 0.00477628567821512\n",
      "Episode 1653; Testing Loss 0.005874190005038535; Training Loss 0.004776276320080113\n",
      "Episode 1654; Testing Loss 0.005874262917768699; Training Loss 0.004776266596492515\n",
      "Episode 1655; Testing Loss 0.005874328767801875; Training Loss 0.00477625490150194\n",
      "Episode 1656; Testing Loss 0.005874279538492819; Training Loss 0.004776243500778849\n",
      "Episode 1657; Testing Loss 0.005874167353672134; Training Loss 0.00477623255177986\n",
      "Episode 1658; Testing Loss 0.005874108036861386; Training Loss 0.004776220870626199\n",
      "Episode 1659; Testing Loss 0.005874167985882664; Training Loss 0.004776208947321958\n",
      "Episode 1660; Testing Loss 0.005874154097415621; Training Loss 0.0047762007535531534\n",
      "Episode 1661; Testing Loss 0.005874131525979256; Training Loss 0.004776190276958018\n",
      "Episode 1662; Testing Loss 0.0058741231638094245; Training Loss 0.0047761774488346955\n",
      "Episode 1663; Testing Loss 0.005874102403100911; Training Loss 0.004776165875404689\n",
      "Episode 1664; Testing Loss 0.0058740660671596145; Training Loss 0.004776157282858631\n",
      "Episode 1665; Testing Loss 0.005874114488478853; Training Loss 0.004776146826000428\n",
      "Episode 1666; Testing Loss 0.005874175981721559; Training Loss 0.004776134080529762\n",
      "Episode 1667; Testing Loss 0.005874132345131579; Training Loss 0.004776121778682333\n",
      "Episode 1668; Testing Loss 0.00587401692672445; Training Loss 0.004776113349330137\n",
      "Episode 1669; Testing Loss 0.005873966796235637; Training Loss 0.004776101420866058\n",
      "Episode 1670; Testing Loss 0.00587399047165302; Training Loss 0.004776089511769299\n",
      "Episode 1671; Testing Loss 0.00587400098472186; Training Loss 0.0047760811938614214\n",
      "Episode 1672; Testing Loss 0.005874027492090769; Training Loss 0.004776070297380882\n",
      "Episode 1673; Testing Loss 0.005874041141884499; Training Loss 0.004776058269680402\n",
      "Episode 1674; Testing Loss 0.00587402268645199; Training Loss 0.004776048378064931\n",
      "Episode 1675; Testing Loss 0.005874003283754341; Training Loss 0.00477603760097\n",
      "Episode 1676; Testing Loss 0.0058739908818616175; Training Loss 0.004776025327589018\n",
      "Episode 1677; Testing Loss 0.0058739507525357145; Training Loss 0.004776013598393449\n",
      "Episode 1678; Testing Loss 0.005873859123917576; Training Loss 0.004776003089603983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1679; Testing Loss 0.005873855279867498; Training Loss 0.004775992221627999\n",
      "Episode 1680; Testing Loss 0.005873911430832023; Training Loss 0.004775980215144844\n",
      "Episode 1681; Testing Loss 0.005873877432217975; Training Loss 0.004775970628619917\n",
      "Episode 1682; Testing Loss 0.005873858383271839; Training Loss 0.004775961969720023\n",
      "Episode 1683; Testing Loss 0.005873928383044373; Training Loss 0.004775949546117981\n",
      "Episode 1684; Testing Loss 0.005873929154833097; Training Loss 0.00477593756549765\n",
      "Episode 1685; Testing Loss 0.005873816761205483; Training Loss 0.004775926949753289\n",
      "Episode 1686; Testing Loss 0.005873760380548704; Training Loss 0.004775916064796052\n",
      "Episode 1687; Testing Loss 0.005873778922823187; Training Loss 0.00477590369791827\n",
      "Episode 1688; Testing Loss 0.005873815508133596; Training Loss 0.004775894033180257\n",
      "Episode 1689; Testing Loss 0.005873826020097176; Training Loss 0.004775884503915496\n",
      "Episode 1690; Testing Loss 0.005873759524912075; Training Loss 0.004775871810458617\n",
      "Episode 1691; Testing Loss 0.005873727603105072; Training Loss 0.00477586068950822\n",
      "Episode 1692; Testing Loss 0.005873706000044463; Training Loss 0.004775851675333564\n",
      "Episode 1693; Testing Loss 0.005873730959424224; Training Loss 0.0047758390557375106\n",
      "Episode 1694; Testing Loss 0.005873752606086078; Training Loss 0.0047758270075953955\n",
      "Episode 1695; Testing Loss 0.005873708065409893; Training Loss 0.0047758178261182675\n",
      "Episode 1696; Testing Loss 0.0058736536479088875; Training Loss 0.0047758087691662085\n",
      "Episode 1697; Testing Loss 0.005873702468245307; Training Loss 0.004775796298343279\n",
      "Episode 1698; Testing Loss 0.005873699508751566; Training Loss 0.004775785626582257\n",
      "Episode 1699; Testing Loss 0.005873560770576793; Training Loss 0.004775774779423721\n",
      "Episode 1700; Testing Loss 0.0058735080453331005; Training Loss 0.0047757640137084485\n",
      "Episode 1701; Testing Loss 0.00587360529519793; Training Loss 0.004775752493482501\n",
      "Episode 1702; Testing Loss 0.00587366094942558; Training Loss 0.004775743742996562\n",
      "Episode 1703; Testing Loss 0.005873586046892034; Training Loss 0.004775732959443959\n",
      "Episode 1704; Testing Loss 0.00587354329467703; Training Loss 0.004775721213892508\n",
      "Episode 1705; Testing Loss 0.005873599799913934; Training Loss 0.004775709806166363\n",
      "Episode 1706; Testing Loss 0.005873614389945098; Training Loss 0.004775699229900264\n",
      "Episode 1707; Testing Loss 0.0058735109323367765; Training Loss 0.004775687270838958\n",
      "Episode 1708; Testing Loss 0.0058734531383377595; Training Loss 0.0047756756046186495\n",
      "Episode 1709; Testing Loss 0.005873505222725759; Training Loss 0.004775667182670772\n",
      "Episode 1710; Testing Loss 0.005873610024159318; Training Loss 0.00477565744688194\n",
      "Episode 1711; Testing Loss 0.005873586867857718; Training Loss 0.004775645200109613\n",
      "Episode 1712; Testing Loss 0.005873394449244435; Training Loss 0.004775633033283527\n",
      "Episode 1713; Testing Loss 0.005873330018878844; Training Loss 0.004775624145797564\n",
      "Episode 1714; Testing Loss 0.00587345957112304; Training Loss 0.004775612787099949\n",
      "Episode 1715; Testing Loss 0.005873536745818522; Training Loss 0.004775601077797244\n",
      "Episode 1716; Testing Loss 0.005873432056291772; Training Loss 0.004775591348424461\n",
      "Episode 1717; Testing Loss 0.005873386029773362; Training Loss 0.004775582221132572\n",
      "Episode 1718; Testing Loss 0.005873457186615367; Training Loss 0.004775568494277719\n",
      "Episode 1719; Testing Loss 0.005873505612594194; Training Loss 0.0047755575412789224\n",
      "Episode 1720; Testing Loss 0.005873426742651169; Training Loss 0.004775547188449975\n",
      "Episode 1721; Testing Loss 0.005873349679344065; Training Loss 0.0047755353305851626\n",
      "Episode 1722; Testing Loss 0.00587332474660067; Training Loss 0.004775524413474185\n",
      "Episode 1723; Testing Loss 0.005873314096242203; Training Loss 0.004775516913699323\n",
      "Episode 1724; Testing Loss 0.005873341606169515; Training Loss 0.0047755067377896025\n",
      "Episode 1725; Testing Loss 0.005873374540748587; Training Loss 0.004775493850307932\n",
      "Episode 1726; Testing Loss 0.005873366628825397; Training Loss 0.004775480769798326\n",
      "Episode 1727; Testing Loss 0.005873313460516297; Training Loss 0.004775471892742276\n",
      "Episode 1728; Testing Loss 0.0058732802941445315; Training Loss 0.00477546191763749\n",
      "Episode 1729; Testing Loss 0.0058733468530473825; Training Loss 0.0047754481401978705\n",
      "Episode 1730; Testing Loss 0.005873401867026285; Training Loss 0.004775440585887017\n",
      "Episode 1731; Testing Loss 0.005873331501561506; Training Loss 0.004775430797828816\n",
      "Episode 1732; Testing Loss 0.0058732189526020965; Training Loss 0.004775418085099799\n",
      "Episode 1733; Testing Loss 0.005873211023071227; Training Loss 0.0047754066089779985\n",
      "Episode 1734; Testing Loss 0.005873216151053306; Training Loss 0.00477539646674708\n",
      "Episode 1735; Testing Loss 0.005873213461308912; Training Loss 0.004775384236691134\n",
      "Episode 1736; Testing Loss 0.005873226175895144; Training Loss 0.0047753730094469565\n",
      "Episode 1737; Testing Loss 0.0058732918299102274; Training Loss 0.004775362803038114\n",
      "Episode 1738; Testing Loss 0.005873266949278142; Training Loss 0.004775354079475037\n",
      "Episode 1739; Testing Loss 0.005873179218114241; Training Loss 0.004775341844966874\n",
      "Episode 1740; Testing Loss 0.005873119415638089; Training Loss 0.004775329952388331\n",
      "Episode 1741; Testing Loss 0.005873131926080349; Training Loss 0.004775321621978189\n",
      "Episode 1742; Testing Loss 0.0058731193753339345; Training Loss 0.004775310710236945\n",
      "Episode 1743; Testing Loss 0.005873090186493488; Training Loss 0.004775298253892121\n",
      "Episode 1744; Testing Loss 0.005873114429120275; Training Loss 0.0047752862103661925\n",
      "Episode 1745; Testing Loss 0.005873165361906083; Training Loss 0.004775276064133012\n",
      "Episode 1746; Testing Loss 0.005873152739675602; Training Loss 0.0047752659486839525\n",
      "Episode 1747; Testing Loss 0.0058730835658547245; Training Loss 0.004775253858280549\n",
      "Episode 1748; Testing Loss 0.005873096090011296; Training Loss 0.004775242805705257\n",
      "Episode 1749; Testing Loss 0.005873084513619121; Training Loss 0.004775231643610317\n",
      "Episode 1750; Testing Loss 0.0058730361231653545; Training Loss 0.004775220995160052\n",
      "Episode 1751; Testing Loss 0.0058729809373352215; Training Loss 0.004775209424622121\n",
      "Episode 1752; Testing Loss 0.005873002342485675; Training Loss 0.00477519875950994\n",
      "Episode 1753; Testing Loss 0.005873091723046722; Training Loss 0.004775188347515767\n",
      "Episode 1754; Testing Loss 0.005873128605827007; Training Loss 0.0047751769853717815\n",
      "Episode 1755; Testing Loss 0.0058730194557091156; Training Loss 0.004775165665673103\n",
      "Episode 1756; Testing Loss 0.00587285855001245; Training Loss 0.004775154299022249\n",
      "Episode 1757; Testing Loss 0.005872895206651867; Training Loss 0.00477514452661973\n",
      "Episode 1758; Testing Loss 0.005873110943660684; Training Loss 0.0047751320413659615\n",
      "Episode 1759; Testing Loss 0.005873194425527571; Training Loss 0.004775117970120936\n",
      "Episode 1760; Testing Loss 0.005873065114504792; Training Loss 0.0047751074373343855\n",
      "Episode 1761; Testing Loss 0.0058730277270720955; Training Loss 0.004775098393256569\n",
      "Episode 1762; Testing Loss 0.005873215968974387; Training Loss 0.004775081541294938\n",
      "Episode 1763; Testing Loss 0.005873315081560955; Training Loss 0.004775068273762225\n",
      "Episode 1764; Testing Loss 0.005873239904988964; Training Loss 0.0047750515757054674\n",
      "Episode 1765; Testing Loss 0.005873214635709924; Training Loss 0.0047750437703663814\n",
      "Episode 1766; Testing Loss 0.005873386309183431; Training Loss 0.004775032224292918\n",
      "Episode 1767; Testing Loss 0.0058735935386666155; Training Loss 0.004775019092109514\n",
      "Episode 1768; Testing Loss 0.0058735925515323245; Training Loss 0.004775001261152938\n",
      "Episode 1769; Testing Loss 0.005873447354605258; Training Loss 0.004774989692760954\n",
      "Episode 1770; Testing Loss 0.005873394869516988; Training Loss 0.004774981035613344\n",
      "Episode 1771; Testing Loss 0.005873566368688667; Training Loss 0.004774967473124853\n",
      "Episode 1772; Testing Loss 0.005873753574316229; Training Loss 0.004774954944098058\n",
      "Episode 1773; Testing Loss 0.005873757435984052; Training Loss 0.0047749396190207025\n",
      "Episode 1774; Testing Loss 0.005873713356245367; Training Loss 0.0047749232077610075\n",
      "Episode 1775; Testing Loss 0.005873754140529714; Training Loss 0.00477491331942754\n",
      "Episode 1776; Testing Loss 0.005873918498081041; Training Loss 0.004774901187292536\n",
      "Episode 1777; Testing Loss 0.00587402043333664; Training Loss 0.0047748852300417794\n",
      "Episode 1778; Testing Loss 0.005873989793550666; Training Loss 0.004774867605846044\n",
      "Episode 1779; Testing Loss 0.00587392417316134; Training Loss 0.0047748564018948725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1780; Testing Loss 0.0058739424985917435; Training Loss 0.004774846284779424\n",
      "Episode 1781; Testing Loss 0.005874014075300499; Training Loss 0.004774834389224334\n",
      "Episode 1782; Testing Loss 0.005874086757688613; Training Loss 0.004774820705504518\n",
      "Episode 1783; Testing Loss 0.005874137133742158; Training Loss 0.004774805913193193\n",
      "Episode 1784; Testing Loss 0.00587419789425596; Training Loss 0.004774789810528728\n",
      "Episode 1785; Testing Loss 0.005874243643317207; Training Loss 0.0047747779980374504\n",
      "Episode 1786; Testing Loss 0.0058743109521110625; Training Loss 0.0047747668852269306\n",
      "Episode 1787; Testing Loss 0.005874376122475151; Training Loss 0.004774752301916918\n",
      "Episode 1788; Testing Loss 0.005874421595132985; Training Loss 0.004774738374862259\n",
      "Episode 1789; Testing Loss 0.005874478950519516; Training Loss 0.004774727489487013\n",
      "Episode 1790; Testing Loss 0.005874467579397771; Training Loss 0.0047747150878097245\n",
      "Episode 1791; Testing Loss 0.005874368689763618; Training Loss 0.004774702602524045\n",
      "Episode 1792; Testing Loss 0.0058743527969910515; Training Loss 0.004774688203031661\n",
      "Episode 1793; Testing Loss 0.005874449993142122; Training Loss 0.004774671687111603\n",
      "Episode 1794; Testing Loss 0.005874533306161995; Training Loss 0.004774654541799877\n",
      "Episode 1795; Testing Loss 0.005874550393601911; Training Loss 0.004774644501690869\n",
      "Episode 1796; Testing Loss 0.005874618638272524; Training Loss 0.004774630557726127\n",
      "Episode 1797; Testing Loss 0.005874695487251426; Training Loss 0.004774613803377124\n",
      "Episode 1798; Testing Loss 0.005874729161910048; Training Loss 0.004774595086869295\n",
      "Episode 1799; Testing Loss 0.005874724712239906; Training Loss 0.0047745793195488285\n",
      "Episode 1800; Testing Loss 0.005874720367491084; Training Loss 0.004774565977436625\n",
      "Episode 1801; Testing Loss 0.005874757690759048; Training Loss 0.004774551181860748\n",
      "Episode 1802; Testing Loss 0.005874812464847885; Training Loss 0.004774534753541931\n",
      "Episode 1803; Testing Loss 0.0058748858765889835; Training Loss 0.004774519598769556\n",
      "Episode 1804; Testing Loss 0.005875007238351029; Training Loss 0.004774502392190248\n",
      "Episode 1805; Testing Loss 0.0058750979636666345; Training Loss 0.004774488608718468\n",
      "Episode 1806; Testing Loss 0.0058751181564051696; Training Loss 0.004774474710492288\n",
      "Episode 1807; Testing Loss 0.005875175671069801; Training Loss 0.004774458026171166\n",
      "Episode 1808; Testing Loss 0.0058752940933327405; Training Loss 0.004774440481833741\n",
      "Episode 1809; Testing Loss 0.005875366211309448; Training Loss 0.004774428519116852\n",
      "Episode 1810; Testing Loss 0.005875459954851375; Training Loss 0.004774413699661577\n",
      "Episode 1811; Testing Loss 0.0058755039055763715; Training Loss 0.004774398385233057\n",
      "Episode 1812; Testing Loss 0.005875447701828099; Training Loss 0.004774385580738809\n",
      "Episode 1813; Testing Loss 0.005875413194819089; Training Loss 0.004774372593565463\n",
      "Episode 1814; Testing Loss 0.0058754661706335734; Training Loss 0.00477435628869186\n",
      "Episode 1815; Testing Loss 0.0058755096641137155; Training Loss 0.004774342175503791\n",
      "Episode 1816; Testing Loss 0.005875485881512993; Training Loss 0.004774328107782501\n",
      "Episode 1817; Testing Loss 0.005875535363650383; Training Loss 0.004774314912016293\n",
      "Episode 1818; Testing Loss 0.0058755978558115984; Training Loss 0.004774299260240708\n",
      "Episode 1819; Testing Loss 0.005875603834878633; Training Loss 0.004774283148584785\n",
      "Episode 1820; Testing Loss 0.0058756261654559815; Training Loss 0.004774267444071848\n",
      "Episode 1821; Testing Loss 0.005875686380242025; Training Loss 0.004774252711947466\n",
      "Episode 1822; Testing Loss 0.005875780507283706; Training Loss 0.0047742381605865155\n",
      "Episode 1823; Testing Loss 0.005875828147964983; Training Loss 0.004774222550135466\n",
      "Episode 1824; Testing Loss 0.005875768444683354; Training Loss 0.004774208901526382\n",
      "Episode 1825; Testing Loss 0.0058757334239876915; Training Loss 0.00477419467628965\n",
      "Episode 1826; Testing Loss 0.005875794103991964; Training Loss 0.004774178693584746\n",
      "Episode 1827; Testing Loss 0.005875870713169748; Training Loss 0.0047741640594251965\n",
      "Episode 1828; Testing Loss 0.005875897773140216; Training Loss 0.00477415050065731\n",
      "Episode 1829; Testing Loss 0.005875927432196237; Training Loss 0.004774135738812114\n",
      "Episode 1830; Testing Loss 0.00587601396992477; Training Loss 0.004774119098605022\n",
      "Episode 1831; Testing Loss 0.0058760809799828185; Training Loss 0.0047741076349132605\n",
      "Episode 1832; Testing Loss 0.00587603757774628; Training Loss 0.0047740941619714445\n",
      "Episode 1833; Testing Loss 0.00587598506135693; Training Loss 0.00477407986556992\n",
      "Episode 1834; Testing Loss 0.005876055288897087; Training Loss 0.004774064748229823\n",
      "Episode 1835; Testing Loss 0.005876191802304521; Training Loss 0.004774049855565348\n",
      "Episode 1836; Testing Loss 0.005876237994930369; Training Loss 0.0047740351071048565\n",
      "Episode 1837; Testing Loss 0.005876178268147441; Training Loss 0.004774022102733574\n",
      "Episode 1838; Testing Loss 0.005876178943402483; Training Loss 0.004774009347555172\n",
      "Episode 1839; Testing Loss 0.005876249277984938; Training Loss 0.004773994185430054\n",
      "Episode 1840; Testing Loss 0.005876234143018976; Training Loss 0.004773978574628813\n",
      "Episode 1841; Testing Loss 0.005876154175635284; Training Loss 0.004773966082435003\n",
      "Episode 1842; Testing Loss 0.005876106074296165; Training Loss 0.004773952941623725\n",
      "Episode 1843; Testing Loss 0.0058761853475111136; Training Loss 0.004773937880800796\n",
      "Episode 1844; Testing Loss 0.005876293118466286; Training Loss 0.004773924269128755\n",
      "Episode 1845; Testing Loss 0.005876290762680637; Training Loss 0.0047739094985668985\n",
      "Episode 1846; Testing Loss 0.005876238465183255; Training Loss 0.004773895146959165\n",
      "Episode 1847; Testing Loss 0.0058762004074943185; Training Loss 0.00477388083883379\n",
      "Episode 1848; Testing Loss 0.005876266443182208; Training Loss 0.004773865718142223\n",
      "Episode 1849; Testing Loss 0.005876375255934109; Training Loss 0.004773851157726057\n",
      "Episode 1850; Testing Loss 0.005876448015664804; Training Loss 0.004773837955774788\n",
      "Episode 1851; Testing Loss 0.005876363545137867; Training Loss 0.004773822546102606\n",
      "Episode 1852; Testing Loss 0.005876276477759116; Training Loss 0.004773807358357843\n",
      "Episode 1853; Testing Loss 0.005876317195369053; Training Loss 0.004773793103492835\n",
      "Episode 1854; Testing Loss 0.005876459567512412; Training Loss 0.004773778674528416\n",
      "Episode 1855; Testing Loss 0.005876524521619826; Training Loss 0.004773762988694446\n",
      "Episode 1856; Testing Loss 0.005876437856601344; Training Loss 0.004773747724752858\n",
      "Episode 1857; Testing Loss 0.0058763909093190745; Training Loss 0.004773733452427867\n",
      "Episode 1858; Testing Loss 0.005876520255610266; Training Loss 0.0047737193054692385\n",
      "Episode 1859; Testing Loss 0.005876707127659328; Training Loss 0.004773704136287916\n",
      "Episode 1860; Testing Loss 0.005876759158146343; Training Loss 0.0047736893903531745\n",
      "Episode 1861; Testing Loss 0.0058766510109126; Training Loss 0.004773676380061839\n",
      "Episode 1862; Testing Loss 0.005876593279518845; Training Loss 0.004773662059857304\n",
      "Episode 1863; Testing Loss 0.005876715631726252; Training Loss 0.004773643704945299\n",
      "Episode 1864; Testing Loss 0.0058768235680413; Training Loss 0.00477362942713107\n",
      "Episode 1865; Testing Loss 0.0058767656751148815; Training Loss 0.004773615108637073\n",
      "Episode 1866; Testing Loss 0.005876706311721364; Training Loss 0.004773599320941895\n",
      "Episode 1867; Testing Loss 0.0058766979084212585; Training Loss 0.004773585539277882\n",
      "Episode 1868; Testing Loss 0.005876781890805376; Training Loss 0.004773571023568775\n",
      "Episode 1869; Testing Loss 0.005876872240360738; Training Loss 0.004773558007633835\n",
      "Episode 1870; Testing Loss 0.00587688049543152; Training Loss 0.004773543961195341\n",
      "Episode 1871; Testing Loss 0.0058768388531754215; Training Loss 0.004773531017642191\n",
      "Episode 1872; Testing Loss 0.005876840510925154; Training Loss 0.004773516362430584\n",
      "Episode 1873; Testing Loss 0.005876897268609562; Training Loss 0.0047735044577801936\n",
      "Episode 1874; Testing Loss 0.005876931347677983; Training Loss 0.004773492114527567\n",
      "Episode 1875; Testing Loss 0.0058768298891402875; Training Loss 0.004773477065658903\n",
      "Episode 1876; Testing Loss 0.005876761577327058; Training Loss 0.004773461811236005\n",
      "Episode 1877; Testing Loss 0.005876756414995875; Training Loss 0.004773451210935417\n",
      "Episode 1878; Testing Loss 0.005876786404744266; Training Loss 0.004773439148134932\n",
      "Episode 1879; Testing Loss 0.005876797310966504; Training Loss 0.004773423358882195\n",
      "Episode 1880; Testing Loss 0.005876729761243905; Training Loss 0.0047734080857055345\n",
      "Episode 1881; Testing Loss 0.005876602668269024; Training Loss 0.004773396194116064\n",
      "Episode 1882; Testing Loss 0.005876591304501513; Training Loss 0.0047733819298166145\n",
      "Episode 1883; Testing Loss 0.005876682126613644; Training Loss 0.004773365685959262\n",
      "Episode 1884; Testing Loss 0.0058767310324855035; Training Loss 0.004773354012505793\n",
      "Episode 1885; Testing Loss 0.005876647658385489; Training Loss 0.004773341707093185\n",
      "Episode 1886; Testing Loss 0.00587660510614214; Training Loss 0.00477332706614552\n",
      "Episode 1887; Testing Loss 0.0058766556041562786; Training Loss 0.0047733133569745\n",
      "Episode 1888; Testing Loss 0.005876593635764141; Training Loss 0.004773300978334804\n",
      "Episode 1889; Testing Loss 0.005876466864790389; Training Loss 0.004773286794741257\n",
      "Episode 1890; Testing Loss 0.005876465467072279; Training Loss 0.004773271999094167\n",
      "Episode 1891; Testing Loss 0.005876575493515497; Training Loss 0.004773259398920278\n",
      "Episode 1892; Testing Loss 0.0058765177080412736; Training Loss 0.004773246772279939\n",
      "Episode 1893; Testing Loss 0.005876384503662436; Training Loss 0.004773233636211451\n",
      "Episode 1894; Testing Loss 0.005876369850545727; Training Loss 0.004773219332992101\n",
      "Episode 1895; Testing Loss 0.005876439117890527; Training Loss 0.004773208230118459\n",
      "Episode 1896; Testing Loss 0.005876497927716027; Training Loss 0.004773194794378991\n",
      "Episode 1897; Testing Loss 0.005876451279822439; Training Loss 0.0047731799195416145\n",
      "Episode 1898; Testing Loss 0.005876422972883432; Training Loss 0.004773168772602414\n",
      "Episode 1899; Testing Loss 0.005876435502138138; Training Loss 0.004773156014842471\n",
      "Episode 1900; Testing Loss 0.005876458635322518; Training Loss 0.004773144098738831\n",
      "Episode 1901; Testing Loss 0.005876430982896593; Training Loss 0.004773129960715976\n",
      "Episode 1902; Testing Loss 0.005876395716454413; Training Loss 0.0047731165347676685\n",
      "Episode 1903; Testing Loss 0.005876427758228191; Training Loss 0.0047731027064185205\n",
      "Episode 1904; Testing Loss 0.005876436338345444; Training Loss 0.004773089059857828\n",
      "Episode 1905; Testing Loss 0.00587639573883738; Training Loss 0.004773076728675443\n",
      "Episode 1906; Testing Loss 0.005876304143781876; Training Loss 0.004773063524522086\n",
      "Episode 1907; Testing Loss 0.00587627101459322; Training Loss 0.0047730498724004485\n",
      "Episode 1908; Testing Loss 0.005876291737245926; Training Loss 0.0047730374358273655\n",
      "Episode 1909; Testing Loss 0.005876321816959393; Training Loss 0.004773024869769265\n",
      "Episode 1910; Testing Loss 0.0058762881054823255; Training Loss 0.0047730105048625175\n",
      "Episode 1911; Testing Loss 0.005876173522721854; Training Loss 0.004772999247367071\n",
      "Episode 1912; Testing Loss 0.005876152917037412; Training Loss 0.004772986557179129\n",
      "Episode 1913; Testing Loss 0.0058762298965228206; Training Loss 0.00477297311605229\n",
      "Episode 1914; Testing Loss 0.005876221420660301; Training Loss 0.00477295865761222\n",
      "Episode 1915; Testing Loss 0.005876086436622257; Training Loss 0.004772947466156526\n",
      "Episode 1916; Testing Loss 0.005876052590624409; Training Loss 0.004772934379085345\n",
      "Episode 1917; Testing Loss 0.0058761507517594595; Training Loss 0.004772920300300827\n",
      "Episode 1918; Testing Loss 0.005876204804085077; Training Loss 0.004772908471476179\n",
      "Episode 1919; Testing Loss 0.005876128716862266; Training Loss 0.004772894627746232\n",
      "Episode 1920; Testing Loss 0.005875972722292003; Training Loss 0.004772884021121141\n",
      "Episode 1921; Testing Loss 0.005875966708718752; Training Loss 0.004772871513794451\n",
      "Episode 1922; Testing Loss 0.005876094144447881; Training Loss 0.004772856163682194\n",
      "Episode 1923; Testing Loss 0.005876077889567021; Training Loss 0.004772844680643604\n",
      "Episode 1924; Testing Loss 0.0058758929273609265; Training Loss 0.004772831902010294\n",
      "Episode 1925; Testing Loss 0.005875845211233358; Training Loss 0.004772818550543809\n",
      "Episode 1926; Testing Loss 0.005875949378449852; Training Loss 0.0047728061910050175\n",
      "Episode 1927; Testing Loss 0.0058760295659933065; Training Loss 0.004772795602863859\n",
      "Episode 1928; Testing Loss 0.005875906409929433; Training Loss 0.004772780172384098\n",
      "Episode 1929; Testing Loss 0.005875778560535553; Training Loss 0.004772769825939192\n",
      "Episode 1930; Testing Loss 0.005875811090205948; Training Loss 0.004772756929682441\n",
      "Episode 1931; Testing Loss 0.005875918164337438; Training Loss 0.004772745019079603\n",
      "Episode 1932; Testing Loss 0.005875891645851254; Training Loss 0.00477273148999677\n",
      "Episode 1933; Testing Loss 0.005875792676277876; Training Loss 0.004772719975443627\n",
      "Episode 1934; Testing Loss 0.0058758218555247195; Training Loss 0.004772707807027736\n",
      "Episode 1935; Testing Loss 0.005875945673211381; Training Loss 0.004772694755163647\n",
      "Episode 1936; Testing Loss 0.005875972791014086; Training Loss 0.004772681895396654\n",
      "Episode 1937; Testing Loss 0.005875808355437622; Training Loss 0.00477266919045631\n",
      "Episode 1938; Testing Loss 0.005875667580770977; Training Loss 0.004772659259149515\n",
      "Episode 1939; Testing Loss 0.00587576471743295; Training Loss 0.004772645020515955\n",
      "Episode 1940; Testing Loss 0.005875887402426235; Training Loss 0.004772631983215382\n",
      "Episode 1941; Testing Loss 0.00587585567494615; Training Loss 0.0047726184137688405\n",
      "Episode 1942; Testing Loss 0.005875741896349311; Training Loss 0.004772606450554679\n",
      "Episode 1943; Testing Loss 0.005875723047129893; Training Loss 0.004772593985225528\n",
      "Episode 1944; Testing Loss 0.005875850717025021; Training Loss 0.004772581280900522\n",
      "Episode 1945; Testing Loss 0.0058758849695302985; Training Loss 0.004772571363985163\n",
      "Episode 1946; Testing Loss 0.005875733805521783; Training Loss 0.004772557482937286\n",
      "Episode 1947; Testing Loss 0.005875643604396671; Training Loss 0.004772545292223849\n",
      "Episode 1948; Testing Loss 0.005875758667061744; Training Loss 0.004772532145231588\n",
      "Episode 1949; Testing Loss 0.0058759091387655055; Training Loss 0.004772521528617564\n",
      "Episode 1950; Testing Loss 0.005875801822398909; Training Loss 0.004772506425040735\n",
      "Episode 1951; Testing Loss 0.00587560993384051; Training Loss 0.0047724965961499916\n",
      "Episode 1952; Testing Loss 0.005875660937980878; Training Loss 0.004772483193932671\n",
      "Episode 1953; Testing Loss 0.005875869669535851; Training Loss 0.0047724708970883324\n",
      "Episode 1954; Testing Loss 0.00587582080940954; Training Loss 0.004772458116000759\n",
      "Episode 1955; Testing Loss 0.005875642542307439; Training Loss 0.004772445970690105\n",
      "Episode 1956; Testing Loss 0.00587559450578806; Training Loss 0.004772433084168741\n",
      "Episode 1957; Testing Loss 0.0058756566529334095; Training Loss 0.0047724221109250575\n",
      "Episode 1958; Testing Loss 0.005875644206157233; Training Loss 0.00477240992793288\n",
      "Episode 1959; Testing Loss 0.005875533336326882; Training Loss 0.004772396935049671\n",
      "Episode 1960; Testing Loss 0.0058754764245098155; Training Loss 0.004772385255662527\n",
      "Episode 1961; Testing Loss 0.005875534351147506; Training Loss 0.0047723725646300185\n",
      "Episode 1962; Testing Loss 0.005875559391561703; Training Loss 0.0047723615289897315\n",
      "Episode 1963; Testing Loss 0.005875536343140352; Training Loss 0.00477234855577009\n",
      "Episode 1964; Testing Loss 0.005875537042607746; Training Loss 0.004772334926811362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1965; Testing Loss 0.0058755486177856945; Training Loss 0.004772324120305001\n",
      "Episode 1966; Testing Loss 0.0058755144908267795; Training Loss 0.004772311284990099\n",
      "Episode 1967; Testing Loss 0.005875514269522061; Training Loss 0.004772298654970428\n",
      "Episode 1968; Testing Loss 0.005875527944969875; Training Loss 0.004772287344970082\n",
      "Episode 1969; Testing Loss 0.005875570284116099; Training Loss 0.004772277256318497\n",
      "Episode 1970; Testing Loss 0.005875601293222623; Training Loss 0.004772264416535733\n",
      "Episode 1971; Testing Loss 0.005875586931473097; Training Loss 0.004772249717251893\n",
      "Episode 1972; Testing Loss 0.005875508434187223; Training Loss 0.004772240382051365\n",
      "Episode 1973; Testing Loss 0.005875477628759075; Training Loss 0.0047722285129557325\n",
      "Episode 1974; Testing Loss 0.005875494153353982; Training Loss 0.004772217192257383\n",
      "Episode 1975; Testing Loss 0.0058755219331531; Training Loss 0.004772203971191343\n",
      "Episode 1976; Testing Loss 0.005875488865070762; Training Loss 0.004772190625552426\n",
      "Episode 1977; Testing Loss 0.005875462827950871; Training Loss 0.004772178964120735\n",
      "Episode 1978; Testing Loss 0.0058754283134531465; Training Loss 0.004772166225781015\n",
      "Episode 1979; Testing Loss 0.005875418153914591; Training Loss 0.004772153028306569\n",
      "Episode 1980; Testing Loss 0.005875414082579525; Training Loss 0.004772142805016626\n",
      "Episode 1981; Testing Loss 0.005875451871580495; Training Loss 0.004772130863601608\n",
      "Episode 1982; Testing Loss 0.005875488429011801; Training Loss 0.0047721172914432265\n",
      "Episode 1983; Testing Loss 0.005875448739753489; Training Loss 0.004772104922395123\n",
      "Episode 1984; Testing Loss 0.005875359948642625; Training Loss 0.004772093125370883\n",
      "Episode 1985; Testing Loss 0.005875310727673915; Training Loss 0.004772080766893232\n",
      "Episode 1986; Testing Loss 0.005875393556968332; Training Loss 0.004772068038357015\n",
      "Episode 1987; Testing Loss 0.00587544836445055; Training Loss 0.004772056293995039\n",
      "Episode 1988; Testing Loss 0.005875396708080413; Training Loss 0.004772043953875495\n",
      "Episode 1989; Testing Loss 0.005875334998444321; Training Loss 0.004772032261436576\n",
      "Episode 1990; Testing Loss 0.005875377854141691; Training Loss 0.0047720191799507415\n",
      "Episode 1991; Testing Loss 0.0058754409463117555; Training Loss 0.004772008926161958\n",
      "Episode 1992; Testing Loss 0.005875432826267223; Training Loss 0.0047719962378477915\n",
      "Episode 1993; Testing Loss 0.00587531713720519; Training Loss 0.004771985254965936\n",
      "Episode 1994; Testing Loss 0.005875306138331683; Training Loss 0.004771972752366796\n",
      "Episode 1995; Testing Loss 0.005875372625554316; Training Loss 0.004771961658077981\n",
      "Episode 1996; Testing Loss 0.005875315323081662; Training Loss 0.004771949511554304\n",
      "Episode 1997; Testing Loss 0.005875205107920639; Training Loss 0.00477193606145962\n",
      "Episode 1998; Testing Loss 0.005875222897516348; Training Loss 0.004771927983492356\n",
      "Episode 1999; Testing Loss 0.005875380713969606; Training Loss 0.004771916696477305\n",
      "Episode 2000; Testing Loss 0.005875403508804798; Training Loss 0.004771902928934459\n",
      "Episode 2001; Testing Loss 0.005875274578343466; Training Loss 0.004771889982003384\n",
      "Episode 2002; Testing Loss 0.0058751843011244295; Training Loss 0.004771880747073587\n",
      "Episode 2003; Testing Loss 0.0058751915680903925; Training Loss 0.004771869502614965\n",
      "Episode 2004; Testing Loss 0.005875240067494645; Training Loss 0.004771855644067667\n",
      "Episode 2005; Testing Loss 0.005875246854998163; Training Loss 0.0047718433915641175\n",
      "Episode 2006; Testing Loss 0.005875221361070686; Training Loss 0.004771831687750556\n",
      "Episode 2007; Testing Loss 0.005875230391753475; Training Loss 0.004771819708523977\n",
      "Episode 2008; Testing Loss 0.00587524913040294; Training Loss 0.004771806417299411\n",
      "Episode 2009; Testing Loss 0.005875217025692371; Training Loss 0.004771793720272465\n",
      "Episode 2010; Testing Loss 0.005875201170968105; Training Loss 0.0047717831836372555\n",
      "Episode 2011; Testing Loss 0.005875202823031595; Training Loss 0.004771772677149769\n",
      "Episode 2012; Testing Loss 0.005875248250057291; Training Loss 0.004771760315004714\n",
      "Episode 2013; Testing Loss 0.005875227079412369; Training Loss 0.004771746080405249\n",
      "Episode 2014; Testing Loss 0.005875142524673662; Training Loss 0.00477173492687554\n",
      "Episode 2015; Testing Loss 0.0058751550798866944; Training Loss 0.004771724530889989\n",
      "Episode 2016; Testing Loss 0.005875245023914319; Training Loss 0.00477171169454797\n",
      "Episode 2017; Testing Loss 0.005875250895615346; Training Loss 0.004771700685544369\n",
      "Episode 2018; Testing Loss 0.00587514026948076; Training Loss 0.004771688372234643\n",
      "Episode 2019; Testing Loss 0.00587506548315418; Training Loss 0.004771676953927636\n",
      "Episode 2020; Testing Loss 0.005875143770922467; Training Loss 0.004771663274511622\n",
      "Episode 2021; Testing Loss 0.0058752597553635676; Training Loss 0.004771651285049019\n",
      "Episode 2022; Testing Loss 0.005875267488047639; Training Loss 0.004771639950978743\n",
      "Episode 2023; Testing Loss 0.005875144394455491; Training Loss 0.004771626438154836\n",
      "Episode 2024; Testing Loss 0.005875034245506274; Training Loss 0.004771616605204794\n",
      "Episode 2025; Testing Loss 0.005875121791399011; Training Loss 0.004771603225176026\n",
      "Episode 2026; Testing Loss 0.005875201480878824; Training Loss 0.004771592825972155\n",
      "Episode 2027; Testing Loss 0.005875114533408015; Training Loss 0.004771581557095141\n",
      "Episode 2028; Testing Loss 0.0058750520153845525; Training Loss 0.004771569657887481\n",
      "Episode 2029; Testing Loss 0.005875112446333329; Training Loss 0.0047715576028839485\n",
      "Episode 2030; Testing Loss 0.005875196140538047; Training Loss 0.004771546391621703\n",
      "Episode 2031; Testing Loss 0.0058751303742440655; Training Loss 0.004771532601168998\n",
      "Episode 2032; Testing Loss 0.005875032490796935; Training Loss 0.004771522024709659\n",
      "Episode 2033; Testing Loss 0.005875023951636624; Training Loss 0.004771511125120907\n",
      "Episode 2034; Testing Loss 0.005875048860760772; Training Loss 0.004771499806590772\n",
      "Episode 2035; Testing Loss 0.005875068834247125; Training Loss 0.004771486179352636\n",
      "Episode 2036; Testing Loss 0.005875052788029842; Training Loss 0.004771474492623271\n",
      "Episode 2037; Testing Loss 0.0058750295399696015; Training Loss 0.004771463788938617\n",
      "Episode 2038; Testing Loss 0.005875044568743378; Training Loss 0.004771450775850616\n",
      "Episode 2039; Testing Loss 0.005875112991101271; Training Loss 0.004771438770002938\n",
      "Episode 2040; Testing Loss 0.005875105920844511; Training Loss 0.004771427224266274\n",
      "Episode 2041; Testing Loss 0.005875018641388128; Training Loss 0.00477141492149989\n",
      "Episode 2042; Testing Loss 0.005875036482142598; Training Loss 0.0047714025785149565\n",
      "Episode 2043; Testing Loss 0.005875125006660258; Training Loss 0.004771390765980996\n",
      "Episode 2044; Testing Loss 0.005875101210868836; Training Loss 0.0047713802614827916\n",
      "Episode 2045; Testing Loss 0.005875003124431711; Training Loss 0.0047713684358767555\n",
      "Episode 2046; Testing Loss 0.005874976057140511; Training Loss 0.004771355990225331\n",
      "Episode 2047; Testing Loss 0.005875066922077386; Training Loss 0.004771344148804289\n",
      "Episode 2048; Testing Loss 0.005875061132520313; Training Loss 0.004771333336871623\n",
      "Episode 2049; Testing Loss 0.005874992299430458; Training Loss 0.004771319848373366\n",
      "Episode 2050; Testing Loss 0.005874969239155471; Training Loss 0.004771309631199463\n",
      "Episode 2051; Testing Loss 0.005875061202404525; Training Loss 0.004771298477896638\n",
      "Episode 2052; Testing Loss 0.005875088105681692; Training Loss 0.004771286221523397\n",
      "Episode 2053; Testing Loss 0.005875022561114401; Training Loss 0.004771274296828655\n",
      "Episode 2054; Testing Loss 0.0058750159593809635; Training Loss 0.004771263264723599\n",
      "Episode 2055; Testing Loss 0.005875008012137802; Training Loss 0.004771252410549492\n",
      "Episode 2056; Testing Loss 0.005874922623620206; Training Loss 0.0047712402035386325\n",
      "Episode 2057; Testing Loss 0.005874842260829693; Training Loss 0.004771227007564433\n",
      "Episode 2058; Testing Loss 0.005874882196640691; Training Loss 0.004771216670410268\n",
      "Episode 2059; Testing Loss 0.005874954103161727; Training Loss 0.004771206297300677\n",
      "Episode 2060; Testing Loss 0.005874955143719909; Training Loss 0.004771195027591379\n",
      "Episode 2061; Testing Loss 0.005874876189187792; Training Loss 0.004771181633716103\n",
      "Episode 2062; Testing Loss 0.005874872833214352; Training Loss 0.004771168984919927\n",
      "Episode 2063; Testing Loss 0.005874982644720449; Training Loss 0.004771159392479224\n",
      "Episode 2064; Testing Loss 0.005874981830867962; Training Loss 0.004771148721581433\n",
      "Episode 2065; Testing Loss 0.0058748873281084905; Training Loss 0.004771136740167978\n",
      "Episode 2066; Testing Loss 0.005874870308726678; Training Loss 0.004771124073797573\n",
      "Episode 2067; Testing Loss 0.005874979336530719; Training Loss 0.004771111420337809\n",
      "Episode 2068; Testing Loss 0.0058750625478924505; Training Loss 0.00477110192604774\n",
      "Episode 2069; Testing Loss 0.005874984247670772; Training Loss 0.004771087439406123\n",
      "Episode 2070; Testing Loss 0.005874889111314696; Training Loss 0.0047710750964599195\n",
      "Episode 2071; Testing Loss 0.005874879554784504; Training Loss 0.004771064875660668\n",
      "Episode 2072; Testing Loss 0.005874917319215263; Training Loss 0.004771052456189788\n",
      "Episode 2073; Testing Loss 0.005874954196673587; Training Loss 0.0047710400526867385\n",
      "Episode 2074; Testing Loss 0.0058749393124354074; Training Loss 0.004771028797196462\n",
      "Episode 2075; Testing Loss 0.005874846771941308; Training Loss 0.004771016451941643\n",
      "Episode 2076; Testing Loss 0.0058747915503141145; Training Loss 0.004771006632570424\n",
      "Episode 2077; Testing Loss 0.0058748766050511604; Training Loss 0.004770994160639544\n",
      "Episode 2078; Testing Loss 0.0058749694315822845; Training Loss 0.00477098268570251\n",
      "Episode 2079; Testing Loss 0.005874895227341278; Training Loss 0.004770970114862066\n",
      "Episode 2080; Testing Loss 0.005874751594687345; Training Loss 0.004770959311270611\n",
      "Episode 2081; Testing Loss 0.005874759168621602; Training Loss 0.004770946514637979\n",
      "Episode 2082; Testing Loss 0.0058749044259875215; Training Loss 0.00477093675321259\n",
      "Episode 2083; Testing Loss 0.005875007965755655; Training Loss 0.004770926635554266\n",
      "Episode 2084; Testing Loss 0.005874935830405839; Training Loss 0.004770912838913147\n",
      "Episode 2085; Testing Loss 0.00587481257582164; Training Loss 0.0047709002579300365\n",
      "Episode 2086; Testing Loss 0.005874830196664831; Training Loss 0.004770890858231377\n",
      "Episode 2087; Testing Loss 0.00587488147659488; Training Loss 0.00477088001170014\n",
      "Episode 2088; Testing Loss 0.00587484701393164; Training Loss 0.004770868002602912\n",
      "Episode 2089; Testing Loss 0.005874731687502093; Training Loss 0.0047708550340483154\n",
      "Episode 2090; Testing Loss 0.005874698113759528; Training Loss 0.00477084549306784\n",
      "Episode 2091; Testing Loss 0.005874801622330782; Training Loss 0.004770834870601161\n",
      "Episode 2092; Testing Loss 0.005874896797874469; Training Loss 0.004770823810495754\n",
      "Episode 2093; Testing Loss 0.005874838296013571; Training Loss 0.0047708095592601495\n",
      "Episode 2094; Testing Loss 0.00587473163273901; Training Loss 0.004770797005000566\n",
      "Episode 2095; Testing Loss 0.005874729700653781; Training Loss 0.004770785648044961\n",
      "Episode 2096; Testing Loss 0.00587483376012005; Training Loss 0.0047707741207337\n",
      "Episode 2097; Testing Loss 0.005874883624288641; Training Loss 0.00477076410851138\n",
      "Episode 2098; Testing Loss 0.005874759139936955; Training Loss 0.00477075112930517\n",
      "Episode 2099; Testing Loss 0.005874652316029154; Training Loss 0.004770740111140552\n",
      "Episode 2100; Testing Loss 0.005874714046210495; Training Loss 0.004770729943022155\n",
      "Episode 2101; Testing Loss 0.00587486395467375; Training Loss 0.004770719251296994\n",
      "Episode 2102; Testing Loss 0.005874849476015937; Training Loss 0.0047707052121257225\n",
      "Episode 2103; Testing Loss 0.005874693719066265; Training Loss 0.004770694178007213\n",
      "Episode 2104; Testing Loss 0.005874590338652607; Training Loss 0.004770685681886059\n",
      "Episode 2105; Testing Loss 0.005874682990988386; Training Loss 0.004770671416297282\n",
      "Episode 2106; Testing Loss 0.005874808901890999; Training Loss 0.004770657850941126\n",
      "Episode 2107; Testing Loss 0.005874818081862072; Training Loss 0.004770649081661517\n",
      "Episode 2108; Testing Loss 0.00587473132568448; Training Loss 0.004770638408492764\n",
      "Episode 2109; Testing Loss 0.005874706657645753; Training Loss 0.004770624827535863\n",
      "Episode 2110; Testing Loss 0.005874761890482631; Training Loss 0.004770611116060406\n",
      "Episode 2111; Testing Loss 0.005874755580840604; Training Loss 0.004770600922188661\n",
      "Episode 2112; Testing Loss 0.005874682496729902; Training Loss 0.004770588301607757\n",
      "Episode 2113; Testing Loss 0.00587464480880971; Training Loss 0.004770577373071539\n",
      "Episode 2114; Testing Loss 0.005874722841439086; Training Loss 0.004770566862060247\n",
      "Episode 2115; Testing Loss 0.005874823069570032; Training Loss 0.004770555033339197\n",
      "Episode 2116; Testing Loss 0.005874794882094599; Training Loss 0.004770544986498457\n",
      "Episode 2117; Testing Loss 0.005874678953035265; Training Loss 0.004770533629112034\n",
      "Episode 2118; Testing Loss 0.0058746633998357466; Training Loss 0.004770520098964145\n",
      "Episode 2119; Testing Loss 0.005874723091135511; Training Loss 0.00477050868136299\n",
      "Episode 2120; Testing Loss 0.005874666712158453; Training Loss 0.0047704992553715495\n",
      "Episode 2121; Testing Loss 0.005874612256024635; Training Loss 0.004770488940806916\n",
      "Episode 2122; Testing Loss 0.005874633640738223; Training Loss 0.00477047423148029\n",
      "Episode 2123; Testing Loss 0.005874643454224907; Training Loss 0.004770460101802476\n",
      "Episode 2124; Testing Loss 0.005874640589587703; Training Loss 0.004770449133339522\n",
      "Episode 2125; Testing Loss 0.0058746436892505895; Training Loss 0.004770435214460833\n",
      "Episode 2126; Testing Loss 0.0058746724500479425; Training Loss 0.0047704226319356955\n",
      "Episode 2127; Testing Loss 0.0058747282514694; Training Loss 0.004770410389609711\n",
      "Episode 2128; Testing Loss 0.005874731372173206; Training Loss 0.00477039647695882\n",
      "Episode 2129; Testing Loss 0.005874693718715248; Training Loss 0.004770382176858825\n",
      "Episode 2130; Testing Loss 0.0058747480677948626; Training Loss 0.004770368149576027\n",
      "Episode 2131; Testing Loss 0.005874857948067724; Training Loss 0.0047703576895506365\n",
      "Episode 2132; Testing Loss 0.005874910962297525; Training Loss 0.004770344017464498\n",
      "Episode 2133; Testing Loss 0.005874882580605802; Training Loss 0.0047703267225332625\n",
      "Episode 2134; Testing Loss 0.005874890629907856; Training Loss 0.004770317843770405\n",
      "Episode 2135; Testing Loss 0.0058749241057716225; Training Loss 0.00477030723697846\n",
      "Episode 2136; Testing Loss 0.005874979792928357; Training Loss 0.0047702935844825605\n",
      "Episode 2137; Testing Loss 0.005875046181332614; Training Loss 0.004770278025859851\n",
      "Episode 2138; Testing Loss 0.005875102557500654; Training Loss 0.004770263782547591\n",
      "Episode 2139; Testing Loss 0.0058751568246530154; Training Loss 0.004770254921983793\n",
      "Episode 2140; Testing Loss 0.005875214850869235; Training Loss 0.004770244342821006\n",
      "Episode 2141; Testing Loss 0.005875166302611288; Training Loss 0.004770230063233044\n",
      "Episode 2142; Testing Loss 0.005875124373725837; Training Loss 0.004770214442039172\n",
      "Episode 2143; Testing Loss 0.0058751404787731306; Training Loss 0.0047702041864053695\n",
      "Episode 2144; Testing Loss 0.005875158046854398; Training Loss 0.004770192783642808\n",
      "Episode 2145; Testing Loss 0.00587516612929205; Training Loss 0.004770179117808695\n",
      "Episode 2146; Testing Loss 0.005875170495721431; Training Loss 0.004770167724088369\n",
      "Episode 2147; Testing Loss 0.005875224659721344; Training Loss 0.004770157733138291\n",
      "Episode 2148; Testing Loss 0.005875272631158033; Training Loss 0.0047701449580303664\n",
      "Episode 2149; Testing Loss 0.005875243714766655; Training Loss 0.0047701337999165325\n",
      "Episode 2150; Testing Loss 0.005875179488848052; Training Loss 0.004770122850259199\n",
      "Episode 2151; Testing Loss 0.00587518044956213; Training Loss 0.004770109970313878\n",
      "Episode 2152; Testing Loss 0.005875260481306115; Training Loss 0.004770094937469502\n",
      "Episode 2153; Testing Loss 0.005875273325162555; Training Loss 0.004770081600156445\n",
      "Episode 2154; Testing Loss 0.005875268121322682; Training Loss 0.004770069609064276\n",
      "Episode 2155; Testing Loss 0.005875278770304932; Training Loss 0.004770057758049959\n",
      "Episode 2156; Testing Loss 0.005875319381444169; Training Loss 0.004770048243336347\n",
      "Episode 2157; Testing Loss 0.005875310663881241; Training Loss 0.004770037410259578\n",
      "Episode 2158; Testing Loss 0.005875264645191086; Training Loss 0.004770023608767082\n",
      "Episode 2159; Testing Loss 0.005875226341409444; Training Loss 0.004770008420309735\n",
      "Episode 2160; Testing Loss 0.005875224352494076; Training Loss 0.00477000257547679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2161; Testing Loss 0.005875246442543395; Training Loss 0.0047699929423615445\n",
      "Episode 2162; Testing Loss 0.005875239480603688; Training Loss 0.004769979600962854\n",
      "Episode 2163; Testing Loss 0.0058752037839860626; Training Loss 0.004769963960480727\n",
      "Episode 2164; Testing Loss 0.00587519406951106; Training Loss 0.00476995274499208\n",
      "Episode 2165; Testing Loss 0.005875225403435609; Training Loss 0.004769943362506878\n",
      "Episode 2166; Testing Loss 0.005875242850456001; Training Loss 0.004769931957868101\n",
      "Episode 2167; Testing Loss 0.00587524958820442; Training Loss 0.004769918266721177\n",
      "Episode 2168; Testing Loss 0.005875241359686903; Training Loss 0.004769904829151949\n",
      "Episode 2169; Testing Loss 0.005875255751349927; Training Loss 0.0047698930069462915\n",
      "Episode 2170; Testing Loss 0.0058752735219750555; Training Loss 0.00476988287518623\n",
      "Episode 2171; Testing Loss 0.005875178834023853; Training Loss 0.004769870436999848\n",
      "Episode 2172; Testing Loss 0.005875083960830918; Training Loss 0.004769858602927472\n",
      "Episode 2173; Testing Loss 0.005875086038077166; Training Loss 0.004769843703996228\n",
      "Episode 2174; Testing Loss 0.005875199464838943; Training Loss 0.004769833709146166\n",
      "Episode 2175; Testing Loss 0.005875254569190089; Training Loss 0.0047698226522431275\n",
      "Episode 2176; Testing Loss 0.005875172703805644; Training Loss 0.004769809197452022\n",
      "Episode 2177; Testing Loss 0.00587509707458127; Training Loss 0.004769797118559117\n",
      "Episode 2178; Testing Loss 0.005875118997332308; Training Loss 0.004769786996794262\n",
      "Episode 2179; Testing Loss 0.005875234765339853; Training Loss 0.004769774624142603\n",
      "Episode 2180; Testing Loss 0.0058752520103088094; Training Loss 0.004769761516361301\n",
      "Episode 2181; Testing Loss 0.005875178633776828; Training Loss 0.004769750180354336\n",
      "Episode 2182; Testing Loss 0.0058750904436730205; Training Loss 0.004769738270313861\n",
      "Episode 2183; Testing Loss 0.0058750392507994; Training Loss 0.00476972702131378\n",
      "Episode 2184; Testing Loss 0.0058750692557621356; Training Loss 0.004769715667307185\n",
      "Episode 2185; Testing Loss 0.005875089580074768; Training Loss 0.004769703416739537\n",
      "Episode 2186; Testing Loss 0.005875024326406278; Training Loss 0.004769691201373501\n",
      "Episode 2187; Testing Loss 0.005874932490972199; Training Loss 0.004769681720788842\n",
      "Episode 2188; Testing Loss 0.005874904412179029; Training Loss 0.004769669686918144\n",
      "Episode 2189; Testing Loss 0.00587497150307319; Training Loss 0.004769658635664312\n",
      "Episode 2190; Testing Loss 0.005875053828600959; Training Loss 0.004769646554739614\n",
      "Episode 2191; Testing Loss 0.0058750763087547534; Training Loss 0.004769633436276077\n",
      "Episode 2192; Testing Loss 0.005875021975065913; Training Loss 0.004769623532994741\n",
      "Episode 2193; Testing Loss 0.005874976400163464; Training Loss 0.0047696116726361855\n",
      "Episode 2194; Testing Loss 0.005874987139882778; Training Loss 0.004769600771214218\n",
      "Episode 2195; Testing Loss 0.005874945785273933; Training Loss 0.0047695874598204764\n",
      "Episode 2196; Testing Loss 0.0058748954820151515; Training Loss 0.004769575990330311\n",
      "Episode 2197; Testing Loss 0.00587487786430305; Training Loss 0.004769564926358156\n",
      "Episode 2198; Testing Loss 0.005874892211004797; Training Loss 0.004769554012881405\n",
      "Episode 2199; Testing Loss 0.005874891808321189; Training Loss 0.004769541292488997\n",
      "Episode 2200; Testing Loss 0.005874880396003962; Training Loss 0.0047695288242906786\n",
      "Episode 2201; Testing Loss 0.005874853650237247; Training Loss 0.004769517047317333\n",
      "Episode 2202; Testing Loss 0.005874846274606915; Training Loss 0.004769506231661019\n",
      "Episode 2203; Testing Loss 0.005874874025061788; Training Loss 0.004769492919197664\n",
      "Episode 2204; Testing Loss 0.005874934119304294; Training Loss 0.0047694822011112\n",
      "Episode 2205; Testing Loss 0.0058749840461736575; Training Loss 0.004769470532561035\n",
      "Episode 2206; Testing Loss 0.005874942410711165; Training Loss 0.004769458229573711\n",
      "Episode 2207; Testing Loss 0.00587491489910177; Training Loss 0.0047694460971928546\n",
      "Episode 2208; Testing Loss 0.00587493030796232; Training Loss 0.004769436134436083\n",
      "Episode 2209; Testing Loss 0.005874879286127376; Training Loss 0.0047694245672434\n",
      "Episode 2210; Testing Loss 0.0058748213261597725; Training Loss 0.004769412434672072\n",
      "Episode 2211; Testing Loss 0.005874831348901674; Training Loss 0.0047694015930654565\n",
      "Episode 2212; Testing Loss 0.005874863008252647; Training Loss 0.00476938919293116\n",
      "Episode 2213; Testing Loss 0.005874844816026929; Training Loss 0.004769380049369961\n",
      "Episode 2214; Testing Loss 0.0058748678522561075; Training Loss 0.004769368446009054\n",
      "Episode 2215; Testing Loss 0.005874876463446134; Training Loss 0.004769355598019458\n",
      "Episode 2216; Testing Loss 0.005874904651045286; Training Loss 0.004769344768400234\n",
      "Episode 2217; Testing Loss 0.005874872714115145; Training Loss 0.004769333756229697\n",
      "Episode 2218; Testing Loss 0.005874804918283795; Training Loss 0.004769321662036452\n",
      "Episode 2219; Testing Loss 0.005874842571979325; Training Loss 0.004769307298727147\n",
      "Episode 2220; Testing Loss 0.005874911374759939; Training Loss 0.004769300136612704\n",
      "Episode 2221; Testing Loss 0.005874899511812913; Training Loss 0.004769289478102801\n",
      "Episode 2222; Testing Loss 0.005874794861392332; Training Loss 0.004769275660710239\n",
      "Episode 2223; Testing Loss 0.005874769541764188; Training Loss 0.004769262626834248\n",
      "Episode 2224; Testing Loss 0.005874824489084944; Training Loss 0.004769252680559592\n",
      "Episode 2225; Testing Loss 0.005874834159212101; Training Loss 0.004769244417742898\n",
      "Episode 2226; Testing Loss 0.005874744080934503; Training Loss 0.004769232774973897\n",
      "Episode 2227; Testing Loss 0.00587470281807796; Training Loss 0.004769218689556748\n",
      "Episode 2228; Testing Loss 0.005874765360663383; Training Loss 0.004769204678611085\n",
      "Episode 2229; Testing Loss 0.0058747996387209424; Training Loss 0.00476919695982114\n",
      "Episode 2230; Testing Loss 0.005874767454629303; Training Loss 0.004769187919132423\n",
      "Episode 2231; Testing Loss 0.005874673201383252; Training Loss 0.004769175389456926\n",
      "Episode 2232; Testing Loss 0.005874644009508296; Training Loss 0.004769161837300135\n",
      "Episode 2233; Testing Loss 0.005874744219223021; Training Loss 0.004769149057949631\n",
      "Episode 2234; Testing Loss 0.005874779668994639; Training Loss 0.004769138722935668\n",
      "Episode 2235; Testing Loss 0.0058747030469851885; Training Loss 0.0047691277146689\n",
      "Episode 2236; Testing Loss 0.005874618515347728; Training Loss 0.004769115523337392\n",
      "Episode 2237; Testing Loss 0.005874668518280714; Training Loss 0.004769103178554955\n",
      "Episode 2238; Testing Loss 0.005874824414845667; Training Loss 0.004769092729923485\n",
      "Episode 2239; Testing Loss 0.0058748404072889; Training Loss 0.004769080114750901\n",
      "Episode 2240; Testing Loss 0.005874649615508664; Training Loss 0.004769069357822\n",
      "Episode 2241; Testing Loss 0.005874563656392622; Training Loss 0.004769058692791874\n",
      "Episode 2242; Testing Loss 0.005874643710982298; Training Loss 0.004769046450992063\n",
      "Episode 2243; Testing Loss 0.005874646178106226; Training Loss 0.004769034270696752\n",
      "Episode 2244; Testing Loss 0.00587454451624842; Training Loss 0.004769023003451458\n",
      "Episode 2245; Testing Loss 0.005874505592721094; Training Loss 0.004769011226617131\n",
      "Episode 2246; Testing Loss 0.00587455816178746; Training Loss 0.004769001791161634\n",
      "Episode 2247; Testing Loss 0.005874582444432352; Training Loss 0.004768992080823186\n",
      "Episode 2248; Testing Loss 0.005874494435905807; Training Loss 0.004768979743877996\n",
      "Episode 2249; Testing Loss 0.005874447375243712; Training Loss 0.004768966899489765\n",
      "Episode 2250; Testing Loss 0.005874554526048224; Training Loss 0.004768953850724786\n",
      "Episode 2251; Testing Loss 0.005874679491731571; Training Loss 0.004768943686020164\n",
      "Episode 2252; Testing Loss 0.00587463097333199; Training Loss 0.004768930987047074\n",
      "Episode 2253; Testing Loss 0.0058744681595788144; Training Loss 0.0047689198563961935\n",
      "Episode 2254; Testing Loss 0.005874456237966536; Training Loss 0.004768911038331546\n",
      "Episode 2255; Testing Loss 0.005874640788751395; Training Loss 0.004768897416803694\n",
      "Episode 2256; Testing Loss 0.005874697129508291; Training Loss 0.004768886074497689\n",
      "Episode 2257; Testing Loss 0.00587454662554279; Training Loss 0.004768874086197092\n",
      "Episode 2258; Testing Loss 0.005874396540102104; Training Loss 0.004768863397753391\n",
      "Episode 2259; Testing Loss 0.005874445593022097; Training Loss 0.004768852304450922\n",
      "Episode 2260; Testing Loss 0.005874590896145164; Training Loss 0.0047688421443001965\n",
      "Episode 2261; Testing Loss 0.0058745460965907255; Training Loss 0.004768830343058859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2262; Testing Loss 0.00587443228274697; Training Loss 0.004768817222304525\n",
      "Episode 2263; Testing Loss 0.005874393819227313; Training Loss 0.0047688075749486546\n",
      "Episode 2264; Testing Loss 0.005874466702637741; Training Loss 0.004768796442739859\n",
      "Episode 2265; Testing Loss 0.005874460102417938; Training Loss 0.004768784365302973\n",
      "Episode 2266; Testing Loss 0.005874365548128758; Training Loss 0.004768770663190897\n",
      "Episode 2267; Testing Loss 0.005874340722410532; Training Loss 0.0047687629860470775\n",
      "Episode 2268; Testing Loss 0.005874370362878695; Training Loss 0.004768753303242717\n",
      "Episode 2269; Testing Loss 0.00587439506027042; Training Loss 0.004768741904415097\n",
      "Episode 2270; Testing Loss 0.005874345661435382; Training Loss 0.004768728787917902\n",
      "Episode 2271; Testing Loss 0.005874316871036931; Training Loss 0.004768715298516315\n",
      "Episode 2272; Testing Loss 0.005874386140181228; Training Loss 0.004768704170021517\n",
      "Episode 2273; Testing Loss 0.0058744670283267944; Training Loss 0.004768694686798732\n",
      "Episode 2274; Testing Loss 0.005874407179455364; Training Loss 0.004768682957955184\n",
      "Episode 2275; Testing Loss 0.005874342788648949; Training Loss 0.00476867016067659\n",
      "Episode 2276; Testing Loss 0.005874358981247218; Training Loss 0.004768658737005562\n",
      "Episode 2277; Testing Loss 0.005874323253126206; Training Loss 0.0047686485231662945\n",
      "Episode 2278; Testing Loss 0.0058742192647262045; Training Loss 0.004768638461859799\n",
      "Episode 2279; Testing Loss 0.005874211622560199; Training Loss 0.004768626590200354\n",
      "Episode 2280; Testing Loss 0.005874262753624215; Training Loss 0.004768613734924107\n",
      "Episode 2281; Testing Loss 0.0058742832888752225; Training Loss 0.0047686024348317785\n",
      "Episode 2282; Testing Loss 0.005874195808325975; Training Loss 0.004768589546614561\n",
      "Episode 2283; Testing Loss 0.005874145510476856; Training Loss 0.004768578734864867\n",
      "Episode 2284; Testing Loss 0.0058741842243123; Training Loss 0.004768566825064072\n",
      "Episode 2285; Testing Loss 0.005874233268507157; Training Loss 0.004768556605987784\n",
      "Episode 2286; Testing Loss 0.005874281912318918; Training Loss 0.0047685452688087165\n",
      "Episode 2287; Testing Loss 0.005874291003519733; Training Loss 0.004768532799157825\n",
      "Episode 2288; Testing Loss 0.005874260989180947; Training Loss 0.004768521890827164\n",
      "Episode 2289; Testing Loss 0.00587427761126083; Training Loss 0.00476851119327666\n",
      "Episode 2290; Testing Loss 0.005874248373820091; Training Loss 0.004768501246590784\n",
      "Episode 2291; Testing Loss 0.005874174681967206; Training Loss 0.004768489443968759\n",
      "Episode 2292; Testing Loss 0.005874172857264924; Training Loss 0.004768477723461238\n",
      "Episode 2293; Testing Loss 0.005874263526844127; Training Loss 0.00476846732675379\n",
      "Episode 2294; Testing Loss 0.005874307618350256; Training Loss 0.004768456706788088\n",
      "Episode 2295; Testing Loss 0.0058742134829739175; Training Loss 0.004768442708755012\n",
      "Episode 2296; Testing Loss 0.005874137213307455; Training Loss 0.004768433143440229\n",
      "Episode 2297; Testing Loss 0.005874083037549921; Training Loss 0.004768423224996255\n",
      "Episode 2298; Testing Loss 0.005874095912642461; Training Loss 0.004768410837356159\n",
      "Episode 2299; Testing Loss 0.005874158739954047; Training Loss 0.0047683978174716205\n",
      "Episode 2300; Testing Loss 0.005874157654929123; Training Loss 0.004768388181710792\n",
      "Episode 2301; Testing Loss 0.005874079985988539; Training Loss 0.004768378550074253\n",
      "Episode 2302; Testing Loss 0.005874081228156204; Training Loss 0.004768365629101185\n",
      "Episode 2303; Testing Loss 0.005874150828030208; Training Loss 0.004768353073450294\n",
      "Episode 2304; Testing Loss 0.005874106661299431; Training Loss 0.0047683418496478865\n",
      "Episode 2305; Testing Loss 0.005874028355711729; Training Loss 0.004768330652747509\n",
      "Episode 2306; Testing Loss 0.0058740815033373394; Training Loss 0.004768320654341389\n",
      "Episode 2307; Testing Loss 0.00587419614626731; Training Loss 0.00476830909262548\n",
      "Episode 2308; Testing Loss 0.005874194670929734; Training Loss 0.004768297844194962\n",
      "Episode 2309; Testing Loss 0.00587403565611304; Training Loss 0.004768284986688746\n",
      "Episode 2310; Testing Loss 0.005873954044868265; Training Loss 0.0047682748290102505\n",
      "Episode 2311; Testing Loss 0.005874044366413246; Training Loss 0.004768262277074843\n",
      "Episode 2312; Testing Loss 0.0058741025535654166; Training Loss 0.004768253408451822\n",
      "Episode 2313; Testing Loss 0.005873947064137288; Training Loss 0.004768241795534289\n",
      "Episode 2314; Testing Loss 0.005873843992947527; Training Loss 0.004768229753096341\n",
      "Episode 2315; Testing Loss 0.005873951215928059; Training Loss 0.0047682180467755195\n",
      "Episode 2316; Testing Loss 0.005874066262087455; Training Loss 0.004768207841058385\n",
      "Episode 2317; Testing Loss 0.005874014022272986; Training Loss 0.0047681944561246345\n",
      "Episode 2318; Testing Loss 0.005873924725656959; Training Loss 0.004768186097230556\n",
      "Episode 2319; Testing Loss 0.0058739016615867515; Training Loss 0.004768175703554177\n",
      "Episode 2320; Testing Loss 0.0058739793894430885; Training Loss 0.004768162523338742\n",
      "Episode 2321; Testing Loss 0.005873964923106219; Training Loss 0.004768148959252185\n",
      "Episode 2322; Testing Loss 0.005873905753262867; Training Loss 0.004768139288126342\n",
      "Episode 2323; Testing Loss 0.005873867998069238; Training Loss 0.004768128868515333\n",
      "Episode 2324; Testing Loss 0.005873933968319452; Training Loss 0.004768117002486593\n",
      "Episode 2325; Testing Loss 0.0058739481138072374; Training Loss 0.004768102755339365\n",
      "Episode 2326; Testing Loss 0.00587392313888831; Training Loss 0.0047680947363916435\n",
      "Episode 2327; Testing Loss 0.005873867480311495; Training Loss 0.004768085865736881\n",
      "Episode 2328; Testing Loss 0.005873836445283454; Training Loss 0.004768073677170912\n",
      "Episode 2329; Testing Loss 0.0058738796164009704; Training Loss 0.004768060635297361\n",
      "Episode 2330; Testing Loss 0.005873893035895752; Training Loss 0.004768048519612288\n",
      "Episode 2331; Testing Loss 0.005873807883868599; Training Loss 0.004768039185500461\n",
      "Episode 2332; Testing Loss 0.005873783954358039; Training Loss 0.00476803023152231\n",
      "Episode 2333; Testing Loss 0.005873883133751196; Training Loss 0.004768018685949747\n",
      "Episode 2334; Testing Loss 0.005873932772454509; Training Loss 0.004768005554487064\n",
      "Episode 2335; Testing Loss 0.005873825324035874; Training Loss 0.004767991842389252\n",
      "Episode 2336; Testing Loss 0.00587371654850832; Training Loss 0.004767982681309881\n",
      "Episode 2337; Testing Loss 0.005873761228948872; Training Loss 0.0047679709450658995\n",
      "Episode 2338; Testing Loss 0.00587392289747555; Training Loss 0.004767958357014064\n",
      "Episode 2339; Testing Loss 0.005873959405907639; Training Loss 0.004767947324929634\n",
      "Episode 2340; Testing Loss 0.005873777112989796; Training Loss 0.00476793630589301\n",
      "Episode 2341; Testing Loss 0.005873693734485912; Training Loss 0.004767924172591197\n",
      "Episode 2342; Testing Loss 0.005873785298648831; Training Loss 0.004767913742489233\n",
      "Episode 2343; Testing Loss 0.00587378752831778; Training Loss 0.004767903272064796\n",
      "Episode 2344; Testing Loss 0.0058736906994941215; Training Loss 0.004767891491635305\n",
      "Episode 2345; Testing Loss 0.005873662484104438; Training Loss 0.004767879258227481\n",
      "Episode 2346; Testing Loss 0.005873734815535151; Training Loss 0.0047678694034099765\n",
      "Episode 2347; Testing Loss 0.005873808337166888; Training Loss 0.004767858482961337\n",
      "Episode 2348; Testing Loss 0.005873758881656913; Training Loss 0.0047678485337092034\n",
      "Episode 2349; Testing Loss 0.005873651128328376; Training Loss 0.004767835009484946\n",
      "Episode 2350; Testing Loss 0.005873601740517101; Training Loss 0.004767823011917429\n",
      "Episode 2351; Testing Loss 0.005873614815167098; Training Loss 0.004767811219813956\n",
      "Episode 2352; Testing Loss 0.005873664992770627; Training Loss 0.00476780297804086\n",
      "Episode 2353; Testing Loss 0.005873653751333601; Training Loss 0.004767791741368597\n",
      "Episode 2354; Testing Loss 0.005873603187668694; Training Loss 0.004767777804263674\n",
      "Episode 2355; Testing Loss 0.0058735652622904175; Training Loss 0.004767765900086014\n",
      "Episode 2356; Testing Loss 0.005873549253781334; Training Loss 0.004767755340955551\n",
      "Episode 2357; Testing Loss 0.0058736053591063155; Training Loss 0.004767743095762395\n",
      "Episode 2358; Testing Loss 0.005873658639709228; Training Loss 0.004767731433709875\n",
      "Episode 2359; Testing Loss 0.0058736648980079525; Training Loss 0.004767720631865757\n",
      "Episode 2360; Testing Loss 0.005873579275379117; Training Loss 0.004767710738067499\n",
      "Episode 2361; Testing Loss 0.005873623198278759; Training Loss 0.004767698924118523\n",
      "Episode 2362; Testing Loss 0.005873678119723904; Training Loss 0.004767688399542596\n",
      "Episode 2363; Testing Loss 0.005873559683137384; Training Loss 0.004767677919724013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2364; Testing Loss 0.00587344927083233; Training Loss 0.004767667832711102\n",
      "Episode 2365; Testing Loss 0.0058735027814293805; Training Loss 0.004767654247846729\n",
      "Episode 2366; Testing Loss 0.00587357570788095; Training Loss 0.004767646113287202\n",
      "Episode 2367; Testing Loss 0.005873521843991168; Training Loss 0.00476763514917051\n",
      "Episode 2368; Testing Loss 0.005873404278454732; Training Loss 0.004767623653089853\n",
      "Episode 2369; Testing Loss 0.005873460656842732; Training Loss 0.004767610802858792\n",
      "Episode 2370; Testing Loss 0.005873580456693375; Training Loss 0.004767597381707071\n",
      "Episode 2371; Testing Loss 0.005873631170513532; Training Loss 0.00476758803592883\n",
      "Episode 2372; Testing Loss 0.0058735869811598175; Training Loss 0.004767575763599585\n",
      "Episode 2373; Testing Loss 0.005873547806675652; Training Loss 0.004767566309899428\n",
      "Episode 2374; Testing Loss 0.0058735628638423056; Training Loss 0.0047675543144137635\n",
      "Episode 2375; Testing Loss 0.005873569028755455; Training Loss 0.004767542956306658\n",
      "Episode 2376; Testing Loss 0.005873470696789518; Training Loss 0.004767531976154283\n",
      "Episode 2377; Testing Loss 0.0058733996917640025; Training Loss 0.004767521311506063\n",
      "Episode 2378; Testing Loss 0.005873451772066456; Training Loss 0.004767508382811182\n",
      "Episode 2379; Testing Loss 0.005873490146051118; Training Loss 0.0047674996582154055\n",
      "Episode 2380; Testing Loss 0.005873357512366944; Training Loss 0.004767488898049212\n",
      "Episode 2381; Testing Loss 0.005873287395487097; Training Loss 0.004767478709217376\n",
      "Episode 2382; Testing Loss 0.005873387145664734; Training Loss 0.0047674651554223705\n",
      "Episode 2383; Testing Loss 0.005873474854786129; Training Loss 0.004767451999111956\n",
      "Episode 2384; Testing Loss 0.005873452472092497; Training Loss 0.0047674420527648465\n",
      "Episode 2385; Testing Loss 0.005873432554929103; Training Loss 0.004767430315085181\n",
      "Episode 2386; Testing Loss 0.005873473333146434; Training Loss 0.004767419765714828\n",
      "Episode 2387; Testing Loss 0.005873488478546372; Training Loss 0.004767408543268822\n",
      "Episode 2388; Testing Loss 0.005873388485059993; Training Loss 0.004767396333458457\n",
      "Episode 2389; Testing Loss 0.005873363894620832; Training Loss 0.00476738512198054\n",
      "Episode 2390; Testing Loss 0.005873414386341436; Training Loss 0.0047673736554282\n",
      "Episode 2391; Testing Loss 0.005873446012726623; Training Loss 0.004767362780933924\n",
      "Episode 2392; Testing Loss 0.005873344613114212; Training Loss 0.004767351033533456\n",
      "Episode 2393; Testing Loss 0.005873289601534173; Training Loss 0.0047673413282979605\n",
      "Episode 2394; Testing Loss 0.005873379133524516; Training Loss 0.004767329761831753\n",
      "Episode 2395; Testing Loss 0.005873368410910159; Training Loss 0.004767318277677294\n",
      "Episode 2396; Testing Loss 0.005873230247129996; Training Loss 0.004767308483927957\n",
      "Episode 2397; Testing Loss 0.0058732177189814135; Training Loss 0.004767296973353612\n",
      "Episode 2398; Testing Loss 0.0058733259435842355; Training Loss 0.004767286443208454\n",
      "Episode 2399; Testing Loss 0.005873371211402937; Training Loss 0.004767275170255171\n",
      "Episode 2400; Testing Loss 0.00587327026469179; Training Loss 0.004767262168262921\n",
      "Episode 2401; Testing Loss 0.005873148912695448; Training Loss 0.004767253884938479\n",
      "Episode 2402; Testing Loss 0.005873144170780798; Training Loss 0.004767242674131086\n",
      "Episode 2403; Testing Loss 0.005873270622940414; Training Loss 0.004767231050764711\n",
      "Episode 2404; Testing Loss 0.005873335930875584; Training Loss 0.004767219740546894\n",
      "Episode 2405; Testing Loss 0.005873215165370796; Training Loss 0.004767206099457162\n",
      "Episode 2406; Testing Loss 0.005873160222506813; Training Loss 0.0047671965306599695\n",
      "Episode 2407; Testing Loss 0.005873237739892243; Training Loss 0.004767185509899599\n",
      "Episode 2408; Testing Loss 0.005873260328328467; Training Loss 0.00476717436240751\n",
      "Episode 2409; Testing Loss 0.005873182029603852; Training Loss 0.004767162967561132\n",
      "Episode 2410; Testing Loss 0.005873173113222497; Training Loss 0.004767151247787636\n",
      "Episode 2411; Testing Loss 0.005873253477507556; Training Loss 0.0047671388734752\n",
      "Episode 2412; Testing Loss 0.005873323164355323; Training Loss 0.0047671290442351496\n",
      "Episode 2413; Testing Loss 0.005873259794492655; Training Loss 0.004767117587714467\n",
      "Episode 2414; Testing Loss 0.005873153551126987; Training Loss 0.004767106211841351\n",
      "Episode 2415; Testing Loss 0.005873172092545849; Training Loss 0.004767095133279609\n",
      "Episode 2416; Testing Loss 0.005873231095348658; Training Loss 0.004767083815259861\n",
      "Episode 2417; Testing Loss 0.005873163928728008; Training Loss 0.004767073537600118\n",
      "Episode 2418; Testing Loss 0.0058729975411881165; Training Loss 0.004767063250587632\n",
      "Episode 2419; Testing Loss 0.005872981381863083; Training Loss 0.00476705113702502\n",
      "Episode 2420; Testing Loss 0.005873102197458476; Training Loss 0.004767039126299814\n",
      "Episode 2421; Testing Loss 0.005873220443706663; Training Loss 0.004767029150498269\n",
      "Episode 2422; Testing Loss 0.005873108453373274; Training Loss 0.0047670165097644344\n",
      "Episode 2423; Testing Loss 0.005873003862933852; Training Loss 0.004767006236776484\n",
      "Episode 2424; Testing Loss 0.005873074808327283; Training Loss 0.004766993378118773\n",
      "Episode 2425; Testing Loss 0.005873234453174485; Training Loss 0.004766985538754478\n",
      "Episode 2426; Testing Loss 0.00587316348739194; Training Loss 0.004766973385409432\n",
      "Episode 2427; Testing Loss 0.0058729916125191415; Training Loss 0.004766960564961872\n",
      "Episode 2428; Testing Loss 0.005872992578986594; Training Loss 0.0047669504230775225\n",
      "Episode 2429; Testing Loss 0.005873077201424299; Training Loss 0.004766940155046174\n",
      "Episode 2430; Testing Loss 0.005873038951190382; Training Loss 0.00476692791551326\n",
      "Episode 2431; Testing Loss 0.005872964376969446; Training Loss 0.004766916933875513\n",
      "Episode 2432; Testing Loss 0.005872990460433833; Training Loss 0.00476690664864602\n",
      "Episode 2433; Testing Loss 0.00587301907690014; Training Loss 0.004766893680907852\n",
      "Episode 2434; Testing Loss 0.005872988804715899; Training Loss 0.004766882314139936\n",
      "Episode 2435; Testing Loss 0.005873006534624251; Training Loss 0.004766872786853581\n",
      "Episode 2436; Testing Loss 0.005873035845628744; Training Loss 0.0047668615133937605\n",
      "Episode 2437; Testing Loss 0.005873029827290248; Training Loss 0.0047668488937111855\n",
      "Episode 2438; Testing Loss 0.005873031140559813; Training Loss 0.004766839147945797\n",
      "Episode 2439; Testing Loss 0.0058729667413182154; Training Loss 0.004766829315484694\n",
      "Episode 2440; Testing Loss 0.005872899713149512; Training Loss 0.004766816736133588\n",
      "Episode 2441; Testing Loss 0.005872893574717164; Training Loss 0.0047668068198931865\n",
      "Episode 2442; Testing Loss 0.005872939421890086; Training Loss 0.0047667973153830015\n",
      "Episode 2443; Testing Loss 0.005872944352732056; Training Loss 0.004766785781684098\n",
      "Episode 2444; Testing Loss 0.005872898315290863; Training Loss 0.004766772512626845\n",
      "Episode 2445; Testing Loss 0.0058729104297481705; Training Loss 0.004766760256464723\n",
      "Episode 2446; Testing Loss 0.005872921182668977; Training Loss 0.004766750217840624\n",
      "Episode 2447; Testing Loss 0.005872883006119812; Training Loss 0.0047667391579594865\n",
      "Episode 2448; Testing Loss 0.0058728990110670605; Training Loss 0.004766727166861734\n",
      "Episode 2449; Testing Loss 0.005872962358991213; Training Loss 0.0047667162216375415\n",
      "Episode 2450; Testing Loss 0.00587295930512286; Training Loss 0.004766706618086644\n",
      "Episode 2451; Testing Loss 0.005872856087488786; Training Loss 0.004766694132291149\n",
      "Episode 2452; Testing Loss 0.00587275122257185; Training Loss 0.004766684573253355\n",
      "Episode 2453; Testing Loss 0.005872757035643506; Training Loss 0.004766674147136642\n",
      "Episode 2454; Testing Loss 0.0058728171488591375; Training Loss 0.004766662330765961\n",
      "Episode 2455; Testing Loss 0.005872770707724922; Training Loss 0.004766650143787166\n",
      "Episode 2456; Testing Loss 0.005872750160659028; Training Loss 0.004766638907724587\n",
      "Episode 2457; Testing Loss 0.0058727812699299895; Training Loss 0.004766628232932511\n",
      "Episode 2458; Testing Loss 0.005872757480133817; Training Loss 0.004766616269638164\n",
      "Episode 2459; Testing Loss 0.005872731588569684; Training Loss 0.004766605920683349\n",
      "Episode 2460; Testing Loss 0.005872818141868925; Training Loss 0.004766595301795068\n",
      "Episode 2461; Testing Loss 0.005872871237435859; Training Loss 0.004766584343000829\n",
      "Episode 2462; Testing Loss 0.005872837038161046; Training Loss 0.004766572247432258\n",
      "Episode 2463; Testing Loss 0.005872711436545436; Training Loss 0.004766561187494578\n",
      "Episode 2464; Testing Loss 0.005872680790670124; Training Loss 0.00476655087599952\n",
      "Episode 2465; Testing Loss 0.0058726953493742975; Training Loss 0.004766538363927006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2466; Testing Loss 0.005872707073745337; Training Loss 0.004766530008726855\n",
      "Episode 2467; Testing Loss 0.005872695543616235; Training Loss 0.004766519549576865\n",
      "Episode 2468; Testing Loss 0.005872675926531519; Training Loss 0.004766507656406202\n",
      "Episode 2469; Testing Loss 0.005872623633486755; Training Loss 0.004766496236288431\n",
      "Episode 2470; Testing Loss 0.005872604106358384; Training Loss 0.004766483342188295\n",
      "Episode 2471; Testing Loss 0.005872671399848265; Training Loss 0.004766474326331219\n",
      "Episode 2472; Testing Loss 0.005872668481434582; Training Loss 0.004766463350257825\n",
      "Episode 2473; Testing Loss 0.0058726121587426355; Training Loss 0.004766453317975734\n",
      "Episode 2474; Testing Loss 0.005872622199752556; Training Loss 0.004766441604126578\n",
      "Episode 2475; Testing Loss 0.005872724582593377; Training Loss 0.004766429155904946\n",
      "Episode 2476; Testing Loss 0.0058727232048436636; Training Loss 0.00476642015620774\n",
      "Episode 2477; Testing Loss 0.005872590267140941; Training Loss 0.004766408987810169\n",
      "Episode 2478; Testing Loss 0.005872499131483023; Training Loss 0.004766398104936644\n",
      "Episode 2479; Testing Loss 0.005872574110739177; Training Loss 0.00476638501450802\n",
      "Episode 2480; Testing Loss 0.0058726116612327324; Training Loss 0.004766375458435771\n",
      "Episode 2481; Testing Loss 0.0058725876500440215; Training Loss 0.00476636549778534\n",
      "Episode 2482; Testing Loss 0.0058725434409315; Training Loss 0.004766353411429487\n",
      "Episode 2483; Testing Loss 0.005872497547614658; Training Loss 0.004766340443367524\n",
      "Episode 2484; Testing Loss 0.005872525773446688; Training Loss 0.004766330836887807\n",
      "Episode 2485; Testing Loss 0.005872549151210179; Training Loss 0.004766319274488886\n",
      "Episode 2486; Testing Loss 0.00587258111084284; Training Loss 0.004766307616147516\n",
      "Episode 2487; Testing Loss 0.005872613562459911; Training Loss 0.004766296686420302\n",
      "Episode 2488; Testing Loss 0.005872575441716206; Training Loss 0.004766285596422741\n",
      "Episode 2489; Testing Loss 0.005872515709308846; Training Loss 0.00476627475082509\n",
      "Episode 2490; Testing Loss 0.0058725142275839574; Training Loss 0.004766262101297575\n",
      "Episode 2491; Testing Loss 0.005872522182113354; Training Loss 0.004766251628828638\n",
      "Episode 2492; Testing Loss 0.00587242692759856; Training Loss 0.004766240356525591\n",
      "Episode 2493; Testing Loss 0.005872374936013547; Training Loss 0.004766228135451299\n",
      "Episode 2494; Testing Loss 0.005872405410298078; Training Loss 0.004766217923859459\n",
      "Episode 2495; Testing Loss 0.005872514786927086; Training Loss 0.004766207682600107\n",
      "Episode 2496; Testing Loss 0.005872504858020016; Training Loss 0.004766195213235403\n",
      "Episode 2497; Testing Loss 0.005872376032887086; Training Loss 0.004766185232144453\n",
      "Episode 2498; Testing Loss 0.0058723118903317205; Training Loss 0.004766174738838974\n",
      "Episode 2499; Testing Loss 0.005872391226348562; Training Loss 0.004766162254866051\n",
      "Episode 2500; Testing Loss 0.005872449744264852; Training Loss 0.004766150337371707\n",
      "Episode 2501; Testing Loss 0.005872422864066088; Training Loss 0.004766138298477486\n",
      "Episode 2502; Testing Loss 0.0058723688038803115; Training Loss 0.004766129235917011\n",
      "Episode 2503; Testing Loss 0.005872366319804662; Training Loss 0.004766118477092471\n",
      "Episode 2504; Testing Loss 0.005872400921358234; Training Loss 0.004766105457191224\n",
      "Episode 2505; Testing Loss 0.005872400375015272; Training Loss 0.004766096197768697\n",
      "Episode 2506; Testing Loss 0.00587229781199652; Training Loss 0.004766085003002987\n",
      "Episode 2507; Testing Loss 0.005872213489405254; Training Loss 0.004766074258858387\n",
      "Episode 2508; Testing Loss 0.005872262626967698; Training Loss 0.004766061011412853\n",
      "Episode 2509; Testing Loss 0.005872395577077349; Training Loss 0.004766049907167155\n",
      "Episode 2510; Testing Loss 0.005872397751047672; Training Loss 0.004766040410863212\n",
      "Episode 2511; Testing Loss 0.005872319563223296; Training Loss 0.004766027378156142\n",
      "Episode 2512; Testing Loss 0.005872295823749201; Training Loss 0.004766016284220236\n",
      "Episode 2513; Testing Loss 0.005872249568418946; Training Loss 0.004766006124298806\n",
      "Episode 2514; Testing Loss 0.005872228796324562; Training Loss 0.0047659936275045705\n",
      "Episode 2515; Testing Loss 0.005872242623082148; Training Loss 0.0047659819604703784\n",
      "Episode 2516; Testing Loss 0.005872301886422318; Training Loss 0.004765970968473508\n",
      "Episode 2517; Testing Loss 0.0058722980061784716; Training Loss 0.004765959781304253\n",
      "Episode 2518; Testing Loss 0.005872205029220044; Training Loss 0.0047659486837761675\n",
      "Episode 2519; Testing Loss 0.005872130410403571; Training Loss 0.004765937784440096\n",
      "Episode 2520; Testing Loss 0.0058721958021492335; Training Loss 0.0047659255876081035\n",
      "Episode 2521; Testing Loss 0.005872222373094203; Training Loss 0.004765913651625375\n",
      "Episode 2522; Testing Loss 0.005872197304101967; Training Loss 0.004765902286953582\n",
      "Episode 2523; Testing Loss 0.005872123348122648; Training Loss 0.004765891082024099\n",
      "Episode 2524; Testing Loss 0.005872102480446236; Training Loss 0.004765879726667179\n",
      "Episode 2525; Testing Loss 0.005872128134314266; Training Loss 0.004765869587346691\n",
      "Episode 2526; Testing Loss 0.0058721769552128435; Training Loss 0.004765859415293223\n",
      "Episode 2527; Testing Loss 0.005872209762983974; Training Loss 0.004765846878248497\n",
      "Episode 2528; Testing Loss 0.005872174200112843; Training Loss 0.004765837225458427\n",
      "Episode 2529; Testing Loss 0.005872081309883596; Training Loss 0.0047658264215651265\n",
      "Episode 2530; Testing Loss 0.005872049543354058; Training Loss 0.004765813653080192\n",
      "Episode 2531; Testing Loss 0.005872109522181436; Training Loss 0.004765803335659654\n",
      "Episode 2532; Testing Loss 0.0058721858681999375; Training Loss 0.004765794808531024\n",
      "Episode 2533; Testing Loss 0.00587219914881585; Training Loss 0.004765782545279763\n",
      "Episode 2534; Testing Loss 0.005872125398886967; Training Loss 0.004765771651306555\n",
      "Episode 2535; Testing Loss 0.0058720672664855445; Training Loss 0.004765761365344726\n",
      "Episode 2536; Testing Loss 0.005872094549339775; Training Loss 0.004765748753852679\n",
      "Episode 2537; Testing Loss 0.005872128263303419; Training Loss 0.004765736512016102\n",
      "Episode 2538; Testing Loss 0.00587212824773683; Training Loss 0.004765725346030267\n",
      "Episode 2539; Testing Loss 0.005872076796498891; Training Loss 0.004765714496930747\n",
      "Episode 2540; Testing Loss 0.0058720771596843056; Training Loss 0.004765702677494082\n",
      "Episode 2541; Testing Loss 0.005872095670022307; Training Loss 0.00476569070104678\n",
      "Episode 2542; Testing Loss 0.005872025733675438; Training Loss 0.004765679862402076\n",
      "Episode 2543; Testing Loss 0.005871993641030078; Training Loss 0.004765668016390777\n",
      "Episode 2544; Testing Loss 0.005872079080643429; Training Loss 0.00476565677982986\n",
      "Episode 2545; Testing Loss 0.0058721408473210845; Training Loss 0.004765646151130221\n",
      "Episode 2546; Testing Loss 0.005872102042547278; Training Loss 0.004765634183326503\n",
      "Episode 2547; Testing Loss 0.005872023639929154; Training Loss 0.00476562395921392\n",
      "Episode 2548; Testing Loss 0.005871992379240256; Training Loss 0.004765612125889851\n",
      "Episode 2549; Testing Loss 0.005872060022136331; Training Loss 0.004765599928892305\n",
      "Episode 2550; Testing Loss 0.0058720544298430525; Training Loss 0.0047655902722445595\n",
      "Episode 2551; Testing Loss 0.005871957055014248; Training Loss 0.004765578634342988\n",
      "Episode 2552; Testing Loss 0.0058718478390695755; Training Loss 0.004765567859100738\n",
      "Episode 2553; Testing Loss 0.0058718791310166975; Training Loss 0.0047655569638339005\n",
      "Episode 2554; Testing Loss 0.005871990355588974; Training Loss 0.0047655458358755965\n",
      "Episode 2555; Testing Loss 0.005872002403802422; Training Loss 0.004765532915227935\n",
      "Episode 2556; Testing Loss 0.0058719205398546715; Training Loss 0.004765521310037879\n",
      "Episode 2557; Testing Loss 0.0058718821776872505; Training Loss 0.004765512246005143\n",
      "Episode 2558; Testing Loss 0.005872021227698102; Training Loss 0.004765499844671099\n",
      "Episode 2559; Testing Loss 0.005872130727481; Training Loss 0.004765491970765252\n",
      "Episode 2560; Testing Loss 0.005871973182223982; Training Loss 0.004765477863480805\n",
      "Episode 2561; Testing Loss 0.005871787221383461; Training Loss 0.004765468213822873\n",
      "Episode 2562; Testing Loss 0.0058718643810471375; Training Loss 0.004765455878546457\n",
      "Episode 2563; Testing Loss 0.005872047883263857; Training Loss 0.004765445556310478\n",
      "Episode 2564; Testing Loss 0.005871979171799047; Training Loss 0.004765431030165769\n",
      "Episode 2565; Testing Loss 0.005871770246580797; Training Loss 0.004765421882944173\n",
      "Episode 2566; Testing Loss 0.005871808224717761; Training Loss 0.0047654111801802975\n",
      "Episode 2567; Testing Loss 0.00587199528670568; Training Loss 0.004765398634539508\n",
      "Episode 2568; Testing Loss 0.005872044781447108; Training Loss 0.00476538749604337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2569; Testing Loss 0.005871892807354131; Training Loss 0.004765377785756099\n",
      "Episode 2570; Testing Loss 0.005871840874081597; Training Loss 0.004765368130828558\n",
      "Episode 2571; Testing Loss 0.0058719931579343555; Training Loss 0.004765352630344731\n",
      "Episode 2572; Testing Loss 0.005872116846130132; Training Loss 0.004765342326412747\n",
      "Episode 2573; Testing Loss 0.005871969230862763; Training Loss 0.004765329659414583\n",
      "Episode 2574; Testing Loss 0.005871764163932016; Training Loss 0.004765320191052486\n",
      "Episode 2575; Testing Loss 0.005871829182567018; Training Loss 0.004765306970914618\n",
      "Episode 2576; Testing Loss 0.005872027223086761; Training Loss 0.004765295149223692\n",
      "Episode 2577; Testing Loss 0.005872038110166801; Training Loss 0.004765285464197508\n",
      "Episode 2578; Testing Loss 0.005871858309655184; Training Loss 0.004765273275933069\n",
      "Episode 2579; Testing Loss 0.005871746441918267; Training Loss 0.004765262044351037\n",
      "Episode 2580; Testing Loss 0.005871846062507339; Training Loss 0.00476524862277515\n",
      "Episode 2581; Testing Loss 0.005871922074123762; Training Loss 0.004765238579010754\n",
      "Episode 2582; Testing Loss 0.00587186360357412; Training Loss 0.0047652250490913\n",
      "Episode 2583; Testing Loss 0.005871754964987284; Training Loss 0.0047652163677227395\n",
      "Episode 2584; Testing Loss 0.0058718422242799466; Training Loss 0.004765204480805402\n",
      "Episode 2585; Testing Loss 0.005871972943758347; Training Loss 0.004765192636561745\n",
      "Episode 2586; Testing Loss 0.005871957739081486; Training Loss 0.004765180681559571\n",
      "Episode 2587; Testing Loss 0.005871833364795375; Training Loss 0.004765170256409108\n",
      "Episode 2588; Testing Loss 0.005871814690779283; Training Loss 0.004765158138624736\n",
      "Episode 2589; Testing Loss 0.005871908956584403; Training Loss 0.004765146763368502\n",
      "Episode 2590; Testing Loss 0.005871891092063812; Training Loss 0.004765135450155946\n",
      "Episode 2591; Testing Loss 0.005871731849857525; Training Loss 0.00476512316813241\n",
      "Episode 2592; Testing Loss 0.005871645399076403; Training Loss 0.004765114645319487\n",
      "Episode 2593; Testing Loss 0.0058717937110629795; Training Loss 0.004765101123782755\n",
      "Episode 2594; Testing Loss 0.005871899183144153; Training Loss 0.004765090315241359\n",
      "Episode 2595; Testing Loss 0.00587177449135776; Training Loss 0.004765078417032097\n",
      "Episode 2596; Testing Loss 0.0058717302966036266; Training Loss 0.004765068203396317\n",
      "Episode 2597; Testing Loss 0.005871814037724322; Training Loss 0.004765054331726949\n",
      "Episode 2598; Testing Loss 0.005871899600572668; Training Loss 0.00476504394228135\n",
      "Episode 2599; Testing Loss 0.0058719040155964936; Training Loss 0.004765032296442823\n",
      "Episode 2600; Testing Loss 0.005871850740696855; Training Loss 0.004765019920451916\n",
      "Episode 2601; Testing Loss 0.00587179275079491; Training Loss 0.004765010464259049\n",
      "Episode 2602; Testing Loss 0.00587179780232468; Training Loss 0.004764998618518402\n",
      "Episode 2603; Testing Loss 0.005871776819763309; Training Loss 0.004764988208460854\n",
      "Episode 2604; Testing Loss 0.005871712758087555; Training Loss 0.004764977378474482\n",
      "Episode 2605; Testing Loss 0.00587171861891912; Training Loss 0.004764965532032882\n",
      "Episode 2606; Testing Loss 0.005871754884677637; Training Loss 0.004764953558017206\n",
      "Episode 2607; Testing Loss 0.0058717894004704585; Training Loss 0.004764942262490298\n",
      "Episode 2608; Testing Loss 0.005871749270409973; Training Loss 0.004764929811760295\n",
      "Episode 2609; Testing Loss 0.0058717116177648175; Training Loss 0.004764918281281089\n",
      "Episode 2610; Testing Loss 0.0058717905823451755; Training Loss 0.004764906823568502\n",
      "Episode 2611; Testing Loss 0.005871889581754327; Training Loss 0.00476489658376284\n",
      "Episode 2612; Testing Loss 0.0058718276857484255; Training Loss 0.004764886332676234\n",
      "Episode 2613; Testing Loss 0.005871727784042689; Training Loss 0.004764875696874985\n",
      "Episode 2614; Testing Loss 0.005871757117114491; Training Loss 0.004764862177624407\n",
      "Episode 2615; Testing Loss 0.005871821874218325; Training Loss 0.004764852233153092\n",
      "Episode 2616; Testing Loss 0.005871783552952643; Training Loss 0.004764841365817639\n",
      "Episode 2617; Testing Loss 0.005871700747649292; Training Loss 0.004764829707621927\n",
      "Episode 2618; Testing Loss 0.005871723071990585; Training Loss 0.004764816994757608\n",
      "Episode 2619; Testing Loss 0.0058717860386738145; Training Loss 0.004764807208379247\n",
      "Episode 2620; Testing Loss 0.0058717597911842785; Training Loss 0.004764794817686346\n",
      "Episode 2621; Testing Loss 0.005871697950041437; Training Loss 0.0047647820805677065\n",
      "Episode 2622; Testing Loss 0.0058717437306471925; Training Loss 0.004764772853660574\n",
      "Episode 2623; Testing Loss 0.005871843399884358; Training Loss 0.004764761935560575\n",
      "Episode 2624; Testing Loss 0.0058717948495893724; Training Loss 0.00476474878280318\n",
      "Episode 2625; Testing Loss 0.005871711098270047; Training Loss 0.004764735681241558\n",
      "Episode 2626; Testing Loss 0.005871618293243661; Training Loss 0.004764725279686311\n",
      "Episode 2627; Testing Loss 0.005871638458672337; Training Loss 0.004764713675164371\n",
      "Episode 2628; Testing Loss 0.005871745450951696; Training Loss 0.004764703334623782\n",
      "Episode 2629; Testing Loss 0.005871774859287928; Training Loss 0.004764691666862029\n",
      "Episode 2630; Testing Loss 0.005871679572787446; Training Loss 0.004764679796748512\n",
      "Episode 2631; Testing Loss 0.0058715646837686165; Training Loss 0.004764669109253392\n",
      "Episode 2632; Testing Loss 0.005871569441025648; Training Loss 0.004764658373493445\n",
      "Episode 2633; Testing Loss 0.0058716574927577475; Training Loss 0.0047646455927310444\n",
      "Episode 2634; Testing Loss 0.0058716930088687355; Training Loss 0.004764635077584645\n",
      "Episode 2635; Testing Loss 0.005871584207681688; Training Loss 0.004764623417893167\n",
      "Episode 2636; Testing Loss 0.0058715205742590725; Training Loss 0.004764611780252875\n",
      "Episode 2637; Testing Loss 0.005871556505093046; Training Loss 0.004764602296790994\n",
      "Episode 2638; Testing Loss 0.005871641775592636; Training Loss 0.004764590396181349\n",
      "Episode 2639; Testing Loss 0.00587166026282922; Training Loss 0.004764578510137937\n",
      "Episode 2640; Testing Loss 0.005871616820905928; Training Loss 0.004764568204858926\n",
      "Episode 2641; Testing Loss 0.005871563519255759; Training Loss 0.004764557492352823\n",
      "Episode 2642; Testing Loss 0.005871574343095253; Training Loss 0.00476454495678654\n",
      "Episode 2643; Testing Loss 0.005871631634301099; Training Loss 0.004764532488531225\n",
      "Episode 2644; Testing Loss 0.005871580076998802; Training Loss 0.004764524367913297\n",
      "Episode 2645; Testing Loss 0.00587152496638961; Training Loss 0.00476451434036935\n",
      "Episode 2646; Testing Loss 0.005871524337740016; Training Loss 0.0047645006522675625\n",
      "Episode 2647; Testing Loss 0.005871571040949867; Training Loss 0.004764487941307892\n",
      "Episode 2648; Testing Loss 0.005871614547101307; Training Loss 0.004764479596393708\n",
      "Episode 2649; Testing Loss 0.005871556880864863; Training Loss 0.004764470086179711\n",
      "Episode 2650; Testing Loss 0.005871510664226303; Training Loss 0.004764457417050891\n",
      "Episode 2651; Testing Loss 0.005871519768085631; Training Loss 0.004764442989982629\n",
      "Episode 2652; Testing Loss 0.005871545905670409; Training Loss 0.004764432358085245\n",
      "Episode 2653; Testing Loss 0.0058715047822541395; Training Loss 0.004764424904685288\n",
      "Episode 2654; Testing Loss 0.005871484867335575; Training Loss 0.004764414941622099\n",
      "Episode 2655; Testing Loss 0.005871500956666995; Training Loss 0.004764401169969984\n",
      "Episode 2656; Testing Loss 0.005871526360419077; Training Loss 0.00476438548289875\n",
      "Episode 2657; Testing Loss 0.005871536865781133; Training Loss 0.004764375431014616\n",
      "Episode 2658; Testing Loss 0.005871492917590178; Training Loss 0.004764366642513236\n",
      "Episode 2659; Testing Loss 0.005871476379818958; Training Loss 0.004764353434774153\n",
      "Episode 2660; Testing Loss 0.0058714846802345; Training Loss 0.004764341885403625\n",
      "Episode 2661; Testing Loss 0.005871428949383629; Training Loss 0.004764329710930556\n",
      "Episode 2662; Testing Loss 0.005871403589593064; Training Loss 0.004764319509691569\n",
      "Episode 2663; Testing Loss 0.005871420655624722; Training Loss 0.004764305269476462\n",
      "Episode 2664; Testing Loss 0.005871435380660701; Training Loss 0.0047642965206878135\n",
      "Episode 2665; Testing Loss 0.005871432618186193; Training Loss 0.004764287184018227\n",
      "Episode 2666; Testing Loss 0.005871410301104346; Training Loss 0.004764275447237387\n",
      "Episode 2667; Testing Loss 0.005871403572598766; Training Loss 0.004764261221600211\n",
      "Episode 2668; Testing Loss 0.005871438544417778; Training Loss 0.0047642511338838855\n",
      "Episode 2669; Testing Loss 0.0058714733700248704; Training Loss 0.004764241395610269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2670; Testing Loss 0.005871448324718005; Training Loss 0.004764231585934774\n",
      "Episode 2671; Testing Loss 0.0058714100855476745; Training Loss 0.004764218108658194\n",
      "Episode 2672; Testing Loss 0.005871388018285968; Training Loss 0.004764206061419434\n",
      "Episode 2673; Testing Loss 0.0058713231680429575; Training Loss 0.004764194619791844\n",
      "Episode 2674; Testing Loss 0.0058712565023801054; Training Loss 0.004764184938272303\n",
      "Episode 2675; Testing Loss 0.005871242562134861; Training Loss 0.0047641730190162036\n",
      "Episode 2676; Testing Loss 0.005871251238069289; Training Loss 0.004764159151834164\n",
      "Episode 2677; Testing Loss 0.005871253206331699; Training Loss 0.004764150916975172\n",
      "Episode 2678; Testing Loss 0.0058712243746629365; Training Loss 0.0047641409944168284\n",
      "Episode 2679; Testing Loss 0.005871205313597489; Training Loss 0.004764127959389205\n",
      "Episode 2680; Testing Loss 0.005871241629636449; Training Loss 0.004764113838550715\n",
      "Episode 2681; Testing Loss 0.005871320202884727; Training Loss 0.004764104749148576\n",
      "Episode 2682; Testing Loss 0.005871315351109005; Training Loss 0.004764094352245652\n",
      "Episode 2683; Testing Loss 0.005871274626756487; Training Loss 0.004764081932525168\n",
      "Episode 2684; Testing Loss 0.005871288770702295; Training Loss 0.0047640723414204495\n",
      "Episode 2685; Testing Loss 0.005871301476685327; Training Loss 0.004764058259000232\n",
      "Episode 2686; Testing Loss 0.00587127936069545; Training Loss 0.004764048434122506\n",
      "Episode 2687; Testing Loss 0.005871243976357295; Training Loss 0.0047640378390320995\n",
      "Episode 2688; Testing Loss 0.00587121460905181; Training Loss 0.004764029336135312\n",
      "Episode 2689; Testing Loss 0.005871226352752518; Training Loss 0.004764017053732477\n",
      "Episode 2690; Testing Loss 0.00587123798297948; Training Loss 0.004764002445079167\n",
      "Episode 2691; Testing Loss 0.005871210097888933; Training Loss 0.0047639913951179344\n",
      "Episode 2692; Testing Loss 0.0058711916174557545; Training Loss 0.004763978073790958\n",
      "Episode 2693; Testing Loss 0.005871226376572348; Training Loss 0.004763966756158422\n",
      "Episode 2694; Testing Loss 0.005871261379867605; Training Loss 0.004763958119214341\n",
      "Episode 2695; Testing Loss 0.005871281519897498; Training Loss 0.004763948545309893\n",
      "Episode 2696; Testing Loss 0.005871266759264244; Training Loss 0.0047639337956836385\n",
      "Episode 2697; Testing Loss 0.005871226580363548; Training Loss 0.004763921424251699\n",
      "Episode 2698; Testing Loss 0.005871176603744146; Training Loss 0.004763912043503923\n",
      "Episode 2699; Testing Loss 0.005871132487596285; Training Loss 0.00476390081140238\n",
      "Episode 2700; Testing Loss 0.005871120067373248; Training Loss 0.004763887677458568\n",
      "Episode 2701; Testing Loss 0.005871154507383802; Training Loss 0.004763875783847679\n",
      "Episode 2702; Testing Loss 0.005871173078847239; Training Loss 0.004763867565073348\n",
      "Episode 2703; Testing Loss 0.005871101273911439; Training Loss 0.004763855794518946\n",
      "Episode 2704; Testing Loss 0.005871074407773021; Training Loss 0.004763843970776411\n",
      "Episode 2705; Testing Loss 0.00587116653901842; Training Loss 0.004763829792872242\n",
      "Episode 2706; Testing Loss 0.005871246881408308; Training Loss 0.004763820074947156\n",
      "Episode 2707; Testing Loss 0.005871272697495055; Training Loss 0.004763809560937856\n",
      "Episode 2708; Testing Loss 0.005871228289878496; Training Loss 0.0047637994492763895\n",
      "Episode 2709; Testing Loss 0.005871143778377645; Training Loss 0.004763787367444267\n",
      "Episode 2710; Testing Loss 0.005871097778401868; Training Loss 0.004763773768048047\n",
      "Episode 2711; Testing Loss 0.005871122133631601; Training Loss 0.004763764731099648\n",
      "Episode 2712; Testing Loss 0.005871117975662063; Training Loss 0.004763755208819651\n",
      "Episode 2713; Testing Loss 0.005871076452248629; Training Loss 0.00476374221248036\n",
      "Episode 2714; Testing Loss 0.005871055732956281; Training Loss 0.004763730434196368\n",
      "Episode 2715; Testing Loss 0.0058710551406960096; Training Loss 0.004763720665724429\n",
      "Episode 2716; Testing Loss 0.005871113709677995; Training Loss 0.004763709697206245\n",
      "Episode 2717; Testing Loss 0.005871159206747344; Training Loss 0.004763697146275282\n",
      "Episode 2718; Testing Loss 0.005871146810729484; Training Loss 0.004763685684885426\n",
      "Episode 2719; Testing Loss 0.005871153478434904; Training Loss 0.00476367448213745\n",
      "Episode 2720; Testing Loss 0.005871171435979498; Training Loss 0.004763663792721226\n",
      "Episode 2721; Testing Loss 0.005871110954215083; Training Loss 0.0047636511719905345\n",
      "Episode 2722; Testing Loss 0.005871028729684176; Training Loss 0.004763640088832755\n",
      "Episode 2723; Testing Loss 0.005871016318284181; Training Loss 0.004763626755462216\n",
      "Episode 2724; Testing Loss 0.005871078863204314; Training Loss 0.004763617613850967\n",
      "Episode 2725; Testing Loss 0.00587115452381696; Training Loss 0.004763608247719823\n",
      "Episode 2726; Testing Loss 0.005871123297028419; Training Loss 0.004763595327481138\n",
      "Episode 2727; Testing Loss 0.005871048775186442; Training Loss 0.00476358368288661\n",
      "Episode 2728; Testing Loss 0.005870979336330817; Training Loss 0.004763572556878858\n",
      "Episode 2729; Testing Loss 0.005870951205411904; Training Loss 0.004763561533433513\n",
      "Episode 2730; Testing Loss 0.0058709761027481155; Training Loss 0.004763547351015837\n",
      "Episode 2731; Testing Loss 0.005870997552432922; Training Loss 0.004763540200145239\n",
      "Episode 2732; Testing Loss 0.0058710238827902415; Training Loss 0.004763530601716353\n",
      "Episode 2733; Testing Loss 0.005871008010668025; Training Loss 0.004763517130966683\n",
      "Episode 2734; Testing Loss 0.005870949701193884; Training Loss 0.004763503213592451\n",
      "Episode 2735; Testing Loss 0.005870907367873832; Training Loss 0.004763492343807692\n",
      "Episode 2736; Testing Loss 0.005870898120964483; Training Loss 0.004763481286542556\n",
      "Episode 2737; Testing Loss 0.005870930680212282; Training Loss 0.0047634683727070136\n",
      "Episode 2738; Testing Loss 0.005870977530304224; Training Loss 0.004763458082385919\n",
      "Episode 2739; Testing Loss 0.005870912098662573; Training Loss 0.004763447120612203\n",
      "Episode 2740; Testing Loss 0.005870832902944281; Training Loss 0.004763436326258133\n",
      "Episode 2741; Testing Loss 0.005870841933914235; Training Loss 0.00476342429910033\n",
      "Episode 2742; Testing Loss 0.005870949913005938; Training Loss 0.004763413890913952\n",
      "Episode 2743; Testing Loss 0.005870913460311585; Training Loss 0.0047634032707707845\n",
      "Episode 2744; Testing Loss 0.005870827492336356; Training Loss 0.004763390571227925\n",
      "Episode 2745; Testing Loss 0.0058708438682133724; Training Loss 0.004763381102717845\n",
      "Episode 2746; Testing Loss 0.005870884886930182; Training Loss 0.0047633711021354466\n",
      "Episode 2747; Testing Loss 0.005870856833196659; Training Loss 0.004763359344063129\n",
      "Episode 2748; Testing Loss 0.0058707925415354975; Training Loss 0.004763346614828617\n",
      "Episode 2749; Testing Loss 0.005870816308749783; Training Loss 0.004763334792719546\n",
      "Episode 2750; Testing Loss 0.005870842701675512; Training Loss 0.004763324125960656\n",
      "Episode 2751; Testing Loss 0.00587079975770431; Training Loss 0.004763312242245886\n",
      "Episode 2752; Testing Loss 0.005870768009475716; Training Loss 0.004763303158580229\n",
      "Episode 2753; Testing Loss 0.00587085585129167; Training Loss 0.004763291570443675\n",
      "Episode 2754; Testing Loss 0.005870913978235391; Training Loss 0.004763280028509042\n",
      "Episode 2755; Testing Loss 0.005870853167895101; Training Loss 0.004763268421186649\n",
      "Episode 2756; Testing Loss 0.005870771575740097; Training Loss 0.004763257392061612\n",
      "Episode 2757; Testing Loss 0.0058707248362618; Training Loss 0.004763248540938651\n",
      "Episode 2758; Testing Loss 0.005870732339699056; Training Loss 0.004763237074448949\n",
      "Episode 2759; Testing Loss 0.005870721220597706; Training Loss 0.00476322516222866\n",
      "Episode 2760; Testing Loss 0.00587071175202737; Training Loss 0.004763214600050014\n",
      "Episode 2761; Testing Loss 0.005870672727024804; Training Loss 0.00476320337564244\n",
      "Episode 2762; Testing Loss 0.005870661138693151; Training Loss 0.004763190452774387\n",
      "Episode 2763; Testing Loss 0.005870700717419145; Training Loss 0.004763179646164804\n",
      "Episode 2764; Testing Loss 0.00587070413739857; Training Loss 0.004763170537453661\n",
      "Episode 2765; Testing Loss 0.00587071851574231; Training Loss 0.004763159118620202\n",
      "Episode 2766; Testing Loss 0.005870744469652997; Training Loss 0.004763146700547231\n",
      "Episode 2767; Testing Loss 0.005870777666896394; Training Loss 0.004763134629603172\n",
      "Episode 2768; Testing Loss 0.00587073321537085; Training Loss 0.004763122865238083\n",
      "Episode 2769; Testing Loss 0.005870636730152056; Training Loss 0.004763109465353414\n",
      "Episode 2770; Testing Loss 0.005870604864206979; Training Loss 0.0047631031746765235\n",
      "Episode 2771; Testing Loss 0.005870657828537688; Training Loss 0.004763093144832983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2772; Testing Loss 0.005870686071939122; Training Loss 0.004763079002010515\n",
      "Episode 2773; Testing Loss 0.005870661283821573; Training Loss 0.004763066622874761\n",
      "Episode 2774; Testing Loss 0.005870603322599292; Training Loss 0.00476305949694785\n",
      "Episode 2775; Testing Loss 0.005870634875877735; Training Loss 0.004763049207280829\n",
      "Episode 2776; Testing Loss 0.005870720597463802; Training Loss 0.004763036815185405\n",
      "Episode 2777; Testing Loss 0.005870742261968576; Training Loss 0.0047630254321001505\n",
      "Episode 2778; Testing Loss 0.005870658765934401; Training Loss 0.004763014041074768\n",
      "Episode 2779; Testing Loss 0.0058705660423991465; Training Loss 0.004763000228562548\n",
      "Episode 2780; Testing Loss 0.005870566211387169; Training Loss 0.004762989945991837\n",
      "Episode 2781; Testing Loss 0.005870541958165564; Training Loss 0.004762981857159225\n",
      "Episode 2782; Testing Loss 0.005870494031415742; Training Loss 0.004762969476770308\n",
      "Episode 2783; Testing Loss 0.0058704888431803754; Training Loss 0.00476295745366158\n",
      "Episode 2784; Testing Loss 0.005870559690954447; Training Loss 0.004762945206269614\n",
      "Episode 2785; Testing Loss 0.005870597551814668; Training Loss 0.004762934363419924\n",
      "Episode 2786; Testing Loss 0.0058705361308985645; Training Loss 0.004762924440265011\n",
      "Episode 2787; Testing Loss 0.00587045743828973; Training Loss 0.004762913477271206\n",
      "Episode 2788; Testing Loss 0.005870484236517633; Training Loss 0.004762901229624529\n",
      "Episode 2789; Testing Loss 0.005870543472580882; Training Loss 0.004762889043284024\n",
      "Episode 2790; Testing Loss 0.005870554006054553; Training Loss 0.004762877125946179\n",
      "Episode 2791; Testing Loss 0.005870512623176626; Training Loss 0.004762867678652246\n",
      "Episode 2792; Testing Loss 0.005870529692851269; Training Loss 0.004762857207775553\n",
      "Episode 2793; Testing Loss 0.0058705451367103475; Training Loss 0.004762845404547558\n",
      "Episode 2794; Testing Loss 0.0058704706333069095; Training Loss 0.00476283356743323\n",
      "Episode 2795; Testing Loss 0.005870395308135468; Training Loss 0.004762820627673046\n",
      "Episode 2796; Testing Loss 0.005870395596251833; Training Loss 0.004762810887386638\n",
      "Episode 2797; Testing Loss 0.005870429055435307; Training Loss 0.0047628005542964405\n",
      "Episode 2798; Testing Loss 0.005870423423370892; Training Loss 0.004762789620793199\n",
      "Episode 2799; Testing Loss 0.005870367782472281; Training Loss 0.004762776804421857\n",
      "Episode 2800; Testing Loss 0.005870317080253013; Training Loss 0.0047627662328963705\n",
      "Episode 2801; Testing Loss 0.00587038250037064; Training Loss 0.004762755152469447\n",
      "Episode 2802; Testing Loss 0.005870436031849928; Training Loss 0.004762745138096771\n",
      "Episode 2803; Testing Loss 0.0058704481001712935; Training Loss 0.0047627327748461035\n",
      "Episode 2804; Testing Loss 0.005870389004846805; Training Loss 0.004762720338693375\n",
      "Episode 2805; Testing Loss 0.005870370865461727; Training Loss 0.004762709742350345\n",
      "Episode 2806; Testing Loss 0.0058703905290879545; Training Loss 0.004762698861564635\n",
      "Episode 2807; Testing Loss 0.005870361961353581; Training Loss 0.004762688443739074\n",
      "Episode 2808; Testing Loss 0.005870308814768269; Training Loss 0.0047626771224755235\n",
      "Episode 2809; Testing Loss 0.005870299341147101; Training Loss 0.004762665545620094\n",
      "Episode 2810; Testing Loss 0.005870285282505664; Training Loss 0.0047626539527695355\n",
      "Episode 2811; Testing Loss 0.0058702703302535; Training Loss 0.004762642777388156\n",
      "Episode 2812; Testing Loss 0.005870268111308755; Training Loss 0.004762631095478697\n",
      "Episode 2813; Testing Loss 0.005870267218111239; Training Loss 0.004762620523116415\n",
      "Episode 2814; Testing Loss 0.005870327542412132; Training Loss 0.004762609502747994\n",
      "Episode 2815; Testing Loss 0.005870338018427498; Training Loss 0.004762598053299253\n",
      "Episode 2816; Testing Loss 0.0058703062097962425; Training Loss 0.004762588559098395\n",
      "Episode 2817; Testing Loss 0.005870251994886596; Training Loss 0.004762577123809484\n",
      "Episode 2818; Testing Loss 0.005870202153535192; Training Loss 0.004762564865942925\n",
      "Episode 2819; Testing Loss 0.005870205099583247; Training Loss 0.004762553789208169\n",
      "Episode 2820; Testing Loss 0.005870223209989587; Training Loss 0.004762543142991993\n",
      "Episode 2821; Testing Loss 0.005870213745915393; Training Loss 0.004762531436508658\n",
      "Episode 2822; Testing Loss 0.005870225262794373; Training Loss 0.004762521925393671\n",
      "Episode 2823; Testing Loss 0.005870192411105543; Training Loss 0.004762510542965441\n",
      "Episode 2824; Testing Loss 0.005870221300820738; Training Loss 0.004762497950535761\n",
      "Episode 2825; Testing Loss 0.005870246037911818; Training Loss 0.004762487688714073\n",
      "Episode 2826; Testing Loss 0.005870283889604176; Training Loss 0.0047624765181542485\n",
      "Episode 2827; Testing Loss 0.005870246895646093; Training Loss 0.004762464279092895\n",
      "Episode 2828; Testing Loss 0.005870087784613243; Training Loss 0.004762453940623527\n",
      "Episode 2829; Testing Loss 0.005870065403010086; Training Loss 0.004762443329296136\n",
      "Episode 2830; Testing Loss 0.005870142612193444; Training Loss 0.004762432268696716\n",
      "Episode 2831; Testing Loss 0.005870151363593268; Training Loss 0.004762420505606197\n",
      "Episode 2832; Testing Loss 0.005870104203980497; Training Loss 0.004762408359275158\n",
      "Episode 2833; Testing Loss 0.005870117551415415; Training Loss 0.0047623983738316325\n",
      "Episode 2834; Testing Loss 0.005870170374012889; Training Loss 0.0047623868975930714\n",
      "Episode 2835; Testing Loss 0.005870176699373436; Training Loss 0.004762375549001801\n",
      "Episode 2836; Testing Loss 0.005870186633212609; Training Loss 0.004762362656740827\n",
      "Episode 2837; Testing Loss 0.00587016045076138; Training Loss 0.004762352200757567\n",
      "Episode 2838; Testing Loss 0.005870163733742199; Training Loss 0.004762341166693882\n",
      "Episode 2839; Testing Loss 0.0058701429517584724; Training Loss 0.004762328861501097\n",
      "Episode 2840; Testing Loss 0.005870085648692297; Training Loss 0.004762319046239502\n",
      "Episode 2841; Testing Loss 0.005870091619007983; Training Loss 0.004762307460343335\n",
      "Episode 2842; Testing Loss 0.005870082066173627; Training Loss 0.004762296095218199\n",
      "Episode 2843; Testing Loss 0.005870053514459713; Training Loss 0.004762284706944903\n",
      "Episode 2844; Testing Loss 0.005870028206584546; Training Loss 0.004762273236490141\n",
      "Episode 2845; Testing Loss 0.005870009901662952; Training Loss 0.004762262718101163\n",
      "Episode 2846; Testing Loss 0.005870053696676762; Training Loss 0.004762250826072443\n",
      "Episode 2847; Testing Loss 0.0058701309655632035; Training Loss 0.004762238945285585\n",
      "Episode 2848; Testing Loss 0.005870114752793141; Training Loss 0.004762226774486794\n",
      "Episode 2849; Testing Loss 0.0058699672781777614; Training Loss 0.0047622171371309386\n",
      "Episode 2850; Testing Loss 0.005869934978984026; Training Loss 0.004762205769752657\n",
      "Episode 2851; Testing Loss 0.005870016301456965; Training Loss 0.004762195158966918\n",
      "Episode 2852; Testing Loss 0.005869985089990024; Training Loss 0.004762183708766977\n",
      "Episode 2853; Testing Loss 0.005869876612757141; Training Loss 0.004762173093991323\n",
      "Episode 2854; Testing Loss 0.005869894758179496; Training Loss 0.004762161428668352\n",
      "Episode 2855; Testing Loss 0.005870026981591914; Training Loss 0.00476214911100369\n",
      "Episode 2856; Testing Loss 0.005870062024649679; Training Loss 0.004762137808240893\n",
      "Episode 2857; Testing Loss 0.005869936604386503; Training Loss 0.004762126103881096\n",
      "Episode 2858; Testing Loss 0.0058698864742981445; Training Loss 0.004762115592362066\n",
      "Episode 2859; Testing Loss 0.005870023354104464; Training Loss 0.004762103114098191\n",
      "Episode 2860; Testing Loss 0.005870044512994008; Training Loss 0.0047620925706260185\n",
      "Episode 2861; Testing Loss 0.005869901120404838; Training Loss 0.004762081201765112\n",
      "Episode 2862; Testing Loss 0.005869787773667877; Training Loss 0.004762072318032052\n",
      "Episode 2863; Testing Loss 0.005869837036111846; Training Loss 0.004762059655689224\n",
      "Episode 2864; Testing Loss 0.005869969562094947; Training Loss 0.004762048039783921\n",
      "Episode 2865; Testing Loss 0.0058699792916379785; Training Loss 0.004762035656793591\n",
      "Episode 2866; Testing Loss 0.0058698902713351445; Training Loss 0.0047620271316106775\n",
      "Episode 2867; Testing Loss 0.0058699295473717186; Training Loss 0.004762016372148877\n",
      "Episode 2868; Testing Loss 0.005869995696264547; Training Loss 0.0047620036583106585\n",
      "Episode 2869; Testing Loss 0.005869952356106323; Training Loss 0.004761990778092237\n",
      "Episode 2870; Testing Loss 0.005869785684498349; Training Loss 0.00476197959023038\n",
      "Episode 2871; Testing Loss 0.005869744776762707; Training Loss 0.004761969345285436\n",
      "Episode 2872; Testing Loss 0.0058698157770344615; Training Loss 0.004761956401677171\n",
      "Episode 2873; Testing Loss 0.005869879928742203; Training Loss 0.004761945453179794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2874; Testing Loss 0.0058698112843332435; Training Loss 0.004761935579335838\n",
      "Episode 2875; Testing Loss 0.005869795112251714; Training Loss 0.004761924186215178\n",
      "Episode 2876; Testing Loss 0.005869896466466894; Training Loss 0.004761913114291373\n",
      "Episode 2877; Testing Loss 0.0058698923719522796; Training Loss 0.004761901961474876\n",
      "Episode 2878; Testing Loss 0.005869794486713324; Training Loss 0.004761888401338593\n",
      "Episode 2879; Testing Loss 0.005869757886005165; Training Loss 0.00476187904184683\n",
      "Episode 2880; Testing Loss 0.0058698112422865874; Training Loss 0.004761868157666108\n",
      "Episode 2881; Testing Loss 0.005869867620091752; Training Loss 0.004761856578306692\n",
      "Episode 2882; Testing Loss 0.0058698065539065794; Training Loss 0.004761842829095551\n",
      "Episode 2883; Testing Loss 0.00586970076450287; Training Loss 0.00476183351236002\n",
      "Episode 2884; Testing Loss 0.005869759558010495; Training Loss 0.004761822300339879\n",
      "Episode 2885; Testing Loss 0.005869843213937971; Training Loss 0.004761813109896389\n",
      "Episode 2886; Testing Loss 0.005869776268435445; Training Loss 0.00476180095349258\n",
      "Episode 2887; Testing Loss 0.005869667179441082; Training Loss 0.004761788376277558\n",
      "Episode 2888; Testing Loss 0.005869682233618161; Training Loss 0.004761774668581852\n",
      "Episode 2889; Testing Loss 0.0058697776947423735; Training Loss 0.004761764522047715\n",
      "Episode 2890; Testing Loss 0.005869800429901617; Training Loss 0.004761753110073377\n",
      "Episode 2891; Testing Loss 0.0058696574287802296; Training Loss 0.004761742058775527\n",
      "Episode 2892; Testing Loss 0.0058696085467472466; Training Loss 0.004761732503499893\n",
      "Episode 2893; Testing Loss 0.005869733794314707; Training Loss 0.004761719276695146\n",
      "Episode 2894; Testing Loss 0.005869810459235318; Training Loss 0.004761709566986574\n",
      "Episode 2895; Testing Loss 0.005869687147807465; Training Loss 0.004761699011530671\n",
      "Episode 2896; Testing Loss 0.005869542961568363; Training Loss 0.004761688745802408\n",
      "Episode 2897; Testing Loss 0.005869562205696554; Training Loss 0.004761675033061162\n",
      "Episode 2898; Testing Loss 0.0058696920395740135; Training Loss 0.004761664147722194\n",
      "Episode 2899; Testing Loss 0.005869728927729719; Training Loss 0.004761654551355393\n",
      "Episode 2900; Testing Loss 0.005869593475151181; Training Loss 0.004761641733657938\n",
      "Episode 2901; Testing Loss 0.005869519871144483; Training Loss 0.004761631022949093\n",
      "Episode 2902; Testing Loss 0.005869660733178352; Training Loss 0.004761618070869208\n",
      "Episode 2903; Testing Loss 0.005869726182714438; Training Loss 0.00476160843459243\n",
      "Episode 2904; Testing Loss 0.0058696070227659235; Training Loss 0.0047615953414230985\n",
      "Episode 2905; Testing Loss 0.00586947997162733; Training Loss 0.004761586383341495\n",
      "Episode 2906; Testing Loss 0.005869541186315314; Training Loss 0.004761575946762462\n",
      "Episode 2907; Testing Loss 0.0058696880841783245; Training Loss 0.00476156454469863\n",
      "Episode 2908; Testing Loss 0.005869670989834964; Training Loss 0.004761551835035986\n",
      "Episode 2909; Testing Loss 0.005869525275662727; Training Loss 0.004761541141688111\n",
      "Episode 2910; Testing Loss 0.005869470329853147; Training Loss 0.004761529619107714\n",
      "Episode 2911; Testing Loss 0.005869554099955402; Training Loss 0.004761519217605903\n",
      "Episode 2912; Testing Loss 0.00586959353183366; Training Loss 0.004761510340878955\n",
      "Episode 2913; Testing Loss 0.005869511709118734; Training Loss 0.004761497790748615\n",
      "Episode 2914; Testing Loss 0.0058694300074945225; Training Loss 0.004761487984358563\n",
      "Episode 2915; Testing Loss 0.005869389600659904; Training Loss 0.004761477670895894\n",
      "Episode 2916; Testing Loss 0.005869426895048877; Training Loss 0.0047614643020430095\n",
      "Episode 2917; Testing Loss 0.005869487310756913; Training Loss 0.0047614533583231885\n",
      "Episode 2918; Testing Loss 0.005869476258285863; Training Loss 0.00476144028295066\n",
      "Episode 2919; Testing Loss 0.0058694705885088076; Training Loss 0.00476143090665981\n",
      "Episode 2920; Testing Loss 0.005869493070299049; Training Loss 0.004761416815088382\n",
      "Episode 2921; Testing Loss 0.005869499959192335; Training Loss 0.004761408440921577\n",
      "Episode 2922; Testing Loss 0.0058694576893237385; Training Loss 0.0047613996800232525\n",
      "Episode 2923; Testing Loss 0.005869416872246781; Training Loss 0.004761388053085282\n",
      "Episode 2924; Testing Loss 0.0058694262732857895; Training Loss 0.004761373909725598\n",
      "Episode 2925; Testing Loss 0.005869462451804869; Training Loss 0.004761364329285687\n",
      "Episode 2926; Testing Loss 0.005869500285184867; Training Loss 0.0047613538647091206\n",
      "Episode 2927; Testing Loss 0.005869453012624513; Training Loss 0.004761341135219127\n",
      "Episode 2928; Testing Loss 0.005869390128375942; Training Loss 0.004761327968032611\n",
      "Episode 2929; Testing Loss 0.005869409483052628; Training Loss 0.00476131805499502\n",
      "Episode 2930; Testing Loss 0.005869479565812949; Training Loss 0.004761306988547755\n",
      "Episode 2931; Testing Loss 0.005869431687572034; Training Loss 0.0047612948327036084\n",
      "Episode 2932; Testing Loss 0.005869333709903548; Training Loss 0.0047612829202200845\n",
      "Episode 2933; Testing Loss 0.005869323163062395; Training Loss 0.004761272621582689\n",
      "Episode 2934; Testing Loss 0.005869336992383719; Training Loss 0.004761262032996216\n",
      "Episode 2935; Testing Loss 0.005869336189082671; Training Loss 0.004761249018410252\n",
      "Episode 2936; Testing Loss 0.0058693299953303055; Training Loss 0.004761239771819736\n",
      "Episode 2937; Testing Loss 0.005869394985625478; Training Loss 0.004761230102408413\n",
      "Episode 2938; Testing Loss 0.005869442727087476; Training Loss 0.004761217419481264\n",
      "Episode 2939; Testing Loss 0.005869360310882417; Training Loss 0.0047612073513716565\n",
      "Episode 2940; Testing Loss 0.005869268683002268; Training Loss 0.0047611971366339385\n",
      "Episode 2941; Testing Loss 0.00586923815043403; Training Loss 0.004761185811751158\n",
      "Episode 2942; Testing Loss 0.00586927793079641; Training Loss 0.004761173171203622\n",
      "Episode 2943; Testing Loss 0.0058693350796508795; Training Loss 0.004761163255544677\n",
      "Episode 2944; Testing Loss 0.005869322188475443; Training Loss 0.004761153338516707\n",
      "Episode 2945; Testing Loss 0.005869217225830216; Training Loss 0.004761140533368805\n",
      "Episode 2946; Testing Loss 0.005869233105762532; Training Loss 0.004761129583280912\n",
      "Episode 2947; Testing Loss 0.005869278003754814; Training Loss 0.004761119416015034\n",
      "Episode 2948; Testing Loss 0.005869305247059869; Training Loss 0.004761107665211188\n",
      "Episode 2949; Testing Loss 0.005869258660548302; Training Loss 0.004761096061905028\n",
      "Episode 2950; Testing Loss 0.0058691486139491; Training Loss 0.004761088143473369\n",
      "Episode 2951; Testing Loss 0.0058691376797487494; Training Loss 0.004761078170755743\n",
      "Episode 2952; Testing Loss 0.005869243535725976; Training Loss 0.0047610655730233225\n",
      "Episode 2953; Testing Loss 0.0058692752555618575; Training Loss 0.0047610548887590245\n",
      "Episode 2954; Testing Loss 0.0058691246941284135; Training Loss 0.004761043022144251\n",
      "Episode 2955; Testing Loss 0.005869056582519574; Training Loss 0.004761034102303287\n",
      "Episode 2956; Testing Loss 0.005869136354449104; Training Loss 0.004761021790765491\n",
      "Episode 2957; Testing Loss 0.00586924721909151; Training Loss 0.004761010141774369\n",
      "Episode 2958; Testing Loss 0.005869207537328039; Training Loss 0.004760998627816953\n",
      "Episode 2959; Testing Loss 0.005869104847845507; Training Loss 0.004760987064035458\n",
      "Episode 2960; Testing Loss 0.005869094574509082; Training Loss 0.004760976559471313\n",
      "Episode 2961; Testing Loss 0.005869131277081979; Training Loss 0.004760966431589787\n",
      "Episode 2962; Testing Loss 0.005869072770752366; Training Loss 0.004760955108823054\n",
      "Episode 2963; Testing Loss 0.005868952897654058; Training Loss 0.004760943253033873\n",
      "Episode 2964; Testing Loss 0.005868949909441471; Training Loss 0.004760934530677755\n",
      "Episode 2965; Testing Loss 0.005869093943576712; Training Loss 0.004760923953499536\n",
      "Episode 2966; Testing Loss 0.005869137634871119; Training Loss 0.00476091193549596\n",
      "Episode 2967; Testing Loss 0.005869042846116757; Training Loss 0.004760899871256096\n",
      "Episode 2968; Testing Loss 0.005868975137455728; Training Loss 0.0047608897998381635\n",
      "Episode 2969; Testing Loss 0.005869018698203786; Training Loss 0.004760878977380542\n",
      "Episode 2970; Testing Loss 0.005869047991623042; Training Loss 0.00476086724388775\n",
      "Episode 2971; Testing Loss 0.005869000048649636; Training Loss 0.004760855507671565\n",
      "Episode 2972; Testing Loss 0.0058690029864627334; Training Loss 0.0047608455375031\n",
      "Episode 2973; Testing Loss 0.0058690203157992045; Training Loss 0.0047608332487625755\n",
      "Episode 2974; Testing Loss 0.005869005902663072; Training Loss 0.004760824353831073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2975; Testing Loss 0.005868887741548041; Training Loss 0.0047608141567991\n",
      "Episode 2976; Testing Loss 0.005868831621295895; Training Loss 0.004760802979768925\n",
      "Episode 2977; Testing Loss 0.00586894840255188; Training Loss 0.004760789822464865\n",
      "Episode 2978; Testing Loss 0.005869042900509211; Training Loss 0.004760780922064603\n",
      "Episode 2979; Testing Loss 0.005868945939510451; Training Loss 0.004760768971102175\n",
      "Episode 2980; Testing Loss 0.005868876346707216; Training Loss 0.004760756733218765\n",
      "Episode 2981; Testing Loss 0.00586892445999966; Training Loss 0.004760747859537682\n",
      "Episode 2982; Testing Loss 0.0058688918823660355; Training Loss 0.004760737691314851\n",
      "Episode 2983; Testing Loss 0.0058687898733731534; Training Loss 0.004760726444659096\n",
      "Episode 2984; Testing Loss 0.005868807855421648; Training Loss 0.004760714168893806\n",
      "Episode 2985; Testing Loss 0.005868891852866571; Training Loss 0.004760703990007416\n",
      "Episode 2986; Testing Loss 0.005868923415306672; Training Loss 0.004760693362909794\n",
      "Episode 2987; Testing Loss 0.005868800693321082; Training Loss 0.00476068082167053\n",
      "Episode 2988; Testing Loss 0.005868770519018723; Training Loss 0.004760670543984044\n",
      "Episode 2989; Testing Loss 0.0058687602547720106; Training Loss 0.004760660367759353\n",
      "Episode 2990; Testing Loss 0.00586876012453301; Training Loss 0.004760649709985952\n",
      "Episode 2991; Testing Loss 0.005868692787337319; Training Loss 0.004760637371633016\n",
      "Episode 2992; Testing Loss 0.005868705320049241; Training Loss 0.004760628185734696\n",
      "Episode 2993; Testing Loss 0.005868768277624365; Training Loss 0.004760618385939906\n",
      "Episode 2994; Testing Loss 0.0058687567233211176; Training Loss 0.004760604597236792\n",
      "Episode 2995; Testing Loss 0.005868713807657835; Training Loss 0.004760594187075046\n",
      "Episode 2996; Testing Loss 0.005868662508637679; Training Loss 0.0047605849578707575\n",
      "Episode 2997; Testing Loss 0.005868719915641349; Training Loss 0.0047605730765789885\n",
      "Episode 2998; Testing Loss 0.005868769933684389; Training Loss 0.0047605618967021535\n",
      "Episode 2999; Testing Loss 0.005868692960086688; Training Loss 0.004760550648762914\n",
      "Episode 3000; Testing Loss 0.005868648120889742; Training Loss 0.0047605413490626304\n",
      "Episode 3001; Testing Loss 0.005868732636313034; Training Loss 0.004760528619668271\n",
      "Episode 3002; Testing Loss 0.005868749717321754; Training Loss 0.004760519525688684\n",
      "Episode 3003; Testing Loss 0.0058686056794140145; Training Loss 0.004760508029987043\n",
      "Episode 3004; Testing Loss 0.0058685024863756674; Training Loss 0.004760498821711494\n",
      "Episode 3005; Testing Loss 0.005868535690236728; Training Loss 0.004760486175848566\n",
      "Episode 3006; Testing Loss 0.005868644391547069; Training Loss 0.004760476821712215\n",
      "Episode 3007; Testing Loss 0.005868681480570725; Training Loss 0.0047604656913704775\n",
      "Episode 3008; Testing Loss 0.00586863101754315; Training Loss 0.004760453958204397\n",
      "Episode 3009; Testing Loss 0.005868589638991961; Training Loss 0.004760442532974711\n",
      "Episode 3010; Testing Loss 0.005868566623038069; Training Loss 0.004760432628139089\n",
      "Episode 3011; Testing Loss 0.005868522437490775; Training Loss 0.00476042284396234\n",
      "Episode 3012; Testing Loss 0.0058684579961419195; Training Loss 0.004760410666959304\n",
      "Episode 3013; Testing Loss 0.005868457059954712; Training Loss 0.00476039984799218\n",
      "Episode 3014; Testing Loss 0.005868533213713189; Training Loss 0.004760390842648806\n",
      "Episode 3015; Testing Loss 0.005868570373609957; Training Loss 0.004760379057158427\n",
      "Episode 3016; Testing Loss 0.005868526224713996; Training Loss 0.004760368514165461\n",
      "Episode 3017; Testing Loss 0.005868440546496251; Training Loss 0.004760358537757634\n",
      "Episode 3018; Testing Loss 0.005868472193815975; Training Loss 0.004760345985381587\n",
      "Episode 3019; Testing Loss 0.005868537803356261; Training Loss 0.00476033692858784\n",
      "Episode 3020; Testing Loss 0.005868462983570182; Training Loss 0.004760325401366446\n",
      "Episode 3021; Testing Loss 0.005868365836602166; Training Loss 0.004760315054249592\n",
      "Episode 3022; Testing Loss 0.0058683967175872966; Training Loss 0.004760300142130817\n",
      "Episode 3023; Testing Loss 0.0058684807060198785; Training Loss 0.004760292316122473\n",
      "Episode 3024; Testing Loss 0.005868496749528825; Training Loss 0.0047602839700033855\n",
      "Episode 3025; Testing Loss 0.005868440416365902; Training Loss 0.004760273386421431\n",
      "Episode 3026; Testing Loss 0.005868429087716553; Training Loss 0.004760260195410755\n",
      "Episode 3027; Testing Loss 0.005868474257823394; Training Loss 0.004760248670922876\n",
      "Episode 3028; Testing Loss 0.0058685072128517955; Training Loss 0.004760237876591387\n",
      "Episode 3029; Testing Loss 0.0058684523824516525; Training Loss 0.004760227212359198\n",
      "Episode 3030; Testing Loss 0.005868337578714883; Training Loss 0.004760218128370445\n",
      "Episode 3031; Testing Loss 0.005868284405962841; Training Loss 0.004760206900142404\n",
      "Episode 3032; Testing Loss 0.0058683013980875874; Training Loss 0.004760193418226627\n",
      "Episode 3033; Testing Loss 0.005868319563402172; Training Loss 0.00476018323547202\n",
      "Episode 3034; Testing Loss 0.005868319928644786; Training Loss 0.004760175352244845\n",
      "Episode 3035; Testing Loss 0.005868314968084911; Training Loss 0.004760164780611232\n",
      "Episode 3036; Testing Loss 0.005868314546447357; Training Loss 0.004760150733924126\n",
      "Episode 3037; Testing Loss 0.005868308186566597; Training Loss 0.004760142118826465\n",
      "Episode 3038; Testing Loss 0.005868301759638037; Training Loss 0.004760130223799945\n",
      "Episode 3039; Testing Loss 0.0058682722408962915; Training Loss 0.004760121245885442\n",
      "Episode 3040; Testing Loss 0.0058681826801019864; Training Loss 0.004760110965674745\n",
      "Episode 3041; Testing Loss 0.005868143229114386; Training Loss 0.004760098965405068\n",
      "Episode 3042; Testing Loss 0.005868200493799679; Training Loss 0.004760086918180664\n",
      "Episode 3043; Testing Loss 0.005868241382674533; Training Loss 0.004760075591237173\n",
      "Episode 3044; Testing Loss 0.005868231503741491; Training Loss 0.0047600674920846965\n",
      "Episode 3045; Testing Loss 0.005868167099906182; Training Loss 0.00476005738443182\n",
      "Episode 3046; Testing Loss 0.005868153669962472; Training Loss 0.004760045925054294\n",
      "Episode 3047; Testing Loss 0.005868222253602777; Training Loss 0.0047600313154112905\n",
      "Episode 3048; Testing Loss 0.005868227373803577; Training Loss 0.004760019672600858\n",
      "Episode 3049; Testing Loss 0.005868177331249491; Training Loss 0.0047600129786862\n",
      "Episode 3050; Testing Loss 0.005868121527458447; Training Loss 0.004760000258728327\n",
      "Episode 3051; Testing Loss 0.005868073444770682; Training Loss 0.004759990820835737\n",
      "Episode 3052; Testing Loss 0.005868038379135248; Training Loss 0.004759979977670561\n",
      "Episode 3053; Testing Loss 0.005868025289158513; Training Loss 0.0047599690484457875\n",
      "Episode 3054; Testing Loss 0.005868071304802731; Training Loss 0.00475995777811514\n",
      "Episode 3055; Testing Loss 0.005868123024201885; Training Loss 0.004759946812355133\n",
      "Episode 3056; Testing Loss 0.005868134471425614; Training Loss 0.004759935935520585\n",
      "Episode 3057; Testing Loss 0.005868107577777901; Training Loss 0.0047599217652127684\n",
      "Episode 3058; Testing Loss 0.005868047885885007; Training Loss 0.004759912027579808\n",
      "Episode 3059; Testing Loss 0.005867964248155478; Training Loss 0.004759904291731158\n",
      "Episode 3060; Testing Loss 0.005867965887220276; Training Loss 0.004759892832260894\n",
      "Episode 3061; Testing Loss 0.00586798805858178; Training Loss 0.004759880065027297\n",
      "Episode 3062; Testing Loss 0.005867998271749233; Training Loss 0.004759868894148132\n",
      "Episode 3063; Testing Loss 0.00586798183288653; Training Loss 0.004759858905925768\n",
      "Episode 3064; Testing Loss 0.005867957311516166; Training Loss 0.004759845695218466\n",
      "Episode 3065; Testing Loss 0.005867936358413742; Training Loss 0.004759834740846599\n",
      "Episode 3066; Testing Loss 0.005867916411581653; Training Loss 0.004759825606021105\n",
      "Episode 3067; Testing Loss 0.005867966381629996; Training Loss 0.004759811945825312\n",
      "Episode 3068; Testing Loss 0.005867975822549577; Training Loss 0.004759803775041569\n",
      "Episode 3069; Testing Loss 0.005867929108829738; Training Loss 0.004759794066916437\n",
      "Episode 3070; Testing Loss 0.005867894106022515; Training Loss 0.004759782308911117\n",
      "Episode 3071; Testing Loss 0.005867872474779404; Training Loss 0.004759770353669025\n",
      "Episode 3072; Testing Loss 0.005867869731850036; Training Loss 0.004759758675710861\n",
      "Episode 3073; Testing Loss 0.0058678386753684125; Training Loss 0.004759750202073997\n",
      "Episode 3074; Testing Loss 0.005867741702216616; Training Loss 0.004759739457094227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3075; Testing Loss 0.005867738399871268; Training Loss 0.004759726751743209\n",
      "Episode 3076; Testing Loss 0.005867809016783142; Training Loss 0.004759717247984448\n",
      "Episode 3077; Testing Loss 0.0058679247020211175; Training Loss 0.004759707948455767\n",
      "Episode 3078; Testing Loss 0.00586789294314949; Training Loss 0.0047596945489324985\n",
      "Episode 3079; Testing Loss 0.005867786376316395; Training Loss 0.004759684146164836\n",
      "Episode 3080; Testing Loss 0.005867754808626719; Training Loss 0.004759676935812653\n",
      "Episode 3081; Testing Loss 0.0058678073095103185; Training Loss 0.004759666600817757\n",
      "Episode 3082; Testing Loss 0.00586782977699561; Training Loss 0.0047596539350047255\n",
      "Episode 3083; Testing Loss 0.005867753738842666; Training Loss 0.004759639805933555\n",
      "Episode 3084; Testing Loss 0.005867749061120824; Training Loss 0.004759630402394599\n",
      "Episode 3085; Testing Loss 0.005867817741177778; Training Loss 0.004759619647454154\n",
      "Episode 3086; Testing Loss 0.005867802004304034; Training Loss 0.0047596080601956196\n",
      "Episode 3087; Testing Loss 0.005867728992483569; Training Loss 0.004759596178840542\n",
      "Episode 3088; Testing Loss 0.005867752146187201; Training Loss 0.004759587181417176\n",
      "Episode 3089; Testing Loss 0.005867746170696073; Training Loss 0.004759576459747449\n",
      "Episode 3090; Testing Loss 0.005867689720518151; Training Loss 0.004759565222071656\n",
      "Episode 3091; Testing Loss 0.005867606739275185; Training Loss 0.004759552833854464\n",
      "Episode 3092; Testing Loss 0.005867638283166746; Training Loss 0.004759543368039098\n",
      "Episode 3093; Testing Loss 0.0058677289498580655; Training Loss 0.00475953159989199\n",
      "Episode 3094; Testing Loss 0.005867710306907183; Training Loss 0.0047595212083666035\n",
      "Episode 3095; Testing Loss 0.005867564064030448; Training Loss 0.00475951138120587\n",
      "Episode 3096; Testing Loss 0.005867569198863933; Training Loss 0.0047594995118036935\n",
      "Episode 3097; Testing Loss 0.005867634722167453; Training Loss 0.004759487027855614\n",
      "Episode 3098; Testing Loss 0.005867677434502087; Training Loss 0.004759480142547429\n",
      "Episode 3099; Testing Loss 0.00586764693018271; Training Loss 0.004759470582521965\n",
      "Episode 3100; Testing Loss 0.005867574897414982; Training Loss 0.004759457473544401\n",
      "Episode 3101; Testing Loss 0.005867500470099305; Training Loss 0.004759445019014899\n",
      "Episode 3102; Testing Loss 0.005867534611315399; Training Loss 0.004759435409428456\n",
      "Episode 3103; Testing Loss 0.0058675931062643; Training Loss 0.004759424384642269\n",
      "Episode 3104; Testing Loss 0.005867557829963393; Training Loss 0.00475941202366136\n",
      "Episode 3105; Testing Loss 0.005867461100302307; Training Loss 0.00475940376502615\n",
      "Episode 3106; Testing Loss 0.005867514120811041; Training Loss 0.004759393791564388\n",
      "Episode 3107; Testing Loss 0.005867589449113992; Training Loss 0.004759380347477086\n",
      "Episode 3108; Testing Loss 0.0058675111703486394; Training Loss 0.004759371237097652\n",
      "Episode 3109; Testing Loss 0.005867408197653809; Training Loss 0.004759362516778747\n",
      "Episode 3110; Testing Loss 0.005867410787867608; Training Loss 0.004759350793039726\n",
      "Episode 3111; Testing Loss 0.005867499589964448; Training Loss 0.004759338361454774\n",
      "Episode 3112; Testing Loss 0.00586747485943097; Training Loss 0.004759327419467711\n",
      "Episode 3113; Testing Loss 0.005867350802203135; Training Loss 0.004759317211437312\n",
      "Episode 3114; Testing Loss 0.00586731553424039; Training Loss 0.004759304242334118\n",
      "Episode 3115; Testing Loss 0.00586743730919998; Training Loss 0.004759295294928826\n",
      "Episode 3116; Testing Loss 0.005867473058347452; Training Loss 0.00475928649262624\n",
      "Episode 3117; Testing Loss 0.0058673554246009485; Training Loss 0.004759275296487425\n",
      "Episode 3118; Testing Loss 0.005867337381359543; Training Loss 0.004759262572400796\n",
      "Episode 3119; Testing Loss 0.005867397423923547; Training Loss 0.004759250701612686\n",
      "Episode 3120; Testing Loss 0.005867444783876344; Training Loss 0.004759242123450864\n",
      "Episode 3121; Testing Loss 0.0058673574352276076; Training Loss 0.004759230002077249\n",
      "Episode 3122; Testing Loss 0.005867234191627478; Training Loss 0.004759219666569179\n",
      "Episode 3123; Testing Loss 0.005867253739852562; Training Loss 0.004759209680907914\n",
      "Episode 3124; Testing Loss 0.005867340650897199; Training Loss 0.004759200276052403\n",
      "Episode 3125; Testing Loss 0.0058673190912812735; Training Loss 0.004759187872968422\n",
      "Episode 3126; Testing Loss 0.005867258027503784; Training Loss 0.004759175342921671\n",
      "Episode 3127; Testing Loss 0.005867302968809066; Training Loss 0.004759165424573346\n",
      "Episode 3128; Testing Loss 0.0058673233114814455; Training Loss 0.004759153485155044\n",
      "Episode 3129; Testing Loss 0.005867238828560988; Training Loss 0.004759145413667212\n",
      "Episode 3130; Testing Loss 0.005867092821059258; Training Loss 0.004759135462586325\n",
      "Episode 3131; Testing Loss 0.005867083274277267; Training Loss 0.004759125457951108\n",
      "Episode 3132; Testing Loss 0.00586724595278532; Training Loss 0.004759112752743366\n",
      "Episode 3133; Testing Loss 0.005867330529954162; Training Loss 0.004759100004782823\n",
      "Episode 3134; Testing Loss 0.005867254840006934; Training Loss 0.004759089945363993\n",
      "Episode 3135; Testing Loss 0.005867130961538256; Training Loss 0.004759078959067187\n",
      "Episode 3136; Testing Loss 0.0058671297700003105; Training Loss 0.004759066571326031\n",
      "Episode 3137; Testing Loss 0.005867211347754002; Training Loss 0.004759057062131271\n",
      "Episode 3138; Testing Loss 0.0058672313050518845; Training Loss 0.004759045661660024\n",
      "Episode 3139; Testing Loss 0.005867115560128184; Training Loss 0.004759036325774513\n",
      "Episode 3140; Testing Loss 0.005867003009443657; Training Loss 0.004759026556231337\n",
      "Episode 3141; Testing Loss 0.005867034660028828; Training Loss 0.0047590149868739695\n",
      "Episode 3142; Testing Loss 0.005867184737220965; Training Loss 0.004759003252303034\n",
      "Episode 3143; Testing Loss 0.005867198315429869; Training Loss 0.004758993960797156\n",
      "Episode 3144; Testing Loss 0.005867111908864776; Training Loss 0.004758983904920291\n",
      "Episode 3145; Testing Loss 0.005867024084081849; Training Loss 0.004758971230838881\n",
      "Episode 3146; Testing Loss 0.005867001448259926; Training Loss 0.004758960575576855\n",
      "Episode 3147; Testing Loss 0.0058670216141005; Training Loss 0.004758951461204874\n",
      "Episode 3148; Testing Loss 0.005867022130055645; Training Loss 0.004758940175740371\n",
      "Episode 3149; Testing Loss 0.005866973071392923; Training Loss 0.0047589276854806415\n",
      "Episode 3150; Testing Loss 0.005866984042760616; Training Loss 0.004758918119098102\n",
      "Episode 3151; Testing Loss 0.005867047153111471; Training Loss 0.004758907957644837\n",
      "Episode 3152; Testing Loss 0.005867046819427463; Training Loss 0.004758894700905089\n",
      "Episode 3153; Testing Loss 0.005867004302797111; Training Loss 0.0047588868153689515\n",
      "Episode 3154; Testing Loss 0.005866970391391236; Training Loss 0.004758877282782866\n",
      "Episode 3155; Testing Loss 0.005866932500026972; Training Loss 0.004758865638880761\n",
      "Episode 3156; Testing Loss 0.005866912528099313; Training Loss 0.004758853516238388\n",
      "Episode 3157; Testing Loss 0.005866918887743053; Training Loss 0.0047588433148329865\n",
      "Episode 3158; Testing Loss 0.005866939199249612; Training Loss 0.004758832582323911\n",
      "Episode 3159; Testing Loss 0.00586695917462037; Training Loss 0.004758819901251964\n",
      "Episode 3160; Testing Loss 0.005866931195482951; Training Loss 0.0047588092051286356\n",
      "Episode 3161; Testing Loss 0.005866963863147516; Training Loss 0.004758800019971891\n",
      "Episode 3162; Testing Loss 0.005867015725419059; Training Loss 0.004758788576155538\n",
      "Episode 3163; Testing Loss 0.005866986330540834; Training Loss 0.004758778495883864\n",
      "Episode 3164; Testing Loss 0.005866880386057887; Training Loss 0.004758767601348497\n",
      "Episode 3165; Testing Loss 0.005866814651417266; Training Loss 0.00475875600555748\n",
      "Episode 3166; Testing Loss 0.005866855967089208; Training Loss 0.0047587430543607236\n",
      "Episode 3167; Testing Loss 0.005866949820113488; Training Loss 0.00475873426964637\n",
      "Episode 3168; Testing Loss 0.0058668954628049485; Training Loss 0.004758724377863492\n",
      "Episode 3169; Testing Loss 0.005866772744130133; Training Loss 0.004758712562690479\n",
      "Episode 3170; Testing Loss 0.005866804689236285; Training Loss 0.004758701820436481\n",
      "Episode 3171; Testing Loss 0.005866880893254916; Training Loss 0.004758693307615765\n",
      "Episode 3172; Testing Loss 0.005866852366994814; Training Loss 0.00475868097020116\n",
      "Episode 3173; Testing Loss 0.005866757287260292; Training Loss 0.004758669545454657\n",
      "Episode 3174; Testing Loss 0.005866780945607213; Training Loss 0.004758662404624827\n",
      "Episode 3175; Testing Loss 0.005866885833773202; Training Loss 0.0047586518177181476\n",
      "Episode 3176; Testing Loss 0.005866907457029271; Training Loss 0.004758638456301507\n",
      "Episode 3177; Testing Loss 0.005866819645224135; Training Loss 0.004758627681564053\n",
      "Episode 3178; Testing Loss 0.005866691525890087; Training Loss 0.004758620010413163\n",
      "Episode 3179; Testing Loss 0.005866720310068979; Training Loss 0.004758606784309154\n",
      "Episode 3180; Testing Loss 0.005866835195544677; Training Loss 0.004758595164326737\n",
      "Episode 3181; Testing Loss 0.005866819228674171; Training Loss 0.004758587840943092\n",
      "Episode 3182; Testing Loss 0.005866698589703091; Training Loss 0.00475857790246139\n",
      "Episode 3183; Testing Loss 0.005866669513846901; Training Loss 0.004758564056585873\n",
      "Episode 3184; Testing Loss 0.005866741149161694; Training Loss 0.004758552977271695\n",
      "Episode 3185; Testing Loss 0.005866722437151006; Training Loss 0.004758544207151091\n",
      "Episode 3186; Testing Loss 0.005866630969092414; Training Loss 0.004758533404802463\n",
      "Episode 3187; Testing Loss 0.005866648032836289; Training Loss 0.004758520729854463\n",
      "Episode 3188; Testing Loss 0.005866762248550052; Training Loss 0.004758507546840251\n",
      "Episode 3189; Testing Loss 0.005866775232216785; Training Loss 0.004758500270448347\n",
      "Episode 3190; Testing Loss 0.005866711530771779; Training Loss 0.004758491317757354\n",
      "Episode 3191; Testing Loss 0.0058666242046299414; Training Loss 0.004758480046615591\n",
      "Episode 3192; Testing Loss 0.0058665580502929625; Training Loss 0.004758465147484224\n",
      "Episode 3193; Testing Loss 0.005866616911063088; Training Loss 0.0047584561794666155\n",
      "Episode 3194; Testing Loss 0.005866623020528646; Training Loss 0.004758447199611185\n",
      "Episode 3195; Testing Loss 0.0058665892871573155; Training Loss 0.004758437030116577\n",
      "Episode 3196; Testing Loss 0.005866595657806974; Training Loss 0.004758425903086765\n",
      "Episode 3197; Testing Loss 0.0058666206427472126; Training Loss 0.004758411922164853\n",
      "Episode 3198; Testing Loss 0.005866601598119344; Training Loss 0.00475839992977142\n",
      "Episode 3199; Testing Loss 0.0058666068296069525; Training Loss 0.004758390748148136\n",
      "Episode 3200; Testing Loss 0.005866615943432875; Training Loss 0.004758378159400873\n",
      "Episode 3201; Testing Loss 0.005866609457928677; Training Loss 0.004758367270080366\n",
      "Episode 3202; Testing Loss 0.005866578686478732; Training Loss 0.004758358707292392\n",
      "Episode 3203; Testing Loss 0.005866535577307324; Training Loss 0.004758346455483979\n",
      "Episode 3204; Testing Loss 0.005866518089626344; Training Loss 0.00475833209439612\n",
      "Episode 3205; Testing Loss 0.00586654359303176; Training Loss 0.004758324891651366\n",
      "Episode 3206; Testing Loss 0.005866602893034637; Training Loss 0.004758314724471633\n",
      "Episode 3207; Testing Loss 0.005866602512897586; Training Loss 0.004758301394066283\n",
      "Episode 3208; Testing Loss 0.005866510353259588; Training Loss 0.004758289608659151\n",
      "Episode 3209; Testing Loss 0.005866426689026393; Training Loss 0.004758278197962803\n",
      "Episode 3210; Testing Loss 0.005866456170683728; Training Loss 0.004758267523166832\n",
      "Episode 3211; Testing Loss 0.005866494544578286; Training Loss 0.004758255904345663\n",
      "Episode 3212; Testing Loss 0.005866482384437015; Training Loss 0.00475824254479544\n",
      "Episode 3213; Testing Loss 0.005866494215421543; Training Loss 0.004758231505207716\n",
      "Episode 3214; Testing Loss 0.005866557195636302; Training Loss 0.004758219441191322\n",
      "Episode 3215; Testing Loss 0.005866584566055654; Training Loss 0.004758208167233608\n",
      "Episode 3216; Testing Loss 0.005866592550704636; Training Loss 0.004758196769169546\n",
      "Episode 3217; Testing Loss 0.005866608690334915; Training Loss 0.004758184129671617\n",
      "Episode 3218; Testing Loss 0.005866613025531425; Training Loss 0.004758175976819257\n",
      "Episode 3219; Testing Loss 0.005866546215937353; Training Loss 0.004758165127754475\n",
      "Episode 3220; Testing Loss 0.005866579394217178; Training Loss 0.0047581512501767885\n",
      "Episode 3221; Testing Loss 0.00586667854488015; Training Loss 0.004758139518237504\n",
      "Episode 3222; Testing Loss 0.0058667014383086994; Training Loss 0.004758130980266329\n",
      "Episode 3223; Testing Loss 0.005866687268736896; Training Loss 0.004758119710946949\n",
      "Episode 3224; Testing Loss 0.005866664622325344; Training Loss 0.004758107120382177\n",
      "Episode 3225; Testing Loss 0.005866760864457042; Training Loss 0.004758094124567693\n",
      "Episode 3226; Testing Loss 0.005866825402306054; Training Loss 0.00475808397449243\n",
      "Episode 3227; Testing Loss 0.005866825985447099; Training Loss 0.004758072907441622\n",
      "Episode 3228; Testing Loss 0.0058667443063922416; Training Loss 0.004758060359668138\n",
      "Episode 3229; Testing Loss 0.005866737608202598; Training Loss 0.00475804940943456\n",
      "Episode 3230; Testing Loss 0.005866759968070314; Training Loss 0.004758038527272379\n",
      "Episode 3231; Testing Loss 0.005866801671616859; Training Loss 0.004758027202640857\n",
      "Episode 3232; Testing Loss 0.005866847641709131; Training Loss 0.004758018714488526\n",
      "Episode 3233; Testing Loss 0.005866816398022586; Training Loss 0.004758007609796602\n",
      "Episode 3234; Testing Loss 0.005866782537524838; Training Loss 0.004757996020667657\n",
      "Episode 3235; Testing Loss 0.005866835087402193; Training Loss 0.004757982088547162\n",
      "Episode 3236; Testing Loss 0.005866843259738909; Training Loss 0.004757974135086789\n",
      "Episode 3237; Testing Loss 0.005866761178127877; Training Loss 0.004757963960876045\n",
      "Episode 3238; Testing Loss 0.005866727930357682; Training Loss 0.004757951739620594\n",
      "Episode 3239; Testing Loss 0.005866762943135833; Training Loss 0.004757938165211538\n",
      "Episode 3240; Testing Loss 0.005866764793366429; Training Loss 0.0047579273042173045\n",
      "Episode 3241; Testing Loss 0.005866725833781837; Training Loss 0.004757915592869402\n",
      "Episode 3242; Testing Loss 0.005866714629731944; Training Loss 0.00475790614107308\n",
      "Episode 3243; Testing Loss 0.005866814879238742; Training Loss 0.0047578930952174485\n",
      "Episode 3244; Testing Loss 0.005866889958068286; Training Loss 0.004757883193202232\n",
      "Episode 3245; Testing Loss 0.005866786375624256; Training Loss 0.0047578711388092665\n",
      "Episode 3246; Testing Loss 0.005866675748869403; Training Loss 0.004757860474805426\n",
      "Episode 3247; Testing Loss 0.005866712854518312; Training Loss 0.0047578501314585915\n",
      "Episode 3248; Testing Loss 0.0058668306688199; Training Loss 0.004757840393933885\n",
      "Episode 3249; Testing Loss 0.005866775164079361; Training Loss 0.0047578270082656495\n",
      "Episode 3250; Testing Loss 0.005866680753736421; Training Loss 0.004757816799389718\n",
      "Episode 3251; Testing Loss 0.005866674134001589; Training Loss 0.004757806838706942\n",
      "Episode 3252; Testing Loss 0.005866776213385428; Training Loss 0.004757796067555317\n",
      "Episode 3253; Testing Loss 0.0058668053520934006; Training Loss 0.00475778398223897\n",
      "Episode 3254; Testing Loss 0.005866736977153492; Training Loss 0.0047577717255652815\n",
      "Episode 3255; Testing Loss 0.00586674852525825; Training Loss 0.0047577614713179275\n",
      "Episode 3256; Testing Loss 0.005866771756474011; Training Loss 0.004757749838361351\n",
      "Episode 3257; Testing Loss 0.005866718626755219; Training Loss 0.004757740721198925\n",
      "Episode 3258; Testing Loss 0.005866628329863803; Training Loss 0.004757730925881619\n",
      "Episode 3259; Testing Loss 0.005866604133716265; Training Loss 0.004757718870278504\n",
      "Episode 3260; Testing Loss 0.005866684038605159; Training Loss 0.004757705834278617\n",
      "Episode 3261; Testing Loss 0.005866773347325812; Training Loss 0.00475769339992794\n",
      "Episode 3262; Testing Loss 0.005866768547966764; Training Loss 0.004757682254899854\n",
      "Episode 3263; Testing Loss 0.005866706022397836; Training Loss 0.004757671236349764\n",
      "Episode 3264; Testing Loss 0.005866624579190985; Training Loss 0.004757659525721649\n",
      "Episode 3265; Testing Loss 0.0058666129261869164; Training Loss 0.00475765001306803\n",
      "Episode 3266; Testing Loss 0.005866704142860973; Training Loss 0.004757638635197102\n",
      "Episode 3267; Testing Loss 0.005866719523701753; Training Loss 0.00475762900479824\n",
      "Episode 3268; Testing Loss 0.005866606410735282; Training Loss 0.004757617396971941\n",
      "Episode 3269; Testing Loss 0.005866565115497672; Training Loss 0.004757607002549627\n",
      "Episode 3270; Testing Loss 0.005866623708747897; Training Loss 0.004757593936504844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3271; Testing Loss 0.005866711437214043; Training Loss 0.0047575835353711226\n",
      "Episode 3272; Testing Loss 0.005866754013763014; Training Loss 0.004757573551080776\n",
      "Episode 3273; Testing Loss 0.005866690951691952; Training Loss 0.004757560767567091\n",
      "Episode 3274; Testing Loss 0.005866555013279116; Training Loss 0.004757550659232569\n",
      "Episode 3275; Testing Loss 0.00586654642461699; Training Loss 0.0047575404137825345\n",
      "Episode 3276; Testing Loss 0.005866630366720746; Training Loss 0.004757529245626767\n",
      "Episode 3277; Testing Loss 0.005866659671494823; Training Loss 0.004757518589092688\n",
      "Episode 3278; Testing Loss 0.005866564231696312; Training Loss 0.004757505515849969\n",
      "Episode 3279; Testing Loss 0.005866537552340189; Training Loss 0.004757494461077505\n",
      "Episode 3280; Testing Loss 0.005866647699451461; Training Loss 0.0047574833385695715\n",
      "Episode 3281; Testing Loss 0.0058666928924115125; Training Loss 0.004757472863219354\n",
      "Episode 3282; Testing Loss 0.005866550211322119; Training Loss 0.004757460682609548\n",
      "Episode 3283; Testing Loss 0.005866513316174129; Training Loss 0.004757450410073644\n",
      "Episode 3284; Testing Loss 0.005866587576799873; Training Loss 0.004757438144099125\n",
      "Episode 3285; Testing Loss 0.005866643188620658; Training Loss 0.004757430112204638\n",
      "Episode 3286; Testing Loss 0.005866579479063481; Training Loss 0.004757418070580637\n",
      "Episode 3287; Testing Loss 0.005866451379310495; Training Loss 0.00475740665290867\n",
      "Episode 3288; Testing Loss 0.0058664538817000235; Training Loss 0.004757396434303469\n",
      "Episode 3289; Testing Loss 0.005866561151453204; Training Loss 0.004757384554100662\n",
      "Episode 3290; Testing Loss 0.005866590898768497; Training Loss 0.004757374096301192\n",
      "Episode 3291; Testing Loss 0.005866498795528391; Training Loss 0.004757361920106562\n",
      "Episode 3292; Testing Loss 0.005866461022096581; Training Loss 0.004757352760525887\n",
      "Episode 3293; Testing Loss 0.005866557285497881; Training Loss 0.004757341375367763\n",
      "Episode 3294; Testing Loss 0.005866592405327536; Training Loss 0.0047573295284455445\n",
      "Episode 3295; Testing Loss 0.005866489534572064; Training Loss 0.004757317667084473\n",
      "Episode 3296; Testing Loss 0.0058664122725118496; Training Loss 0.004757307449420275\n",
      "Episode 3297; Testing Loss 0.005866492251074265; Training Loss 0.0047572954886788196\n",
      "Episode 3298; Testing Loss 0.005866557198748059; Training Loss 0.004757286657614237\n",
      "Episode 3299; Testing Loss 0.005866452282633493; Training Loss 0.004757274785612484\n",
      "Episode 3300; Testing Loss 0.0058663820458812795; Training Loss 0.004757263798523983\n",
      "Episode 3301; Testing Loss 0.005866487185186888; Training Loss 0.004757253286348802\n",
      "Episode 3302; Testing Loss 0.005866514010190464; Training Loss 0.004757243208712649\n",
      "Episode 3303; Testing Loss 0.005866362063478179; Training Loss 0.004757230847738718\n",
      "Episode 3304; Testing Loss 0.00586628887285449; Training Loss 0.004757220123515479\n",
      "Episode 3305; Testing Loss 0.005866356910839576; Training Loss 0.004757209749390938\n",
      "Episode 3306; Testing Loss 0.005866458728663627; Training Loss 0.00475719731410671\n",
      "Episode 3307; Testing Loss 0.005866489011304283; Training Loss 0.00475718629696505\n",
      "Episode 3308; Testing Loss 0.0058663984942730705; Training Loss 0.004757175829274179\n",
      "Episode 3309; Testing Loss 0.005866333558451755; Training Loss 0.004757164582823003\n",
      "Episode 3310; Testing Loss 0.005866405187141405; Training Loss 0.004757153627137133\n",
      "Episode 3311; Testing Loss 0.005866496899386702; Training Loss 0.004757144905067592\n",
      "Episode 3312; Testing Loss 0.005866384355092451; Training Loss 0.004757130456462634\n",
      "Episode 3313; Testing Loss 0.005866261183944232; Training Loss 0.0047571202102500345\n",
      "Episode 3314; Testing Loss 0.005866310978891969; Training Loss 0.004757109170841133\n",
      "Episode 3315; Testing Loss 0.005866431181899758; Training Loss 0.0047570990896592434\n",
      "Episode 3316; Testing Loss 0.005866384280278046; Training Loss 0.004757086694541491\n",
      "Episode 3317; Testing Loss 0.005866309592179355; Training Loss 0.004757077874035906\n",
      "Episode 3318; Testing Loss 0.005866356277984834; Training Loss 0.004757065536135048\n",
      "Episode 3319; Testing Loss 0.005866429240978354; Training Loss 0.004757055490454666\n",
      "Episode 3320; Testing Loss 0.0058662966791422876; Training Loss 0.004757044011585382\n",
      "Episode 3321; Testing Loss 0.00586619453402659; Training Loss 0.004757033319802353\n",
      "Episode 3322; Testing Loss 0.005866248281319704; Training Loss 0.004757023209101222\n",
      "Episode 3323; Testing Loss 0.005866340771299109; Training Loss 0.004757012140547707\n",
      "Episode 3324; Testing Loss 0.005866312900494421; Training Loss 0.004756999705757192\n",
      "Episode 3325; Testing Loss 0.005866244293964178; Training Loss 0.004756989275728051\n",
      "Episode 3326; Testing Loss 0.0058662838165407915; Training Loss 0.0047569778488943\n",
      "Episode 3327; Testing Loss 0.005866401694969406; Training Loss 0.004756967572286813\n",
      "Episode 3328; Testing Loss 0.005866336027428412; Training Loss 0.004756954491882588\n",
      "Episode 3329; Testing Loss 0.0058661576406603055; Training Loss 0.004756948343270674\n",
      "Episode 3330; Testing Loss 0.005866193100041665; Training Loss 0.004756936742940059\n",
      "Episode 3331; Testing Loss 0.005866333004720089; Training Loss 0.004756924325117686\n",
      "Episode 3332; Testing Loss 0.0058663023784269975; Training Loss 0.004756913070677152\n",
      "Episode 3333; Testing Loss 0.0058661493456721395; Training Loss 0.004756901308736721\n",
      "Episode 3334; Testing Loss 0.005866109436740741; Training Loss 0.004756892579672577\n",
      "Episode 3335; Testing Loss 0.005866276281728938; Training Loss 0.004756879362050675\n",
      "Episode 3336; Testing Loss 0.005866341754166459; Training Loss 0.004756870545574659\n",
      "Episode 3337; Testing Loss 0.005866213579270282; Training Loss 0.004756859430172477\n",
      "Episode 3338; Testing Loss 0.005866116996584461; Training Loss 0.00475684841636421\n",
      "Episode 3339; Testing Loss 0.005866206275416654; Training Loss 0.004756838019105865\n",
      "Episode 3340; Testing Loss 0.005866357307660494; Training Loss 0.004756827725306152\n",
      "Episode 3341; Testing Loss 0.005866274823977683; Training Loss 0.0047568145393254765\n",
      "Episode 3342; Testing Loss 0.005866032613690628; Training Loss 0.004756806159514473\n",
      "Episode 3343; Testing Loss 0.005865979500954341; Training Loss 0.004756796454654474\n",
      "Episode 3344; Testing Loss 0.005866144174124623; Training Loss 0.004756785256209013\n",
      "Episode 3345; Testing Loss 0.005866252352946654; Training Loss 0.004756775059630891\n",
      "Episode 3346; Testing Loss 0.005866146817674757; Training Loss 0.004756759570207664\n",
      "Episode 3347; Testing Loss 0.005866035624226504; Training Loss 0.004756753541766932\n",
      "Episode 3348; Testing Loss 0.005866123873441669; Training Loss 0.004756742090872004\n",
      "Episode 3349; Testing Loss 0.005866225561371907; Training Loss 0.004756732568024138\n",
      "Episode 3350; Testing Loss 0.005866116569769557; Training Loss 0.0047567171101721125\n",
      "Episode 3351; Testing Loss 0.005865997914771489; Training Loss 0.004756707276557998\n",
      "Episode 3352; Testing Loss 0.005866023054081025; Training Loss 0.004756697791227997\n",
      "Episode 3353; Testing Loss 0.005866133273246072; Training Loss 0.004756687803155943\n",
      "Episode 3354; Testing Loss 0.005866142028892178; Training Loss 0.004756675797688796\n",
      "Episode 3355; Testing Loss 0.005866044729888899; Training Loss 0.004756661448134839\n",
      "Episode 3356; Testing Loss 0.005865996785485681; Training Loss 0.004756652479467898\n",
      "Episode 3357; Testing Loss 0.005866060783308176; Training Loss 0.004756642289921736\n",
      "Episode 3358; Testing Loss 0.005866142755273238; Training Loss 0.004756630484007685\n",
      "Episode 3359; Testing Loss 0.005866136333472284; Training Loss 0.004756618183271107\n",
      "Episode 3360; Testing Loss 0.005866005470619855; Training Loss 0.004756607176326097\n",
      "Episode 3361; Testing Loss 0.005865947796193486; Training Loss 0.004756596412201214\n",
      "Episode 3362; Testing Loss 0.00586596106418336; Training Loss 0.004756584412976768\n",
      "Episode 3363; Testing Loss 0.00586599079631337; Training Loss 0.004756576006710268\n",
      "Episode 3364; Testing Loss 0.005866031791748887; Training Loss 0.004756565906997227\n",
      "Episode 3365; Testing Loss 0.005865991869296647; Training Loss 0.004756551677957509\n",
      "Episode 3366; Testing Loss 0.005865952318550886; Training Loss 0.004756542891000849\n",
      "Episode 3367; Testing Loss 0.0058659247125775835; Training Loss 0.004756534545015706\n",
      "Episode 3368; Testing Loss 0.00586596605819188; Training Loss 0.00475652312569915\n",
      "Episode 3369; Testing Loss 0.0058660189086155645; Training Loss 0.004756511805421436\n",
      "Episode 3370; Testing Loss 0.0058659111505541105; Training Loss 0.0047564980657767405\n",
      "Episode 3371; Testing Loss 0.0058657937779873015; Training Loss 0.004756491189953791\n",
      "Episode 3372; Testing Loss 0.005865851482090774; Training Loss 0.00475648231119123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3373; Testing Loss 0.005865916126196084; Training Loss 0.004756470532234001\n",
      "Episode 3374; Testing Loss 0.0058658846730813025; Training Loss 0.004756454838465285\n",
      "Episode 3375; Testing Loss 0.005865826542622182; Training Loss 0.004756446910756843\n",
      "Episode 3376; Testing Loss 0.005865875907410481; Training Loss 0.004756438038929458\n",
      "Episode 3377; Testing Loss 0.005865936967131416; Training Loss 0.004756427523405514\n",
      "Episode 3378; Testing Loss 0.0058658805766077255; Training Loss 0.004756414992924009\n",
      "Episode 3379; Testing Loss 0.005865782448874467; Training Loss 0.004756401750701096\n",
      "Episode 3380; Testing Loss 0.005865803448202802; Training Loss 0.004756391283473213\n",
      "Episode 3381; Testing Loss 0.005865889644953303; Training Loss 0.0047563826536865935\n",
      "Episode 3382; Testing Loss 0.005865881733827728; Training Loss 0.004756369264519084\n",
      "Episode 3383; Testing Loss 0.005865817602586788; Training Loss 0.004756356145520589\n",
      "Episode 3384; Testing Loss 0.005865822348573707; Training Loss 0.004756345848769322\n",
      "Episode 3385; Testing Loss 0.0058658640055615; Training Loss 0.0047563349126302\n",
      "Episode 3386; Testing Loss 0.0058658481016391805; Training Loss 0.004756322913337813\n",
      "Episode 3387; Testing Loss 0.005865815295147518; Training Loss 0.004756314087553389\n",
      "Episode 3388; Testing Loss 0.00586578475164798; Training Loss 0.004756302783416088\n",
      "Episode 3389; Testing Loss 0.005865776841743459; Training Loss 0.004756288596401198\n",
      "Episode 3390; Testing Loss 0.005865805722723219; Training Loss 0.004756279687457344\n",
      "Episode 3391; Testing Loss 0.0058657893579710544; Training Loss 0.004756269851042967\n",
      "Episode 3392; Testing Loss 0.005865737475317476; Training Loss 0.004756258475176827\n",
      "Episode 3393; Testing Loss 0.005865779341003737; Training Loss 0.004756244855309678\n",
      "Episode 3394; Testing Loss 0.005865826075680289; Training Loss 0.004756233202181922\n",
      "Episode 3395; Testing Loss 0.005865770658801143; Training Loss 0.00475622226477211\n",
      "Episode 3396; Testing Loss 0.005865704779746927; Training Loss 0.00475620995786992\n",
      "Episode 3397; Testing Loss 0.005865753544105025; Training Loss 0.004756198410279206\n",
      "Episode 3398; Testing Loss 0.005865832296103799; Training Loss 0.004756187957363166\n",
      "Episode 3399; Testing Loss 0.005865830504745107; Training Loss 0.004756175725219359\n",
      "Episode 3400; Testing Loss 0.0058657433206690274; Training Loss 0.004756164884369994\n",
      "Episode 3401; Testing Loss 0.005865670257573488; Training Loss 0.004756154633149401\n",
      "Episode 3402; Testing Loss 0.005865684788829544; Training Loss 0.004756142524749372\n",
      "Episode 3403; Testing Loss 0.005865756722161518; Training Loss 0.00475613122525955\n",
      "Episode 3404; Testing Loss 0.005865798033936278; Training Loss 0.0047561223938447205\n",
      "Episode 3405; Testing Loss 0.005865763171104181; Training Loss 0.004756110700554558\n",
      "Episode 3406; Testing Loss 0.005865694472016328; Training Loss 0.004756097496812181\n",
      "Episode 3407; Testing Loss 0.005865644487475178; Training Loss 0.004756085884190176\n",
      "Episode 3408; Testing Loss 0.0058656642114761175; Training Loss 0.0047560758827459235\n",
      "Episode 3409; Testing Loss 0.005865716815477483; Training Loss 0.004756065357724561\n",
      "Episode 3410; Testing Loss 0.0058656679905905625; Training Loss 0.004756052895838964\n",
      "Episode 3411; Testing Loss 0.005865634724876338; Training Loss 0.004756040945339273\n",
      "Episode 3412; Testing Loss 0.005865697790044749; Training Loss 0.004756029678973251\n",
      "Episode 3413; Testing Loss 0.005865698433461228; Training Loss 0.0047560194282271605\n",
      "Episode 3414; Testing Loss 0.005865663652894105; Training Loss 0.0047560069713742294\n",
      "Episode 3415; Testing Loss 0.005865639399181356; Training Loss 0.004755996448680234\n",
      "Episode 3416; Testing Loss 0.00586563859310748; Training Loss 0.004755984984310758\n",
      "Episode 3417; Testing Loss 0.005865642589486251; Training Loss 0.004755972153606166\n",
      "Episode 3418; Testing Loss 0.005865621607835873; Training Loss 0.004755961249670176\n",
      "Episode 3419; Testing Loss 0.005865614168420664; Training Loss 0.004755949597093533\n",
      "Episode 3420; Testing Loss 0.005865596185171859; Training Loss 0.0047559393115075895\n",
      "Episode 3421; Testing Loss 0.005865578517027505; Training Loss 0.004755927260045594\n",
      "Episode 3422; Testing Loss 0.005865556753958104; Training Loss 0.004755915920824977\n",
      "Episode 3423; Testing Loss 0.0058655618220554455; Training Loss 0.004755905337534043\n",
      "Episode 3424; Testing Loss 0.005865611712510782; Training Loss 0.004755893267383427\n",
      "Episode 3425; Testing Loss 0.005865616159118014; Training Loss 0.0047558828613335465\n",
      "Episode 3426; Testing Loss 0.005865565996504266; Training Loss 0.004755872645691712\n",
      "Episode 3427; Testing Loss 0.005865588953074885; Training Loss 0.00475585934364909\n",
      "Episode 3428; Testing Loss 0.0058656437850190335; Training Loss 0.004755850079599188\n",
      "Episode 3429; Testing Loss 0.005865613591819948; Training Loss 0.00475583899680413\n",
      "Episode 3430; Testing Loss 0.005865521579795672; Training Loss 0.0047558273392896795\n",
      "Episode 3431; Testing Loss 0.005865469528570323; Training Loss 0.0047558149097347265\n",
      "Episode 3432; Testing Loss 0.005865535469866294; Training Loss 0.004755805511864889\n",
      "Episode 3433; Testing Loss 0.005865627514602062; Training Loss 0.0047557962461494034\n",
      "Episode 3434; Testing Loss 0.005865591849386259; Training Loss 0.00475578258988814\n",
      "Episode 3435; Testing Loss 0.005865471236885013; Training Loss 0.004755769486848905\n",
      "Episode 3436; Testing Loss 0.0058654130718788534; Training Loss 0.004755760896745155\n",
      "Episode 3437; Testing Loss 0.005865513710378832; Training Loss 0.004755749028918284\n",
      "Episode 3438; Testing Loss 0.005865548977726326; Training Loss 0.004755735885155618\n",
      "Episode 3439; Testing Loss 0.005865476488368396; Training Loss 0.004755726112850979\n",
      "Episode 3440; Testing Loss 0.005865475039878369; Training Loss 0.004755716114608783\n",
      "Episode 3441; Testing Loss 0.005865531094668415; Training Loss 0.004755703271728919\n",
      "Episode 3442; Testing Loss 0.005865548558625113; Training Loss 0.00475569388404369\n",
      "Episode 3443; Testing Loss 0.005865420644181967; Training Loss 0.004755682054923005\n",
      "Episode 3444; Testing Loss 0.005865361581778634; Training Loss 0.004755673085625623\n",
      "Episode 3445; Testing Loss 0.005865413786703293; Training Loss 0.004755660910288326\n",
      "Episode 3446; Testing Loss 0.005865489797237061; Training Loss 0.004755649866744497\n",
      "Episode 3447; Testing Loss 0.005865463791731468; Training Loss 0.004755637252862493\n",
      "Episode 3448; Testing Loss 0.005865376012258158; Training Loss 0.00475562594548832\n",
      "Episode 3449; Testing Loss 0.0058654080338126705; Training Loss 0.0047556168304120575\n",
      "Episode 3450; Testing Loss 0.005865488350108932; Training Loss 0.004755607870606997\n",
      "Episode 3451; Testing Loss 0.0058654875077688295; Training Loss 0.004755596218959354\n",
      "Episode 3452; Testing Loss 0.005865395206834817; Training Loss 0.004755581998420149\n",
      "Episode 3453; Testing Loss 0.005865361033102973; Training Loss 0.004755568079036408\n",
      "Episode 3454; Testing Loss 0.005865446358875314; Training Loss 0.004755557182777094\n",
      "Episode 3455; Testing Loss 0.005865460304585809; Training Loss 0.004755546433927465\n",
      "Episode 3456; Testing Loss 0.005865327380431443; Training Loss 0.004755533291218129\n",
      "Episode 3457; Testing Loss 0.005865244490846427; Training Loss 0.004755524146519078\n",
      "Episode 3458; Testing Loss 0.005865371860108372; Training Loss 0.004755510738421\n",
      "Episode 3459; Testing Loss 0.00586548625530071; Training Loss 0.004755499607959023\n",
      "Episode 3460; Testing Loss 0.005865439440099644; Training Loss 0.004755488439612796\n",
      "Episode 3461; Testing Loss 0.005865244479541213; Training Loss 0.004755477381419304\n",
      "Episode 3462; Testing Loss 0.005865273951633357; Training Loss 0.004755465169363462\n",
      "Episode 3463; Testing Loss 0.00586539262673503; Training Loss 0.004755455045933474\n",
      "Episode 3464; Testing Loss 0.005865401068739702; Training Loss 0.004755443917803593\n",
      "Episode 3465; Testing Loss 0.00586527939389715; Training Loss 0.004755432412900169\n",
      "Episode 3466; Testing Loss 0.00586521192019773; Training Loss 0.004755420668068392\n",
      "Episode 3467; Testing Loss 0.005865308967128819; Training Loss 0.004755409774003841\n",
      "Episode 3468; Testing Loss 0.005865351100494412; Training Loss 0.004755399117249323\n",
      "Episode 3469; Testing Loss 0.005865333054641892; Training Loss 0.0047553866365714105\n",
      "Episode 3470; Testing Loss 0.005865291243760488; Training Loss 0.00475537376405051\n",
      "Episode 3471; Testing Loss 0.005865232972395677; Training Loss 0.004755366834012857\n",
      "Episode 3472; Testing Loss 0.005865286314395294; Training Loss 0.004755356346081583\n",
      "Episode 3473; Testing Loss 0.005865305714345166; Training Loss 0.004755343085256457\n",
      "Episode 3474; Testing Loss 0.005865313074752429; Training Loss 0.004755329613390787\n",
      "Episode 3475; Testing Loss 0.005865270382440241; Training Loss 0.004755319523964275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3476; Testing Loss 0.0058652177630390805; Training Loss 0.004755308580430795\n",
      "Episode 3477; Testing Loss 0.005865254326533175; Training Loss 0.004755295345568068\n",
      "Episode 3478; Testing Loss 0.005865254139941218; Training Loss 0.004755284387154258\n",
      "Episode 3479; Testing Loss 0.005865275923346985; Training Loss 0.004755273587361552\n",
      "Episode 3480; Testing Loss 0.005865289102470237; Training Loss 0.004755261720436889\n",
      "Episode 3481; Testing Loss 0.005865221198478442; Training Loss 0.004755249571390095\n",
      "Episode 3482; Testing Loss 0.005865195391288254; Training Loss 0.004755238674878338\n",
      "Episode 3483; Testing Loss 0.005865254012648578; Training Loss 0.004755228035230423\n",
      "Episode 3484; Testing Loss 0.005865220799024549; Training Loss 0.004755216187513007\n",
      "Episode 3485; Testing Loss 0.00586519421239971; Training Loss 0.004755206147211786\n",
      "Episode 3486; Testing Loss 0.005865233805490911; Training Loss 0.004755194443669784\n",
      "Episode 3487; Testing Loss 0.005865259194731246; Training Loss 0.0047551824314090415\n",
      "Episode 3488; Testing Loss 0.005865225887050917; Training Loss 0.004755173575120867\n",
      "Episode 3489; Testing Loss 0.005865198007807076; Training Loss 0.004755162547474501\n",
      "Episode 3490; Testing Loss 0.0058652357216405285; Training Loss 0.0047551493700356785\n",
      "Episode 3491; Testing Loss 0.005865270849218739; Training Loss 0.004755140528070173\n",
      "Episode 3492; Testing Loss 0.00586519455314969; Training Loss 0.004755129794702753\n",
      "Episode 3493; Testing Loss 0.005865124747268342; Training Loss 0.004755118147119954\n",
      "Episode 3494; Testing Loss 0.005865170348788528; Training Loss 0.004755107145130201\n",
      "Episode 3495; Testing Loss 0.005865186010480692; Training Loss 0.004755094706461395\n",
      "Episode 3496; Testing Loss 0.005865130336683196; Training Loss 0.0047550864612597695\n",
      "Episode 3497; Testing Loss 0.005865131488417474; Training Loss 0.00475507593697892\n",
      "Episode 3498; Testing Loss 0.005865221904962043; Training Loss 0.004755063741174357\n",
      "Episode 3499; Testing Loss 0.005865251346266438; Training Loss 0.004755049830359911\n",
      "Episode 3500; Testing Loss 0.005865153761082735; Training Loss 0.004755039406994559\n",
      "Episode 3501; Testing Loss 0.00586506177005781; Training Loss 0.004755031191362979\n",
      "Episode 3502; Testing Loss 0.005865015784759294; Training Loss 0.004755018132390449\n",
      "Episode 3503; Testing Loss 0.0058650690508876835; Training Loss 0.004755004694859903\n",
      "Episode 3504; Testing Loss 0.00586515102620276; Training Loss 0.004754995164279756\n",
      "Episode 3505; Testing Loss 0.005865155776759236; Training Loss 0.004754984638031717\n",
      "Episode 3506; Testing Loss 0.005865096455929167; Training Loss 0.004754971927260737\n",
      "Episode 3507; Testing Loss 0.005865057168452216; Training Loss 0.004754960280341835\n",
      "Episode 3508; Testing Loss 0.005865100611602012; Training Loss 0.004754948758542803\n",
      "Episode 3509; Testing Loss 0.005865174255897434; Training Loss 0.004754937474270575\n",
      "Episode 3510; Testing Loss 0.005865115322459559; Training Loss 0.004754924530212232\n",
      "Episode 3511; Testing Loss 0.005864963611467882; Training Loss 0.004754917362870044\n",
      "Episode 3512; Testing Loss 0.005865018202582141; Training Loss 0.004754905779379302\n",
      "Episode 3513; Testing Loss 0.005865163374746578; Training Loss 0.004754893067031448\n",
      "Episode 3514; Testing Loss 0.005865119972002796; Training Loss 0.004754881493384662\n",
      "Episode 3515; Testing Loss 0.005864968311083029; Training Loss 0.004754872774591132\n",
      "Episode 3516; Testing Loss 0.005864971271923724; Training Loss 0.004754860643808156\n",
      "Episode 3517; Testing Loss 0.005865057921531172; Training Loss 0.004754848265785407\n",
      "Episode 3518; Testing Loss 0.005865057257112403; Training Loss 0.0047548356379792395\n",
      "Episode 3519; Testing Loss 0.0058649554448974; Training Loss 0.004754824295549375\n",
      "Episode 3520; Testing Loss 0.005864928682155629; Training Loss 0.00475481358973298\n",
      "Episode 3521; Testing Loss 0.00586495488646789; Training Loss 0.004754801884579646\n",
      "Episode 3522; Testing Loss 0.00586503070468954; Training Loss 0.004754789959714551\n",
      "Episode 3523; Testing Loss 0.005865019332374182; Training Loss 0.0047547807453023495\n",
      "Episode 3524; Testing Loss 0.005864976543637091; Training Loss 0.004754769738657751\n",
      "Episode 3525; Testing Loss 0.005864969311477017; Training Loss 0.004754756259981094\n",
      "Episode 3526; Testing Loss 0.005864893259071203; Training Loss 0.004754747900455363\n",
      "Episode 3527; Testing Loss 0.005864877589825449; Training Loss 0.004754739148631277\n",
      "Episode 3528; Testing Loss 0.005864899733613179; Training Loss 0.004754725750346572\n",
      "Episode 3529; Testing Loss 0.005864906275384149; Training Loss 0.0047547123330856506\n",
      "Episode 3530; Testing Loss 0.005864902195290409; Training Loss 0.004754704586835592\n",
      "Episode 3531; Testing Loss 0.005864987676628576; Training Loss 0.004754694923565522\n",
      "Episode 3532; Testing Loss 0.00586497564907658; Training Loss 0.004754682109859875\n",
      "Episode 3533; Testing Loss 0.005864876028920267; Training Loss 0.004754668170803254\n",
      "Episode 3534; Testing Loss 0.005864826661665466; Training Loss 0.004754658441901516\n",
      "Episode 3535; Testing Loss 0.0058648230434797605; Training Loss 0.004754651560716535\n",
      "Episode 3536; Testing Loss 0.005864848214330575; Training Loss 0.0047546381486376675\n",
      "Episode 3537; Testing Loss 0.005864858671293081; Training Loss 0.004754623151735833\n",
      "Episode 3538; Testing Loss 0.0058648523311218455; Training Loss 0.00475461843216057\n",
      "Episode 3539; Testing Loss 0.005864830743936702; Training Loss 0.004754609510309226\n",
      "Episode 3540; Testing Loss 0.005864830951631311; Training Loss 0.004754598080325544\n",
      "Episode 3541; Testing Loss 0.005864906328757359; Training Loss 0.004754584156285486\n",
      "Episode 3542; Testing Loss 0.005864884621978678; Training Loss 0.004754567746979236\n",
      "Episode 3543; Testing Loss 0.005864754288895263; Training Loss 0.004754559531472948\n",
      "Episode 3544; Testing Loss 0.0058646258838047135; Training Loss 0.0047545547167791745\n",
      "Episode 3545; Testing Loss 0.005864637186533223; Training Loss 0.00475454209316186\n",
      "Episode 3546; Testing Loss 0.005864777654619475; Training Loss 0.004754525053265588\n",
      "Episode 3547; Testing Loss 0.00586484990808066; Training Loss 0.004754513621859639\n",
      "Episode 3548; Testing Loss 0.005864784656192408; Training Loss 0.004754505211236211\n",
      "Episode 3549; Testing Loss 0.005864707830457055; Training Loss 0.004754494027671571\n",
      "Episode 3550; Testing Loss 0.005864727637144441; Training Loss 0.004754480467526545\n",
      "Episode 3551; Testing Loss 0.005864836674627551; Training Loss 0.004754467488491291\n",
      "Episode 3552; Testing Loss 0.005864816640166358; Training Loss 0.004754457068061866\n",
      "Episode 3553; Testing Loss 0.005864721314689391; Training Loss 0.004754446143838762\n",
      "Episode 3554; Testing Loss 0.005864643541518322; Training Loss 0.004754434723332624\n",
      "Episode 3555; Testing Loss 0.005864681905527135; Training Loss 0.004754423052382554\n",
      "Episode 3556; Testing Loss 0.005864812250244215; Training Loss 0.004754411769857716\n",
      "Episode 3557; Testing Loss 0.005864828197866151; Training Loss 0.004754400226160154\n",
      "Episode 3558; Testing Loss 0.005864650588384412; Training Loss 0.0047543894272784335\n",
      "Episode 3559; Testing Loss 0.0058646093579055; Training Loss 0.004754377513156136\n",
      "Episode 3560; Testing Loss 0.005864743408245995; Training Loss 0.004754366285571031\n",
      "Episode 3561; Testing Loss 0.005864703928791718; Training Loss 0.0047543546845588225\n",
      "Episode 3562; Testing Loss 0.005864537363116253; Training Loss 0.0047543462244420236\n",
      "Episode 3563; Testing Loss 0.005864566183242637; Training Loss 0.004754333742809358\n",
      "Episode 3564; Testing Loss 0.00586473022096723; Training Loss 0.004754322414336301\n",
      "Episode 3565; Testing Loss 0.0058647342139072035; Training Loss 0.004754310961659979\n",
      "Episode 3566; Testing Loss 0.0058645735190685395; Training Loss 0.004754300110912807\n",
      "Episode 3567; Testing Loss 0.005864570717188528; Training Loss 0.004754288726017308\n",
      "Episode 3568; Testing Loss 0.005864694435202706; Training Loss 0.004754275944529399\n",
      "Episode 3569; Testing Loss 0.005864729763057259; Training Loss 0.004754266369336174\n",
      "Episode 3570; Testing Loss 0.005864536249872243; Training Loss 0.0047542540452636855\n",
      "Episode 3571; Testing Loss 0.005864462617645149; Training Loss 0.004754243674004681\n",
      "Episode 3572; Testing Loss 0.005864601413152993; Training Loss 0.0047542320840706686\n",
      "Episode 3573; Testing Loss 0.005864730150378714; Training Loss 0.004754222348565635\n",
      "Episode 3574; Testing Loss 0.005864613037830677; Training Loss 0.004754209663927619\n",
      "Episode 3575; Testing Loss 0.00586446589036159; Training Loss 0.0047541998409198\n",
      "Episode 3576; Testing Loss 0.005864521080379674; Training Loss 0.004754185574808186\n",
      "Episode 3577; Testing Loss 0.0058646381459999865; Training Loss 0.0047541772650769995\n",
      "Episode 3578; Testing Loss 0.005864594872715676; Training Loss 0.004754165321660502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3579; Testing Loss 0.005864487416009067; Training Loss 0.0047541517521801436\n",
      "Episode 3580; Testing Loss 0.0058644615103746875; Training Loss 0.004754142620726423\n",
      "Episode 3581; Testing Loss 0.005864509728039978; Training Loss 0.004754129925617349\n",
      "Episode 3582; Testing Loss 0.005864571497002841; Training Loss 0.004754121469473567\n",
      "Episode 3583; Testing Loss 0.0058646260248479895; Training Loss 0.004754110459768187\n",
      "Episode 3584; Testing Loss 0.00586456205801759; Training Loss 0.004754097879441638\n",
      "Episode 3585; Testing Loss 0.0058644221607755875; Training Loss 0.004754086523276496\n",
      "Episode 3586; Testing Loss 0.005864386143738515; Training Loss 0.004754077408708417\n",
      "Episode 3587; Testing Loss 0.005864471225237521; Training Loss 0.004754066682601398\n",
      "Episode 3588; Testing Loss 0.005864494699209741; Training Loss 0.004754054097413231\n",
      "Episode 3589; Testing Loss 0.005864438772703246; Training Loss 0.004754042757580878\n",
      "Episode 3590; Testing Loss 0.005864471287094639; Training Loss 0.0047540317525713565\n",
      "Episode 3591; Testing Loss 0.005864561173514476; Training Loss 0.004754020292877505\n",
      "Episode 3592; Testing Loss 0.005864512140750749; Training Loss 0.004754009133043565\n",
      "Episode 3593; Testing Loss 0.005864402261464883; Training Loss 0.004753997459690491\n",
      "Episode 3594; Testing Loss 0.005864403987465675; Training Loss 0.004753987605755667\n",
      "Episode 3595; Testing Loss 0.005864511903546941; Training Loss 0.0047539777146217614\n",
      "Episode 3596; Testing Loss 0.005864483506146258; Training Loss 0.004753964352446542\n",
      "Episode 3597; Testing Loss 0.005864382677579372; Training Loss 0.004753957103535562\n",
      "Episode 3598; Testing Loss 0.0058643916385453815; Training Loss 0.004753946304690315\n",
      "Episode 3599; Testing Loss 0.00586451291415655; Training Loss 0.004753931025008863\n",
      "Episode 3600; Testing Loss 0.005864573521283306; Training Loss 0.004753923788355975\n",
      "Episode 3601; Testing Loss 0.00586443463492394; Training Loss 0.0047539104464536745\n",
      "Episode 3602; Testing Loss 0.005864309230088301; Training Loss 0.004753900380076883\n",
      "Episode 3603; Testing Loss 0.005864394823990743; Training Loss 0.004753888188689924\n",
      "Episode 3604; Testing Loss 0.0058644890440856775; Training Loss 0.0047538773070912406\n",
      "Episode 3605; Testing Loss 0.005864424815756244; Training Loss 0.004753865760564651\n",
      "Episode 3606; Testing Loss 0.0058643193763464226; Training Loss 0.004753854588460593\n",
      "Episode 3607; Testing Loss 0.005864330290197924; Training Loss 0.004753843415203995\n",
      "Episode 3608; Testing Loss 0.005864445055448798; Training Loss 0.004753832858067279\n",
      "Episode 3609; Testing Loss 0.0058644432526348735; Training Loss 0.004753820794378487\n",
      "Episode 3610; Testing Loss 0.005864352339675898; Training Loss 0.004753812016884307\n",
      "Episode 3611; Testing Loss 0.005864325179324957; Training Loss 0.004753801805275262\n",
      "Episode 3612; Testing Loss 0.005864426124388254; Training Loss 0.004753788379993016\n",
      "Episode 3613; Testing Loss 0.005864487740793709; Training Loss 0.004753781741169228\n",
      "Episode 3614; Testing Loss 0.005864372247563377; Training Loss 0.004753768533885519\n",
      "Episode 3615; Testing Loss 0.00586427862205065; Training Loss 0.004753757866132574\n",
      "Episode 3616; Testing Loss 0.005864381370744933; Training Loss 0.00475374724466418\n",
      "Episode 3617; Testing Loss 0.005864472311445329; Training Loss 0.004753734914724252\n",
      "Episode 3618; Testing Loss 0.0058643778847083765; Training Loss 0.004753723636569039\n",
      "Episode 3619; Testing Loss 0.0058642698338674925; Training Loss 0.004753715152841178\n",
      "Episode 3620; Testing Loss 0.00586429528677934; Training Loss 0.00475370221365332\n",
      "Episode 3621; Testing Loss 0.005864379163487758; Training Loss 0.004753693833573896\n",
      "Episode 3622; Testing Loss 0.005864383136059946; Training Loss 0.004753682490132824\n",
      "Episode 3623; Testing Loss 0.005864310951351415; Training Loss 0.004753670034498302\n",
      "Episode 3624; Testing Loss 0.005864265042519411; Training Loss 0.004753658310580379\n",
      "Episode 3625; Testing Loss 0.005864285263092669; Training Loss 0.004753647483082274\n",
      "Episode 3626; Testing Loss 0.0058643162334215335; Training Loss 0.0047536352094649315\n",
      "Episode 3627; Testing Loss 0.0058642703476188985; Training Loss 0.004753624201988449\n",
      "Episode 3628; Testing Loss 0.005864229333930309; Training Loss 0.004753613181057954\n",
      "Episode 3629; Testing Loss 0.005864234853193269; Training Loss 0.004753603032447092\n",
      "Episode 3630; Testing Loss 0.005864225603787459; Training Loss 0.004753593393276411\n",
      "Episode 3631; Testing Loss 0.005864180002010137; Training Loss 0.004753580334412924\n",
      "Episode 3632; Testing Loss 0.00586420956705003; Training Loss 0.004753568453654541\n",
      "Episode 3633; Testing Loss 0.005864285791655974; Training Loss 0.004753558839219768\n",
      "Episode 3634; Testing Loss 0.00586434386296024; Training Loss 0.004753548295700017\n",
      "Episode 3635; Testing Loss 0.005864310261773626; Training Loss 0.00475353746563608\n",
      "Episode 3636; Testing Loss 0.0058642164169856; Training Loss 0.0047535256690923195\n",
      "Episode 3637; Testing Loss 0.005864194473024969; Training Loss 0.0047535140807744075\n",
      "Episode 3638; Testing Loss 0.005864227348494227; Training Loss 0.004753505177240552\n",
      "Episode 3639; Testing Loss 0.0058642017682519155; Training Loss 0.004753493813064114\n",
      "Episode 3640; Testing Loss 0.005864183441163774; Training Loss 0.0047534831679915185\n",
      "Episode 3641; Testing Loss 0.00586423941567217; Training Loss 0.004753470354097026\n",
      "Episode 3642; Testing Loss 0.005864272465281023; Training Loss 0.004753458987149191\n",
      "Episode 3643; Testing Loss 0.005864205134029734; Training Loss 0.004753447641023748\n",
      "Episode 3644; Testing Loss 0.005864165999789298; Training Loss 0.004753437506751482\n",
      "Episode 3645; Testing Loss 0.005864159030936476; Training Loss 0.0047534254465312915\n",
      "Episode 3646; Testing Loss 0.0058641806269351185; Training Loss 0.004753414561194018\n",
      "Episode 3647; Testing Loss 0.005864174182789895; Training Loss 0.004753404547022349\n",
      "Episode 3648; Testing Loss 0.005864187922410377; Training Loss 0.004753393800302325\n",
      "Episode 3649; Testing Loss 0.005864190307631594; Training Loss 0.004753382683039229\n",
      "Episode 3650; Testing Loss 0.005864176655853579; Training Loss 0.004753371369293026\n",
      "Episode 3651; Testing Loss 0.005864162770659738; Training Loss 0.004753360940518309\n",
      "Episode 3652; Testing Loss 0.005864181349238367; Training Loss 0.004753351040827644\n",
      "Episode 3653; Testing Loss 0.005864187887362955; Training Loss 0.004753338922713616\n",
      "Episode 3654; Testing Loss 0.005864168159584476; Training Loss 0.004753329311102632\n",
      "Episode 3655; Testing Loss 0.005864095590058641; Training Loss 0.0047533160815694635\n",
      "Episode 3656; Testing Loss 0.005864084180483645; Training Loss 0.00475330905873373\n",
      "Episode 3657; Testing Loss 0.005864155778559333; Training Loss 0.004753297622789718\n",
      "Episode 3658; Testing Loss 0.005864155900277208; Training Loss 0.004753285630865537\n",
      "Episode 3659; Testing Loss 0.005864125091895081; Training Loss 0.004753276183449873\n",
      "Episode 3660; Testing Loss 0.005864086701588024; Training Loss 0.004753263971317709\n",
      "Episode 3661; Testing Loss 0.005864101918657182; Training Loss 0.004753251042336968\n",
      "Episode 3662; Testing Loss 0.005864166883796589; Training Loss 0.004753241386368797\n",
      "Episode 3663; Testing Loss 0.005864126968802374; Training Loss 0.004753229813642976\n",
      "Episode 3664; Testing Loss 0.005863988730764669; Training Loss 0.00475322048705362\n",
      "Episode 3665; Testing Loss 0.005863948851770113; Training Loss 0.004753209879367547\n",
      "Episode 3666; Testing Loss 0.00586403428270343; Training Loss 0.004753196910971875\n",
      "Episode 3667; Testing Loss 0.005864107718758681; Training Loss 0.004753187275075773\n",
      "Episode 3668; Testing Loss 0.005864029364348379; Training Loss 0.004753174222425005\n",
      "Episode 3669; Testing Loss 0.005863927985099399; Training Loss 0.004753166743315017\n",
      "Episode 3670; Testing Loss 0.005864038335842432; Training Loss 0.004753155493574302\n",
      "Episode 3671; Testing Loss 0.00586414941702066; Training Loss 0.0047531431707018065\n",
      "Episode 3672; Testing Loss 0.0058641001034694246; Training Loss 0.004753135880819385\n",
      "Episode 3673; Testing Loss 0.005863901467528865; Training Loss 0.004753123643434911\n",
      "Episode 3674; Testing Loss 0.005863845285189466; Training Loss 0.004753113039033937\n",
      "Episode 3675; Testing Loss 0.0058639873977997465; Training Loss 0.004753102231950385\n",
      "Episode 3676; Testing Loss 0.005864090105013205; Training Loss 0.0047530900123839296\n",
      "Episode 3677; Testing Loss 0.005864001913296482; Training Loss 0.004753077742532774\n",
      "Episode 3678; Testing Loss 0.005863907296308661; Training Loss 0.004753068945296184\n",
      "Episode 3679; Testing Loss 0.0058639221178894575; Training Loss 0.004753057353476044\n",
      "Episode 3680; Testing Loss 0.005863995112439988; Training Loss 0.004753047688744533\n",
      "Episode 3681; Testing Loss 0.005863970190355664; Training Loss 0.004753035025035071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3682; Testing Loss 0.005863848128689939; Training Loss 0.004753025503543371\n",
      "Episode 3683; Testing Loss 0.005863793248885029; Training Loss 0.004753015594680414\n",
      "Episode 3684; Testing Loss 0.005863896248670902; Training Loss 0.004753003000772371\n",
      "Episode 3685; Testing Loss 0.00586396720630459; Training Loss 0.004752990772599427\n",
      "Episode 3686; Testing Loss 0.005863976266127499; Training Loss 0.004752982467156424\n",
      "Episode 3687; Testing Loss 0.005863926569399819; Training Loss 0.00475297013392079\n",
      "Episode 3688; Testing Loss 0.005863905893221791; Training Loss 0.004752957625328724\n",
      "Episode 3689; Testing Loss 0.0058639002383774; Training Loss 0.004752946937423844\n",
      "Episode 3690; Testing Loss 0.005863890848881122; Training Loss 0.004752936847308043\n",
      "Episode 3691; Testing Loss 0.005863867200323976; Training Loss 0.004752926787956708\n",
      "Episode 3692; Testing Loss 0.005863861895410451; Training Loss 0.004752914387498397\n",
      "Episode 3693; Testing Loss 0.005863905325338273; Training Loss 0.00475290402598935\n",
      "Episode 3694; Testing Loss 0.005863918356478691; Training Loss 0.0047528933150262\n",
      "Episode 3695; Testing Loss 0.005863935956533007; Training Loss 0.0047528810304267735\n",
      "Episode 3696; Testing Loss 0.005863877577443369; Training Loss 0.004752873534179585\n",
      "Episode 3697; Testing Loss 0.005863781895359526; Training Loss 0.004752862388431139\n",
      "Episode 3698; Testing Loss 0.005863745343781403; Training Loss 0.004752849587381257\n",
      "Episode 3699; Testing Loss 0.005863862829547844; Training Loss 0.004752839915763453\n",
      "Episode 3700; Testing Loss 0.005863912281167411; Training Loss 0.004752828611506268\n",
      "Episode 3701; Testing Loss 0.005863789157063891; Training Loss 0.004752816868630609\n",
      "Episode 3702; Testing Loss 0.005863729941781958; Training Loss 0.004752806652208533\n",
      "Episode 3703; Testing Loss 0.005863845648273241; Training Loss 0.004752796204576438\n",
      "Episode 3704; Testing Loss 0.005863911542915078; Training Loss 0.004752786734652574\n",
      "Episode 3705; Testing Loss 0.005863769949978008; Training Loss 0.004752772371226772\n",
      "Episode 3706; Testing Loss 0.005863630222002467; Training Loss 0.004752763217783419\n",
      "Episode 3707; Testing Loss 0.005863697547089025; Training Loss 0.004752751479995299\n",
      "Episode 3708; Testing Loss 0.00586385870328468; Training Loss 0.004752742523894493\n",
      "Episode 3709; Testing Loss 0.005863776037899164; Training Loss 0.0047527298841561864\n",
      "Episode 3710; Testing Loss 0.005863662534017209; Training Loss 0.004752720193662477\n",
      "Episode 3711; Testing Loss 0.005863734920632803; Training Loss 0.0047527081717248995\n",
      "Episode 3712; Testing Loss 0.0058638590853230055; Training Loss 0.004752699184885595\n",
      "Episode 3713; Testing Loss 0.005863763252606646; Training Loss 0.004752687049731806\n",
      "Episode 3714; Testing Loss 0.005863616374906304; Training Loss 0.004752675752372934\n",
      "Episode 3715; Testing Loss 0.005863619999102639; Training Loss 0.004752666104662828\n",
      "Episode 3716; Testing Loss 0.005863689673127794; Training Loss 0.004752655239510515\n",
      "Episode 3717; Testing Loss 0.0058637484176175736; Training Loss 0.004752643931622079\n",
      "Episode 3718; Testing Loss 0.005863752370965731; Training Loss 0.004752632569811684\n",
      "Episode 3719; Testing Loss 0.005863713153268555; Training Loss 0.004752624799266417\n",
      "Episode 3720; Testing Loss 0.005863637647820707; Training Loss 0.004752612328696371\n",
      "Episode 3721; Testing Loss 0.005863570269447792; Training Loss 0.004752602525833178\n",
      "Episode 3722; Testing Loss 0.005863647362560558; Training Loss 0.004752593325305266\n",
      "Episode 3723; Testing Loss 0.005863759051993362; Training Loss 0.004752581916594317\n",
      "Episode 3724; Testing Loss 0.005863719339444163; Training Loss 0.004752567226576322\n",
      "Episode 3725; Testing Loss 0.0058635805877221255; Training Loss 0.004752562445927633\n",
      "Episode 3726; Testing Loss 0.005863567729528422; Training Loss 0.004752552535092408\n",
      "Episode 3727; Testing Loss 0.0058637035772936275; Training Loss 0.00475253583583841\n",
      "Episode 3728; Testing Loss 0.005863732206751891; Training Loss 0.004752527054756058\n",
      "Episode 3729; Testing Loss 0.0058636314727752974; Training Loss 0.004752519008162082\n",
      "Episode 3730; Testing Loss 0.00586359768702872; Training Loss 0.0047525083400706955\n",
      "Episode 3731; Testing Loss 0.005863662414775593; Training Loss 0.004752493748886339\n",
      "Episode 3732; Testing Loss 0.005863645304203151; Training Loss 0.00475248405908666\n",
      "Episode 3733; Testing Loss 0.005863496422994893; Training Loss 0.004752473095092145\n",
      "Episode 3734; Testing Loss 0.005863445518590655; Training Loss 0.004752461470132476\n",
      "Episode 3735; Testing Loss 0.005863624373172156; Training Loss 0.004752450136321368\n",
      "Episode 3736; Testing Loss 0.005863708564911521; Training Loss 0.004752439836574218\n",
      "Episode 3737; Testing Loss 0.005863533323254206; Training Loss 0.00475243058175392\n",
      "Episode 3738; Testing Loss 0.005863320375630785; Training Loss 0.004752421255422085\n",
      "Episode 3739; Testing Loss 0.0058634284287388885; Training Loss 0.004752407376988756\n",
      "Episode 3740; Testing Loss 0.005863732685159443; Training Loss 0.004752397374065555\n",
      "Episode 3741; Testing Loss 0.005863716180722973; Training Loss 0.0047523863182674445\n",
      "Episode 3742; Testing Loss 0.005863438016293416; Training Loss 0.004752374907185571\n",
      "Episode 3743; Testing Loss 0.005863368332717567; Training Loss 0.004752366053844777\n",
      "Episode 3744; Testing Loss 0.005863567506868868; Training Loss 0.004752354026597455\n",
      "Episode 3745; Testing Loss 0.00586363887702276; Training Loss 0.004752345692024041\n",
      "Episode 3746; Testing Loss 0.005863422973046305; Training Loss 0.004752332811790904\n",
      "Episode 3747; Testing Loss 0.005863310104292632; Training Loss 0.004752324147179109\n",
      "Episode 3748; Testing Loss 0.005863464888093543; Training Loss 0.004752310145269468\n",
      "Episode 3749; Testing Loss 0.005863652360320032; Training Loss 0.004752299881162762\n",
      "Episode 3750; Testing Loss 0.0058635395992546165; Training Loss 0.004752287230842018\n",
      "Episode 3751; Testing Loss 0.005863366778466408; Training Loss 0.004752278266362735\n",
      "Episode 3752; Testing Loss 0.005863369308971983; Training Loss 0.004752265671254388\n",
      "Episode 3753; Testing Loss 0.005863527622442717; Training Loss 0.004752256300344521\n",
      "Episode 3754; Testing Loss 0.0058634875954205615; Training Loss 0.004752244138962778\n",
      "Episode 3755; Testing Loss 0.005863377439320918; Training Loss 0.004752232670323732\n",
      "Episode 3756; Testing Loss 0.005863320135184733; Training Loss 0.004752222562293868\n",
      "Episode 3757; Testing Loss 0.005863413908649534; Training Loss 0.004752212438381555\n",
      "Episode 3758; Testing Loss 0.005863531429311287; Training Loss 0.004752201454420756\n",
      "Episode 3759; Testing Loss 0.005863451630908032; Training Loss 0.0047521924115614955\n",
      "Episode 3760; Testing Loss 0.005863312163864449; Training Loss 0.004752178964451269\n",
      "Episode 3761; Testing Loss 0.005863307084766775; Training Loss 0.004752169951959467\n",
      "Episode 3762; Testing Loss 0.005863410446154481; Training Loss 0.004752159198412387\n",
      "Episode 3763; Testing Loss 0.005863427617531906; Training Loss 0.004752147293513766\n",
      "Episode 3764; Testing Loss 0.005863325803226059; Training Loss 0.004752136301736714\n",
      "Episode 3765; Testing Loss 0.005863272634879666; Training Loss 0.004752125279087287\n",
      "Episode 3766; Testing Loss 0.005863360000529256; Training Loss 0.00475211464424043\n",
      "Episode 3767; Testing Loss 0.005863385298293083; Training Loss 0.004752103034991357\n",
      "Episode 3768; Testing Loss 0.005863294088633521; Training Loss 0.004752093955650719\n",
      "Episode 3769; Testing Loss 0.005863245345256095; Training Loss 0.004752083170480944\n",
      "Episode 3770; Testing Loss 0.005863257361686955; Training Loss 0.0047520708901903205\n",
      "Episode 3771; Testing Loss 0.005863229681293494; Training Loss 0.004752061124394491\n",
      "Episode 3772; Testing Loss 0.005863193637633265; Training Loss 0.004752049475662897\n",
      "Episode 3773; Testing Loss 0.0058631888390410685; Training Loss 0.004752038269221307\n",
      "Episode 3774; Testing Loss 0.0058632827190933995; Training Loss 0.004752028595395042\n",
      "Episode 3775; Testing Loss 0.005863301270600156; Training Loss 0.004752017172331662\n",
      "Episode 3776; Testing Loss 0.005863242778537149; Training Loss 0.0047520067306233114\n",
      "Episode 3777; Testing Loss 0.005863234819546273; Training Loss 0.00475199612097454\n",
      "Episode 3778; Testing Loss 0.0058631798180908655; Training Loss 0.004751985319921153\n",
      "Episode 3779; Testing Loss 0.005863144208931684; Training Loss 0.004751974328614502\n",
      "Episode 3780; Testing Loss 0.00586312033337137; Training Loss 0.0047519644828437095\n",
      "Episode 3781; Testing Loss 0.005863177387547223; Training Loss 0.004751953218363034\n",
      "Episode 3782; Testing Loss 0.005863154433881819; Training Loss 0.004751943221669075\n",
      "Episode 3783; Testing Loss 0.00586313290051781; Training Loss 0.004751931152575796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3784; Testing Loss 0.005863159127413162; Training Loss 0.004751921633714309\n",
      "Episode 3785; Testing Loss 0.0058631541876641895; Training Loss 0.004751911945652458\n",
      "Episode 3786; Testing Loss 0.005863109058778913; Training Loss 0.004751899729864391\n",
      "Episode 3787; Testing Loss 0.005863078335661591; Training Loss 0.004751888615008934\n",
      "Episode 3788; Testing Loss 0.005863102676386058; Training Loss 0.004751881216327037\n",
      "Episode 3789; Testing Loss 0.005863077977606715; Training Loss 0.004751867829812083\n",
      "Episode 3790; Testing Loss 0.005863048374989427; Training Loss 0.004751859196165573\n",
      "Episode 3791; Testing Loss 0.0058630870329715165; Training Loss 0.004751851242571606\n",
      "Episode 3792; Testing Loss 0.005863098632812342; Training Loss 0.004751840158830403\n",
      "Episode 3793; Testing Loss 0.00586307649000348; Training Loss 0.004751824865768136\n",
      "Episode 3794; Testing Loss 0.005863047411670626; Training Loss 0.004751815193913306\n",
      "Episode 3795; Testing Loss 0.005862999915973752; Training Loss 0.004751806493240084\n",
      "Episode 3796; Testing Loss 0.005862969779041215; Training Loss 0.004751794140833959\n",
      "Episode 3797; Testing Loss 0.005863024311203887; Training Loss 0.004751783380648133\n",
      "Episode 3798; Testing Loss 0.0058630729663794; Training Loss 0.004751769701608769\n",
      "Episode 3799; Testing Loss 0.005863055585884553; Training Loss 0.0047517631745306824\n",
      "Episode 3800; Testing Loss 0.0058629413875682535; Training Loss 0.004751751704841699\n",
      "Episode 3801; Testing Loss 0.0058629450759609385; Training Loss 0.004751740493757256\n",
      "Episode 3802; Testing Loss 0.005863064105004346; Training Loss 0.004751729436427351\n",
      "Episode 3803; Testing Loss 0.005863107353967261; Training Loss 0.004751720334791564\n",
      "Episode 3804; Testing Loss 0.005863030375330059; Training Loss 0.004751708016329809\n",
      "Episode 3805; Testing Loss 0.005862969370924883; Training Loss 0.00475169960360604\n",
      "Episode 3806; Testing Loss 0.005862949625447897; Training Loss 0.0047516885544207806\n",
      "Episode 3807; Testing Loss 0.005862976039247787; Training Loss 0.00475167460812661\n",
      "Episode 3808; Testing Loss 0.005863016330422645; Training Loss 0.004751663704663385\n",
      "Episode 3809; Testing Loss 0.0058629953182206315; Training Loss 0.004751653836539586\n",
      "Episode 3810; Testing Loss 0.005862916055852161; Training Loss 0.004751645352899404\n",
      "Episode 3811; Testing Loss 0.005862878868077137; Training Loss 0.004751634664785391\n",
      "Episode 3812; Testing Loss 0.005862894284816145; Training Loss 0.004751619272470025\n",
      "Episode 3813; Testing Loss 0.005862913191985624; Training Loss 0.004751610048714357\n",
      "Episode 3814; Testing Loss 0.005862907592415924; Training Loss 0.00475159968008293\n",
      "Episode 3815; Testing Loss 0.005862880329229942; Training Loss 0.004751590426960845\n",
      "Episode 3816; Testing Loss 0.005862830670991278; Training Loss 0.004751578710433458\n",
      "Episode 3817; Testing Loss 0.005862853270058758; Training Loss 0.004751566752276112\n",
      "Episode 3818; Testing Loss 0.005862937531877158; Training Loss 0.00475155626299278\n",
      "Episode 3819; Testing Loss 0.005862939786806824; Training Loss 0.00475154593159251\n",
      "Episode 3820; Testing Loss 0.005862871918677042; Training Loss 0.004751533052497187\n",
      "Episode 3821; Testing Loss 0.005862853192034374; Training Loss 0.0047515237705180015\n",
      "Episode 3822; Testing Loss 0.005862941531658903; Training Loss 0.004751514095691822\n",
      "Episode 3823; Testing Loss 0.005862958177129799; Training Loss 0.00475150269412233\n",
      "Episode 3824; Testing Loss 0.005862834234557995; Training Loss 0.0047514901225310276\n",
      "Episode 3825; Testing Loss 0.005862759454430616; Training Loss 0.004751479592428351\n",
      "Episode 3826; Testing Loss 0.005862824023807942; Training Loss 0.004751469866942947\n",
      "Episode 3827; Testing Loss 0.0058628873417030415; Training Loss 0.004751458107347061\n",
      "Episode 3828; Testing Loss 0.0058628568541958664; Training Loss 0.004751447633703743\n",
      "Episode 3829; Testing Loss 0.005862824575658509; Training Loss 0.004751438213089067\n",
      "Episode 3830; Testing Loss 0.005862822621340225; Training Loss 0.004751425433457865\n",
      "Episode 3831; Testing Loss 0.005862801524826113; Training Loss 0.0047514144382606125\n",
      "Episode 3832; Testing Loss 0.0058627801207385595; Training Loss 0.004751403390533821\n",
      "Episode 3833; Testing Loss 0.005862775623378593; Training Loss 0.004751393531697556\n",
      "Episode 3834; Testing Loss 0.005862727157677442; Training Loss 0.0047513817340849055\n",
      "Episode 3835; Testing Loss 0.0058627188525001836; Training Loss 0.004751371732970302\n",
      "Episode 3836; Testing Loss 0.005862752712233785; Training Loss 0.004751361047191227\n",
      "Episode 3837; Testing Loss 0.0058627162874595965; Training Loss 0.004751351389916379\n",
      "Episode 3838; Testing Loss 0.005862673792910357; Training Loss 0.004751340204346377\n",
      "Episode 3839; Testing Loss 0.005862752010075002; Training Loss 0.004751330077203978\n",
      "Episode 3840; Testing Loss 0.005862832218137578; Training Loss 0.00475132092285134\n",
      "Episode 3841; Testing Loss 0.005862725783095691; Training Loss 0.004751306873796875\n",
      "Episode 3842; Testing Loss 0.005862545249793318; Training Loss 0.004751301383344958\n",
      "Episode 3843; Testing Loss 0.005862551120066582; Training Loss 0.004751290108622349\n",
      "Episode 3844; Testing Loss 0.005862682689829149; Training Loss 0.004751274550700729\n",
      "Episode 3845; Testing Loss 0.005862775991512752; Training Loss 0.0047512662690363605\n",
      "Episode 3846; Testing Loss 0.005862726315701013; Training Loss 0.0047512542250316425\n",
      "Episode 3847; Testing Loss 0.005862645691594783; Training Loss 0.004751246795726439\n",
      "Episode 3848; Testing Loss 0.005862647862510309; Training Loss 0.004751233699968697\n",
      "Episode 3849; Testing Loss 0.005862721320865318; Training Loss 0.004751224450006396\n",
      "Episode 3850; Testing Loss 0.005862683568863439; Training Loss 0.004751213908536927\n",
      "Episode 3851; Testing Loss 0.005862561659714612; Training Loss 0.0047511991540149226\n",
      "Episode 3852; Testing Loss 0.005862500633226765; Training Loss 0.004751191080912321\n",
      "Episode 3853; Testing Loss 0.0058626015683019925; Training Loss 0.004751180479898987\n",
      "Episode 3854; Testing Loss 0.005862702007484687; Training Loss 0.004751170257221492\n",
      "Episode 3855; Testing Loss 0.005862604181025301; Training Loss 0.004751156790130944\n",
      "Episode 3856; Testing Loss 0.00586246427681431; Training Loss 0.004751148832137788\n",
      "Episode 3857; Testing Loss 0.005862411450859763; Training Loss 0.00475113883836692\n",
      "Episode 3858; Testing Loss 0.0058624817920179696; Training Loss 0.004751125900356337\n",
      "Episode 3859; Testing Loss 0.00586248879840011; Training Loss 0.004751115448185402\n",
      "Episode 3860; Testing Loss 0.005862483909405202; Training Loss 0.0047511055158808895\n",
      "Episode 3861; Testing Loss 0.005862471724200151; Training Loss 0.004751094890139444\n",
      "Episode 3862; Testing Loss 0.005862528927807529; Training Loss 0.0047510837021226835\n",
      "Episode 3863; Testing Loss 0.005862548482209755; Training Loss 0.004751072046981386\n",
      "Episode 3864; Testing Loss 0.005862520861298706; Training Loss 0.004751061756523512\n",
      "Episode 3865; Testing Loss 0.005862479890751346; Training Loss 0.00475105001252655\n",
      "Episode 3866; Testing Loss 0.005862444167358667; Training Loss 0.004751039533517293\n",
      "Episode 3867; Testing Loss 0.005862466237577504; Training Loss 0.004751028076865241\n",
      "Episode 3868; Testing Loss 0.0058624369089607265; Training Loss 0.004751019653329253\n",
      "Episode 3869; Testing Loss 0.005862392764333455; Training Loss 0.004751006523699066\n",
      "Episode 3870; Testing Loss 0.005862389627531062; Training Loss 0.004750997911318107\n",
      "Episode 3871; Testing Loss 0.005862495826533525; Training Loss 0.004750987204122907\n",
      "Episode 3872; Testing Loss 0.005862552392668413; Training Loss 0.004750976328171182\n",
      "Episode 3873; Testing Loss 0.005862429991309862; Training Loss 0.004750968101684746\n",
      "Episode 3874; Testing Loss 0.0058622609759700055; Training Loss 0.0047509571027968996\n",
      "Episode 3875; Testing Loss 0.0058623144307445975; Training Loss 0.004750946159558126\n",
      "Episode 3876; Testing Loss 0.005862505813874335; Training Loss 0.004750936946796299\n",
      "Episode 3877; Testing Loss 0.005862499508236128; Training Loss 0.004750925484937943\n",
      "Episode 3878; Testing Loss 0.005862308315906403; Training Loss 0.004750911161838131\n",
      "Episode 3879; Testing Loss 0.0058622331212517835; Training Loss 0.004750903007310638\n",
      "Episode 3880; Testing Loss 0.005862403205665781; Training Loss 0.004750890153248057\n",
      "Episode 3881; Testing Loss 0.005862556466272489; Training Loss 0.004750881469519043\n",
      "Episode 3882; Testing Loss 0.00586241459800177; Training Loss 0.004750868815776677\n",
      "Episode 3883; Testing Loss 0.00586226351054013; Training Loss 0.004750857107790425\n",
      "Episode 3884; Testing Loss 0.005862258316079055; Training Loss 0.0047508470437048035\n",
      "Episode 3885; Testing Loss 0.005862336090666333; Training Loss 0.004750837225989398\n",
      "Episode 3886; Testing Loss 0.005862375958928129; Training Loss 0.004750825162606354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3887; Testing Loss 0.005862352496126567; Training Loss 0.004750818044539229\n",
      "Episode 3888; Testing Loss 0.0058622545867043985; Training Loss 0.004750804835243035\n",
      "Episode 3889; Testing Loss 0.005862255809174714; Training Loss 0.004750796845070793\n",
      "Episode 3890; Testing Loss 0.005862370520438328; Training Loss 0.004750787629564247\n",
      "Episode 3891; Testing Loss 0.005862400823067064; Training Loss 0.004750776974088001\n",
      "Episode 3892; Testing Loss 0.005862296881882935; Training Loss 0.00475076307336503\n",
      "Episode 3893; Testing Loss 0.005862198075517623; Training Loss 0.004750753171597561\n",
      "Episode 3894; Testing Loss 0.005862169482229497; Training Loss 0.0047507431511628825\n",
      "Episode 3895; Testing Loss 0.0058622705703544106; Training Loss 0.004750729390547993\n",
      "Episode 3896; Testing Loss 0.005862348601460663; Training Loss 0.004750720341018633\n",
      "Episode 3897; Testing Loss 0.005862321467520294; Training Loss 0.00475070765917959\n",
      "Episode 3898; Testing Loss 0.0058622465519079575; Training Loss 0.004750701024082651\n",
      "Episode 3899; Testing Loss 0.00586219349546296; Training Loss 0.004750688477115831\n",
      "Episode 3900; Testing Loss 0.005862208340606719; Training Loss 0.0047506781639185\n",
      "Episode 3901; Testing Loss 0.005862273061584112; Training Loss 0.004750670755452864\n",
      "Episode 3902; Testing Loss 0.005862240198466889; Training Loss 0.004750659581513263\n",
      "Episode 3903; Testing Loss 0.0058621655751114506; Training Loss 0.004750644965459765\n",
      "Episode 3904; Testing Loss 0.005862158122558405; Training Loss 0.0047506366567433575\n",
      "Episode 3905; Testing Loss 0.005862138072648277; Training Loss 0.004750627454656747\n",
      "Episode 3906; Testing Loss 0.005862145205434623; Training Loss 0.004750612560187836\n",
      "Episode 3907; Testing Loss 0.0058621865396743; Training Loss 0.004750601620562304\n",
      "Episode 3908; Testing Loss 0.005862205556061473; Training Loss 0.004750591931550385\n",
      "Episode 3909; Testing Loss 0.005862192574623556; Training Loss 0.004750583710292622\n",
      "Episode 3910; Testing Loss 0.005862151014988813; Training Loss 0.004750572292768224\n",
      "Episode 3911; Testing Loss 0.00586213400787887; Training Loss 0.004750562814546788\n",
      "Episode 3912; Testing Loss 0.005862172944214183; Training Loss 0.004750551179255655\n",
      "Episode 3913; Testing Loss 0.005862185153399023; Training Loss 0.004750536861704198\n",
      "Episode 3914; Testing Loss 0.005862120019452626; Training Loss 0.004750533106250965\n",
      "Episode 3915; Testing Loss 0.005862036183538737; Training Loss 0.004750522078912435\n",
      "Episode 3916; Testing Loss 0.0058620478951969535; Training Loss 0.004750508198641872\n",
      "Episode 3917; Testing Loss 0.0058621186009350625; Training Loss 0.004750498726737463\n",
      "Episode 3918; Testing Loss 0.00586210778432082; Training Loss 0.004750487455695306\n",
      "Episode 3919; Testing Loss 0.005862049735806389; Training Loss 0.004750476039773555\n",
      "Episode 3920; Testing Loss 0.005862021715243369; Training Loss 0.004750465535006007\n",
      "Episode 3921; Testing Loss 0.005862075922218; Training Loss 0.0047504535934682615\n",
      "Episode 3922; Testing Loss 0.0058621189894349345; Training Loss 0.004750443035622388\n",
      "Episode 3923; Testing Loss 0.0058621331640517785; Training Loss 0.00475043452841666\n",
      "Episode 3924; Testing Loss 0.005862088947524121; Training Loss 0.004750423543262429\n",
      "Episode 3925; Testing Loss 0.005862005429955179; Training Loss 0.004750410856200006\n",
      "Episode 3926; Testing Loss 0.005861970694921071; Training Loss 0.004750398942739222\n",
      "Episode 3927; Testing Loss 0.0058620625697838056; Training Loss 0.004750389205302398\n",
      "Episode 3928; Testing Loss 0.0058620641679781385; Training Loss 0.0047503787290770465\n",
      "Episode 3929; Testing Loss 0.005861995410428073; Training Loss 0.00475036683515618\n",
      "Episode 3930; Testing Loss 0.005861927042871306; Training Loss 0.004750355617908606\n",
      "Episode 3931; Testing Loss 0.005862017678763406; Training Loss 0.004750347020544992\n",
      "Episode 3932; Testing Loss 0.005862045244923888; Training Loss 0.004750335523294554\n",
      "Episode 3933; Testing Loss 0.005862005243427109; Training Loss 0.004750325400011161\n",
      "Episode 3934; Testing Loss 0.005861933324916853; Training Loss 0.00475031584474161\n",
      "Episode 3935; Testing Loss 0.005861889446322163; Training Loss 0.004750304187534965\n",
      "Episode 3936; Testing Loss 0.005861949214539035; Training Loss 0.0047502925783648176\n",
      "Episode 3937; Testing Loss 0.005861972739524347; Training Loss 0.004750283811869777\n",
      "Episode 3938; Testing Loss 0.005861925397753527; Training Loss 0.004750273437889236\n",
      "Episode 3939; Testing Loss 0.005861879880879346; Training Loss 0.004750261927854545\n",
      "Episode 3940; Testing Loss 0.005861928161686106; Training Loss 0.004750250793571318\n",
      "Episode 3941; Testing Loss 0.005862012757195407; Training Loss 0.004750240208094193\n",
      "Episode 3942; Testing Loss 0.00586197396721701; Training Loss 0.00475023017432458\n",
      "Episode 3943; Testing Loss 0.005861852279189115; Training Loss 0.004750217972927724\n",
      "Episode 3944; Testing Loss 0.005861799058977697; Training Loss 0.004750210362061685\n",
      "Episode 3945; Testing Loss 0.005861890666681628; Training Loss 0.004750198853551726\n",
      "Episode 3946; Testing Loss 0.005861994313675708; Training Loss 0.0047501871121743335\n",
      "Episode 3947; Testing Loss 0.0058619209555117666; Training Loss 0.004750179321984539\n",
      "Episode 3948; Testing Loss 0.0058618044759276115; Training Loss 0.004750167621752583\n",
      "Episode 3949; Testing Loss 0.00586174584333899; Training Loss 0.0047501553911678115\n",
      "Episode 3950; Testing Loss 0.00586185677657857; Training Loss 0.004750144953551334\n",
      "Episode 3951; Testing Loss 0.00586196882993298; Training Loss 0.004750134265981067\n",
      "Episode 3952; Testing Loss 0.005861893878682597; Training Loss 0.004750123883184491\n",
      "Episode 3953; Testing Loss 0.005861723626017415; Training Loss 0.004750111622770273\n",
      "Episode 3954; Testing Loss 0.005861745213175703; Training Loss 0.0047501024663188975\n",
      "Episode 3955; Testing Loss 0.0058618796538925065; Training Loss 0.004750094369277479\n",
      "Episode 3956; Testing Loss 0.00586182734158246; Training Loss 0.0047500808845968925\n",
      "Episode 3957; Testing Loss 0.005861647426585941; Training Loss 0.004750070042034412\n",
      "Episode 3958; Testing Loss 0.005861658417265294; Training Loss 0.00475005958523896\n",
      "Episode 3959; Testing Loss 0.005861865061428141; Training Loss 0.004750049277066929\n",
      "Episode 3960; Testing Loss 0.0058618673940288865; Training Loss 0.004750037222832514\n",
      "Episode 3961; Testing Loss 0.005861713245404555; Training Loss 0.004750030250059797\n",
      "Episode 3962; Testing Loss 0.005861740995807464; Training Loss 0.004750019118581847\n",
      "Episode 3963; Testing Loss 0.005861881483918648; Training Loss 0.004750006535871495\n",
      "Episode 3964; Testing Loss 0.005861803238685413; Training Loss 0.004749995026151478\n",
      "Episode 3965; Testing Loss 0.005861658208145272; Training Loss 0.004749984761925657\n",
      "Episode 3966; Testing Loss 0.0058616029805499265; Training Loss 0.004749977018466773\n",
      "Episode 3967; Testing Loss 0.005861647918085957; Training Loss 0.004749963146054536\n",
      "Episode 3968; Testing Loss 0.005861722490354496; Training Loss 0.004749954787034982\n",
      "Episode 3969; Testing Loss 0.005861779615772413; Training Loss 0.004749947395512247\n",
      "Episode 3970; Testing Loss 0.005861743531267191; Training Loss 0.004749935704544671\n",
      "Episode 3971; Testing Loss 0.005861653949481188; Training Loss 0.004749921495817669\n",
      "Episode 3972; Testing Loss 0.005861591324546839; Training Loss 0.004749911987395004\n",
      "Episode 3973; Testing Loss 0.005861593240369101; Training Loss 0.004749901589077153\n",
      "Episode 3974; Testing Loss 0.005861612953329608; Training Loss 0.004749888483289971\n",
      "Episode 3975; Testing Loss 0.0058616591463623345; Training Loss 0.0047498801512824\n",
      "Episode 3976; Testing Loss 0.005861703657523607; Training Loss 0.004749869072616663\n",
      "Episode 3977; Testing Loss 0.005861685995376781; Training Loss 0.004749861168335258\n",
      "Episode 3978; Testing Loss 0.00586161275503734; Training Loss 0.0047498479727158555\n",
      "Episode 3979; Testing Loss 0.005861564445594241; Training Loss 0.004749839288776078\n",
      "Episode 3980; Testing Loss 0.005861561807060111; Training Loss 0.00474983095316826\n",
      "Episode 3981; Testing Loss 0.005861565890741288; Training Loss 0.004749819250732627\n",
      "Episode 3982; Testing Loss 0.0058615387578716425; Training Loss 0.004749805006440988\n",
      "Episode 3983; Testing Loss 0.005861541688886916; Training Loss 0.004749798941815433\n",
      "Episode 3984; Testing Loss 0.005861538506708661; Training Loss 0.004749788181066532\n",
      "Episode 3985; Testing Loss 0.00586154733818524; Training Loss 0.0047497732513264775\n",
      "Episode 3986; Testing Loss 0.005861557787056091; Training Loss 0.004749764414534707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3987; Testing Loss 0.0058615785829857076; Training Loss 0.004749755874044386\n",
      "Episode 3988; Testing Loss 0.005861600962726275; Training Loss 0.004749744120336763\n",
      "Episode 3989; Testing Loss 0.005861567600539646; Training Loss 0.00474973049112448\n",
      "Episode 3990; Testing Loss 0.005861546312314746; Training Loss 0.004749722313539522\n",
      "Episode 3991; Testing Loss 0.005861466258307045; Training Loss 0.004749712126685137\n",
      "Episode 3992; Testing Loss 0.0058614451980177834; Training Loss 0.004749698653169241\n",
      "Episode 3993; Testing Loss 0.0058614675479104015; Training Loss 0.004749689827261837\n",
      "Episode 3994; Testing Loss 0.005861514639591267; Training Loss 0.004749679982968421\n",
      "Episode 3995; Testing Loss 0.005861547138003079; Training Loss 0.00474966883123447\n",
      "Episode 3996; Testing Loss 0.0058615336094815975; Training Loss 0.004749654736900851\n",
      "Episode 3997; Testing Loss 0.005861466280900373; Training Loss 0.004749649150970731\n",
      "Episode 3998; Testing Loss 0.005861340726208395; Training Loss 0.004749639545166963\n",
      "Episode 3999; Testing Loss 0.005861317870221836; Training Loss 0.004749625293363949\n",
      "Episode 4000; Testing Loss 0.005861444343759897; Training Loss 0.004749615305403313\n",
      "Episode 4001; Testing Loss 0.005861523463507725; Training Loss 0.004749606659552787\n",
      "Episode 4002; Testing Loss 0.00586140296189855; Training Loss 0.004749592895709664\n",
      "Episode 4003; Testing Loss 0.005861323502978274; Training Loss 0.004749581322118047\n",
      "Episode 4004; Testing Loss 0.005861397368397862; Training Loss 0.004749574191246046\n",
      "Episode 4005; Testing Loss 0.005861466347319476; Training Loss 0.004749561134376208\n",
      "Episode 4006; Testing Loss 0.005861417053374879; Training Loss 0.004749550680636217\n",
      "Episode 4007; Testing Loss 0.005861297120773296; Training Loss 0.004749543600746561\n",
      "Episode 4008; Testing Loss 0.005861313365230856; Training Loss 0.004749531983147282\n",
      "Episode 4009; Testing Loss 0.005861413513165333; Training Loss 0.004749520149077549\n",
      "Episode 4010; Testing Loss 0.005861397604806125; Training Loss 0.004749511013537893\n",
      "Episode 4011; Testing Loss 0.005861283858775731; Training Loss 0.0047494972393470015\n",
      "Episode 4012; Testing Loss 0.005861295497955538; Training Loss 0.004749486642646132\n",
      "Episode 4013; Testing Loss 0.00586134877747738; Training Loss 0.004749477383526777\n",
      "Episode 4014; Testing Loss 0.005861315908453794; Training Loss 0.004749464783459364\n",
      "Episode 4015; Testing Loss 0.005861235965260507; Training Loss 0.0047494535034892675\n",
      "Episode 4016; Testing Loss 0.005861259290118742; Training Loss 0.004749444175586515\n",
      "Episode 4017; Testing Loss 0.005861348231249357; Training Loss 0.004749434112917359\n",
      "Episode 4018; Testing Loss 0.005861316804727002; Training Loss 0.0047494230752355765\n",
      "Episode 4019; Testing Loss 0.005861233892246229; Training Loss 0.004749413508347379\n",
      "Episode 4020; Testing Loss 0.005861233931509599; Training Loss 0.004749401720142252\n",
      "Episode 4021; Testing Loss 0.005861255937540363; Training Loss 0.0047493903380053555\n",
      "Episode 4022; Testing Loss 0.005861206025653389; Training Loss 0.004749381515495578\n",
      "Episode 4023; Testing Loss 0.005861191176174156; Training Loss 0.004749369647363375\n",
      "Episode 4024; Testing Loss 0.005861239046591855; Training Loss 0.004749357142756246\n",
      "Episode 4025; Testing Loss 0.005861237938760034; Training Loss 0.004749348116644461\n",
      "Episode 4026; Testing Loss 0.005861181081190599; Training Loss 0.004749336624287043\n",
      "Episode 4027; Testing Loss 0.005861166987435272; Training Loss 0.004749326514850073\n",
      "Episode 4028; Testing Loss 0.0058611970132496525; Training Loss 0.0047493162257284385\n",
      "Episode 4029; Testing Loss 0.005861218439656093; Training Loss 0.004749304879497221\n",
      "Episode 4030; Testing Loss 0.005861168482993885; Training Loss 0.004749295061703877\n",
      "Episode 4031; Testing Loss 0.005861073253785007; Training Loss 0.004749283446237022\n",
      "Episode 4032; Testing Loss 0.005861048229800741; Training Loss 0.004749274575787881\n",
      "Episode 4033; Testing Loss 0.005861074840214907; Training Loss 0.004749262949869168\n",
      "Episode 4034; Testing Loss 0.005861084580684839; Training Loss 0.004749253039310911\n",
      "Episode 4035; Testing Loss 0.005861032311527031; Training Loss 0.004749242677836348\n",
      "Episode 4036; Testing Loss 0.005861068289455279; Training Loss 0.004749232849030987\n",
      "Episode 4037; Testing Loss 0.005861169144153191; Training Loss 0.004749222222099071\n",
      "Episode 4038; Testing Loss 0.0058611847906801596; Training Loss 0.004749210228687485\n",
      "Episode 4039; Testing Loss 0.005861056830078171; Training Loss 0.004749198898279607\n",
      "Episode 4040; Testing Loss 0.00586092989779439; Training Loss 0.004749190146195666\n",
      "Episode 4041; Testing Loss 0.005860964550379737; Training Loss 0.004749177849139611\n",
      "Episode 4042; Testing Loss 0.005861063125151503; Training Loss 0.004749167396417003\n",
      "Episode 4043; Testing Loss 0.00586100143536527; Training Loss 0.004749157525777259\n",
      "Episode 4044; Testing Loss 0.0058609479052806205; Training Loss 0.004749148242594605\n",
      "Episode 4045; Testing Loss 0.005860987560125805; Training Loss 0.0047491363769354145\n",
      "Episode 4046; Testing Loss 0.005861062673662142; Training Loss 0.004749126574103188\n",
      "Episode 4047; Testing Loss 0.0058609489700502376; Training Loss 0.004749113829170479\n",
      "Episode 4048; Testing Loss 0.005860824580527035; Training Loss 0.00474910689961463\n",
      "Episode 4049; Testing Loss 0.005860897672753658; Training Loss 0.004749097452058158\n",
      "Episode 4050; Testing Loss 0.005861063193026418; Training Loss 0.004749086652186244\n",
      "Episode 4051; Testing Loss 0.005861040705049965; Training Loss 0.00474907269353299\n",
      "Episode 4052; Testing Loss 0.005860847723596531; Training Loss 0.004749062056730171\n",
      "Episode 4053; Testing Loss 0.005860789577567049; Training Loss 0.004749052873729668\n",
      "Episode 4054; Testing Loss 0.005860942267109296; Training Loss 0.004749040333443196\n",
      "Episode 4055; Testing Loss 0.005861017041367352; Training Loss 0.004749030996382766\n",
      "Episode 4056; Testing Loss 0.005860874452440708; Training Loss 0.004749017381531639\n",
      "Episode 4057; Testing Loss 0.005860756162481572; Training Loss 0.004749011403794355\n",
      "Episode 4058; Testing Loss 0.005860791911562471; Training Loss 0.004748999396768302\n",
      "Episode 4059; Testing Loss 0.005860883477002204; Training Loss 0.004748987364447222\n",
      "Episode 4060; Testing Loss 0.005860905247804661; Training Loss 0.004748978060721658\n",
      "Episode 4061; Testing Loss 0.0058608814691747954; Training Loss 0.004748967272797073\n",
      "Episode 4062; Testing Loss 0.0058608431652542575; Training Loss 0.004748954103560695\n",
      "Episode 4063; Testing Loss 0.0058608071027394055; Training Loss 0.004748945553915381\n",
      "Episode 4064; Testing Loss 0.005860790156826171; Training Loss 0.004748934624058943\n",
      "Episode 4065; Testing Loss 0.0058608225305991956; Training Loss 0.00474892318452596\n",
      "Episode 4066; Testing Loss 0.005860811078253899; Training Loss 0.00474891300619673\n",
      "Episode 4067; Testing Loss 0.005860743595940811; Training Loss 0.004748902910616721\n",
      "Episode 4068; Testing Loss 0.005860728762571941; Training Loss 0.004748892432739332\n",
      "Episode 4069; Testing Loss 0.005860756543841757; Training Loss 0.004748881432561432\n",
      "Episode 4070; Testing Loss 0.005860850758610447; Training Loss 0.004748870904446979\n",
      "Episode 4071; Testing Loss 0.005860811816885697; Training Loss 0.004748861555901879\n",
      "Episode 4072; Testing Loss 0.0058607459491933816; Training Loss 0.004748851745136715\n",
      "Episode 4073; Testing Loss 0.00586073059728922; Training Loss 0.004748841657065633\n",
      "Episode 4074; Testing Loss 0.005860715051332142; Training Loss 0.004748829854358414\n",
      "Episode 4075; Testing Loss 0.00586070671067729; Training Loss 0.0047488196858105295\n",
      "Episode 4076; Testing Loss 0.005860723022187525; Training Loss 0.004748807490515578\n",
      "Episode 4077; Testing Loss 0.005860723026940953; Training Loss 0.004748798381624145\n",
      "Episode 4078; Testing Loss 0.005860662021021744; Training Loss 0.004748788572966783\n",
      "Episode 4079; Testing Loss 0.005860622946997088; Training Loss 0.004748777685935783\n",
      "Episode 4080; Testing Loss 0.005860642569078019; Training Loss 0.0047487661874227455\n",
      "Episode 4081; Testing Loss 0.0058606459933532255; Training Loss 0.004748753981379242\n",
      "Episode 4082; Testing Loss 0.00586065280841725; Training Loss 0.004748745747231926\n",
      "Episode 4083; Testing Loss 0.005860651509610697; Training Loss 0.004748735480213806\n",
      "Episode 4084; Testing Loss 0.00586068492612385; Training Loss 0.0047487248176468115\n",
      "Episode 4085; Testing Loss 0.005860709107925631; Training Loss 0.004748716449901051\n",
      "Episode 4086; Testing Loss 0.005860660061703261; Training Loss 0.004748703751577619\n",
      "Episode 4087; Testing Loss 0.005860609727957898; Training Loss 0.004748693189141311\n",
      "Episode 4088; Testing Loss 0.005860594050757745; Training Loss 0.0047486824615454335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4089; Testing Loss 0.005860556036612077; Training Loss 0.004748670012782104\n",
      "Episode 4090; Testing Loss 0.00586048113834058; Training Loss 0.00474865905187853\n",
      "Episode 4091; Testing Loss 0.005860469555323478; Training Loss 0.004748647710049751\n",
      "Episode 4092; Testing Loss 0.005860601361420748; Training Loss 0.004748637654869658\n",
      "Episode 4093; Testing Loss 0.005860631276476037; Training Loss 0.004748628059530624\n",
      "Episode 4094; Testing Loss 0.005860543569245413; Training Loss 0.004748616558156531\n",
      "Episode 4095; Testing Loss 0.005860520166114922; Training Loss 0.004748603953066556\n",
      "Episode 4096; Testing Loss 0.005860468535360433; Training Loss 0.004748595582536879\n",
      "Episode 4097; Testing Loss 0.005860514546320653; Training Loss 0.004748584556179726\n",
      "Episode 4098; Testing Loss 0.005860549856684725; Training Loss 0.00474857779484611\n",
      "Episode 4099; Testing Loss 0.005860453744722201; Training Loss 0.004748563440208869\n",
      "Episode 4100; Testing Loss 0.005860401563519429; Training Loss 0.004748556936939774\n",
      "Episode 4101; Testing Loss 0.005860567824139298; Training Loss 0.004748547490645806\n",
      "Episode 4102; Testing Loss 0.005860669861288774; Training Loss 0.004748537803495659\n",
      "Episode 4103; Testing Loss 0.005860529141204429; Training Loss 0.004748522255927406\n",
      "Episode 4104; Testing Loss 0.005860350971565017; Training Loss 0.004748514542123596\n",
      "Episode 4105; Testing Loss 0.005860320405783309; Training Loss 0.00474850458833319\n",
      "Episode 4106; Testing Loss 0.005860466982243511; Training Loss 0.00474848882317638\n",
      "Episode 4107; Testing Loss 0.005860538976952772; Training Loss 0.0047484822725628465\n",
      "Episode 4108; Testing Loss 0.005860426767734904; Training Loss 0.004748471656441691\n",
      "Episode 4109; Testing Loss 0.0058603486947116515; Training Loss 0.004748460953109276\n",
      "Episode 4110; Testing Loss 0.005860430393470762; Training Loss 0.004748447708363735\n",
      "Episode 4111; Testing Loss 0.0058604823756079755; Training Loss 0.004748441630427238\n",
      "Episode 4112; Testing Loss 0.005860347720638308; Training Loss 0.004748431619712587\n",
      "Episode 4113; Testing Loss 0.00586022089297953; Training Loss 0.0047484169582519276\n",
      "Episode 4114; Testing Loss 0.005860304556740902; Training Loss 0.004748407391499722\n",
      "Episode 4115; Testing Loss 0.005860524088850622; Training Loss 0.004748398649492851\n",
      "Episode 4116; Testing Loss 0.005860551061710471; Training Loss 0.004748387014188415\n",
      "Episode 4117; Testing Loss 0.005860356372282197; Training Loss 0.00474837294822458\n",
      "Episode 4118; Testing Loss 0.005860323972690717; Training Loss 0.004748369091459493\n",
      "Episode 4119; Testing Loss 0.00586039310251747; Training Loss 0.004748357485609944\n",
      "Episode 4120; Testing Loss 0.005860399358130391; Training Loss 0.004748343306255897\n",
      "Episode 4121; Testing Loss 0.00586033998596479; Training Loss 0.0047483369304624715\n",
      "Episode 4122; Testing Loss 0.005860326352935751; Training Loss 0.0047483295939159086\n",
      "Episode 4123; Testing Loss 0.005860354341961923; Training Loss 0.0047483177682560995\n",
      "Episode 4124; Testing Loss 0.005860347484885134; Training Loss 0.004748304706558108\n",
      "Episode 4125; Testing Loss 0.005860278522096063; Training Loss 0.004748292496360535\n",
      "Episode 4126; Testing Loss 0.0058602696995188; Training Loss 0.0047482845916008804\n",
      "Episode 4127; Testing Loss 0.005860260767369132; Training Loss 0.004748273335027152\n",
      "Episode 4128; Testing Loss 0.005860256418038417; Training Loss 0.004748258557198598\n",
      "Episode 4129; Testing Loss 0.005860225827313607; Training Loss 0.0047482501550280875\n",
      "Episode 4130; Testing Loss 0.005860238285642278; Training Loss 0.004748244146592839\n",
      "Episode 4131; Testing Loss 0.005860239824686671; Training Loss 0.004748233493907145\n",
      "Episode 4132; Testing Loss 0.005860232207440771; Training Loss 0.00474821905046002\n",
      "Episode 4133; Testing Loss 0.005860252329265127; Training Loss 0.0047482053649181965\n",
      "Episode 4134; Testing Loss 0.0058602693592914945; Training Loss 0.004748196013827456\n",
      "Episode 4135; Testing Loss 0.00586021922033901; Training Loss 0.004748183989805067\n",
      "Episode 4136; Testing Loss 0.005860109142432858; Training Loss 0.004748175976591698\n",
      "Episode 4137; Testing Loss 0.005860119901432913; Training Loss 0.004748165306305927\n",
      "Episode 4138; Testing Loss 0.005860192900772409; Training Loss 0.004748153714264079\n",
      "Episode 4139; Testing Loss 0.0058602331371468905; Training Loss 0.004748143991439157\n",
      "Episode 4140; Testing Loss 0.005860151711071333; Training Loss 0.0047481322612844155\n",
      "Episode 4141; Testing Loss 0.005860096586142798; Training Loss 0.004748122327670993\n",
      "Episode 4142; Testing Loss 0.005860218673980274; Training Loss 0.0047481121357778825\n",
      "Episode 4143; Testing Loss 0.005860258380011588; Training Loss 0.004748102053691098\n",
      "Episode 4144; Testing Loss 0.005860106575376784; Training Loss 0.004748088948422802\n",
      "Episode 4145; Testing Loss 0.005860004848203232; Training Loss 0.004748080195445929\n",
      "Episode 4146; Testing Loss 0.00586010001414077; Training Loss 0.004748068062905266\n",
      "Episode 4147; Testing Loss 0.005860235647312986; Training Loss 0.004748058704757033\n",
      "Episode 4148; Testing Loss 0.005860176386673271; Training Loss 0.004748046803427053\n",
      "Episode 4149; Testing Loss 0.005860020852947962; Training Loss 0.004748036799896238\n",
      "Episode 4150; Testing Loss 0.005859979042508409; Training Loss 0.004748027472286165\n",
      "Episode 4151; Testing Loss 0.005860117254317687; Training Loss 0.004748016052261253\n",
      "Episode 4152; Testing Loss 0.005860199410771701; Training Loss 0.004748005883691086\n",
      "Episode 4153; Testing Loss 0.005860093623074002; Training Loss 0.004747992970458408\n",
      "Episode 4154; Testing Loss 0.0058600021475967755; Training Loss 0.004747987031514214\n",
      "Episode 4155; Testing Loss 0.005860012186115049; Training Loss 0.0047479751881206655\n",
      "Episode 4156; Testing Loss 0.005860035662607727; Training Loss 0.0047479608317657655\n",
      "Episode 4157; Testing Loss 0.005860032193235372; Training Loss 0.0047479546378986865\n",
      "Episode 4158; Testing Loss 0.005860065002683738; Training Loss 0.004747945996098271\n",
      "Episode 4159; Testing Loss 0.005860075231607811; Training Loss 0.004747934700554722\n",
      "Episode 4160; Testing Loss 0.005860053271998824; Training Loss 0.004747922707196171\n",
      "Episode 4161; Testing Loss 0.0058599750325793966; Training Loss 0.004747911026420182\n",
      "Episode 4162; Testing Loss 0.005859872554478954; Training Loss 0.004747901060609848\n",
      "Episode 4163; Testing Loss 0.00585987484450915; Training Loss 0.004747890507266623\n",
      "Episode 4164; Testing Loss 0.00586001575138141; Training Loss 0.004747880473077445\n",
      "Episode 4165; Testing Loss 0.005860047550360454; Training Loss 0.004747869844644983\n",
      "Episode 4166; Testing Loss 0.0058598874273935175; Training Loss 0.0047478568758612906\n",
      "Episode 4167; Testing Loss 0.005859798504181825; Training Loss 0.004747848046156675\n",
      "Episode 4168; Testing Loss 0.005859904383017625; Training Loss 0.004747837709869451\n",
      "Episode 4169; Testing Loss 0.005860004746659568; Training Loss 0.00474782701366922\n",
      "Episode 4170; Testing Loss 0.005859958111119098; Training Loss 0.004747815311091453\n",
      "Episode 4171; Testing Loss 0.005859808751103513; Training Loss 0.004747805845301275\n",
      "Episode 4172; Testing Loss 0.005859826300008317; Training Loss 0.004747794491095047\n",
      "Episode 4173; Testing Loss 0.0058599402262387214; Training Loss 0.004747783742088012\n",
      "Episode 4174; Testing Loss 0.005859907473710876; Training Loss 0.0047477727178561685\n",
      "Episode 4175; Testing Loss 0.005859801781044441; Training Loss 0.004747764917380527\n",
      "Episode 4176; Testing Loss 0.005859888706822928; Training Loss 0.004747753614796547\n",
      "Episode 4177; Testing Loss 0.005860012302693203; Training Loss 0.004747742868382527\n",
      "Episode 4178; Testing Loss 0.005859879989567294; Training Loss 0.004747733224746966\n",
      "Episode 4179; Testing Loss 0.005859696218486705; Training Loss 0.004747722398660319\n",
      "Episode 4180; Testing Loss 0.005859734368986562; Training Loss 0.004747712156955456\n",
      "Episode 4181; Testing Loss 0.0058599105578883625; Training Loss 0.004747702498599746\n",
      "Episode 4182; Testing Loss 0.005859954267052172; Training Loss 0.004747690075093952\n",
      "Episode 4183; Testing Loss 0.005859819159325441; Training Loss 0.004747679514996303\n",
      "Episode 4184; Testing Loss 0.005859766126225592; Training Loss 0.004747669364444496\n",
      "Episode 4185; Testing Loss 0.0058598505582980525; Training Loss 0.0047476593313186535\n",
      "Episode 4186; Testing Loss 0.005859889860599317; Training Loss 0.004747648444308554\n",
      "Episode 4187; Testing Loss 0.005859776834044968; Training Loss 0.00474763590800885\n",
      "Episode 4188; Testing Loss 0.005859621873182587; Training Loss 0.004747627006334459\n",
      "Episode 4189; Testing Loss 0.0058596345497371255; Training Loss 0.004747616400422966\n",
      "Episode 4190; Testing Loss 0.005859827981205651; Training Loss 0.0047476061055332615\n",
      "Episode 4191; Testing Loss 0.005859921920749082; Training Loss 0.004747595925642312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4192; Testing Loss 0.005859765522817053; Training Loss 0.004747584325130857\n",
      "Episode 4193; Testing Loss 0.005859658455780661; Training Loss 0.004747575428881923\n",
      "Episode 4194; Testing Loss 0.005859770823351208; Training Loss 0.0047475626101499534\n",
      "Episode 4195; Testing Loss 0.005859824122270593; Training Loss 0.004747558123588916\n",
      "Episode 4196; Testing Loss 0.0058596177952077505; Training Loss 0.00474754372487371\n",
      "Episode 4197; Testing Loss 0.005859471899251839; Training Loss 0.004747537453900877\n",
      "Episode 4198; Testing Loss 0.0058596214479822405; Training Loss 0.0047475258479323865\n",
      "Episode 4199; Testing Loss 0.0058597935873232155; Training Loss 0.004747516330260319\n",
      "Episode 4200; Testing Loss 0.005859676377493276; Training Loss 0.004747500881890099\n",
      "Episode 4201; Testing Loss 0.005859451290942408; Training Loss 0.004747497794091069\n",
      "Episode 4202; Testing Loss 0.005859455904865788; Training Loss 0.004747489084098324\n",
      "Episode 4203; Testing Loss 0.0058597342271175515; Training Loss 0.0047474708653413925\n",
      "Episode 4204; Testing Loss 0.005859875149658933; Training Loss 0.004747462769110762\n",
      "Episode 4205; Testing Loss 0.005859716349042103; Training Loss 0.004747453708801725\n",
      "Episode 4206; Testing Loss 0.005859569417376737; Training Loss 0.004747444848642223\n",
      "Episode 4207; Testing Loss 0.005859605576607844; Training Loss 0.004747432188596598\n",
      "Episode 4208; Testing Loss 0.005859688977842742; Training Loss 0.004747418912410784\n",
      "Episode 4209; Testing Loss 0.005859593794705823; Training Loss 0.004747410601758024\n",
      "Episode 4210; Testing Loss 0.005859425220944892; Training Loss 0.004747402524215324\n",
      "Episode 4211; Testing Loss 0.005859424863497107; Training Loss 0.004747387517238999\n",
      "Episode 4212; Testing Loss 0.005859535024806732; Training Loss 0.004747375851058019\n",
      "Episode 4213; Testing Loss 0.0058596185832482; Training Loss 0.004747368429217932\n",
      "Episode 4214; Testing Loss 0.005859572484487763; Training Loss 0.004747358103955808\n",
      "Episode 4215; Testing Loss 0.005859506441470098; Training Loss 0.0047473460477146226\n",
      "Episode 4216; Testing Loss 0.005859558262160033; Training Loss 0.0047473331033901865\n",
      "Episode 4217; Testing Loss 0.005859567141049661; Training Loss 0.004747324076607089\n",
      "Episode 4218; Testing Loss 0.005859478362818203; Training Loss 0.004747310114294423\n",
      "Episode 4219; Testing Loss 0.005859432544796201; Training Loss 0.0047473020841789304\n",
      "Episode 4220; Testing Loss 0.0058594953095345; Training Loss 0.0047472949443177325\n",
      "Episode 4221; Testing Loss 0.005859511608888839; Training Loss 0.004747282294464236\n",
      "Episode 4222; Testing Loss 0.005859439542108467; Training Loss 0.0047472737907096295\n",
      "Episode 4223; Testing Loss 0.005859413685059928; Training Loss 0.004747265098136793\n",
      "Episode 4224; Testing Loss 0.005859439692081712; Training Loss 0.004747253436070741\n",
      "Episode 4225; Testing Loss 0.00585948621996336; Training Loss 0.004747239266473592\n",
      "Episode 4226; Testing Loss 0.0058594988410024425; Training Loss 0.004747231172832102\n",
      "Episode 4227; Testing Loss 0.005859489598637472; Training Loss 0.004747223013793701\n",
      "Episode 4228; Testing Loss 0.005859512889108486; Training Loss 0.004747207648576696\n",
      "Episode 4229; Testing Loss 0.00585954395767135; Training Loss 0.004747199299818052\n",
      "Episode 4230; Testing Loss 0.005859495611923313; Training Loss 0.0047471923213744105\n",
      "Episode 4231; Testing Loss 0.005859372205238723; Training Loss 0.004747181862249976\n",
      "Episode 4232; Testing Loss 0.005859325575098415; Training Loss 0.004747168627740853\n",
      "Episode 4233; Testing Loss 0.005859388199778878; Training Loss 0.004747157437624116\n",
      "Episode 4234; Testing Loss 0.005859414766179665; Training Loss 0.004747148527175373\n",
      "Episode 4235; Testing Loss 0.005859392810967817; Training Loss 0.004747134386955565\n",
      "Episode 4236; Testing Loss 0.005859405320050067; Training Loss 0.004747128754575235\n",
      "Episode 4237; Testing Loss 0.005859457996082916; Training Loss 0.004747118900752733\n",
      "Episode 4238; Testing Loss 0.005859478409688616; Training Loss 0.004747104937430205\n",
      "Episode 4239; Testing Loss 0.00585943975525628; Training Loss 0.004747092879633153\n",
      "Episode 4240; Testing Loss 0.005859372615399031; Training Loss 0.004747087956370222\n",
      "Episode 4241; Testing Loss 0.005859316558971586; Training Loss 0.004747076620968515\n",
      "Episode 4242; Testing Loss 0.005859304557255451; Training Loss 0.00474705789674192\n",
      "Episode 4243; Testing Loss 0.005859285294571271; Training Loss 0.004747054850272653\n",
      "Episode 4244; Testing Loss 0.005859350872459048; Training Loss 0.004747049869013176\n",
      "Episode 4245; Testing Loss 0.0058593755189038435; Training Loss 0.004747039907487057\n",
      "Episode 4246; Testing Loss 0.0058593175046315095; Training Loss 0.004747025559760582\n",
      "Episode 4247; Testing Loss 0.0058592739148884704; Training Loss 0.004747009447475232\n",
      "Episode 4248; Testing Loss 0.005859319894736709; Training Loss 0.004747000244992302\n",
      "Episode 4249; Testing Loss 0.005859318454648246; Training Loss 0.004746993430117367\n",
      "Episode 4250; Testing Loss 0.005859296910801678; Training Loss 0.004746979839077604\n",
      "Episode 4251; Testing Loss 0.005859331045458377; Training Loss 0.004746967309403474\n",
      "Episode 4252; Testing Loss 0.00585939646294377; Training Loss 0.004746960212269122\n",
      "Episode 4253; Testing Loss 0.005859383563158667; Training Loss 0.004746951174558734\n",
      "Episode 4254; Testing Loss 0.005859287855556564; Training Loss 0.004746937894880896\n",
      "Episode 4255; Testing Loss 0.005859240824088941; Training Loss 0.004746922761457488\n",
      "Episode 4256; Testing Loss 0.00585927854028906; Training Loss 0.004746917793107838\n",
      "Episode 4257; Testing Loss 0.005859280412754209; Training Loss 0.004746912751720259\n",
      "Episode 4258; Testing Loss 0.005859210844509491; Training Loss 0.0047468967821419055\n",
      "Episode 4259; Testing Loss 0.005859140838198248; Training Loss 0.004746883553013184\n",
      "Episode 4260; Testing Loss 0.005859221080323754; Training Loss 0.004746876490956092\n",
      "Episode 4261; Testing Loss 0.005859336150599496; Training Loss 0.0047468683838752445\n",
      "Episode 4262; Testing Loss 0.005859327154745105; Training Loss 0.004746855165884604\n",
      "Episode 4263; Testing Loss 0.005859234487946606; Training Loss 0.004746839024910407\n",
      "Episode 4264; Testing Loss 0.005859195021356207; Training Loss 0.004746832868096848\n",
      "Episode 4265; Testing Loss 0.005859162022359545; Training Loss 0.004746827336361408\n",
      "Episode 4266; Testing Loss 0.005859115212601845; Training Loss 0.004746813895681662\n",
      "Episode 4267; Testing Loss 0.005859098153287494; Training Loss 0.004746800795970242\n",
      "Episode 4268; Testing Loss 0.005859190681384629; Training Loss 0.004746791511536753\n",
      "Episode 4269; Testing Loss 0.005859289005599187; Training Loss 0.004746782816801306\n",
      "Episode 4270; Testing Loss 0.005859259171512843; Training Loss 0.004746769961577869\n",
      "Episode 4271; Testing Loss 0.005859190432398943; Training Loss 0.004746757507647854\n",
      "Episode 4272; Testing Loss 0.0058592200510820935; Training Loss 0.004746751041188964\n",
      "Episode 4273; Testing Loss 0.005859240198590442; Training Loss 0.004746742453002345\n",
      "Episode 4274; Testing Loss 0.005859133129500329; Training Loss 0.004746727757927463\n",
      "Episode 4275; Testing Loss 0.005859007783397758; Training Loss 0.004746719038359125\n",
      "Episode 4276; Testing Loss 0.005859057977550213; Training Loss 0.004746710144524549\n",
      "Episode 4277; Testing Loss 0.0058591574175019005; Training Loss 0.004746700770933288\n",
      "Episode 4278; Testing Loss 0.005859144047475486; Training Loss 0.004746689079952572\n",
      "Episode 4279; Testing Loss 0.005859074240148717; Training Loss 0.004746674022598901\n",
      "Episode 4280; Testing Loss 0.005859040035881876; Training Loss 0.004746667456580186\n",
      "Episode 4281; Testing Loss 0.0058590553397302415; Training Loss 0.004746657600143654\n",
      "Episode 4282; Testing Loss 0.005859104974338759; Training Loss 0.004746644900593907\n",
      "Episode 4283; Testing Loss 0.0058591216753547766; Training Loss 0.004746633064568497\n",
      "Episode 4284; Testing Loss 0.005859155517145897; Training Loss 0.004746625397434902\n",
      "Episode 4285; Testing Loss 0.005859154245390712; Training Loss 0.004746612393902095\n",
      "Episode 4286; Testing Loss 0.005859088452202498; Training Loss 0.004746599514335453\n",
      "Episode 4287; Testing Loss 0.005859048083748137; Training Loss 0.0047465889809991755\n",
      "Episode 4288; Testing Loss 0.005859014583643319; Training Loss 0.004746580687977638\n",
      "Episode 4289; Testing Loss 0.005858991230392065; Training Loss 0.00474656634772382\n",
      "Episode 4290; Testing Loss 0.005859002235377542; Training Loss 0.004746558223018941\n",
      "Episode 4291; Testing Loss 0.005859027591748438; Training Loss 0.004746549014143392\n",
      "Episode 4292; Testing Loss 0.005859047503403931; Training Loss 0.00474653588339498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4293; Testing Loss 0.005859084611479226; Training Loss 0.004746525057153914\n",
      "Episode 4294; Testing Loss 0.005859041928380077; Training Loss 0.004746516836638179\n",
      "Episode 4295; Testing Loss 0.005859046638499485; Training Loss 0.004746505359050429\n",
      "Episode 4296; Testing Loss 0.005859112485523056; Training Loss 0.004746494298579243\n",
      "Episode 4297; Testing Loss 0.005859042313058172; Training Loss 0.004746482932377874\n",
      "Episode 4298; Testing Loss 0.005858923737494521; Training Loss 0.004746472152616907\n",
      "Episode 4299; Testing Loss 0.005858913392587013; Training Loss 0.004746462507527603\n",
      "Episode 4300; Testing Loss 0.005858997329669352; Training Loss 0.004746450575932077\n",
      "Episode 4301; Testing Loss 0.005859070398974406; Training Loss 0.0047464412160306375\n",
      "Episode 4302; Testing Loss 0.005858986321453921; Training Loss 0.004746430152385987\n",
      "Episode 4303; Testing Loss 0.005858894020077522; Training Loss 0.004746418731987988\n",
      "Episode 4304; Testing Loss 0.0058589716539219; Training Loss 0.004746407810129346\n",
      "Episode 4305; Testing Loss 0.005859028211282309; Training Loss 0.004746398627797941\n",
      "Episode 4306; Testing Loss 0.0058589698853086; Training Loss 0.004746388425828977\n",
      "Episode 4307; Testing Loss 0.0058589016770892195; Training Loss 0.0047463762418187\n",
      "Episode 4308; Testing Loss 0.005858906914704916; Training Loss 0.004746367783967072\n",
      "Episode 4309; Testing Loss 0.005858945484348874; Training Loss 0.0047463586498902935\n",
      "Episode 4310; Testing Loss 0.00585897422455804; Training Loss 0.004746346408493158\n",
      "Episode 4311; Testing Loss 0.0058589847299134095; Training Loss 0.0047463383548533985\n",
      "Episode 4312; Testing Loss 0.005858977417759768; Training Loss 0.004746325965076857\n",
      "Episode 4313; Testing Loss 0.005858959706736082; Training Loss 0.004746316678347936\n",
      "Episode 4314; Testing Loss 0.005858946133269276; Training Loss 0.004746307773578407\n",
      "Episode 4315; Testing Loss 0.005858935336731111; Training Loss 0.00474629850326436\n",
      "Episode 4316; Testing Loss 0.005858878037851197; Training Loss 0.004746285547400251\n",
      "Episode 4317; Testing Loss 0.0058588667795480024; Training Loss 0.004746275721215\n",
      "Episode 4318; Testing Loss 0.005858883707289349; Training Loss 0.004746264626096925\n",
      "Episode 4319; Testing Loss 0.005858952391890543; Training Loss 0.0047462505122226484\n",
      "Episode 4320; Testing Loss 0.005858973695238133; Training Loss 0.004746240904299391\n",
      "Episode 4321; Testing Loss 0.005858857034697029; Training Loss 0.004746229597062093\n",
      "Episode 4322; Testing Loss 0.005858788050594911; Training Loss 0.00474622089479313\n",
      "Episode 4323; Testing Loss 0.005858904498678187; Training Loss 0.004746210234479116\n",
      "Episode 4324; Testing Loss 0.005858915399286627; Training Loss 0.004746200079300919\n",
      "Episode 4325; Testing Loss 0.005858769270698486; Training Loss 0.004746188786527298\n",
      "Episode 4326; Testing Loss 0.005858772502277869; Training Loss 0.004746177288566906\n",
      "Episode 4327; Testing Loss 0.005858894104781119; Training Loss 0.004746170338883666\n",
      "Episode 4328; Testing Loss 0.005858838143002711; Training Loss 0.004746157194405506\n",
      "Episode 4329; Testing Loss 0.005858705924797877; Training Loss 0.004746151420740046\n",
      "Episode 4330; Testing Loss 0.005858804134037327; Training Loss 0.004746141209453648\n",
      "Episode 4331; Testing Loss 0.005858980261309168; Training Loss 0.004746130509204631\n",
      "Episode 4332; Testing Loss 0.005858911214859318; Training Loss 0.00474611654959147\n",
      "Episode 4333; Testing Loss 0.005858719563331355; Training Loss 0.004746109149515287\n",
      "Episode 4334; Testing Loss 0.00585861779866563; Training Loss 0.004746100871468314\n",
      "Episode 4335; Testing Loss 0.00585875046953646; Training Loss 0.004746084340491693\n",
      "Episode 4336; Testing Loss 0.005858898257597837; Training Loss 0.004746081189265488\n",
      "Episode 4337; Testing Loss 0.005858822204440599; Training Loss 0.0047460724504713595\n",
      "Episode 4338; Testing Loss 0.005858671834567705; Training Loss 0.0047460633571355035\n",
      "Episode 4339; Testing Loss 0.005858741858652781; Training Loss 0.0047460491906544085\n",
      "Episode 4340; Testing Loss 0.005858895653673779; Training Loss 0.0047460343039564985\n",
      "Episode 4341; Testing Loss 0.005858839025111199; Training Loss 0.004746027380128661\n",
      "Episode 4342; Testing Loss 0.005858657605130214; Training Loss 0.0047460181413480455\n",
      "Episode 4343; Testing Loss 0.005858555967086725; Training Loss 0.004746004089098166\n",
      "Episode 4344; Testing Loss 0.005858669800993097; Training Loss 0.004745992644468679\n",
      "Episode 4345; Testing Loss 0.005858820419387424; Training Loss 0.004745988262636929\n",
      "Episode 4346; Testing Loss 0.0058587321613089716; Training Loss 0.004745976963879759\n",
      "Episode 4347; Testing Loss 0.0058585722225128565; Training Loss 0.004745964621530792\n",
      "Episode 4348; Testing Loss 0.005858615864786916; Training Loss 0.004745949426563164\n",
      "Episode 4349; Testing Loss 0.005858781980439254; Training Loss 0.004745946362649632\n",
      "Episode 4350; Testing Loss 0.005858696459133316; Training Loss 0.004745937160859518\n",
      "Episode 4351; Testing Loss 0.005858473529203037; Training Loss 0.004745922604279561\n",
      "Episode 4352; Testing Loss 0.00585848132795272; Training Loss 0.004745911136087325\n",
      "Episode 4353; Testing Loss 0.005858718607090331; Training Loss 0.004745902283493109\n",
      "Episode 4354; Testing Loss 0.005858828688970801; Training Loss 0.004745894748045584\n",
      "Episode 4355; Testing Loss 0.0058586233079680715; Training Loss 0.004745878558277104\n",
      "Episode 4356; Testing Loss 0.005858485139851494; Training Loss 0.004745867966300744\n",
      "Episode 4357; Testing Loss 0.005858659896245795; Training Loss 0.004745860944534331\n",
      "Episode 4358; Testing Loss 0.005858762721202872; Training Loss 0.004745854235537055\n",
      "Episode 4359; Testing Loss 0.00585862928544582; Training Loss 0.004745837824280293\n",
      "Episode 4360; Testing Loss 0.005858480812334983; Training Loss 0.004745825837929303\n",
      "Episode 4361; Testing Loss 0.00585857870616617; Training Loss 0.004745818291371069\n",
      "Episode 4362; Testing Loss 0.005858723703568522; Training Loss 0.004745810331163552\n",
      "Episode 4363; Testing Loss 0.005858704177030215; Training Loss 0.004745796600988115\n",
      "Episode 4364; Testing Loss 0.005858579052694123; Training Loss 0.004745782271090025\n",
      "Episode 4365; Testing Loss 0.005858611142234119; Training Loss 0.004745776499304758\n",
      "Episode 4366; Testing Loss 0.005858686846261236; Training Loss 0.004745767596592697\n",
      "Episode 4367; Testing Loss 0.005858616517169174; Training Loss 0.004745752209682597\n",
      "Episode 4368; Testing Loss 0.0058584561295251525; Training Loss 0.004745742160703951\n",
      "Episode 4369; Testing Loss 0.0058584963536810475; Training Loss 0.004745734878324259\n",
      "Episode 4370; Testing Loss 0.005858685793587122; Training Loss 0.00474572375695743\n",
      "Episode 4371; Testing Loss 0.005858703973674354; Training Loss 0.0047457103911489\n",
      "Episode 4372; Testing Loss 0.005858512263907939; Training Loss 0.004745696494223769\n",
      "Episode 4373; Testing Loss 0.005858449871907731; Training Loss 0.004745689740394357\n",
      "Episode 4374; Testing Loss 0.005858548411439009; Training Loss 0.0047456749289916675\n",
      "Episode 4375; Testing Loss 0.005858531794341456; Training Loss 0.004745666194833679\n",
      "Episode 4376; Testing Loss 0.005858425649504291; Training Loss 0.004745658342262643\n",
      "Episode 4377; Testing Loss 0.005858389847360088; Training Loss 0.004745647289755667\n",
      "Episode 4378; Testing Loss 0.005858433286153515; Training Loss 0.004745633110840608\n",
      "Episode 4379; Testing Loss 0.005858450138712363; Training Loss 0.004745622062448011\n",
      "Episode 4380; Testing Loss 0.005858332317395978; Training Loss 0.004745612623514284\n",
      "Episode 4381; Testing Loss 0.005858278178040512; Training Loss 0.004745600531216526\n",
      "Episode 4382; Testing Loss 0.005858394604883389; Training Loss 0.004745589632326473\n",
      "Episode 4383; Testing Loss 0.005858402898887012; Training Loss 0.0047455790553031845\n",
      "Episode 4384; Testing Loss 0.005858272485064818; Training Loss 0.0047455684112149244\n",
      "Episode 4385; Testing Loss 0.005858172292299143; Training Loss 0.004745556034377514\n",
      "Episode 4386; Testing Loss 0.005858235462950096; Training Loss 0.004745547457583123\n",
      "Episode 4387; Testing Loss 0.00585834183561312; Training Loss 0.004745537112831199\n",
      "Episode 4388; Testing Loss 0.005858320835331659; Training Loss 0.004745523154954612\n",
      "Episode 4389; Testing Loss 0.0058582032459579035; Training Loss 0.004745515259599152\n",
      "Episode 4390; Testing Loss 0.005858254813761672; Training Loss 0.004745503414503346\n",
      "Episode 4391; Testing Loss 0.005858357555237093; Training Loss 0.004745494824971081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4392; Testing Loss 0.005858254549103657; Training Loss 0.004745479971713284\n",
      "Episode 4393; Testing Loss 0.005858104062297104; Training Loss 0.004745475282476613\n",
      "Episode 4394; Testing Loss 0.005858138120529338; Training Loss 0.004745466686368504\n",
      "Episode 4395; Testing Loss 0.005858277019343925; Training Loss 0.0047454541298322965\n",
      "Episode 4396; Testing Loss 0.00585830185218832; Training Loss 0.004745438644833165\n",
      "Episode 4397; Testing Loss 0.005858222160905588; Training Loss 0.004745430146181682\n",
      "Episode 4398; Testing Loss 0.005858152629908707; Training Loss 0.004745419859405102\n",
      "Episode 4399; Testing Loss 0.005858151597077651; Training Loss 0.004745404072666708\n",
      "Episode 4400; Testing Loss 0.005858128015617265; Training Loss 0.004745394444827345\n",
      "Episode 4401; Testing Loss 0.005858067673738822; Training Loss 0.004745383088330969\n",
      "Episode 4402; Testing Loss 0.00585806489644031; Training Loss 0.00474537689968433\n",
      "Episode 4403; Testing Loss 0.005858037146334752; Training Loss 0.004745364455131132\n",
      "Episode 4404; Testing Loss 0.005857975335511269; Training Loss 0.004745352519927464\n",
      "Episode 4405; Testing Loss 0.0058580484718808315; Training Loss 0.004745342402145039\n",
      "Episode 4406; Testing Loss 0.0058581206058399065; Training Loss 0.004745331488878925\n",
      "Episode 4407; Testing Loss 0.0058580548887802505; Training Loss 0.004745318994291347\n",
      "Episode 4408; Testing Loss 0.005857887796339794; Training Loss 0.004745310398123689\n",
      "Episode 4409; Testing Loss 0.0058579420776739746; Training Loss 0.004745296687138989\n",
      "Episode 4410; Testing Loss 0.005858065771656032; Training Loss 0.004745292978643445\n",
      "Episode 4411; Testing Loss 0.005857938965689914; Training Loss 0.00474527914898115\n",
      "Episode 4412; Testing Loss 0.005857782350428117; Training Loss 0.0047452702711330355\n",
      "Episode 4413; Testing Loss 0.005857882374785957; Training Loss 0.0047452596098671055\n",
      "Episode 4414; Testing Loss 0.005858055459229454; Training Loss 0.004745247936394786\n",
      "Episode 4415; Testing Loss 0.005858024212675547; Training Loss 0.004745236430449429\n",
      "Episode 4416; Testing Loss 0.005857810502680948; Training Loss 0.004745230885980029\n",
      "Episode 4417; Testing Loss 0.005857713274989725; Training Loss 0.0047452186346755385\n",
      "Episode 4418; Testing Loss 0.005857829315467173; Training Loss 0.004745202461898727\n",
      "Episode 4419; Testing Loss 0.00585793815368189; Training Loss 0.004745196160165368\n",
      "Episode 4420; Testing Loss 0.005857821565194797; Training Loss 0.004745183609349784\n",
      "Episode 4421; Testing Loss 0.005857637608738214; Training Loss 0.004745176333487249\n",
      "Episode 4422; Testing Loss 0.00585759156238777; Training Loss 0.004745165494073165\n",
      "Episode 4423; Testing Loss 0.005857681324591404; Training Loss 0.0047451532315833\n",
      "Episode 4424; Testing Loss 0.005857804087271668; Training Loss 0.004745143174350048\n",
      "Episode 4425; Testing Loss 0.005857763317096756; Training Loss 0.004745129370635534\n",
      "Episode 4426; Testing Loss 0.005857695197295217; Training Loss 0.004745118667524049\n",
      "Episode 4427; Testing Loss 0.005857696108770556; Training Loss 0.004745106730288371\n",
      "Episode 4428; Testing Loss 0.005857781803727671; Training Loss 0.0047450987888771985\n",
      "Episode 4429; Testing Loss 0.005857846750677415; Training Loss 0.004745090094422551\n",
      "Episode 4430; Testing Loss 0.005857769859896447; Training Loss 0.004745078191918344\n",
      "Episode 4431; Testing Loss 0.00585768285729177; Training Loss 0.0047450653536322725\n",
      "Episode 4432; Testing Loss 0.005857623499047986; Training Loss 0.004745054958080543\n",
      "Episode 4433; Testing Loss 0.0058575423874323625; Training Loss 0.0047450450960030395\n",
      "Episode 4434; Testing Loss 0.0058575545170655675; Training Loss 0.004745033616858663\n",
      "Episode 4435; Testing Loss 0.005857582668295966; Training Loss 0.004745023791316656\n",
      "Episode 4436; Testing Loss 0.005857555225441104; Training Loss 0.004745013009345106\n",
      "Episode 4437; Testing Loss 0.005857536774959711; Training Loss 0.004745004018743803\n",
      "Episode 4438; Testing Loss 0.005857516322439106; Training Loss 0.004744990912093337\n",
      "Episode 4439; Testing Loss 0.005857542550947235; Training Loss 0.004744980771428452\n",
      "Episode 4440; Testing Loss 0.0058576084896142695; Training Loss 0.004744972712467326\n",
      "Episode 4441; Testing Loss 0.005857643811954138; Training Loss 0.004744960933599316\n",
      "Episode 4442; Testing Loss 0.005857599053332107; Training Loss 0.0047449489427409414\n",
      "Episode 4443; Testing Loss 0.005857549575965176; Training Loss 0.004744942024173903\n",
      "Episode 4444; Testing Loss 0.005857498145857869; Training Loss 0.004744930282251418\n",
      "Episode 4445; Testing Loss 0.005857484273032043; Training Loss 0.004744916097301495\n",
      "Episode 4446; Testing Loss 0.005857541653820606; Training Loss 0.00474490746342549\n",
      "Episode 4447; Testing Loss 0.005857524811411061; Training Loss 0.004744896033129839\n",
      "Episode 4448; Testing Loss 0.005857453989121062; Training Loss 0.004744886595965424\n",
      "Episode 4449; Testing Loss 0.005857429548975609; Training Loss 0.004744874105080494\n",
      "Episode 4450; Testing Loss 0.0058574876470855845; Training Loss 0.004744866389235416\n",
      "Episode 4451; Testing Loss 0.005857521341909017; Training Loss 0.004744857329916251\n",
      "Episode 4452; Testing Loss 0.005857429591529798; Training Loss 0.004744846193043103\n",
      "Episode 4453; Testing Loss 0.005857312892506086; Training Loss 0.004744833735277439\n",
      "Episode 4454; Testing Loss 0.005857352433114001; Training Loss 0.004744822804952389\n",
      "Episode 4455; Testing Loss 0.005857464754509521; Training Loss 0.004744813191630999\n",
      "Episode 4456; Testing Loss 0.005857475586748514; Training Loss 0.004744798785997361\n",
      "Episode 4457; Testing Loss 0.005857437793740472; Training Loss 0.004744789734829248\n",
      "Episode 4458; Testing Loss 0.005857436305549772; Training Loss 0.0047447778829173915\n",
      "Episode 4459; Testing Loss 0.005857369914975529; Training Loss 0.004744766824082838\n",
      "Episode 4460; Testing Loss 0.005857316981230823; Training Loss 0.004744756498707323\n",
      "Episode 4461; Testing Loss 0.0058572426217025665; Training Loss 0.004744745493468083\n",
      "Episode 4462; Testing Loss 0.005857261219547299; Training Loss 0.004744736064474205\n",
      "Episode 4463; Testing Loss 0.005857313650141255; Training Loss 0.004744725052703192\n",
      "Episode 4464; Testing Loss 0.005857309084086801; Training Loss 0.004744714992101359\n",
      "Episode 4465; Testing Loss 0.005857208486645622; Training Loss 0.004744703354417288\n",
      "Episode 4466; Testing Loss 0.0058572099990064205; Training Loss 0.004744693137748212\n",
      "Episode 4467; Testing Loss 0.005857248465300072; Training Loss 0.004744684788177713\n",
      "Episode 4468; Testing Loss 0.005857227572310173; Training Loss 0.004744674472895876\n",
      "Episode 4469; Testing Loss 0.005857171763823603; Training Loss 0.004744662769943702\n",
      "Episode 4470; Testing Loss 0.005857199347180584; Training Loss 0.0047446547704708105\n",
      "Episode 4471; Testing Loss 0.005857207705408834; Training Loss 0.004744642881354025\n",
      "Episode 4472; Testing Loss 0.005857195160873189; Training Loss 0.004744631757334669\n",
      "Episode 4473; Testing Loss 0.005857153979146663; Training Loss 0.004744623198710475\n",
      "Episode 4474; Testing Loss 0.005857151831933236; Training Loss 0.0047446111267449255\n",
      "Episode 4475; Testing Loss 0.005857185661887516; Training Loss 0.004744597668290992\n",
      "Episode 4476; Testing Loss 0.0058571508169605905; Training Loss 0.0047445921534304885\n",
      "Episode 4477; Testing Loss 0.005857095083939411; Training Loss 0.004744578336625436\n",
      "Episode 4478; Testing Loss 0.005857093156292311; Training Loss 0.004744569845289772\n",
      "Episode 4479; Testing Loss 0.005857126434452065; Training Loss 0.004744562196428855\n",
      "Episode 4480; Testing Loss 0.005857156428878948; Training Loss 0.004744550712061841\n",
      "Episode 4481; Testing Loss 0.005857137685625356; Training Loss 0.004744537908520196\n",
      "Episode 4482; Testing Loss 0.005857053623561268; Training Loss 0.004744526514031751\n",
      "Episode 4483; Testing Loss 0.0058569334256428005; Training Loss 0.004744516277494465\n",
      "Episode 4484; Testing Loss 0.005856946545951997; Training Loss 0.004744506590540346\n",
      "Episode 4485; Testing Loss 0.005857110858040234; Training Loss 0.004744497051402263\n",
      "Episode 4486; Testing Loss 0.005857156167463632; Training Loss 0.004744485317754748\n",
      "Episode 4487; Testing Loss 0.005856976297513757; Training Loss 0.004744474607236764\n",
      "Episode 4488; Testing Loss 0.005856882454924329; Training Loss 0.004744463596563184\n",
      "Episode 4489; Testing Loss 0.005856992122942534; Training Loss 0.004744453452820427\n",
      "Episode 4490; Testing Loss 0.0058570406946259765; Training Loss 0.004744444278193243\n",
      "Episode 4491; Testing Loss 0.005856906356687386; Training Loss 0.004744430181391609\n",
      "Episode 4492; Testing Loss 0.005856806374910547; Training Loss 0.004744423136954589\n",
      "Episode 4493; Testing Loss 0.005856884544662745; Training Loss 0.0047444103578764985\n",
      "Episode 4494; Testing Loss 0.0058570247223562735; Training Loss 0.004744400135299276\n",
      "Episode 4495; Testing Loss 0.005856975087385853; Training Loss 0.004744389571731037\n",
      "Episode 4496; Testing Loss 0.005856878090678763; Training Loss 0.004744380225268432\n",
      "Episode 4497; Testing Loss 0.005856841662501859; Training Loss 0.004744371318060455\n",
      "Episode 4498; Testing Loss 0.0058568314227609095; Training Loss 0.004744357793980581\n",
      "Episode 4499; Testing Loss 0.005856847375237977; Training Loss 0.004744349936250214\n",
      "Episode 4500; Testing Loss 0.005856940878922434; Training Loss 0.004744342007279482\n",
      "Episode 4501; Testing Loss 0.005856974655263953; Training Loss 0.004744329978980488\n",
      "Episode 4502; Testing Loss 0.0058568747453320216; Training Loss 0.004744316116857184\n",
      "Episode 4503; Testing Loss 0.005856868406584253; Training Loss 0.00474430755671495\n",
      "Episode 4504; Testing Loss 0.00585690968356429; Training Loss 0.004744297705448416\n",
      "Episode 4505; Testing Loss 0.005856930985604977; Training Loss 0.0047442851543862624\n",
      "Episode 4506; Testing Loss 0.0058568588397116695; Training Loss 0.004744276019800455\n",
      "Episode 4507; Testing Loss 0.005856723797580228; Training Loss 0.004744263859808618\n",
      "Episode 4508; Testing Loss 0.005856677143150302; Training Loss 0.004744255057571472\n",
      "Episode 4509; Testing Loss 0.00585674277452911; Training Loss 0.004744242290881806\n",
      "Episode 4510; Testing Loss 0.005856787410549576; Training Loss 0.004744233540660347\n",
      "Episode 4511; Testing Loss 0.0058567773821146165; Training Loss 0.004744226246979936\n",
      "Episode 4512; Testing Loss 0.0058567596993838135; Training Loss 0.004744215985351533\n",
      "Episode 4513; Testing Loss 0.005856771043187072; Training Loss 0.004744202352902834\n",
      "Episode 4514; Testing Loss 0.005856735643624717; Training Loss 0.00474419092133412\n",
      "Episode 4515; Testing Loss 0.005856651634198485; Training Loss 0.004744181903678999\n",
      "Episode 4516; Testing Loss 0.005856603903722863; Training Loss 0.004744171149291856\n",
      "Episode 4517; Testing Loss 0.005856687951404183; Training Loss 0.004744159850095264\n",
      "Episode 4518; Testing Loss 0.0058567725359312695; Training Loss 0.004744148363667034\n",
      "Episode 4519; Testing Loss 0.00585675026228847; Training Loss 0.00474413989510367\n",
      "Episode 4520; Testing Loss 0.005856607953165647; Training Loss 0.004744127670990457\n",
      "Episode 4521; Testing Loss 0.005856577694526205; Training Loss 0.00474412006382978\n",
      "Episode 4522; Testing Loss 0.005856654185771445; Training Loss 0.004744111848532752\n",
      "Episode 4523; Testing Loss 0.00585666705347899; Training Loss 0.004744100869206741\n",
      "Episode 4524; Testing Loss 0.005856607523446697; Training Loss 0.004744086918847281\n",
      "Episode 4525; Testing Loss 0.005856556649528947; Training Loss 0.004744076325220527\n",
      "Episode 4526; Testing Loss 0.005856550356671201; Training Loss 0.004744066099385584\n",
      "Episode 4527; Testing Loss 0.005856571343130437; Training Loss 0.004744053837779691\n",
      "Episode 4528; Testing Loss 0.005856644120422443; Training Loss 0.004744044742826162\n",
      "Episode 4529; Testing Loss 0.005856655563711559; Training Loss 0.0047440325904059165\n",
      "Episode 4530; Testing Loss 0.005856574897570437; Training Loss 0.0047440255273244825\n",
      "Episode 4531; Testing Loss 0.005856435350317307; Training Loss 0.004744015042729702\n",
      "Episode 4532; Testing Loss 0.005856458954178506; Training Loss 0.004744005659161089\n",
      "Episode 4533; Testing Loss 0.005856613327816234; Training Loss 0.004743996704157082\n",
      "Episode 4534; Testing Loss 0.005856615965967631; Training Loss 0.004743986241883034\n",
      "Episode 4535; Testing Loss 0.005856486652384703; Training Loss 0.004743971723497991\n",
      "Episode 4536; Testing Loss 0.005856425707390171; Training Loss 0.004743962650500575\n",
      "Episode 4537; Testing Loss 0.005856452392978665; Training Loss 0.004743952202323849\n",
      "Episode 4538; Testing Loss 0.005856501062024224; Training Loss 0.004743938773746988\n",
      "Episode 4539; Testing Loss 0.00585649046658019; Training Loss 0.00474392996617206\n",
      "Episode 4540; Testing Loss 0.0058563908813025894; Training Loss 0.004743920126538158\n",
      "Episode 4541; Testing Loss 0.005856391975814336; Training Loss 0.004743909868283339\n",
      "Episode 4542; Testing Loss 0.005856430882478402; Training Loss 0.004743898134153986\n",
      "Episode 4543; Testing Loss 0.005856393979316079; Training Loss 0.004743889382661965\n",
      "Episode 4544; Testing Loss 0.005856375315571249; Training Loss 0.004743880432812569\n",
      "Episode 4545; Testing Loss 0.005856438711323109; Training Loss 0.004743869010292244\n",
      "Episode 4546; Testing Loss 0.005856498893027039; Training Loss 0.004743855878218199\n",
      "Episode 4547; Testing Loss 0.005856406031509616; Training Loss 0.004743848201412057\n",
      "Episode 4548; Testing Loss 0.005856269390748891; Training Loss 0.0047438381177099965\n",
      "Episode 4549; Testing Loss 0.005856243721321424; Training Loss 0.004743824509173426\n",
      "Episode 4550; Testing Loss 0.005856395969507893; Training Loss 0.004743815013803275\n",
      "Episode 4551; Testing Loss 0.005856416717254653; Training Loss 0.004743802735777539\n",
      "Episode 4552; Testing Loss 0.005856291610340821; Training Loss 0.00474379769389321\n",
      "Episode 4553; Testing Loss 0.005856252119643049; Training Loss 0.0047437861844304525\n",
      "Episode 4554; Testing Loss 0.005856326494936621; Training Loss 0.004743774620022631\n",
      "Episode 4555; Testing Loss 0.0058563709134157454; Training Loss 0.004743766908910165\n",
      "Episode 4556; Testing Loss 0.0058562331860838686; Training Loss 0.004743755297322912\n",
      "Episode 4557; Testing Loss 0.005856139049745394; Training Loss 0.004743743037215266\n",
      "Episode 4558; Testing Loss 0.005856243239133532; Training Loss 0.004743734311006744\n",
      "Episode 4559; Testing Loss 0.005856337986125476; Training Loss 0.004743725425464848\n",
      "Episode 4560; Testing Loss 0.005856249980645814; Training Loss 0.004743709448433288\n",
      "Episode 4561; Testing Loss 0.005856173949330076; Training Loss 0.0047437059256059235\n",
      "Episode 4562; Testing Loss 0.005856276704855773; Training Loss 0.004743698325629918\n",
      "Episode 4563; Testing Loss 0.005856320256495208; Training Loss 0.0047436897435177575\n",
      "Episode 4564; Testing Loss 0.005856173004804185; Training Loss 0.004743676612543794\n",
      "Episode 4565; Testing Loss 0.005856056874808253; Training Loss 0.004743663936520599\n",
      "Episode 4566; Testing Loss 0.005856147257054493; Training Loss 0.004743647172908412\n",
      "Episode 4567; Testing Loss 0.005856222805419631; Training Loss 0.004743639243379612\n",
      "Episode 4568; Testing Loss 0.005856169128739331; Training Loss 0.004743627468372859\n",
      "Episode 4569; Testing Loss 0.005856059576660698; Training Loss 0.004743617730395147\n",
      "Episode 4570; Testing Loss 0.005856009412366971; Training Loss 0.004743605927320689\n",
      "Episode 4571; Testing Loss 0.0058560525028174965; Training Loss 0.00474359994759639\n",
      "Episode 4572; Testing Loss 0.0058560846210572405; Training Loss 0.004743588751732529\n",
      "Episode 4573; Testing Loss 0.005856049443322402; Training Loss 0.00474357403370153\n",
      "Episode 4574; Testing Loss 0.005856032930450609; Training Loss 0.004743565580588133\n",
      "Episode 4575; Testing Loss 0.0058560428309997956; Training Loss 0.004743554639902664\n",
      "Episode 4576; Testing Loss 0.005856071128021503; Training Loss 0.004743542830890109\n",
      "Episode 4577; Testing Loss 0.005856026660532632; Training Loss 0.00474353353685761\n",
      "Episode 4578; Testing Loss 0.005856012247842311; Training Loss 0.004743523808203422\n",
      "Episode 4579; Testing Loss 0.0058559945089337135; Training Loss 0.0047435146599168025\n",
      "Episode 4580; Testing Loss 0.00585600874339534; Training Loss 0.004743503458736119\n",
      "Episode 4581; Testing Loss 0.005856035234732543; Training Loss 0.004743490854429707\n",
      "Episode 4582; Testing Loss 0.005856003359485647; Training Loss 0.004743484641441087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4583; Testing Loss 0.005855978663282832; Training Loss 0.0047434729516782\n",
      "Episode 4584; Testing Loss 0.005856004962336121; Training Loss 0.0047434590163135905\n",
      "Episode 4585; Testing Loss 0.005856052599885869; Training Loss 0.004743450710369663\n",
      "Episode 4586; Testing Loss 0.005856018322555194; Training Loss 0.004743439057082237\n",
      "Episode 4587; Testing Loss 0.005855920289618851; Training Loss 0.004743429932070064\n",
      "Episode 4588; Testing Loss 0.005855904703012787; Training Loss 0.004743418424105198\n",
      "Episode 4589; Testing Loss 0.0058559557669278445; Training Loss 0.0047434083983846935\n",
      "Episode 4590; Testing Loss 0.0058560335959329185; Training Loss 0.004743398851812456\n",
      "Episode 4591; Testing Loss 0.005856003065836895; Training Loss 0.004743387034020618\n",
      "Episode 4592; Testing Loss 0.005855950113664662; Training Loss 0.004743374079135469\n",
      "Episode 4593; Testing Loss 0.005855897039482782; Training Loss 0.004743368354275982\n",
      "Episode 4594; Testing Loss 0.005855817395444427; Training Loss 0.00474335672944933\n",
      "Episode 4595; Testing Loss 0.005855800073693128; Training Loss 0.004743346018951202\n",
      "Episode 4596; Testing Loss 0.005855902082755529; Training Loss 0.004743337605547315\n",
      "Episode 4597; Testing Loss 0.005855958806059065; Training Loss 0.004743327541166573\n",
      "Episode 4598; Testing Loss 0.005855890362165096; Training Loss 0.0047433137252388125\n",
      "Episode 4599; Testing Loss 0.005855766630559183; Training Loss 0.004743303518547114\n",
      "Episode 4600; Testing Loss 0.005855715828228635; Training Loss 0.004743292405920774\n",
      "Episode 4601; Testing Loss 0.0058557656937408605; Training Loss 0.004743282933524665\n",
      "Episode 4602; Testing Loss 0.00585579698363897; Training Loss 0.00474327423409395\n",
      "Episode 4603; Testing Loss 0.00585572041835301; Training Loss 0.004743259636697257\n",
      "Episode 4604; Testing Loss 0.005855698565944242; Training Loss 0.004743253550766663\n",
      "Episode 4605; Testing Loss 0.005855823392966988; Training Loss 0.00474324444126791\n",
      "Episode 4606; Testing Loss 0.005855878135919862; Training Loss 0.0047432333859202735\n",
      "Episode 4607; Testing Loss 0.0058557616339357564; Training Loss 0.004743220009730676\n",
      "Episode 4608; Testing Loss 0.0058556394600459425; Training Loss 0.004743213403402507\n",
      "Episode 4609; Testing Loss 0.005855630284574624; Training Loss 0.004743203099410053\n",
      "Episode 4610; Testing Loss 0.005855704190168228; Training Loss 0.00474318676680657\n",
      "Episode 4611; Testing Loss 0.005855696270639663; Training Loss 0.004743181212612285\n",
      "Episode 4612; Testing Loss 0.005855653077546351; Training Loss 0.004743174103415107\n",
      "Episode 4613; Testing Loss 0.005855665995081944; Training Loss 0.004743162974942473\n",
      "Episode 4614; Testing Loss 0.005855685102291895; Training Loss 0.004743151097155017\n",
      "Episode 4615; Testing Loss 0.005855635959945767; Training Loss 0.004743136130670058\n",
      "Episode 4616; Testing Loss 0.005855581351146544; Training Loss 0.004743129856300491\n",
      "Episode 4617; Testing Loss 0.005855577786607284; Training Loss 0.004743123698761677\n",
      "Episode 4618; Testing Loss 0.005855595651712761; Training Loss 0.004743109278741668\n",
      "Episode 4619; Testing Loss 0.0058555794489417124; Training Loss 0.0047430963872962814\n",
      "Episode 4620; Testing Loss 0.0058555906280615905; Training Loss 0.004743088505522071\n",
      "Episode 4621; Testing Loss 0.005855573632895474; Training Loss 0.004743076992024024\n",
      "Episode 4622; Testing Loss 0.005855525456393819; Training Loss 0.00474306524020254\n",
      "Episode 4623; Testing Loss 0.005855520613662924; Training Loss 0.004743050406357243\n",
      "Episode 4624; Testing Loss 0.005855493530778284; Training Loss 0.0047430424887848115\n",
      "Episode 4625; Testing Loss 0.005855450693272127; Training Loss 0.004743029854155457\n",
      "Episode 4626; Testing Loss 0.005855432859556281; Training Loss 0.004743021478100858\n",
      "Episode 4627; Testing Loss 0.005855472810849228; Training Loss 0.0047430115390527435\n",
      "Episode 4628; Testing Loss 0.005855466704215101; Training Loss 0.004742999566406965\n",
      "Episode 4629; Testing Loss 0.005855436534063637; Training Loss 0.004742989436355224\n",
      "Episode 4630; Testing Loss 0.005855441684325136; Training Loss 0.004742978547888222\n",
      "Episode 4631; Testing Loss 0.005855420615927772; Training Loss 0.004742968531638021\n",
      "Episode 4632; Testing Loss 0.005855427059795476; Training Loss 0.004742959887885984\n",
      "Episode 4633; Testing Loss 0.0058554111598643046; Training Loss 0.004742947725840758\n",
      "Episode 4634; Testing Loss 0.005855354395877314; Training Loss 0.004742935300913684\n",
      "Episode 4635; Testing Loss 0.005855326769544679; Training Loss 0.004742924362897189\n",
      "Episode 4636; Testing Loss 0.005855372386628347; Training Loss 0.004742915360816816\n",
      "Episode 4637; Testing Loss 0.005855374794291783; Training Loss 0.00474290470458116\n",
      "Episode 4638; Testing Loss 0.005855414392779066; Training Loss 0.004742893751031934\n",
      "Episode 4639; Testing Loss 0.005855414592830123; Training Loss 0.004742883471410964\n",
      "Episode 4640; Testing Loss 0.0058552958953357204; Training Loss 0.004742874358588579\n",
      "Episode 4641; Testing Loss 0.0058552390246245775; Training Loss 0.0047428643059956605\n",
      "Episode 4642; Testing Loss 0.005855296729729208; Training Loss 0.004742854578635858\n",
      "Episode 4643; Testing Loss 0.005855304204390585; Training Loss 0.004742841128045637\n",
      "Episode 4644; Testing Loss 0.005855312044714209; Training Loss 0.004742834486247957\n",
      "Episode 4645; Testing Loss 0.005855317593294621; Training Loss 0.004742826350420015\n",
      "Episode 4646; Testing Loss 0.005855314587838447; Training Loss 0.004742814244502697\n",
      "Episode 4647; Testing Loss 0.005855277315533183; Training Loss 0.0047428009100944826\n",
      "Episode 4648; Testing Loss 0.00585521475026537; Training Loss 0.0047427902583807476\n",
      "Episode 4649; Testing Loss 0.00585518226098418; Training Loss 0.004742780164371908\n",
      "Episode 4650; Testing Loss 0.005855214970688333; Training Loss 0.004742768332977617\n",
      "Episode 4651; Testing Loss 0.005855264988504873; Training Loss 0.004742758900131669\n",
      "Episode 4652; Testing Loss 0.005855216190297444; Training Loss 0.004742746652920006\n",
      "Episode 4653; Testing Loss 0.005855168174744001; Training Loss 0.004742738819635358\n",
      "Episode 4654; Testing Loss 0.005855143863643828; Training Loss 0.0047427276239935595\n",
      "Episode 4655; Testing Loss 0.005855159939810349; Training Loss 0.004742717664133963\n",
      "Episode 4656; Testing Loss 0.005855191521203144; Training Loss 0.004742708991891021\n",
      "Episode 4657; Testing Loss 0.005855206728827249; Training Loss 0.00474269759168891\n",
      "Episode 4658; Testing Loss 0.005855183395422475; Training Loss 0.0047426841306817125\n",
      "Episode 4659; Testing Loss 0.005855131198179124; Training Loss 0.004742675527678189\n",
      "Episode 4660; Testing Loss 0.00585505210761922; Training Loss 0.004742664129319989\n",
      "Episode 4661; Testing Loss 0.0058550548651013674; Training Loss 0.0047426540307162115\n",
      "Episode 4662; Testing Loss 0.005855088854829624; Training Loss 0.004742644305716744\n",
      "Episode 4663; Testing Loss 0.005855083578806798; Training Loss 0.004742633538040114\n",
      "Episode 4664; Testing Loss 0.005855072417062554; Training Loss 0.004742621970669251\n",
      "Episode 4665; Testing Loss 0.005855063898580789; Training Loss 0.00474261398819488\n",
      "Episode 4666; Testing Loss 0.005855008186174847; Training Loss 0.00474260106715325\n",
      "Episode 4667; Testing Loss 0.0058549981792660214; Training Loss 0.004742594315716742\n",
      "Episode 4668; Testing Loss 0.00585514185067821; Training Loss 0.004742584680961893\n",
      "Episode 4669; Testing Loss 0.0058551528591831515; Training Loss 0.004742574285910004\n",
      "Episode 4670; Testing Loss 0.005854981897730135; Training Loss 0.004742559975842335\n",
      "Episode 4671; Testing Loss 0.005854880460008756; Training Loss 0.004742549097544617\n",
      "Episode 4672; Testing Loss 0.0058549155373652205; Training Loss 0.0047425387301329005\n",
      "Episode 4673; Testing Loss 0.00585495938100731; Training Loss 0.004742529201687174\n",
      "Episode 4674; Testing Loss 0.005854982105526658; Training Loss 0.004742519762973499\n",
      "Episode 4675; Testing Loss 0.005854936867951093; Training Loss 0.004742507254547294\n",
      "Episode 4676; Testing Loss 0.005854853495099772; Training Loss 0.004742498838692408\n",
      "Episode 4677; Testing Loss 0.005854835615888766; Training Loss 0.004742488345328147\n",
      "Episode 4678; Testing Loss 0.0058549506918340296; Training Loss 0.004742476233073371\n",
      "Episode 4679; Testing Loss 0.00585503159832554; Training Loss 0.004742467115563627\n",
      "Episode 4680; Testing Loss 0.005854953148779192; Training Loss 0.004742457055495903\n",
      "Episode 4681; Testing Loss 0.005854842110162495; Training Loss 0.004742444567947686\n",
      "Episode 4682; Testing Loss 0.005854858507108724; Training Loss 0.004742437847214404\n",
      "Episode 4683; Testing Loss 0.0058548708989632076; Training Loss 0.004742427699307763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4684; Testing Loss 0.0058548493242991185; Training Loss 0.004742412231494237\n",
      "Episode 4685; Testing Loss 0.0058548392311603996; Training Loss 0.004742405262521747\n",
      "Episode 4686; Testing Loss 0.005854875229548944; Training Loss 0.004742397187601978\n",
      "Episode 4687; Testing Loss 0.005854863510030446; Training Loss 0.004742385071042778\n",
      "Episode 4688; Testing Loss 0.005854804253016825; Training Loss 0.004742370120753059\n",
      "Episode 4689; Testing Loss 0.005854837040190517; Training Loss 0.004742365215731995\n",
      "Episode 4690; Testing Loss 0.005854893540249414; Training Loss 0.004742356102172419\n",
      "Episode 4691; Testing Loss 0.005854818783673937; Training Loss 0.004742339085062055\n",
      "Episode 4692; Testing Loss 0.005854727790580732; Training Loss 0.004742332509448552\n",
      "Episode 4693; Testing Loss 0.0058548129589890635; Training Loss 0.00474232486690249\n",
      "Episode 4694; Testing Loss 0.005854854175635065; Training Loss 0.004742314771639787\n",
      "Episode 4695; Testing Loss 0.00585478929471976; Training Loss 0.004742299245854058\n",
      "Episode 4696; Testing Loss 0.005854770203797433; Training Loss 0.004742290573650417\n",
      "Episode 4697; Testing Loss 0.0058547837630428924; Training Loss 0.0047422817528433815\n",
      "Episode 4698; Testing Loss 0.005854748187777567; Training Loss 0.004742266334215321\n",
      "Episode 4699; Testing Loss 0.005854683507390833; Training Loss 0.004742260634324222\n",
      "Episode 4700; Testing Loss 0.005854729708262916; Training Loss 0.004742254610017663\n",
      "Episode 4701; Testing Loss 0.005854779071979859; Training Loss 0.004742244733135728\n",
      "Episode 4702; Testing Loss 0.005854744336475516; Training Loss 0.00474223088393508\n",
      "Episode 4703; Testing Loss 0.0058547140747757555; Training Loss 0.004742216612034635\n",
      "Episode 4704; Testing Loss 0.0058547816633469476; Training Loss 0.0047422091195955295\n",
      "Episode 4705; Testing Loss 0.0058547674986624805; Training Loss 0.0047421995816220195\n",
      "Episode 4706; Testing Loss 0.005854594389544216; Training Loss 0.004742184136673841\n",
      "Episode 4707; Testing Loss 0.0058545081081416; Training Loss 0.00474217741855877\n",
      "Episode 4708; Testing Loss 0.0058545964749201805; Training Loss 0.004742171549039609\n",
      "Episode 4709; Testing Loss 0.005854619265907885; Training Loss 0.0047421627128078825\n",
      "Episode 4710; Testing Loss 0.00585451295375911; Training Loss 0.004742149219525558\n",
      "Episode 4711; Testing Loss 0.005854454470506221; Training Loss 0.004742134077471988\n",
      "Episode 4712; Testing Loss 0.005854555901853314; Training Loss 0.004742125505870281\n",
      "Episode 4713; Testing Loss 0.005854620459259333; Training Loss 0.0047421173890202515\n",
      "Episode 4714; Testing Loss 0.005854587028784543; Training Loss 0.004742101871799572\n",
      "Episode 4715; Testing Loss 0.005854538059285612; Training Loss 0.004742091462671885\n",
      "Episode 4716; Testing Loss 0.005854604762386082; Training Loss 0.00474208356675176\n",
      "Episode 4717; Testing Loss 0.005854665960979367; Training Loss 0.004742074341258318\n",
      "Episode 4718; Testing Loss 0.005854597662271905; Training Loss 0.004742061758963612\n",
      "Episode 4719; Testing Loss 0.005854469455366643; Training Loss 0.004742047327772368\n",
      "Episode 4720; Testing Loss 0.00585447372049414; Training Loss 0.0047420411516597375\n",
      "Episode 4721; Testing Loss 0.005854593518048436; Training Loss 0.004742031276005729\n",
      "Episode 4722; Testing Loss 0.0058545840953160835; Training Loss 0.0047420169292803184\n",
      "Episode 4723; Testing Loss 0.005854382762618852; Training Loss 0.004742010067431073\n",
      "Episode 4724; Testing Loss 0.005854337682332515; Training Loss 0.004742005460168397\n",
      "Episode 4725; Testing Loss 0.005854558100726758; Training Loss 0.004741993302867109\n",
      "Episode 4726; Testing Loss 0.005854643272084016; Training Loss 0.004741981601968297\n",
      "Episode 4727; Testing Loss 0.005854462390736179; Training Loss 0.00474196342085907\n",
      "Episode 4728; Testing Loss 0.005854365376143477; Training Loss 0.004741959249035431\n",
      "Episode 4729; Testing Loss 0.005854500368651766; Training Loss 0.004741949561627436\n",
      "Episode 4730; Testing Loss 0.005854615785321347; Training Loss 0.004741935592242776\n",
      "Episode 4731; Testing Loss 0.005854449353079149; Training Loss 0.004741923632493553\n",
      "Episode 4732; Testing Loss 0.005854331090869609; Training Loss 0.004741918317163776\n",
      "Episode 4733; Testing Loss 0.005854382688919476; Training Loss 0.004741906757897795\n",
      "Episode 4734; Testing Loss 0.005854484237511819; Training Loss 0.00474189490948852\n",
      "Episode 4735; Testing Loss 0.005854508670783553; Training Loss 0.004741880810944905\n",
      "Episode 4736; Testing Loss 0.005854421616255389; Training Loss 0.004741873209410928\n",
      "Episode 4737; Testing Loss 0.0058543009480632475; Training Loss 0.004741864202968248\n",
      "Episode 4738; Testing Loss 0.0058542411372685885; Training Loss 0.004741849482247072\n",
      "Episode 4739; Testing Loss 0.005854293993113746; Training Loss 0.004741841351868066\n",
      "Episode 4740; Testing Loss 0.005854381159643724; Training Loss 0.0047418342931907255\n",
      "Episode 4741; Testing Loss 0.005854420273297894; Training Loss 0.004741824276384187\n",
      "Episode 4742; Testing Loss 0.005854356563535317; Training Loss 0.004741810272415503\n",
      "Episode 4743; Testing Loss 0.005854319025939306; Training Loss 0.004741796417253506\n",
      "Episode 4744; Testing Loss 0.0058543368867568606; Training Loss 0.004741791329448933\n",
      "Episode 4745; Testing Loss 0.005854336926247772; Training Loss 0.004741781024707417\n",
      "Episode 4746; Testing Loss 0.005854313978334091; Training Loss 0.004741765444029088\n",
      "Episode 4747; Testing Loss 0.0058542128049345115; Training Loss 0.004741758700165647\n",
      "Episode 4748; Testing Loss 0.0058542184179168795; Training Loss 0.004741752066062526\n",
      "Episode 4749; Testing Loss 0.005854282658763676; Training Loss 0.004741742748839058\n",
      "Episode 4750; Testing Loss 0.005854249931487292; Training Loss 0.004741730229717674\n",
      "Episode 4751; Testing Loss 0.005854186308759557; Training Loss 0.004741714428142282\n",
      "Episode 4752; Testing Loss 0.0058542196142503606; Training Loss 0.004741705093208451\n",
      "Episode 4753; Testing Loss 0.005854239233385616; Training Loss 0.004741697297032529\n",
      "Episode 4754; Testing Loss 0.005854200487600865; Training Loss 0.004741682829694766\n",
      "Episode 4755; Testing Loss 0.005854187675767921; Training Loss 0.004741674440976095\n",
      "Episode 4756; Testing Loss 0.005854261705067071; Training Loss 0.0047416671881294024\n",
      "Episode 4757; Testing Loss 0.00585425382048239; Training Loss 0.004741657203655492\n",
      "Episode 4758; Testing Loss 0.005854129260560224; Training Loss 0.004741643509136901\n",
      "Episode 4759; Testing Loss 0.0058540921313060944; Training Loss 0.004741629441997928\n",
      "Episode 4760; Testing Loss 0.005854149540034816; Training Loss 0.0047416243870266805\n",
      "Episode 4761; Testing Loss 0.005854172565587247; Training Loss 0.004741616468182166\n",
      "Episode 4762; Testing Loss 0.005854140515639117; Training Loss 0.004741601530308785\n",
      "Episode 4763; Testing Loss 0.005854065537352618; Training Loss 0.004741590865546487\n",
      "Episode 4764; Testing Loss 0.005854067139744178; Training Loss 0.004741585461541873\n",
      "Episode 4765; Testing Loss 0.005854077653452084; Training Loss 0.004741576091502857\n",
      "Episode 4766; Testing Loss 0.0058540702311021065; Training Loss 0.004741561776664921\n",
      "Episode 4767; Testing Loss 0.005854065258381665; Training Loss 0.004741546691861229\n",
      "Episode 4768; Testing Loss 0.005854112550149348; Training Loss 0.004741540926311555\n",
      "Episode 4769; Testing Loss 0.005854119758596695; Training Loss 0.004741532054771067\n",
      "Episode 4770; Testing Loss 0.005854065382395673; Training Loss 0.004741515612779444\n",
      "Episode 4771; Testing Loss 0.0058539787752461565; Training Loss 0.004741507568939697\n",
      "Episode 4772; Testing Loss 0.005853998129962452; Training Loss 0.004741500109231949\n",
      "Episode 4773; Testing Loss 0.005854086263167671; Training Loss 0.004741490930795065\n",
      "Episode 4774; Testing Loss 0.005854122144840582; Training Loss 0.004741478512271036\n",
      "Episode 4775; Testing Loss 0.005854053989135897; Training Loss 0.0047414631305347415\n",
      "Episode 4776; Testing Loss 0.005854004514866237; Training Loss 0.0047414567696454765\n",
      "Episode 4777; Testing Loss 0.0058539806164603515; Training Loss 0.0047414492716687925\n",
      "Episode 4778; Testing Loss 0.005853917238209256; Training Loss 0.0047414344483658055\n",
      "Episode 4779; Testing Loss 0.0058539185560609755; Training Loss 0.0047414249557484215\n",
      "Episode 4780; Testing Loss 0.005854012980603396; Training Loss 0.004741416777732751\n",
      "Episode 4781; Testing Loss 0.005854035837575283; Training Loss 0.00474140624273804\n",
      "Episode 4782; Testing Loss 0.005854007912572016; Training Loss 0.0047413937617586\n",
      "Episode 4783; Testing Loss 0.005853990788499607; Training Loss 0.004741380576536763\n",
      "Episode 4784; Testing Loss 0.005854036035760217; Training Loss 0.0047413753168840354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4785; Testing Loss 0.005854022209064795; Training Loss 0.004741366590212512\n",
      "Episode 4786; Testing Loss 0.005853892136037356; Training Loss 0.004741351478158467\n",
      "Episode 4787; Testing Loss 0.005853848025097947; Training Loss 0.004741342967924661\n",
      "Episode 4788; Testing Loss 0.0058539591803615405; Training Loss 0.004741334083556376\n",
      "Episode 4789; Testing Loss 0.005854041687568822; Training Loss 0.004741325444994344\n",
      "Episode 4790; Testing Loss 0.005854016886817168; Training Loss 0.004741313031442308\n",
      "Episode 4791; Testing Loss 0.0058539165753404695; Training Loss 0.004741298131251509\n",
      "Episode 4792; Testing Loss 0.005853870680489724; Training Loss 0.004741290670833254\n",
      "Episode 4793; Testing Loss 0.005853825459570663; Training Loss 0.0047412835954558745\n",
      "Episode 4794; Testing Loss 0.005853764300327851; Training Loss 0.004741269656343678\n",
      "Episode 4795; Testing Loss 0.005853780761367338; Training Loss 0.004741258095431688\n",
      "Episode 4796; Testing Loss 0.005853887714437915; Training Loss 0.004741249731674017\n",
      "Episode 4797; Testing Loss 0.005853963631375888; Training Loss 0.00474124197488844\n",
      "Episode 4798; Testing Loss 0.00585395011026575; Training Loss 0.004741230622901242\n",
      "Episode 4799; Testing Loss 0.005853906765941499; Training Loss 0.004741216799145375\n",
      "Episode 4800; Testing Loss 0.005853902585512445; Training Loss 0.004741208996696896\n",
      "Episode 4801; Testing Loss 0.005853862953247321; Training Loss 0.004741198699044455\n",
      "Episode 4802; Testing Loss 0.005853755907168728; Training Loss 0.004741184394806883\n",
      "Episode 4803; Testing Loss 0.005853715949332139; Training Loss 0.004741177561436571\n",
      "Episode 4804; Testing Loss 0.005853801215903772; Training Loss 0.0047411706990062195\n",
      "Episode 4805; Testing Loss 0.005853885678450464; Training Loss 0.004741161525552\n",
      "Episode 4806; Testing Loss 0.005853805579352316; Training Loss 0.004741147091975385\n",
      "Episode 4807; Testing Loss 0.005853691098664268; Training Loss 0.004741132244217013\n",
      "Episode 4808; Testing Loss 0.00585370759033417; Training Loss 0.004741125674804622\n",
      "Episode 4809; Testing Loss 0.005853784893863447; Training Loss 0.004741117940859661\n",
      "Episode 4810; Testing Loss 0.005853815882748803; Training Loss 0.004741104467994313\n",
      "Episode 4811; Testing Loss 0.005853789127574502; Training Loss 0.004741092137265782\n",
      "Episode 4812; Testing Loss 0.005853745416815115; Training Loss 0.004741085577621661\n",
      "Episode 4813; Testing Loss 0.005853732189611806; Training Loss 0.0047410754553924555\n",
      "Episode 4814; Testing Loss 0.005853706537850656; Training Loss 0.0047410630283064095\n",
      "Episode 4815; Testing Loss 0.005853692672656871; Training Loss 0.004741048472233366\n",
      "Episode 4816; Testing Loss 0.005853730336546137; Training Loss 0.004741042681994861\n",
      "Episode 4817; Testing Loss 0.005853672915437651; Training Loss 0.004741034444440138\n",
      "Episode 4818; Testing Loss 0.005853561119034355; Training Loss 0.004741019674704124\n",
      "Episode 4819; Testing Loss 0.005853567274306267; Training Loss 0.004741009313867795\n",
      "Episode 4820; Testing Loss 0.005853703977951231; Training Loss 0.004741002008366204\n",
      "Episode 4821; Testing Loss 0.0058537561340442235; Training Loss 0.004740993198951401\n",
      "Episode 4822; Testing Loss 0.005853637893450008; Training Loss 0.004740979192126318\n",
      "Episode 4823; Testing Loss 0.005853582880667948; Training Loss 0.004740966232751048\n",
      "Episode 4824; Testing Loss 0.00585369054962074; Training Loss 0.004740961402737195\n",
      "Episode 4825; Testing Loss 0.005853685222835068; Training Loss 0.004740952661770793\n",
      "Episode 4826; Testing Loss 0.005853519059363894; Training Loss 0.004740937721224723\n",
      "Episode 4827; Testing Loss 0.005853497245141495; Training Loss 0.004740926045215968\n",
      "Episode 4828; Testing Loss 0.005853631677567201; Training Loss 0.004740917739478113\n",
      "Episode 4829; Testing Loss 0.005853698303963403; Training Loss 0.0047409088621038036\n",
      "Episode 4830; Testing Loss 0.005853637763395948; Training Loss 0.004740897638633237\n",
      "Episode 4831; Testing Loss 0.005853606339299255; Training Loss 0.004740883148315777\n",
      "Episode 4832; Testing Loss 0.005853622079203405; Training Loss 0.004740878860234719\n",
      "Episode 4833; Testing Loss 0.005853537407253848; Training Loss 0.00474087038999707\n",
      "Episode 4834; Testing Loss 0.005853402869571672; Training Loss 0.00474085530321619\n",
      "Episode 4835; Testing Loss 0.005853384591666749; Training Loss 0.004740846275159105\n",
      "Episode 4836; Testing Loss 0.005853526331060604; Training Loss 0.004740838551429968\n",
      "Episode 4837; Testing Loss 0.0058535809187440415; Training Loss 0.0047408288365152025\n",
      "Episode 4838; Testing Loss 0.005853471488646093; Training Loss 0.004740813703296555\n",
      "Episode 4839; Testing Loss 0.005853359815369988; Training Loss 0.004740800075360979\n",
      "Episode 4840; Testing Loss 0.005853409448702425; Training Loss 0.004740795696660383\n",
      "Episode 4841; Testing Loss 0.005853530922591064; Training Loss 0.004740785075286617\n",
      "Episode 4842; Testing Loss 0.005853532464961067; Training Loss 0.004740768253206725\n",
      "Episode 4843; Testing Loss 0.005853454892289383; Training Loss 0.004740763579544173\n",
      "Episode 4844; Testing Loss 0.005853447989204481; Training Loss 0.004740758797160864\n",
      "Episode 4845; Testing Loss 0.005853477761204879; Training Loss 0.0047407490300666166\n",
      "Episode 4846; Testing Loss 0.005853494744292852; Training Loss 0.004740735361326061\n",
      "Episode 4847; Testing Loss 0.0058535025298106485; Training Loss 0.0047407204541073085\n",
      "Episode 4848; Testing Loss 0.005853423118114552; Training Loss 0.004740706835935678\n",
      "Episode 4849; Testing Loss 0.0058533783187974; Training Loss 0.004740699052883971\n",
      "Episode 4850; Testing Loss 0.005853349621769047; Training Loss 0.00474068526044213\n",
      "Episode 4851; Testing Loss 0.005853293788270814; Training Loss 0.004740680039389078\n",
      "Episode 4852; Testing Loss 0.0058533137703095815; Training Loss 0.004740672871330026\n",
      "Episode 4853; Testing Loss 0.005853387222169986; Training Loss 0.004740661802979291\n",
      "Episode 4854; Testing Loss 0.005853321398570577; Training Loss 0.004740648155836605\n",
      "Episode 4855; Testing Loss 0.0058532005764724; Training Loss 0.004740635285546879\n",
      "Episode 4856; Testing Loss 0.0058532071712096715; Training Loss 0.0047406241824566216\n",
      "Episode 4857; Testing Loss 0.005853321977248372; Training Loss 0.004740614707616154\n",
      "Episode 4858; Testing Loss 0.005853348111520928; Training Loss 0.004740607232931494\n",
      "Episode 4859; Testing Loss 0.005853215270466238; Training Loss 0.0047405942603934255\n",
      "Episode 4860; Testing Loss 0.005853130064574961; Training Loss 0.004740585551768294\n",
      "Episode 4861; Testing Loss 0.0058532005690976815; Training Loss 0.004740573559906216\n",
      "Episode 4862; Testing Loss 0.005853315480324048; Training Loss 0.004740564195380048\n",
      "Episode 4863; Testing Loss 0.005853313948093062; Training Loss 0.004740555813518264\n",
      "Episode 4864; Testing Loss 0.005853261782376157; Training Loss 0.004740544166902055\n",
      "Episode 4865; Testing Loss 0.0058532154875173764; Training Loss 0.004740530628838634\n",
      "Episode 4866; Testing Loss 0.005853241283166889; Training Loss 0.00474052509085275\n",
      "Episode 4867; Testing Loss 0.005853197979230402; Training Loss 0.00474051584044586\n",
      "Episode 4868; Testing Loss 0.005853177947503068; Training Loss 0.004740499933186818\n",
      "Episode 4869; Testing Loss 0.005853204344564347; Training Loss 0.004740495191869363\n",
      "Episode 4870; Testing Loss 0.005853251936437094; Training Loss 0.004740488547290284\n",
      "Episode 4871; Testing Loss 0.00585326067052173; Training Loss 0.004740477871996492\n",
      "Episode 4872; Testing Loss 0.005853239218595613; Training Loss 0.004740466717666784\n",
      "Episode 4873; Testing Loss 0.005853194721715157; Training Loss 0.0047404519858292435\n",
      "Episode 4874; Testing Loss 0.00585314302499798; Training Loss 0.004740440731587731\n",
      "Episode 4875; Testing Loss 0.005853129806067649; Training Loss 0.004740431928699977\n",
      "Episode 4876; Testing Loss 0.005853144421378396; Training Loss 0.004740419052149529\n",
      "Episode 4877; Testing Loss 0.005853129639441582; Training Loss 0.004740410601503379\n",
      "Episode 4878; Testing Loss 0.0058531368441984725; Training Loss 0.004740403607069667\n",
      "Episode 4879; Testing Loss 0.00585314134079608; Training Loss 0.004740393895703383\n",
      "Episode 4880; Testing Loss 0.005853101409021607; Training Loss 0.00474038153295459\n",
      "Episode 4881; Testing Loss 0.005853014316198648; Training Loss 0.004740366457344888\n",
      "Episode 4882; Testing Loss 0.00585298099274622; Training Loss 0.0047403617868731\n",
      "Episode 4883; Testing Loss 0.0058530291181255745; Training Loss 0.004740352994815463\n",
      "Episode 4884; Testing Loss 0.0058530825871090306; Training Loss 0.0047403381172654415\n",
      "Episode 4885; Testing Loss 0.005853093205873799; Training Loss 0.004740327409290105\n",
      "Episode 4886; Testing Loss 0.005853033439078349; Training Loss 0.004740321154505676\n",
      "Episode 4887; Testing Loss 0.00585296303054533; Training Loss 0.004740311033658273\n",
      "Episode 4888; Testing Loss 0.005852960227163944; Training Loss 0.004740297849671404\n",
      "Episode 4889; Testing Loss 0.005853033056368311; Training Loss 0.004740282721919012\n",
      "Episode 4890; Testing Loss 0.005853013462855849; Training Loss 0.004740279856268398\n",
      "Episode 4891; Testing Loss 0.005852952076386884; Training Loss 0.004740271832276858\n",
      "Episode 4892; Testing Loss 0.005852879896976868; Training Loss 0.0047402567997930625\n",
      "Episode 4893; Testing Loss 0.005852904375733704; Training Loss 0.004740244606643649\n",
      "Episode 4894; Testing Loss 0.005852989699320155; Training Loss 0.004740238692022204\n",
      "Episode 4895; Testing Loss 0.005853037808348682; Training Loss 0.004740230100715369\n",
      "Episode 4896; Testing Loss 0.005852952078441837; Training Loss 0.004740216496063254\n",
      "Episode 4897; Testing Loss 0.005852884574227155; Training Loss 0.004740201834796302\n",
      "Episode 4898; Testing Loss 0.0058528831531478665; Training Loss 0.004740195447615405\n",
      "Episode 4899; Testing Loss 0.005852876134891234; Training Loss 0.004740186434251223\n",
      "Episode 4900; Testing Loss 0.005852845710378043; Training Loss 0.004740171547862055\n",
      "Episode 4901; Testing Loss 0.005852825510822877; Training Loss 0.004740163629971908\n",
      "Episode 4902; Testing Loss 0.005852856224037516; Training Loss 0.004740156192968985\n",
      "Episode 4903; Testing Loss 0.005852899959286763; Training Loss 0.0047401444744929406\n",
      "Episode 4904; Testing Loss 0.005852871245040673; Training Loss 0.004740133623179892\n",
      "Episode 4905; Testing Loss 0.005852891592594227; Training Loss 0.004740121161919612\n",
      "Episode 4906; Testing Loss 0.0058529407013243615; Training Loss 0.004740117122336412\n",
      "Episode 4907; Testing Loss 0.005852880928864592; Training Loss 0.004740105338890126\n",
      "Episode 4908; Testing Loss 0.00585279946717891; Training Loss 0.004740089794393264\n",
      "Episode 4909; Testing Loss 0.005852709452960521; Training Loss 0.004740082600726096\n",
      "Episode 4910; Testing Loss 0.005852726249091133; Training Loss 0.004740075918675069\n",
      "Episode 4911; Testing Loss 0.00585276535401862; Training Loss 0.00474006616845966\n",
      "Episode 4912; Testing Loss 0.00585278419792775; Training Loss 0.004740053263951932\n",
      "Episode 4913; Testing Loss 0.005852719418993613; Training Loss 0.0047400375888422\n",
      "Episode 4914; Testing Loss 0.005852654647287321; Training Loss 0.0047400320760622675\n",
      "Episode 4915; Testing Loss 0.0058526886459810395; Training Loss 0.0047400240832589854\n",
      "Episode 4916; Testing Loss 0.005852759394844285; Training Loss 0.004740009491418131\n",
      "Episode 4917; Testing Loss 0.00585282594612079; Training Loss 0.0047399969606221155\n",
      "Episode 4918; Testing Loss 0.005852825507111086; Training Loss 0.004739989352907948\n",
      "Episode 4919; Testing Loss 0.005852794202118049; Training Loss 0.004739979201556986\n",
      "Episode 4920; Testing Loss 0.005852716414361406; Training Loss 0.004739966852847495\n",
      "Episode 4921; Testing Loss 0.005852629146386939; Training Loss 0.004739954875359312\n",
      "Episode 4922; Testing Loss 0.005852641208194001; Training Loss 0.004739951356177619\n",
      "Episode 4923; Testing Loss 0.005852619444280691; Training Loss 0.0047399421427621906\n",
      "Episode 4924; Testing Loss 0.005852588370641914; Training Loss 0.004739926110800797\n",
      "Episode 4925; Testing Loss 0.005852610747056643; Training Loss 0.004739917653756161\n",
      "Episode 4926; Testing Loss 0.005852643080788075; Training Loss 0.004739910354194568\n",
      "Episode 4927; Testing Loss 0.005852638255690174; Training Loss 0.004739897262938514\n",
      "Episode 4928; Testing Loss 0.005852714557025855; Training Loss 0.004739885220563938\n",
      "Episode 4929; Testing Loss 0.00585276513684228; Training Loss 0.0047398717603484375\n",
      "Episode 4930; Testing Loss 0.005852688436815509; Training Loss 0.004739864546139603\n",
      "Episode 4931; Testing Loss 0.005852560119398251; Training Loss 0.004739853933782548\n",
      "Episode 4932; Testing Loss 0.005852523976112487; Training Loss 0.004739844485054301\n",
      "Episode 4933; Testing Loss 0.005852620079131601; Training Loss 0.00473983377135523\n",
      "Episode 4934; Testing Loss 0.005852688585470261; Training Loss 0.004739823936091557\n",
      "Episode 4935; Testing Loss 0.00585269706464641; Training Loss 0.004739811504723451\n",
      "Episode 4936; Testing Loss 0.005852550541664082; Training Loss 0.004739802852496138\n",
      "Episode 4937; Testing Loss 0.005852435752860386; Training Loss 0.004739790231845328\n",
      "Episode 4938; Testing Loss 0.005852466390425921; Training Loss 0.00473978369242933\n",
      "Episode 4939; Testing Loss 0.005852550891922753; Training Loss 0.004739776782099402\n",
      "Episode 4940; Testing Loss 0.005852508341003281; Training Loss 0.0047397644415031965\n",
      "Episode 4941; Testing Loss 0.005852412430079087; Training Loss 0.004739750479228801\n",
      "Episode 4942; Testing Loss 0.005852452631025433; Training Loss 0.00473973845691192\n",
      "Episode 4943; Testing Loss 0.005852520717655097; Training Loss 0.004739728196196947\n",
      "Episode 4944; Testing Loss 0.005852493248300249; Training Loss 0.004739719032214272\n",
      "Episode 4945; Testing Loss 0.005852439077235755; Training Loss 0.004739710189869576\n",
      "Episode 4946; Testing Loss 0.005852408705562742; Training Loss 0.004739696656079921\n",
      "Episode 4947; Testing Loss 0.00585246030664158; Training Loss 0.004739690111313429\n",
      "Episode 4948; Testing Loss 0.0058524832972297654; Training Loss 0.004739681398110404\n",
      "Episode 4949; Testing Loss 0.0058524708462768015; Training Loss 0.004739670212471143\n",
      "Episode 4950; Testing Loss 0.005852462245886764; Training Loss 0.00473966034143753\n",
      "Episode 4951; Testing Loss 0.005852445670265154; Training Loss 0.004739647829664922\n",
      "Episode 4952; Testing Loss 0.005852427856358081; Training Loss 0.0047396346354137316\n",
      "Episode 4953; Testing Loss 0.005852379645779291; Training Loss 0.004739629249753025\n",
      "Episode 4954; Testing Loss 0.005852277397805124; Training Loss 0.004739615377345539\n",
      "Episode 4955; Testing Loss 0.00585229345789658; Training Loss 0.004739608658201435\n",
      "Episode 4956; Testing Loss 0.005852474087004891; Training Loss 0.0047396005065744535\n",
      "Episode 4957; Testing Loss 0.005852534203381656; Training Loss 0.004739589980702428\n",
      "Episode 4958; Testing Loss 0.005852419290542559; Training Loss 0.004739574987006157\n",
      "Episode 4959; Testing Loss 0.005852261924142283; Training Loss 0.004739564892221388\n",
      "Episode 4960; Testing Loss 0.005852247083205285; Training Loss 0.004739555760898079\n",
      "Episode 4961; Testing Loss 0.0058523028912845255; Training Loss 0.004739543190475424\n",
      "Episode 4962; Testing Loss 0.005852354263259303; Training Loss 0.004739534256063048\n",
      "Episode 4963; Testing Loss 0.005852323405455495; Training Loss 0.00473952224346526\n",
      "Episode 4964; Testing Loss 0.005852250833462302; Training Loss 0.004739514808948964\n",
      "Episode 4965; Testing Loss 0.0058521990789966154; Training Loss 0.004739503324313351\n",
      "Episode 4966; Testing Loss 0.005852228187853853; Training Loss 0.004739493206675518\n",
      "Episode 4967; Testing Loss 0.005852319074929757; Training Loss 0.004739485604797413\n",
      "Episode 4968; Testing Loss 0.005852312084692032; Training Loss 0.004739473587148477\n",
      "Episode 4969; Testing Loss 0.005852225459130218; Training Loss 0.004739461020136872\n",
      "Episode 4970; Testing Loss 0.005852157600151456; Training Loss 0.004739455399383582\n",
      "Episode 4971; Testing Loss 0.005852209994886557; Training Loss 0.004739441721447464\n",
      "Episode 4972; Testing Loss 0.005852309131009736; Training Loss 0.004739430786328605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4973; Testing Loss 0.005852388142718535; Training Loss 0.004739425279553333\n",
      "Episode 4974; Testing Loss 0.005852345761020314; Training Loss 0.004739415284756391\n",
      "Episode 4975; Testing Loss 0.00585222599173047; Training Loss 0.004739401977918857\n",
      "Episode 4976; Testing Loss 0.005852188477219752; Training Loss 0.0047393908319923175\n",
      "Episode 4977; Testing Loss 0.005852195880952493; Training Loss 0.004739379646142818\n",
      "Episode 4978; Testing Loss 0.005852164162369519; Training Loss 0.004739368231973864\n",
      "Episode 4979; Testing Loss 0.0058521105804546225; Training Loss 0.0047393585815462234\n",
      "Episode 4980; Testing Loss 0.005852101743457002; Training Loss 0.004739347952487164\n",
      "Episode 4981; Testing Loss 0.005852157765026041; Training Loss 0.004739340622550475\n",
      "Episode 4982; Testing Loss 0.005852117814107421; Training Loss 0.004739328303046597\n",
      "Episode 4983; Testing Loss 0.005852079908987258; Training Loss 0.004739319084276896\n",
      "Episode 4984; Testing Loss 0.005852080394370732; Training Loss 0.004739311338025591\n",
      "Episode 4985; Testing Loss 0.005852107440296256; Training Loss 0.004739299972771765\n",
      "Episode 4986; Testing Loss 0.005852091508001204; Training Loss 0.004739285674225971\n",
      "Episode 4987; Testing Loss 0.0058520896685837955; Training Loss 0.004739282079180263\n",
      "Episode 4988; Testing Loss 0.005852070134696848; Training Loss 0.004739273221093273\n",
      "Episode 4989; Testing Loss 0.0058520376783069445; Training Loss 0.004739256664094037\n",
      "Episode 4990; Testing Loss 0.005852099484722078; Training Loss 0.004739248306830375\n",
      "Episode 4991; Testing Loss 0.0058522039034903306; Training Loss 0.0047392421129283\n",
      "Episode 4992; Testing Loss 0.005852246244978646; Training Loss 0.004739233695800343\n",
      "Episode 4993; Testing Loss 0.005852059026296557; Training Loss 0.004739220316018405\n",
      "Episode 4994; Testing Loss 0.005851993760971566; Training Loss 0.004739206738030814\n",
      "Episode 4995; Testing Loss 0.005852080711618945; Training Loss 0.004739196309246597\n",
      "Episode 4996; Testing Loss 0.005852112187427618; Training Loss 0.00473918940508852\n",
      "Episode 4997; Testing Loss 0.005852084051116611; Training Loss 0.004739174601506919\n",
      "Episode 4998; Testing Loss 0.005851978960551353; Training Loss 0.004739166886584209\n",
      "Episode 4999; Testing Loss 0.005851980194914352; Training Loss 0.0047391599955290496\n",
      "Episode 5000; Testing Loss 0.005852033037545499; Training Loss 0.004739150396273511\n",
      "Episode 5001; Testing Loss 0.005852035706505116; Training Loss 0.004739137486264428\n",
      "Episode 5002; Testing Loss 0.005851968052637874; Training Loss 0.0047391223986165305\n",
      "Episode 5003; Testing Loss 0.005851926607180379; Training Loss 0.0047391170770826095\n",
      "Episode 5004; Testing Loss 0.005851917552097497; Training Loss 0.004739108456652667\n",
      "Episode 5005; Testing Loss 0.005851907411668955; Training Loss 0.0047390955480349885\n",
      "Episode 5006; Testing Loss 0.005851847131130198; Training Loss 0.004739085476354948\n",
      "Episode 5007; Testing Loss 0.00585190585662788; Training Loss 0.004739078220347374\n",
      "Episode 5008; Testing Loss 0.005852012036440141; Training Loss 0.004739068176151964\n",
      "Episode 5009; Testing Loss 0.005852006885455532; Training Loss 0.004739055046642793\n",
      "Episode 5010; Testing Loss 0.005851938808187288; Training Loss 0.004739039659670022\n",
      "Episode 5011; Testing Loss 0.0058518820558778216; Training Loss 0.004739033531853869\n",
      "Episode 5012; Testing Loss 0.005851839576865031; Training Loss 0.004739022876294018\n",
      "Episode 5013; Testing Loss 0.00585186919317937; Training Loss 0.0047390099776107005\n",
      "Episode 5014; Testing Loss 0.005851905449738037; Training Loss 0.004739001887612557\n",
      "Episode 5015; Testing Loss 0.005851909742522908; Training Loss 0.004738990295625557\n",
      "Episode 5016; Testing Loss 0.005851833564043357; Training Loss 0.004738979056974393\n",
      "Episode 5017; Testing Loss 0.005851771363673176; Training Loss 0.004738971481795744\n",
      "Episode 5018; Testing Loss 0.005851779451682914; Training Loss 0.004738957505806197\n",
      "Episode 5019; Testing Loss 0.005851831516250195; Training Loss 0.00473895214084784\n",
      "Episode 5020; Testing Loss 0.005851909197763653; Training Loss 0.0047389451272881866\n",
      "Episode 5021; Testing Loss 0.0058519221658822765; Training Loss 0.004738933944182698\n",
      "Episode 5022; Testing Loss 0.005851810176515045; Training Loss 0.0047389202059014\n",
      "Episode 5023; Testing Loss 0.005851732689206468; Training Loss 0.004738907854979731\n",
      "Episode 5024; Testing Loss 0.005851750914681319; Training Loss 0.004738903039235961\n",
      "Episode 5025; Testing Loss 0.0058518011336144305; Training Loss 0.004738892004908565\n",
      "Episode 5026; Testing Loss 0.005851806333534635; Training Loss 0.0047388774279944635\n",
      "Episode 5027; Testing Loss 0.005851764830089268; Training Loss 0.004738870475303247\n",
      "Episode 5028; Testing Loss 0.005851796724650707; Training Loss 0.004738859719819358\n",
      "Episode 5029; Testing Loss 0.005851829622447114; Training Loss 0.004738848690934493\n",
      "Episode 5030; Testing Loss 0.005851780162051987; Training Loss 0.004738836987584406\n",
      "Episode 5031; Testing Loss 0.005851726067685432; Training Loss 0.004738830580365644\n",
      "Episode 5032; Testing Loss 0.005851731427671424; Training Loss 0.004738817129602354\n",
      "Episode 5033; Testing Loss 0.005851703160324582; Training Loss 0.004738809584711208\n",
      "Episode 5034; Testing Loss 0.005851636506181591; Training Loss 0.004738800796034232\n",
      "Episode 5035; Testing Loss 0.005851621924390209; Training Loss 0.00473879007445086\n",
      "Episode 5036; Testing Loss 0.005851741704074652; Training Loss 0.004738776327714928\n",
      "Episode 5037; Testing Loss 0.005851776932869336; Training Loss 0.004738772364198824\n",
      "Episode 5038; Testing Loss 0.005851629466452083; Training Loss 0.004738760957450344\n",
      "Episode 5039; Testing Loss 0.0058515561730917084; Training Loss 0.004738747723061095\n",
      "Episode 5040; Testing Loss 0.005851685051960299; Training Loss 0.0047387397209941945\n",
      "Episode 5041; Testing Loss 0.005851879774967022; Training Loss 0.0047387353865440685\n",
      "Episode 5042; Testing Loss 0.005851779040956425; Training Loss 0.004738723436941735\n",
      "Episode 5043; Testing Loss 0.005851520649274155; Training Loss 0.004738712148709005\n",
      "Episode 5044; Testing Loss 0.005851545036313175; Training Loss 0.004738697237012823\n",
      "Episode 5045; Testing Loss 0.005851742165327072; Training Loss 0.0047386877720131115\n",
      "Episode 5046; Testing Loss 0.005851724425333957; Training Loss 0.004738678865014387\n",
      "Episode 5047; Testing Loss 0.00585154598434748; Training Loss 0.0047386623452250805\n",
      "Episode 5048; Testing Loss 0.0058515049927241915; Training Loss 0.004738658035631318\n",
      "Episode 5049; Testing Loss 0.005851625616031619; Training Loss 0.0047386510075598754\n",
      "Episode 5050; Testing Loss 0.00585167125725605; Training Loss 0.004738641829672916\n",
      "Episode 5051; Testing Loss 0.005851590651448351; Training Loss 0.004738628541680824\n",
      "Episode 5052; Testing Loss 0.005851511694201649; Training Loss 0.004738613468415325\n",
      "Episode 5053; Testing Loss 0.005851525611475996; Training Loss 0.004738603191203743\n",
      "Episode 5054; Testing Loss 0.005851518606914353; Training Loss 0.004738596242446165\n",
      "Episode 5055; Testing Loss 0.005851494170115186; Training Loss 0.0047385816769754925\n",
      "Episode 5056; Testing Loss 0.005851493808415094; Training Loss 0.004738574841975551\n",
      "Episode 5057; Testing Loss 0.005851481822488794; Training Loss 0.004738568127478165\n",
      "Episode 5058; Testing Loss 0.005851473571305772; Training Loss 0.004738558836668144\n",
      "Episode 5059; Testing Loss 0.005851505888533195; Training Loss 0.004738545983631874\n",
      "Episode 5060; Testing Loss 0.005851559582335306; Training Loss 0.004738531918441973\n",
      "Episode 5061; Testing Loss 0.005851543791389692; Training Loss 0.004738524422859505\n",
      "Episode 5062; Testing Loss 0.005851434771692346; Training Loss 0.004738514941379437\n",
      "Episode 5063; Testing Loss 0.005851395754550852; Training Loss 0.004738500387599394\n",
      "Episode 5064; Testing Loss 0.005851471395439576; Training Loss 0.004738492244735037\n",
      "Episode 5065; Testing Loss 0.0058515591883879815; Training Loss 0.004738486194072205\n",
      "Episode 5066; Testing Loss 0.005851491732858635; Training Loss 0.0047384765427412046\n",
      "Episode 5067; Testing Loss 0.005851461918977964; Training Loss 0.004738463142594873\n",
      "Episode 5068; Testing Loss 0.005851500500316306; Training Loss 0.0047384491995363515\n",
      "Episode 5069; Testing Loss 0.005851509409868082; Training Loss 0.004738445629710207\n",
      "Episode 5070; Testing Loss 0.00585148035459516; Training Loss 0.004738437500499914\n",
      "Episode 5071; Testing Loss 0.005851408027761913; Training Loss 0.004738421767461109\n",
      "Episode 5072; Testing Loss 0.005851381763663848; Training Loss 0.004738409990320251\n",
      "Episode 5073; Testing Loss 0.005851467402637218; Training Loss 0.004738408658424654\n",
      "Episode 5074; Testing Loss 0.005851407379359173; Training Loss 0.004738392943183887\n",
      "Episode 5075; Testing Loss 0.005851361876228041; Training Loss 0.004738383246414879\n",
      "Episode 5076; Testing Loss 0.005851473360562467; Training Loss 0.004738369350295397\n",
      "Episode 5077; Testing Loss 0.005851449504051317; Training Loss 0.004738363412576419\n",
      "Episode 5078; Testing Loss 0.005851293849155432; Training Loss 0.004738351450333066\n",
      "Episode 5079; Testing Loss 0.005851242945219833; Training Loss 0.004738337296739211\n",
      "Episode 5080; Testing Loss 0.005851366334647567; Training Loss 0.004738331321851894\n",
      "Episode 5081; Testing Loss 0.005851353042992987; Training Loss 0.004738316495246\n",
      "Episode 5082; Testing Loss 0.005851263622069515; Training Loss 0.004738307794885782\n",
      "Episode 5083; Testing Loss 0.005851251498413408; Training Loss 0.004738299252809978\n",
      "Episode 5084; Testing Loss 0.0058513816417461425; Training Loss 0.004738287614928998\n",
      "Episode 5085; Testing Loss 0.005851385951621075; Training Loss 0.0047382761353615055\n",
      "Episode 5086; Testing Loss 0.00585126843875964; Training Loss 0.004738264986957445\n",
      "Episode 5087; Testing Loss 0.005851194751776039; Training Loss 0.004738256001014156\n",
      "Episode 5088; Testing Loss 0.005851236816591783; Training Loss 0.004738245084508093\n",
      "Episode 5089; Testing Loss 0.005851302267729271; Training Loss 0.004738234080549119\n",
      "Episode 5090; Testing Loss 0.005851282670740945; Training Loss 0.004738223306215067\n",
      "Episode 5091; Testing Loss 0.005851212847693024; Training Loss 0.004738214043475583\n",
      "Episode 5092; Testing Loss 0.005851197104993718; Training Loss 0.0047382034634109215\n",
      "Episode 5093; Testing Loss 0.005851242237505806; Training Loss 0.004738193381321326\n",
      "Episode 5094; Testing Loss 0.005851240854316355; Training Loss 0.0047381833839975395\n",
      "Episode 5095; Testing Loss 0.005851151728072104; Training Loss 0.0047381726640693565\n",
      "Episode 5096; Testing Loss 0.005851112877329525; Training Loss 0.004738164174019021\n",
      "Episode 5097; Testing Loss 0.005851140642107934; Training Loss 0.004738153407022893\n",
      "Episode 5098; Testing Loss 0.005851275981133889; Training Loss 0.004738142686137301\n",
      "Episode 5099; Testing Loss 0.005851276708670794; Training Loss 0.004738134085432348\n",
      "Episode 5100; Testing Loss 0.005851128948336746; Training Loss 0.004738122759095443\n",
      "Episode 5101; Testing Loss 0.005851152146183949; Training Loss 0.00473811120425378\n",
      "Episode 5102; Testing Loss 0.005851186132296034; Training Loss 0.0047381030728161095\n",
      "Episode 5103; Testing Loss 0.005851097753280659; Training Loss 0.004738090995172199\n",
      "Episode 5104; Testing Loss 0.005851083560536615; Training Loss 0.004738080380542428\n",
      "Episode 5105; Testing Loss 0.005851138573376358; Training Loss 0.004738073186643412\n",
      "Episode 5106; Testing Loss 0.005851117129620064; Training Loss 0.004738060952133002\n",
      "Episode 5107; Testing Loss 0.005851113353161687; Training Loss 0.004738053718504906\n",
      "Episode 5108; Testing Loss 0.005851208689345573; Training Loss 0.004738045780677452\n",
      "Episode 5109; Testing Loss 0.005851208337714164; Training Loss 0.004738035978151743\n",
      "Episode 5110; Testing Loss 0.005851077283754624; Training Loss 0.004738022035336383\n",
      "Episode 5111; Testing Loss 0.005851037423066784; Training Loss 0.004738012374744382\n",
      "Episode 5112; Testing Loss 0.005851124760055867; Training Loss 0.004738002937157443\n",
      "Episode 5113; Testing Loss 0.005851200593654196; Training Loss 0.004737989460716025\n",
      "Episode 5114; Testing Loss 0.005851109241877736; Training Loss 0.004737981979339648\n",
      "Episode 5115; Testing Loss 0.00585103077278148; Training Loss 0.004737972604620246\n",
      "Episode 5116; Testing Loss 0.005851119053404557; Training Loss 0.004737961051562316\n",
      "Episode 5117; Testing Loss 0.00585114020823283; Training Loss 0.004737947482521932\n",
      "Episode 5118; Testing Loss 0.0058511009285727965; Training Loss 0.004737943161721188\n",
      "Episode 5119; Testing Loss 0.005851016549794046; Training Loss 0.004737934405332209\n",
      "Episode 5120; Testing Loss 0.005851039220474648; Training Loss 0.004737918765631172\n",
      "Episode 5121; Testing Loss 0.005851130925024758; Training Loss 0.004737914762542953\n",
      "Episode 5122; Testing Loss 0.005851114671119154; Training Loss 0.004737904557817347\n",
      "Episode 5123; Testing Loss 0.005851091555441105; Training Loss 0.004737896652135548\n",
      "Episode 5124; Testing Loss 0.0058511724048427765; Training Loss 0.004737885129215307\n",
      "Episode 5125; Testing Loss 0.005851214069551121; Training Loss 0.004737870354692353\n",
      "Episode 5126; Testing Loss 0.005851116570891283; Training Loss 0.004737861321331622\n",
      "Episode 5127; Testing Loss 0.005850960066394475; Training Loss 0.004737852358026246\n",
      "Episode 5128; Testing Loss 0.00585091538929663; Training Loss 0.004737840392525488\n",
      "Episode 5129; Testing Loss 0.005850974547916664; Training Loss 0.0047378277841227095\n",
      "Episode 5130; Testing Loss 0.005851147340953176; Training Loss 0.0047378214226681484\n",
      "Episode 5131; Testing Loss 0.005851160797571214; Training Loss 0.004737810569303391\n",
      "Episode 5132; Testing Loss 0.005851048762294106; Training Loss 0.004737797774666376\n",
      "Episode 5133; Testing Loss 0.005850966520100643; Training Loss 0.004737784828423295\n",
      "Episode 5134; Testing Loss 0.005851075185626211; Training Loss 0.004737782002788142\n",
      "Episode 5135; Testing Loss 0.005851124557796829; Training Loss 0.004737774093205439\n",
      "Episode 5136; Testing Loss 0.005851000378258389; Training Loss 0.004737758311887738\n",
      "Episode 5137; Testing Loss 0.005850897154278053; Training Loss 0.004737747923458242\n",
      "Episode 5138; Testing Loss 0.005850991938510827; Training Loss 0.004737738064749192\n",
      "Episode 5139; Testing Loss 0.0058512208655226395; Training Loss 0.004737729922681959\n",
      "Episode 5140; Testing Loss 0.005851203986679577; Training Loss 0.004737716168894416\n",
      "Episode 5141; Testing Loss 0.005850999093601864; Training Loss 0.004737702278574782\n",
      "Episode 5142; Testing Loss 0.005850887810566577; Training Loss 0.004737697217986501\n",
      "Episode 5143; Testing Loss 0.005850943716376916; Training Loss 0.0047376849283345835\n",
      "Episode 5144; Testing Loss 0.00585103887860159; Training Loss 0.004737671297809205\n",
      "Episode 5145; Testing Loss 0.005850996482359955; Training Loss 0.004737661722009128\n",
      "Episode 5146; Testing Loss 0.0058509125941541475; Training Loss 0.004737650853735794\n",
      "Episode 5147; Testing Loss 0.005850905327089981; Training Loss 0.004737641430354426\n",
      "Episode 5148; Testing Loss 0.00585098683984733; Training Loss 0.004737630113632541\n",
      "Episode 5149; Testing Loss 0.005851000484683194; Training Loss 0.004737621798376546\n",
      "Episode 5150; Testing Loss 0.0058509664896065385; Training Loss 0.004737614030231457\n",
      "Episode 5151; Testing Loss 0.00585094172420407; Training Loss 0.004737602777028336\n",
      "Episode 5152; Testing Loss 0.0058508987995281045; Training Loss 0.004737588805026229\n",
      "Episode 5153; Testing Loss 0.005850886217578603; Training Loss 0.004737583843226894\n",
      "Episode 5154; Testing Loss 0.005850870606507341; Training Loss 0.004737571794288188\n",
      "Episode 5155; Testing Loss 0.005850882575855179; Training Loss 0.00473756069866016\n",
      "Episode 5156; Testing Loss 0.005850959161419687; Training Loss 0.004737551516093983\n",
      "Episode 5157; Testing Loss 0.005850941171686339; Training Loss 0.004737541951047247\n",
      "Episode 5158; Testing Loss 0.005850873702023229; Training Loss 0.004737528868503023\n",
      "Episode 5159; Testing Loss 0.005850856739376586; Training Loss 0.004737523527562811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5160; Testing Loss 0.005850845436901013; Training Loss 0.004737512629032864\n",
      "Episode 5161; Testing Loss 0.00585079853319702; Training Loss 0.004737499529673159\n",
      "Episode 5162; Testing Loss 0.005850733353667169; Training Loss 0.004737490041906263\n",
      "Episode 5163; Testing Loss 0.0058508001456007794; Training Loss 0.004737478586308607\n",
      "Episode 5164; Testing Loss 0.005850912471878119; Training Loss 0.004737466795817814\n",
      "Episode 5165; Testing Loss 0.005850874750483426; Training Loss 0.004737457767425522\n",
      "Episode 5166; Testing Loss 0.0058507666581759325; Training Loss 0.004737446377449107\n",
      "Episode 5167; Testing Loss 0.005850767503790258; Training Loss 0.004737438180590353\n",
      "Episode 5168; Testing Loss 0.005850796620803294; Training Loss 0.004737426359003279\n",
      "Episode 5169; Testing Loss 0.005850834249799719; Training Loss 0.004737417766944618\n",
      "Episode 5170; Testing Loss 0.005850761254992314; Training Loss 0.0047374058534115655\n",
      "Episode 5171; Testing Loss 0.0058507152361973605; Training Loss 0.004737394819703941\n",
      "Episode 5172; Testing Loss 0.00585076992506227; Training Loss 0.004737384843743988\n",
      "Episode 5173; Testing Loss 0.0058507510501372775; Training Loss 0.004737377909072775\n",
      "Episode 5174; Testing Loss 0.005850693271764806; Training Loss 0.004737364292131968\n",
      "Episode 5175; Testing Loss 0.005850731755549487; Training Loss 0.004737354322308396\n",
      "Episode 5176; Testing Loss 0.0058507281346757605; Training Loss 0.00473734591989728\n",
      "Episode 5177; Testing Loss 0.005850693032195837; Training Loss 0.00473733516067268\n",
      "Episode 5178; Testing Loss 0.005850630344203216; Training Loss 0.00473732398269631\n",
      "Episode 5179; Testing Loss 0.0058506127306794335; Training Loss 0.0047373132605197646\n",
      "Episode 5180; Testing Loss 0.005850709394400119; Training Loss 0.004737306193642678\n",
      "Episode 5181; Testing Loss 0.005850687323652519; Training Loss 0.004737293248887464\n",
      "Episode 5182; Testing Loss 0.005850654284825282; Training Loss 0.004737287134818532\n",
      "Episode 5183; Testing Loss 0.005850605502253864; Training Loss 0.004737275009158671\n",
      "Episode 5184; Testing Loss 0.005850608960138529; Training Loss 0.004737262938561303\n",
      "Episode 5185; Testing Loss 0.00585065406842906; Training Loss 0.004737254346400812\n",
      "Episode 5186; Testing Loss 0.005850654433310052; Training Loss 0.004737243587644251\n",
      "Episode 5187; Testing Loss 0.005850551043564168; Training Loss 0.004737232290992577\n",
      "Episode 5188; Testing Loss 0.0058504969668075404; Training Loss 0.004737220625958734\n",
      "Episode 5189; Testing Loss 0.005850611323122687; Training Loss 0.004737210693764205\n",
      "Episode 5190; Testing Loss 0.0058507406404961545; Training Loss 0.004737202382947918\n",
      "Episode 5191; Testing Loss 0.0058506707084926115; Training Loss 0.004737190729732878\n",
      "Episode 5192; Testing Loss 0.005850462152585102; Training Loss 0.004737182016020508\n",
      "Episode 5193; Testing Loss 0.005850406575346166; Training Loss 0.004737173157217251\n",
      "Episode 5194; Testing Loss 0.005850527432239807; Training Loss 0.004737162146978446\n",
      "Episode 5195; Testing Loss 0.00585056315197812; Training Loss 0.0047371499799558296\n",
      "Episode 5196; Testing Loss 0.005850495336429518; Training Loss 0.004737139183347999\n",
      "Episode 5197; Testing Loss 0.005850501853854313; Training Loss 0.004737128562710317\n",
      "Episode 5198; Testing Loss 0.005850513697910518; Training Loss 0.0047371201719505155\n",
      "Episode 5199; Testing Loss 0.00585046766152579; Training Loss 0.004737107921661519\n",
      "Episode 5200; Testing Loss 0.0058504534275472045; Training Loss 0.004737099424391308\n",
      "Episode 5201; Testing Loss 0.005850409977359368; Training Loss 0.004737087555188947\n",
      "Episode 5202; Testing Loss 0.005850409325406793; Training Loss 0.004737078274969431\n",
      "Episode 5203; Testing Loss 0.005850379930554672; Training Loss 0.004737067799981388\n",
      "Episode 5204; Testing Loss 0.005850393847316649; Training Loss 0.004737058597976159\n",
      "Episode 5205; Testing Loss 0.0058504360597754; Training Loss 0.004737050074517438\n",
      "Episode 5206; Testing Loss 0.005850378309744965; Training Loss 0.00473703610486751\n",
      "Episode 5207; Testing Loss 0.005850371571296209; Training Loss 0.004737027852855583\n",
      "Episode 5208; Testing Loss 0.005850372690508874; Training Loss 0.004737021837148459\n",
      "Episode 5209; Testing Loss 0.005850386912709344; Training Loss 0.004737008697124065\n",
      "Episode 5210; Testing Loss 0.005850362727168514; Training Loss 0.004736997276852324\n",
      "Episode 5211; Testing Loss 0.005850334972591877; Training Loss 0.0047369913349624435\n",
      "Episode 5212; Testing Loss 0.005850267674511454; Training Loss 0.004736979885042336\n",
      "Episode 5213; Testing Loss 0.005850282702081571; Training Loss 0.0047369675428419\n",
      "Episode 5214; Testing Loss 0.005850382878030096; Training Loss 0.004736958322759867\n",
      "Episode 5215; Testing Loss 0.00585036689319942; Training Loss 0.004736948210526426\n",
      "Episode 5216; Testing Loss 0.0058502568738751344; Training Loss 0.0047369376687803775\n",
      "Episode 5217; Testing Loss 0.005850188309917551; Training Loss 0.004736924480718329\n",
      "Episode 5218; Testing Loss 0.005850197593182411; Training Loss 0.004736918284635608\n",
      "Episode 5219; Testing Loss 0.005850301081368839; Training Loss 0.004736910486450987\n",
      "Episode 5220; Testing Loss 0.005850272134498248; Training Loss 0.004736898023382085\n",
      "Episode 5221; Testing Loss 0.0058501521480810725; Training Loss 0.004736883302745696\n",
      "Episode 5222; Testing Loss 0.005850098150494581; Training Loss 0.004736877570429854\n",
      "Episode 5223; Testing Loss 0.005850169447846426; Training Loss 0.004736865168301877\n",
      "Episode 5224; Testing Loss 0.005850216468620337; Training Loss 0.004736855222896205\n",
      "Episode 5225; Testing Loss 0.005850217950183072; Training Loss 0.004736846842220269\n",
      "Episode 5226; Testing Loss 0.005850193563237667; Training Loss 0.004736835331394071\n",
      "Episode 5227; Testing Loss 0.005850132833362949; Training Loss 0.0047368228954935655\n",
      "Episode 5228; Testing Loss 0.005850120059353562; Training Loss 0.004736816960237306\n",
      "Episode 5229; Testing Loss 0.00585009160357733; Training Loss 0.0047368065633856165\n",
      "Episode 5230; Testing Loss 0.005850016577067825; Training Loss 0.004736794020917856\n",
      "Episode 5231; Testing Loss 0.0058499684636568825; Training Loss 0.0047367849101947156\n",
      "Episode 5232; Testing Loss 0.005850084962600967; Training Loss 0.00473677432102932\n",
      "Episode 5233; Testing Loss 0.005850171765959781; Training Loss 0.004736762031878679\n",
      "Episode 5234; Testing Loss 0.0058500900686056095; Training Loss 0.004736754478249408\n",
      "Episode 5235; Testing Loss 0.005850022630081543; Training Loss 0.004736745266167752\n",
      "Episode 5236; Testing Loss 0.005850075966412699; Training Loss 0.004736731328261739\n",
      "Episode 5237; Testing Loss 0.005850177163940562; Training Loss 0.004736723284798551\n",
      "Episode 5238; Testing Loss 0.005850135312018895; Training Loss 0.004736712113215202\n",
      "Episode 5239; Testing Loss 0.0058499972207191825; Training Loss 0.004736701643382634\n",
      "Episode 5240; Testing Loss 0.005849867558653751; Training Loss 0.004736692882233734\n",
      "Episode 5241; Testing Loss 0.005850033774291631; Training Loss 0.004736679834026244\n",
      "Episode 5242; Testing Loss 0.005850190403186045; Training Loss 0.0047366706774364265\n",
      "Episode 5243; Testing Loss 0.005850081259326169; Training Loss 0.004736659500685933\n",
      "Episode 5244; Testing Loss 0.005849960076675317; Training Loss 0.004736650353545787\n",
      "Episode 5245; Testing Loss 0.005849930236198901; Training Loss 0.004736639236038989\n",
      "Episode 5246; Testing Loss 0.0058500544502722235; Training Loss 0.004736630855744416\n",
      "Episode 5247; Testing Loss 0.005850029574020306; Training Loss 0.004736623422116246\n",
      "Episode 5248; Testing Loss 0.00584989796234358; Training Loss 0.004736613786877782\n",
      "Episode 5249; Testing Loss 0.00584984644556076; Training Loss 0.004736602171503931\n",
      "Episode 5250; Testing Loss 0.005849929248689365; Training Loss 0.004736592501856172\n",
      "Episode 5251; Testing Loss 0.005849973461254796; Training Loss 0.004736582815238392\n",
      "Episode 5252; Testing Loss 0.005849938995659041; Training Loss 0.0047365706509914385\n",
      "Episode 5253; Testing Loss 0.005849933626139838; Training Loss 0.0047365602039176145\n",
      "Episode 5254; Testing Loss 0.005849953262409836; Training Loss 0.004736548204057931\n",
      "Episode 5255; Testing Loss 0.005849986110079078; Training Loss 0.004736541593688149\n",
      "Episode 5256; Testing Loss 0.005849970963069369; Training Loss 0.004736530440245751\n",
      "Episode 5257; Testing Loss 0.005849928041101009; Training Loss 0.004736520599213659\n",
      "Episode 5258; Testing Loss 0.005849865087463185; Training Loss 0.0047365118431478125\n",
      "Episode 5259; Testing Loss 0.0058497972581853545; Training Loss 0.004736502267519034\n",
      "Episode 5260; Testing Loss 0.005849782475256133; Training Loss 0.004736489337803552\n",
      "Episode 5261; Testing Loss 0.005849776525759717; Training Loss 0.004736481972691187\n",
      "Episode 5262; Testing Loss 0.00584965519406623; Training Loss 0.0047364701513850265\n",
      "Episode 5263; Testing Loss 0.005849677087668235; Training Loss 0.00473645769512737\n",
      "Episode 5264; Testing Loss 0.005849859100799795; Training Loss 0.004736447163865527\n",
      "Episode 5265; Testing Loss 0.0058499009804549775; Training Loss 0.004736436013707498\n",
      "Episode 5266; Testing Loss 0.005849797944537402; Training Loss 0.004736427118923528\n",
      "Episode 5267; Testing Loss 0.005849752679497126; Training Loss 0.004736416342251828\n",
      "Episode 5268; Testing Loss 0.005849831672104648; Training Loss 0.00473640783410387\n",
      "Episode 5269; Testing Loss 0.0058498060466634656; Training Loss 0.004736395169645622\n",
      "Episode 5270; Testing Loss 0.005849742974112655; Training Loss 0.004736387959552952\n",
      "Episode 5271; Testing Loss 0.005849698272261015; Training Loss 0.004736374625820925\n",
      "Episode 5272; Testing Loss 0.00584976572608497; Training Loss 0.004736366483036339\n",
      "Episode 5273; Testing Loss 0.005849776329199064; Training Loss 0.004736354880202038\n",
      "Episode 5274; Testing Loss 0.005849773131182136; Training Loss 0.0047363473365648005\n",
      "Episode 5275; Testing Loss 0.005849738546716672; Training Loss 0.004736335701728723\n",
      "Episode 5276; Testing Loss 0.0058496889754161355; Training Loss 0.004736326424952293\n",
      "Episode 5277; Testing Loss 0.0058496955240014475; Training Loss 0.004736313843764927\n",
      "Episode 5278; Testing Loss 0.00584969886296742; Training Loss 0.004736309289311883\n",
      "Episode 5279; Testing Loss 0.005849575448344066; Training Loss 0.004736296258945076\n",
      "Episode 5280; Testing Loss 0.005849607506308892; Training Loss 0.004736284797460963\n",
      "Episode 5281; Testing Loss 0.0058497274417980485; Training Loss 0.004736276588281972\n",
      "Episode 5282; Testing Loss 0.005849753156548644; Training Loss 0.004736265110070063\n",
      "Episode 5283; Testing Loss 0.005849692894933909; Training Loss 0.004736253786126608\n",
      "Episode 5284; Testing Loss 0.005849616891956748; Training Loss 0.004736243502124633\n",
      "Episode 5285; Testing Loss 0.005849539768984419; Training Loss 0.004736236745914884\n",
      "Episode 5286; Testing Loss 0.005849578269980392; Training Loss 0.004736225778227985\n",
      "Episode 5287; Testing Loss 0.005849646315696453; Training Loss 0.004736213139382616\n",
      "Episode 5288; Testing Loss 0.005849637326158298; Training Loss 0.0047362091817167076\n",
      "Episode 5289; Testing Loss 0.005849550785136541; Training Loss 0.004736197622044438\n",
      "Episode 5290; Testing Loss 0.005849544193278486; Training Loss 0.00473618353015223\n",
      "Episode 5291; Testing Loss 0.005849598474170873; Training Loss 0.0047361770436563306\n",
      "Episode 5292; Testing Loss 0.005849540226102356; Training Loss 0.004736163206567935\n",
      "Episode 5293; Testing Loss 0.005849503956561963; Training Loss 0.0047361549538963805\n",
      "Episode 5294; Testing Loss 0.005849579327789076; Training Loss 0.004736144080650373\n",
      "Episode 5295; Testing Loss 0.005849565623805103; Training Loss 0.0047361361975723935\n",
      "Episode 5296; Testing Loss 0.005849522322859789; Training Loss 0.00473612613092554\n",
      "Episode 5297; Testing Loss 0.005849532085505949; Training Loss 0.00473611600262898\n",
      "Episode 5298; Testing Loss 0.005849551060704374; Training Loss 0.004736105610323687\n",
      "Episode 5299; Testing Loss 0.00584945953030237; Training Loss 0.0047360951072823385\n",
      "Episode 5300; Testing Loss 0.005849363975457597; Training Loss 0.004736087385907124\n",
      "Episode 5301; Testing Loss 0.005849447632646143; Training Loss 0.004736073503102505\n",
      "Episode 5302; Testing Loss 0.005849530354084701; Training Loss 0.004736066018365452\n",
      "Episode 5303; Testing Loss 0.005849476068719574; Training Loss 0.004736053509736512\n",
      "Episode 5304; Testing Loss 0.005849374560546115; Training Loss 0.00473604119837814\n",
      "Episode 5305; Testing Loss 0.005849347914155212; Training Loss 0.004736034228821988\n",
      "Episode 5306; Testing Loss 0.0058494194466158405; Training Loss 0.004736021833337355\n",
      "Episode 5307; Testing Loss 0.005849472817549139; Training Loss 0.004736009998012148\n",
      "Episode 5308; Testing Loss 0.005849483133155068; Training Loss 0.0047360010034076154\n",
      "Episode 5309; Testing Loss 0.005849424113114626; Training Loss 0.004735992832016756\n",
      "Episode 5310; Testing Loss 0.005849351684245622; Training Loss 0.00473597921712268\n",
      "Episode 5311; Testing Loss 0.005849312020168757; Training Loss 0.004735973405407464\n",
      "Episode 5312; Testing Loss 0.005849349327324282; Training Loss 0.004735965654363444\n",
      "Episode 5313; Testing Loss 0.005849377782477741; Training Loss 0.004735954832454139\n",
      "Episode 5314; Testing Loss 0.0058493319526351775; Training Loss 0.0047359415277618225\n",
      "Episode 5315; Testing Loss 0.005849328199055579; Training Loss 0.004735933252387646\n",
      "Episode 5316; Testing Loss 0.005849341021584616; Training Loss 0.0047359236142341605\n",
      "Episode 5317; Testing Loss 0.0058492901999437205; Training Loss 0.0047359091009306535\n",
      "Episode 5318; Testing Loss 0.005849236107713015; Training Loss 0.00473590177738246\n",
      "Episode 5319; Testing Loss 0.0058492527174001; Training Loss 0.00473589271871195\n",
      "Episode 5320; Testing Loss 0.0058493328679211296; Training Loss 0.00473587999584793\n",
      "Episode 5321; Testing Loss 0.005849331046283549; Training Loss 0.004735871266076129\n",
      "Episode 5322; Testing Loss 0.005849196945127427; Training Loss 0.004735861973673757\n",
      "Episode 5323; Testing Loss 0.005849179635499798; Training Loss 0.004735850853983191\n",
      "Episode 5324; Testing Loss 0.005849343263899928; Training Loss 0.00473584104413249\n",
      "Episode 5325; Testing Loss 0.005849333790477214; Training Loss 0.004735830019551653\n",
      "Episode 5326; Testing Loss 0.005849157539908272; Training Loss 0.004735818684220681\n",
      "Episode 5327; Testing Loss 0.005849077019012846; Training Loss 0.004735809991002752\n",
      "Episode 5328; Testing Loss 0.00584923424444671; Training Loss 0.004735796580983392\n",
      "Episode 5329; Testing Loss 0.005849363094253235; Training Loss 0.004735791261057037\n",
      "Episode 5330; Testing Loss 0.005849228069245715; Training Loss 0.004735776538659459\n",
      "Episode 5331; Testing Loss 0.005849121288354047; Training Loss 0.004735768271697908\n",
      "Episode 5332; Testing Loss 0.005849181378099787; Training Loss 0.00473575723937847\n",
      "Episode 5333; Testing Loss 0.005849256514358411; Training Loss 0.004735747096248913\n",
      "Episode 5334; Testing Loss 0.005849209492432477; Training Loss 0.004735738540184184\n",
      "Episode 5335; Testing Loss 0.005849032143959971; Training Loss 0.004735730790619991\n",
      "Episode 5336; Testing Loss 0.005849173233215437; Training Loss 0.00473571889569746\n",
      "Episode 5337; Testing Loss 0.005849325930638145; Training Loss 0.004735707960769686\n",
      "Episode 5338; Testing Loss 0.00584926796682806; Training Loss 0.0047356998514392805\n",
      "Episode 5339; Testing Loss 0.005849096151577325; Training Loss 0.004735688034557496\n",
      "Episode 5340; Testing Loss 0.005849071330973709; Training Loss 0.004735679530546809\n",
      "Episode 5341; Testing Loss 0.005849201131091672; Training Loss 0.004735674467652664\n",
      "Episode 5342; Testing Loss 0.005849186834602891; Training Loss 0.004735660156361529\n",
      "Episode 5343; Testing Loss 0.005849071635942453; Training Loss 0.004735646427482956\n",
      "Episode 5344; Testing Loss 0.00584908282801683; Training Loss 0.004735643805960592\n",
      "Episode 5345; Testing Loss 0.005849153188617011; Training Loss 0.004735633628082071\n",
      "Episode 5346; Testing Loss 0.005849106057821635; Training Loss 0.004735617380807291\n",
      "Episode 5347; Testing Loss 0.005848950412807994; Training Loss 0.004735612412238262\n",
      "Episode 5348; Testing Loss 0.005848929192622498; Training Loss 0.004735606355091804\n",
      "Episode 5349; Testing Loss 0.005849045267715493; Training Loss 0.004735595477344065\n",
      "Episode 5350; Testing Loss 0.005849142915135578; Training Loss 0.004735583239040255\n",
      "Episode 5351; Testing Loss 0.005849042174764238; Training Loss 0.004735567583865845\n",
      "Episode 5352; Testing Loss 0.005848989815975555; Training Loss 0.004735558234152581\n",
      "Episode 5353; Testing Loss 0.005849008536529231; Training Loss 0.004735551365981471\n",
      "Episode 5354; Testing Loss 0.005848947267011271; Training Loss 0.004735535650820482\n",
      "Episode 5355; Testing Loss 0.0058489405166573544; Training Loss 0.00473552989196508\n",
      "Episode 5356; Testing Loss 0.005848994111980598; Training Loss 0.004735522475430852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5357; Testing Loss 0.005848998904426555; Training Loss 0.00473551316252845\n",
      "Episode 5358; Testing Loss 0.00584894892038648; Training Loss 0.004735500567407986\n",
      "Episode 5359; Testing Loss 0.005848948696655643; Training Loss 0.004735486847725585\n",
      "Episode 5360; Testing Loss 0.005848961667124796; Training Loss 0.004735478809447955\n",
      "Episode 5361; Testing Loss 0.005848906773015294; Training Loss 0.004735469831784651\n",
      "Episode 5362; Testing Loss 0.005848801319982453; Training Loss 0.004735454663268104\n",
      "Episode 5363; Testing Loss 0.005848751006515172; Training Loss 0.004735452089838321\n",
      "Episode 5364; Testing Loss 0.005848902943974904; Training Loss 0.004735444165145488\n",
      "Episode 5365; Testing Loss 0.005848967946728161; Training Loss 0.004735435558849664\n",
      "Episode 5366; Testing Loss 0.005848780737044283; Training Loss 0.004735420510279191\n",
      "Episode 5367; Testing Loss 0.00584875185307342; Training Loss 0.004735406777820004\n",
      "Episode 5368; Testing Loss 0.005848926318184476; Training Loss 0.004735402379594925\n",
      "Episode 5369; Testing Loss 0.005848960372512005; Training Loss 0.004735395080561489\n",
      "Episode 5370; Testing Loss 0.005848835517534426; Training Loss 0.004735380525219908\n",
      "Episode 5371; Testing Loss 0.005848840083704836; Training Loss 0.004735365395885515\n",
      "Episode 5372; Testing Loss 0.0058489746123337035; Training Loss 0.004735362002134228\n",
      "Episode 5373; Testing Loss 0.005848904457037429; Training Loss 0.004735349038103605\n",
      "Episode 5374; Testing Loss 0.005848744518352709; Training Loss 0.004735340150101554\n",
      "Episode 5375; Testing Loss 0.005848774897622413; Training Loss 0.004735326293873143\n",
      "Episode 5376; Testing Loss 0.005848896184948485; Training Loss 0.0047353207511338395\n",
      "Episode 5377; Testing Loss 0.0058488531777796116; Training Loss 0.004735309502795561\n",
      "Episode 5378; Testing Loss 0.005848730100422895; Training Loss 0.0047352949709643\n",
      "Episode 5379; Testing Loss 0.005848648318752451; Training Loss 0.004735288155699891\n",
      "Episode 5380; Testing Loss 0.005848741826398942; Training Loss 0.00473528153649348\n",
      "Episode 5381; Testing Loss 0.005848942537820189; Training Loss 0.004735274777978897\n",
      "Episode 5382; Testing Loss 0.005848893081871685; Training Loss 0.004735260595050852\n",
      "Episode 5383; Testing Loss 0.005848710322516637; Training Loss 0.004735244855064371\n",
      "Episode 5384; Testing Loss 0.005848688926823149; Training Loss 0.004735234982265304\n",
      "Episode 5385; Testing Loss 0.005848767064821482; Training Loss 0.004735228754752086\n",
      "Episode 5386; Testing Loss 0.005848802958567749; Training Loss 0.00473521421685672\n",
      "Episode 5387; Testing Loss 0.005848672057349076; Training Loss 0.0047352047332468745\n",
      "Episode 5388; Testing Loss 0.00584865892498459; Training Loss 0.004735198875311008\n",
      "Episode 5389; Testing Loss 0.005848722244705948; Training Loss 0.004735189303373531\n",
      "Episode 5390; Testing Loss 0.005848727445599016; Training Loss 0.004735176501175349\n",
      "Episode 5391; Testing Loss 0.005848632950038499; Training Loss 0.004735162192308112\n",
      "Episode 5392; Testing Loss 0.0058484944258419425; Training Loss 0.004735152864734698\n",
      "Episode 5393; Testing Loss 0.005848395248891912; Training Loss 0.0047351465199105556\n",
      "Episode 5394; Testing Loss 0.005848446754393292; Training Loss 0.004735132566630684\n",
      "Episode 5395; Testing Loss 0.005848568159815479; Training Loss 0.004735121316718186\n",
      "Episode 5396; Testing Loss 0.0058485771195502505; Training Loss 0.00473511205361824\n",
      "Episode 5397; Testing Loss 0.005848538855570187; Training Loss 0.004735101522502803\n",
      "Episode 5398; Testing Loss 0.005848532926487741; Training Loss 0.004735090349068401\n",
      "Episode 5399; Testing Loss 0.005848566479967729; Training Loss 0.004735079120449858\n",
      "Episode 5400; Testing Loss 0.005848586860519321; Training Loss 0.004735071129222255\n",
      "Episode 5401; Testing Loss 0.005848581146862237; Training Loss 0.004735059683339854\n",
      "Episode 5402; Testing Loss 0.005848533634294297; Training Loss 0.004735047714793394\n",
      "Episode 5403; Testing Loss 0.005848467494101974; Training Loss 0.004735040562398872\n",
      "Episode 5404; Testing Loss 0.005848407529143729; Training Loss 0.00473502693003558\n",
      "Episode 5405; Testing Loss 0.005848386103896284; Training Loss 0.00473501725259926\n",
      "Episode 5406; Testing Loss 0.005848429220945198; Training Loss 0.004735008367622409\n",
      "Episode 5407; Testing Loss 0.005848485395293587; Training Loss 0.004734997161607275\n",
      "Episode 5408; Testing Loss 0.005848453748568657; Training Loss 0.004734987516075361\n",
      "Episode 5409; Testing Loss 0.00584838699969566; Training Loss 0.004734977595820635\n",
      "Episode 5410; Testing Loss 0.005848418205846588; Training Loss 0.004734966605728358\n",
      "Episode 5411; Testing Loss 0.005848433941051449; Training Loss 0.0047349577721302345\n",
      "Episode 5412; Testing Loss 0.00584826968107512; Training Loss 0.0047349468844622225\n",
      "Episode 5413; Testing Loss 0.005848256008149706; Training Loss 0.004734938263757749\n",
      "Episode 5414; Testing Loss 0.005848339614163855; Training Loss 0.0047349255447849585\n",
      "Episode 5415; Testing Loss 0.005848402971306403; Training Loss 0.004734920333454178\n",
      "Episode 5416; Testing Loss 0.0058482878751929404; Training Loss 0.004734905880249221\n",
      "Episode 5417; Testing Loss 0.005848199649086941; Training Loss 0.004734898380472702\n",
      "Episode 5418; Testing Loss 0.0058483490568336976; Training Loss 0.004734890188973953\n",
      "Episode 5419; Testing Loss 0.005848386832301978; Training Loss 0.004734879664755687\n",
      "Episode 5420; Testing Loss 0.005848269149408934; Training Loss 0.00473486844604807\n",
      "Episode 5421; Testing Loss 0.005848170877239637; Training Loss 0.004734855548295106\n",
      "Episode 5422; Testing Loss 0.005848214706040274; Training Loss 0.004734846017398863\n",
      "Episode 5423; Testing Loss 0.005848237558895647; Training Loss 0.0047348388905263765\n",
      "Episode 5424; Testing Loss 0.005848209712149616; Training Loss 0.004734826668146094\n",
      "Episode 5425; Testing Loss 0.005848174261004304; Training Loss 0.004734816463511036\n",
      "Episode 5426; Testing Loss 0.005848242315543826; Training Loss 0.004734806945767407\n",
      "Episode 5427; Testing Loss 0.005848267135989069; Training Loss 0.004734802689017994\n",
      "Episode 5428; Testing Loss 0.0058481411679746605; Training Loss 0.004734787854665379\n",
      "Episode 5429; Testing Loss 0.005848069631222073; Training Loss 0.0047347809647383865\n",
      "Episode 5430; Testing Loss 0.005848186626557736; Training Loss 0.004734770071647448\n",
      "Episode 5431; Testing Loss 0.005848239025089981; Training Loss 0.004734761779293345\n",
      "Episode 5432; Testing Loss 0.005848143311575856; Training Loss 0.004734749788494019\n",
      "Episode 5433; Testing Loss 0.005848051106332811; Training Loss 0.004734735951891803\n",
      "Episode 5434; Testing Loss 0.005848067763913526; Training Loss 0.0047347297133862355\n",
      "Episode 5435; Testing Loss 0.005848040383098994; Training Loss 0.004734721778860077\n",
      "Episode 5436; Testing Loss 0.005847991198718428; Training Loss 0.004734706239154351\n",
      "Episode 5437; Testing Loss 0.005847985154453112; Training Loss 0.004734697632842126\n",
      "Episode 5438; Testing Loss 0.005848056425191509; Training Loss 0.0047346915494408\n",
      "Episode 5439; Testing Loss 0.005848038452896595; Training Loss 0.004734682501561196\n",
      "Episode 5440; Testing Loss 0.005847965866094679; Training Loss 0.004734671265576385\n",
      "Episode 5441; Testing Loss 0.0058479931954957005; Training Loss 0.004734655803779004\n",
      "Episode 5442; Testing Loss 0.005848025248668987; Training Loss 0.004734645487568253\n",
      "Episode 5443; Testing Loss 0.005847948228373466; Training Loss 0.004734636916569032\n",
      "Episode 5444; Testing Loss 0.005847861826336557; Training Loss 0.004734622632405896\n",
      "Episode 5445; Testing Loss 0.005847896661993398; Training Loss 0.004734614996021001\n",
      "Episode 5446; Testing Loss 0.005847957370447948; Training Loss 0.004734606703244515\n",
      "Episode 5447; Testing Loss 0.005847908630704879; Training Loss 0.00473459270317078\n",
      "Episode 5448; Testing Loss 0.005847930686205504; Training Loss 0.0047345842135128515\n",
      "Episode 5449; Testing Loss 0.005847931499312763; Training Loss 0.004734572614605963\n",
      "Episode 5450; Testing Loss 0.0058478611075916285; Training Loss 0.004734565179128165\n",
      "Episode 5451; Testing Loss 0.005847775252313733; Training Loss 0.004734556701200943\n",
      "Episode 5452; Testing Loss 0.005847848625035259; Training Loss 0.00473454675973022\n",
      "Episode 5453; Testing Loss 0.005847991933307537; Training Loss 0.004734534721337216\n",
      "Episode 5454; Testing Loss 0.005847941657892555; Training Loss 0.00473452502364606\n",
      "Episode 5455; Testing Loss 0.0058478052387728905; Training Loss 0.004734514585427844\n",
      "Episode 5456; Testing Loss 0.005847810347813633; Training Loss 0.004734506960167099\n",
      "Episode 5457; Testing Loss 0.005847932828325027; Training Loss 0.004734496155769039\n",
      "Episode 5458; Testing Loss 0.005847945598973168; Training Loss 0.00473448738681861\n",
      "Episode 5459; Testing Loss 0.005847838695641175; Training Loss 0.004734475362765326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5460; Testing Loss 0.0058477338832282465; Training Loss 0.004734465513559198\n",
      "Episode 5461; Testing Loss 0.005847714226570025; Training Loss 0.004734454207287864\n",
      "Episode 5462; Testing Loss 0.005847782848655333; Training Loss 0.004734442790852327\n",
      "Episode 5463; Testing Loss 0.0058478040199698; Training Loss 0.004734436433177746\n",
      "Episode 5464; Testing Loss 0.005847739460652113; Training Loss 0.004734422440210591\n",
      "Episode 5465; Testing Loss 0.0058477152505152915; Training Loss 0.004734412539238788\n",
      "Episode 5466; Testing Loss 0.005847728380230298; Training Loss 0.004734407215918997\n",
      "Episode 5467; Testing Loss 0.0058476922600626535; Training Loss 0.0047343937421239525\n",
      "Episode 5468; Testing Loss 0.005847657056991053; Training Loss 0.004734383826349857\n",
      "Episode 5469; Testing Loss 0.00584768558298753; Training Loss 0.0047343762894948155\n",
      "Episode 5470; Testing Loss 0.0058477294193209715; Training Loss 0.00473436543439477\n",
      "Episode 5471; Testing Loss 0.005847723595111478; Training Loss 0.004734354208422652\n",
      "Episode 5472; Testing Loss 0.005847741350695035; Training Loss 0.004734342216629941\n",
      "Episode 5473; Testing Loss 0.005847680357005887; Training Loss 0.004734331336843164\n",
      "Episode 5474; Testing Loss 0.005847583975076853; Training Loss 0.004734322427386362\n",
      "Episode 5475; Testing Loss 0.005847620750965138; Training Loss 0.004734312236694778\n",
      "Episode 5476; Testing Loss 0.0058476905270776714; Training Loss 0.004734302167276324\n",
      "Episode 5477; Testing Loss 0.005847674502203317; Training Loss 0.004734291875313004\n",
      "Episode 5478; Testing Loss 0.005847607598752944; Training Loss 0.004734280685972477\n",
      "Episode 5479; Testing Loss 0.00584754282706913; Training Loss 0.004734271419609545\n",
      "Episode 5480; Testing Loss 0.005847629085919911; Training Loss 0.004734261355178377\n",
      "Episode 5481; Testing Loss 0.005847755201063866; Training Loss 0.004734253104939045\n",
      "Episode 5482; Testing Loss 0.00584761826647949; Training Loss 0.004734240565905172\n",
      "Episode 5483; Testing Loss 0.005847446965781023; Training Loss 0.004734231886378233\n",
      "Episode 5484; Testing Loss 0.005847420253190514; Training Loss 0.004734221197087834\n",
      "Episode 5485; Testing Loss 0.005847550504270539; Training Loss 0.00473421237530166\n",
      "Episode 5486; Testing Loss 0.005847708375724835; Training Loss 0.004734205451337457\n",
      "Episode 5487; Testing Loss 0.005847633088622821; Training Loss 0.004734192888008813\n",
      "Episode 5488; Testing Loss 0.005847497822798067; Training Loss 0.0047341829182484686\n",
      "Episode 5489; Testing Loss 0.005847472605656019; Training Loss 0.004734173531046292\n",
      "Episode 5490; Testing Loss 0.005847464190334115; Training Loss 0.004734161766027491\n",
      "Episode 5491; Testing Loss 0.00584744921582442; Training Loss 0.004734153566052419\n",
      "Episode 5492; Testing Loss 0.0058475133726628836; Training Loss 0.0047341448376706854\n",
      "Episode 5493; Testing Loss 0.005847553986789741; Training Loss 0.004734135125030567\n",
      "Episode 5494; Testing Loss 0.005847462374137984; Training Loss 0.004734122512495925\n",
      "Episode 5495; Testing Loss 0.005847437182002756; Training Loss 0.004734111325904941\n",
      "Episode 5496; Testing Loss 0.005847535277950375; Training Loss 0.00473410525868258\n",
      "Episode 5497; Testing Loss 0.005847439101688155; Training Loss 0.004734093701285106\n",
      "Episode 5498; Testing Loss 0.005847311463831634; Training Loss 0.004734082292343492\n",
      "Episode 5499; Testing Loss 0.005847387951530417; Training Loss 0.004734072695053692\n",
      "Episode 5500; Testing Loss 0.005847450598486089; Training Loss 0.004734062335183176\n",
      "Episode 5501; Testing Loss 0.005847411821559725; Training Loss 0.004734051550832354\n",
      "Episode 5502; Testing Loss 0.005847359043699867; Training Loss 0.004734041319080142\n",
      "Episode 5503; Testing Loss 0.005847313108584744; Training Loss 0.004734034532708084\n",
      "Episode 5504; Testing Loss 0.0058472849165163545; Training Loss 0.004734021386641064\n",
      "Episode 5505; Testing Loss 0.005847342717434242; Training Loss 0.0047340106916149835\n",
      "Episode 5506; Testing Loss 0.005847388560175928; Training Loss 0.004734001987680986\n",
      "Episode 5507; Testing Loss 0.005847304560106701; Training Loss 0.004733991441241382\n",
      "Episode 5508; Testing Loss 0.005847221963340188; Training Loss 0.004733980809802081\n",
      "Episode 5509; Testing Loss 0.0058472505219154215; Training Loss 0.004733971131324561\n",
      "Episode 5510; Testing Loss 0.0058473471884955466; Training Loss 0.004733962661135523\n",
      "Episode 5511; Testing Loss 0.005847332356591341; Training Loss 0.00473395029580271\n",
      "Episode 5512; Testing Loss 0.005847305975841127; Training Loss 0.004733940320075774\n",
      "Episode 5513; Testing Loss 0.005847289929879359; Training Loss 0.004733931353295782\n",
      "Episode 5514; Testing Loss 0.005847332874957827; Training Loss 0.004733924081535753\n",
      "Episode 5515; Testing Loss 0.005847340137481413; Training Loss 0.004733912466577537\n",
      "Episode 5516; Testing Loss 0.005847268725754749; Training Loss 0.004733898780644469\n",
      "Episode 5517; Testing Loss 0.005847173636606326; Training Loss 0.004733889547844367\n",
      "Episode 5518; Testing Loss 0.0058471940988917715; Training Loss 0.004733881265667238\n",
      "Episode 5519; Testing Loss 0.005847285434550087; Training Loss 0.00473387025791501\n",
      "Episode 5520; Testing Loss 0.005847272036310478; Training Loss 0.004733860534555543\n",
      "Episode 5521; Testing Loss 0.00584709696033; Training Loss 0.004733850156365966\n",
      "Episode 5522; Testing Loss 0.005847079204021052; Training Loss 0.00473384132117677\n",
      "Episode 5523; Testing Loss 0.005847235537386338; Training Loss 0.004733828964332926\n",
      "Episode 5524; Testing Loss 0.005847285810415112; Training Loss 0.0047338217671011006\n",
      "Episode 5525; Testing Loss 0.005847114992246175; Training Loss 0.004733808920554951\n",
      "Episode 5526; Testing Loss 0.005847018471593998; Training Loss 0.004733802428650425\n",
      "Episode 5527; Testing Loss 0.005847182878814014; Training Loss 0.004733790894488068\n",
      "Episode 5528; Testing Loss 0.005847297263875413; Training Loss 0.0047337808026519\n",
      "Episode 5529; Testing Loss 0.005847179490807106; Training Loss 0.004733768242948432\n",
      "Episode 5530; Testing Loss 0.005847072470636775; Training Loss 0.004733762039786796\n",
      "Episode 5531; Testing Loss 0.005847049822130137; Training Loss 0.004733751312949026\n",
      "Episode 5532; Testing Loss 0.005847091665976898; Training Loss 0.00473373541089129\n",
      "Episode 5533; Testing Loss 0.005847081979091008; Training Loss 0.004733728384691613\n",
      "Episode 5534; Testing Loss 0.005847056997020629; Training Loss 0.004733717799497712\n",
      "Episode 5535; Testing Loss 0.005847064462906441; Training Loss 0.004733704638203862\n",
      "Episode 5536; Testing Loss 0.0058471458418340545; Training Loss 0.004733697110596563\n",
      "Episode 5537; Testing Loss 0.005847057615625182; Training Loss 0.004733686250200604\n",
      "Episode 5538; Testing Loss 0.005846958148475629; Training Loss 0.004733677056710202\n",
      "Episode 5539; Testing Loss 0.005847040557359162; Training Loss 0.004733668990226136\n",
      "Episode 5540; Testing Loss 0.0058470996288969256; Training Loss 0.004733655922204595\n",
      "Episode 5541; Testing Loss 0.005847072019679457; Training Loss 0.004733644324368096\n",
      "Episode 5542; Testing Loss 0.0058470359895500725; Training Loss 0.004733637870698425\n",
      "Episode 5543; Testing Loss 0.005847024637498988; Training Loss 0.004733626336392067\n",
      "Episode 5544; Testing Loss 0.0058470739360742575; Training Loss 0.004733615207155362\n",
      "Episode 5545; Testing Loss 0.005847064303276547; Training Loss 0.004733606577870335\n",
      "Episode 5546; Testing Loss 0.005847077432409397; Training Loss 0.004733595709627062\n",
      "Episode 5547; Testing Loss 0.00584701230011721; Training Loss 0.004733583176226699\n",
      "Episode 5548; Testing Loss 0.005846944045385549; Training Loss 0.004733572998440372\n",
      "Episode 5549; Testing Loss 0.005846913559541456; Training Loss 0.004733562568970149\n",
      "Episode 5550; Testing Loss 0.005847010791345665; Training Loss 0.004733552416084032\n",
      "Episode 5551; Testing Loss 0.00584707782835969; Training Loss 0.004733543450105097\n",
      "Episode 5552; Testing Loss 0.0058469947725303655; Training Loss 0.004733532230343318\n",
      "Episode 5553; Testing Loss 0.005846889788135522; Training Loss 0.004733522150711513\n",
      "Episode 5554; Testing Loss 0.005846868353502782; Training Loss 0.004733511612139377\n",
      "Episode 5555; Testing Loss 0.005846971672150328; Training Loss 0.004733502325677777\n",
      "Episode 5556; Testing Loss 0.005847040557491571; Training Loss 0.004733491864503757\n",
      "Episode 5557; Testing Loss 0.005846932829610143; Training Loss 0.004733481577245983\n",
      "Episode 5558; Testing Loss 0.005846820147057161; Training Loss 0.004733471147355938\n",
      "Episode 5559; Testing Loss 0.005846840061891887; Training Loss 0.004733462213642923\n",
      "Episode 5560; Testing Loss 0.005847037656127448; Training Loss 0.004733452326313948\n",
      "Episode 5561; Testing Loss 0.005847027310898547; Training Loss 0.004733442086334157\n",
      "Episode 5562; Testing Loss 0.005846791814284127; Training Loss 0.004733432847285784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5563; Testing Loss 0.005846701902741778; Training Loss 0.004733425547346186\n",
      "Episode 5564; Testing Loss 0.0058469351798066905; Training Loss 0.004733409703100802\n",
      "Episode 5565; Testing Loss 0.005847028008970798; Training Loss 0.0047334032423513115\n",
      "Episode 5566; Testing Loss 0.005846828096060762; Training Loss 0.0047333891636703665\n",
      "Episode 5567; Testing Loss 0.005846736808409572; Training Loss 0.004733381397653416\n",
      "Episode 5568; Testing Loss 0.0058468306588444615; Training Loss 0.0047333686881459594\n",
      "Episode 5569; Testing Loss 0.005846967533504023; Training Loss 0.004733361560993797\n",
      "Episode 5570; Testing Loss 0.005846939251492557; Training Loss 0.004733352299531253\n",
      "Episode 5571; Testing Loss 0.005846843641993988; Training Loss 0.004733341762582951\n",
      "Episode 5572; Testing Loss 0.005846844781030517; Training Loss 0.0047333289083312195\n",
      "Episode 5573; Testing Loss 0.005846883633273832; Training Loss 0.004733322444544438\n",
      "Episode 5574; Testing Loss 0.0058467769251280225; Training Loss 0.004733310042763307\n",
      "Episode 5575; Testing Loss 0.005846745820329619; Training Loss 0.004733301097144766\n",
      "Episode 5576; Testing Loss 0.005846889152256676; Training Loss 0.004733291404876135\n",
      "Episode 5577; Testing Loss 0.005846893849211432; Training Loss 0.004733280126659865\n",
      "Episode 5578; Testing Loss 0.0058467569625117585; Training Loss 0.004733267457413271\n",
      "Episode 5579; Testing Loss 0.0058466839830351855; Training Loss 0.004733261409419984\n",
      "Episode 5580; Testing Loss 0.005846706688907618; Training Loss 0.004733247974477235\n",
      "Episode 5581; Testing Loss 0.005846811461415252; Training Loss 0.004733241032207328\n",
      "Episode 5582; Testing Loss 0.005846828513220102; Training Loss 0.004733233352069369\n",
      "Episode 5583; Testing Loss 0.005846757317219628; Training Loss 0.00473322266255341\n",
      "Episode 5584; Testing Loss 0.005846771948559638; Training Loss 0.004733209576688034\n",
      "Episode 5585; Testing Loss 0.005846780061606055; Training Loss 0.0047331990546481476\n",
      "Episode 5586; Testing Loss 0.005846653996360631; Training Loss 0.004733189443286793\n",
      "Episode 5587; Testing Loss 0.005846615304661967; Training Loss 0.004733179081353156\n",
      "Episode 5588; Testing Loss 0.005846732396804187; Training Loss 0.00473316877617913\n",
      "Episode 5589; Testing Loss 0.005846760579844867; Training Loss 0.004733157482382981\n",
      "Episode 5590; Testing Loss 0.0058466246214499955; Training Loss 0.004733148581262219\n",
      "Episode 5591; Testing Loss 0.005846641243552452; Training Loss 0.0047331404452584545\n",
      "Episode 5592; Testing Loss 0.005846734239400794; Training Loss 0.004733128623268734\n",
      "Episode 5593; Testing Loss 0.005846756646230401; Training Loss 0.004733119921339112\n",
      "Episode 5594; Testing Loss 0.00584660125977097; Training Loss 0.004733109604372892\n",
      "Episode 5595; Testing Loss 0.005846509593709138; Training Loss 0.004733097702912561\n",
      "Episode 5596; Testing Loss 0.005846616554449077; Training Loss 0.004733090189917249\n",
      "Episode 5597; Testing Loss 0.005846723777269126; Training Loss 0.004733080691845272\n",
      "Episode 5598; Testing Loss 0.00584667918599393; Training Loss 0.004733066679037898\n",
      "Episode 5599; Testing Loss 0.00584657286205435; Training Loss 0.004733059143030542\n",
      "Episode 5600; Testing Loss 0.005846641582643411; Training Loss 0.004733048342079888\n",
      "Episode 5601; Testing Loss 0.005846718759455161; Training Loss 0.004733037277895494\n",
      "Episode 5602; Testing Loss 0.005846613454888363; Training Loss 0.004733026484829985\n",
      "Episode 5603; Testing Loss 0.005846427564116455; Training Loss 0.004733019351395842\n",
      "Episode 5604; Testing Loss 0.005846584831034445; Training Loss 0.004733006655098017\n",
      "Episode 5605; Testing Loss 0.0058467707070202785; Training Loss 0.004732998503635363\n",
      "Episode 5606; Testing Loss 0.005846737202459648; Training Loss 0.004732988760889433\n",
      "Episode 5607; Testing Loss 0.005846544628554303; Training Loss 0.004732977239410956\n",
      "Episode 5608; Testing Loss 0.005846474556179555; Training Loss 0.004732964536839914\n",
      "Episode 5609; Testing Loss 0.005846531277014725; Training Loss 0.004732959811602436\n",
      "Episode 5610; Testing Loss 0.005846564855340624; Training Loss 0.004732947633840981\n",
      "Episode 5611; Testing Loss 0.005846483941350412; Training Loss 0.004732935824174162\n",
      "Episode 5612; Testing Loss 0.005846507240696324; Training Loss 0.004732926862775832\n",
      "Episode 5613; Testing Loss 0.00584656865846632; Training Loss 0.004732915671025378\n",
      "Episode 5614; Testing Loss 0.005846570606563373; Training Loss 0.0047329042385079135\n",
      "Episode 5615; Testing Loss 0.005846579556590468; Training Loss 0.004732895603734862\n",
      "Episode 5616; Testing Loss 0.005846560082806833; Training Loss 0.00473288448327835\n",
      "Episode 5617; Testing Loss 0.005846483934956553; Training Loss 0.004732873263277893\n",
      "Episode 5618; Testing Loss 0.0058465335258516705; Training Loss 0.004732862736216356\n",
      "Episode 5619; Testing Loss 0.005846550335415025; Training Loss 0.004732855253198849\n",
      "Episode 5620; Testing Loss 0.005846457855099736; Training Loss 0.004732843291190832\n",
      "Episode 5621; Testing Loss 0.00584647717815825; Training Loss 0.0047328339155680455\n",
      "Episode 5622; Testing Loss 0.005846563579753164; Training Loss 0.004732823039677987\n",
      "Episode 5623; Testing Loss 0.005846508434036701; Training Loss 0.004732814797948478\n",
      "Episode 5624; Testing Loss 0.005846429503742639; Training Loss 0.004732804088022925\n",
      "Episode 5625; Testing Loss 0.005846431213011128; Training Loss 0.004732793323911688\n",
      "Episode 5626; Testing Loss 0.005846471855471521; Training Loss 0.004732786369697406\n",
      "Episode 5627; Testing Loss 0.005846420883876718; Training Loss 0.004732772027785867\n",
      "Episode 5628; Testing Loss 0.005846350915499733; Training Loss 0.004732768131526537\n",
      "Episode 5629; Testing Loss 0.005846372476592135; Training Loss 0.0047327581478449334\n",
      "Episode 5630; Testing Loss 0.0058464608263968125; Training Loss 0.004732748521066075\n",
      "Episode 5631; Testing Loss 0.005846605847610517; Training Loss 0.004732738538449789\n",
      "Episode 5632; Testing Loss 0.005846460082139255; Training Loss 0.004732726934353665\n",
      "Episode 5633; Testing Loss 0.005846257968012377; Training Loss 0.004732716514634079\n",
      "Episode 5634; Testing Loss 0.0058462677612978915; Training Loss 0.004732703629740842\n",
      "Episode 5635; Testing Loss 0.005846424836327122; Training Loss 0.004732696429766839\n",
      "Episode 5636; Testing Loss 0.005846415966122498; Training Loss 0.004732682901671332\n",
      "Episode 5637; Testing Loss 0.005846298785774423; Training Loss 0.00473267564665708\n",
      "Episode 5638; Testing Loss 0.005846273028274592; Training Loss 0.004732665285278027\n",
      "Episode 5639; Testing Loss 0.005846384093322775; Training Loss 0.004732654020733296\n",
      "Episode 5640; Testing Loss 0.005846454003704876; Training Loss 0.004732646252270002\n",
      "Episode 5641; Testing Loss 0.005846312707021437; Training Loss 0.004732634663283633\n",
      "Episode 5642; Testing Loss 0.005846215918728281; Training Loss 0.004732623850689733\n",
      "Episode 5643; Testing Loss 0.005846286773883675; Training Loss 0.004732613524100382\n",
      "Episode 5644; Testing Loss 0.0058463288369418704; Training Loss 0.004732601901774337\n",
      "Episode 5645; Testing Loss 0.005846322698915065; Training Loss 0.004732592439847834\n",
      "Episode 5646; Testing Loss 0.0058462740490790996; Training Loss 0.0047325814300673615\n",
      "Episode 5647; Testing Loss 0.00584623149158456; Training Loss 0.004732571645330069\n",
      "Episode 5648; Testing Loss 0.005846253063929707; Training Loss 0.004732560866467261\n",
      "Episode 5649; Testing Loss 0.005846311681731084; Training Loss 0.004732553038298253\n",
      "Episode 5650; Testing Loss 0.005846224140638998; Training Loss 0.004732540596952902\n",
      "Episode 5651; Testing Loss 0.005846201845053038; Training Loss 0.004732532587440838\n",
      "Episode 5652; Testing Loss 0.00584632911497734; Training Loss 0.00473252168789845\n",
      "Episode 5653; Testing Loss 0.005846312880349034; Training Loss 0.004732514641018298\n",
      "Episode 5654; Testing Loss 0.005846085379961128; Training Loss 0.004732502910075543\n",
      "Episode 5655; Testing Loss 0.005846115801886179; Training Loss 0.004732492727138297\n",
      "Episode 5656; Testing Loss 0.005846336139549763; Training Loss 0.004732482943697716\n",
      "Episode 5657; Testing Loss 0.005846315824717162; Training Loss 0.004732473635944785\n",
      "Episode 5658; Testing Loss 0.0058460853744007986; Training Loss 0.004732464528877674\n",
      "Episode 5659; Testing Loss 0.005846075228754159; Training Loss 0.004732453830261376\n",
      "Episode 5660; Testing Loss 0.005846225641582383; Training Loss 0.0047324445834008395\n",
      "Episode 5661; Testing Loss 0.005846219490581126; Training Loss 0.004732432648211862\n",
      "Episode 5662; Testing Loss 0.00584608638186634; Training Loss 0.004732423107367824\n",
      "Episode 5663; Testing Loss 0.005846050874913684; Training Loss 0.004732412058844712\n",
      "Episode 5664; Testing Loss 0.005846137283408105; Training Loss 0.00473240214094707\n",
      "Episode 5665; Testing Loss 0.005846218531553807; Training Loss 0.004732391503259151\n",
      "Episode 5666; Testing Loss 0.005846205899161479; Training Loss 0.004732381665021934\n",
      "Episode 5667; Testing Loss 0.005846139828319952; Training Loss 0.0047323707471484955\n",
      "Episode 5668; Testing Loss 0.005846029615108128; Training Loss 0.004732361173309329\n",
      "Episode 5669; Testing Loss 0.00584603236499357; Training Loss 0.0047323515032990085\n",
      "Episode 5670; Testing Loss 0.00584611660179669; Training Loss 0.004732340896728522\n",
      "Episode 5671; Testing Loss 0.005846094718410254; Training Loss 0.004732330621156948\n",
      "Episode 5672; Testing Loss 0.005846038533701202; Training Loss 0.004732322671649161\n",
      "Episode 5673; Testing Loss 0.005846086966805905; Training Loss 0.004732312241696516\n",
      "Episode 5674; Testing Loss 0.005846134544137464; Training Loss 0.0047323014817261125\n",
      "Episode 5675; Testing Loss 0.00584612929631145; Training Loss 0.004732293580325626\n",
      "Episode 5676; Testing Loss 0.0058460313063321695; Training Loss 0.004732281537203456\n",
      "Episode 5677; Testing Loss 0.00584600244209162; Training Loss 0.004732274871623835\n",
      "Episode 5678; Testing Loss 0.005846019262086212; Training Loss 0.004732265919516882\n",
      "Episode 5679; Testing Loss 0.0058461223994221625; Training Loss 0.00473225444504853\n",
      "Episode 5680; Testing Loss 0.005846116209140912; Training Loss 0.004732241325370778\n",
      "Episode 5681; Testing Loss 0.00584597005866367; Training Loss 0.00473223433037893\n",
      "Episode 5682; Testing Loss 0.005845921878150875; Training Loss 0.004732224205452069\n",
      "Episode 5683; Testing Loss 0.005845926080154827; Training Loss 0.004732214420631047\n",
      "Episode 5684; Testing Loss 0.005846051212720522; Training Loss 0.004732206837250028\n",
      "Episode 5685; Testing Loss 0.005846058126327339; Training Loss 0.004732197585804303\n",
      "Episode 5686; Testing Loss 0.005845867255431485; Training Loss 0.004732183230300761\n",
      "Episode 5687; Testing Loss 0.005845879126697611; Training Loss 0.004732174916320982\n",
      "Episode 5688; Testing Loss 0.005846022200219829; Training Loss 0.004732166408287012\n",
      "Episode 5689; Testing Loss 0.005846064698208829; Training Loss 0.0047321530875585145\n",
      "Episode 5690; Testing Loss 0.005845990739584507; Training Loss 0.004732144659721425\n",
      "Episode 5691; Testing Loss 0.005845912038484133; Training Loss 0.004732136000366952\n",
      "Episode 5692; Testing Loss 0.005845939628813261; Training Loss 0.004732123784310432\n",
      "Episode 5693; Testing Loss 0.00584599462630563; Training Loss 0.0047321133099899396\n",
      "Episode 5694; Testing Loss 0.005845893880015205; Training Loss 0.004732102773407919\n",
      "Episode 5695; Testing Loss 0.005845792318431596; Training Loss 0.004732094835964158\n",
      "Episode 5696; Testing Loss 0.005845911808646991; Training Loss 0.004732083673535852\n",
      "Episode 5697; Testing Loss 0.005846051781587339; Training Loss 0.004732074879501685\n",
      "Episode 5698; Testing Loss 0.005845907351880608; Training Loss 0.004732063228046038\n",
      "Episode 5699; Testing Loss 0.00584569529817554; Training Loss 0.004732055373625549\n",
      "Episode 5700; Testing Loss 0.0058457159315293975; Training Loss 0.00473204462963563\n",
      "Episode 5701; Testing Loss 0.005845939847872997; Training Loss 0.004732034440739535\n",
      "Episode 5702; Testing Loss 0.005846061698755173; Training Loss 0.0047320259745608035\n",
      "Episode 5703; Testing Loss 0.0058458727875698386; Training Loss 0.004732012872549357\n",
      "Episode 5704; Testing Loss 0.00584571728072654; Training Loss 0.0047320068296612125\n",
      "Episode 5705; Testing Loss 0.005845732832254048; Training Loss 0.004731991873259859\n",
      "Episode 5706; Testing Loss 0.00584588356092604; Training Loss 0.00473198500689404\n",
      "Episode 5707; Testing Loss 0.005845794685629965; Training Loss 0.004731974161404062\n",
      "Episode 5708; Testing Loss 0.005845711795095657; Training Loss 0.004731964762687527\n",
      "Episode 5709; Testing Loss 0.005845798609672997; Training Loss 0.00473195152845789\n",
      "Episode 5710; Testing Loss 0.005845818130045003; Training Loss 0.004731945948972117\n",
      "Episode 5711; Testing Loss 0.005845819296198454; Training Loss 0.004731933236804546\n",
      "Episode 5712; Testing Loss 0.005845812743994487; Training Loss 0.004731923820535926\n",
      "Episode 5713; Testing Loss 0.005845875999473727; Training Loss 0.004731919600005954\n",
      "Episode 5714; Testing Loss 0.0058458518457282565; Training Loss 0.004731907615746363\n",
      "Episode 5715; Testing Loss 0.005845777523296382; Training Loss 0.004731896715030512\n",
      "Episode 5716; Testing Loss 0.0058458580050618995; Training Loss 0.00473188550316738\n",
      "Episode 5717; Testing Loss 0.005845852736896309; Training Loss 0.004731876494207859\n",
      "Episode 5718; Testing Loss 0.005845729629925607; Training Loss 0.004731866277939235\n",
      "Episode 5719; Testing Loss 0.00584570161197996; Training Loss 0.00473185508830234\n",
      "Episode 5720; Testing Loss 0.005845764523704259; Training Loss 0.004731847561185192\n",
      "Episode 5721; Testing Loss 0.005845714538433941; Training Loss 0.0047318373403964714\n",
      "Episode 5722; Testing Loss 0.005845589448857649; Training Loss 0.004731823526782463\n",
      "Episode 5723; Testing Loss 0.005845640145668383; Training Loss 0.0047318171164362\n",
      "Episode 5724; Testing Loss 0.0058457397577942275; Training Loss 0.004731809391906969\n",
      "Episode 5725; Testing Loss 0.0058456483199819395; Training Loss 0.004731797475556791\n",
      "Episode 5726; Testing Loss 0.0058455260155756755; Training Loss 0.004731786453436648\n",
      "Episode 5727; Testing Loss 0.005845545949952447; Training Loss 0.004731774729355542\n",
      "Episode 5728; Testing Loss 0.005845635504783718; Training Loss 0.004731765788602599\n",
      "Episode 5729; Testing Loss 0.005845632706484347; Training Loss 0.004731755486519467\n",
      "Episode 5730; Testing Loss 0.005845590985451807; Training Loss 0.004731745808073211\n",
      "Episode 5731; Testing Loss 0.005845609759478718; Training Loss 0.004731735126885875\n",
      "Episode 5732; Testing Loss 0.005845635198588761; Training Loss 0.0047317237343464285\n",
      "Episode 5733; Testing Loss 0.005845652545245749; Training Loss 0.004731717625428236\n",
      "Episode 5734; Testing Loss 0.005845633517339091; Training Loss 0.004731706684063484\n",
      "Episode 5735; Testing Loss 0.005845583924253213; Training Loss 0.004731698249248797\n",
      "Episode 5736; Testing Loss 0.00584552636833098; Training Loss 0.004731685590592559\n",
      "Episode 5737; Testing Loss 0.00584552459987465; Training Loss 0.004731676050771467\n",
      "Episode 5738; Testing Loss 0.005845599555220386; Training Loss 0.004731670841631518\n",
      "Episode 5739; Testing Loss 0.005845570599901213; Training Loss 0.0047316566582567944\n",
      "Episode 5740; Testing Loss 0.005845466543486457; Training Loss 0.004731644788673153\n",
      "Episode 5741; Testing Loss 0.005845502773532089; Training Loss 0.004731640154093866\n",
      "Episode 5742; Testing Loss 0.005845581173285642; Training Loss 0.004731628737107264\n",
      "Episode 5743; Testing Loss 0.005845531307360896; Training Loss 0.00473161908494008\n",
      "Episode 5744; Testing Loss 0.005845501078069352; Training Loss 0.0047316089592494125\n",
      "Episode 5745; Testing Loss 0.005845534360383482; Training Loss 0.004731600485866923\n",
      "Episode 5746; Testing Loss 0.005845548571485204; Training Loss 0.004731592193540773\n",
      "Episode 5747; Testing Loss 0.0058454556288703; Training Loss 0.004731579630485485\n",
      "Episode 5748; Testing Loss 0.005845349859872855; Training Loss 0.00473156740139359\n",
      "Episode 5749; Testing Loss 0.0058453836617863835; Training Loss 0.004731560231109934\n",
      "Episode 5750; Testing Loss 0.005845566297981184; Training Loss 0.004731554259660681\n",
      "Episode 5751; Testing Loss 0.00584555753214616; Training Loss 0.0047315423920366246\n",
      "Episode 5752; Testing Loss 0.005845384172876285; Training Loss 0.004731529842635243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5753; Testing Loss 0.005845333599116301; Training Loss 0.004731518153957966\n",
      "Episode 5754; Testing Loss 0.005845466207604451; Training Loss 0.004731507101887752\n",
      "Episode 5755; Testing Loss 0.005845581905608203; Training Loss 0.0047315008853078735\n",
      "Episode 5756; Testing Loss 0.005845496236889048; Training Loss 0.0047314901421431155\n",
      "Episode 5757; Testing Loss 0.005845287563987757; Training Loss 0.004731477885891651\n",
      "Episode 5758; Testing Loss 0.005845273068431433; Training Loss 0.0047314684082198\n",
      "Episode 5759; Testing Loss 0.005845446859754893; Training Loss 0.004731457137969091\n",
      "Episode 5760; Testing Loss 0.005845551271516012; Training Loss 0.00473144782910724\n",
      "Episode 5761; Testing Loss 0.005845446060887057; Training Loss 0.004731436938089155\n",
      "Episode 5762; Testing Loss 0.005845307989707667; Training Loss 0.004731429137155827\n",
      "Episode 5763; Testing Loss 0.00584538538351037; Training Loss 0.0047314183170237304\n",
      "Episode 5764; Testing Loss 0.00584549791871926; Training Loss 0.0047314066839273805\n",
      "Episode 5765; Testing Loss 0.005845427198868477; Training Loss 0.00473139675244053\n",
      "Episode 5766; Testing Loss 0.005845305077904657; Training Loss 0.0047313868976335905\n",
      "Episode 5767; Testing Loss 0.005845298469731548; Training Loss 0.004731376278009079\n",
      "Episode 5768; Testing Loss 0.005845396792093016; Training Loss 0.004731367018734368\n",
      "Episode 5769; Testing Loss 0.005845361821031647; Training Loss 0.004731357733810088\n",
      "Episode 5770; Testing Loss 0.005845240711310223; Training Loss 0.0047313486129837035\n",
      "Episode 5771; Testing Loss 0.005845267044131107; Training Loss 0.004731339346056061\n",
      "Episode 5772; Testing Loss 0.0058453939471995; Training Loss 0.004731328031406195\n",
      "Episode 5773; Testing Loss 0.005845369282058099; Training Loss 0.004731315172555429\n",
      "Episode 5774; Testing Loss 0.005845219630996831; Training Loss 0.004731310297869505\n",
      "Episode 5775; Testing Loss 0.005845212056772848; Training Loss 0.004731299902922199\n",
      "Episode 5776; Testing Loss 0.005845372471810416; Training Loss 0.004731289067951684\n",
      "Episode 5777; Testing Loss 0.00584545212735405; Training Loss 0.004731278083366972\n",
      "Episode 5778; Testing Loss 0.005845324983877212; Training Loss 0.004731268063046393\n",
      "Episode 5779; Testing Loss 0.005845179349684643; Training Loss 0.004731260369874489\n",
      "Episode 5780; Testing Loss 0.005845164797327454; Training Loss 0.004731246085177762\n",
      "Episode 5781; Testing Loss 0.005845269493226767; Training Loss 0.004731237076565462\n",
      "Episode 5782; Testing Loss 0.005845261637215266; Training Loss 0.004731229323325441\n",
      "Episode 5783; Testing Loss 0.005845106102686592; Training Loss 0.004731218018618597\n",
      "Episode 5784; Testing Loss 0.005845053018447729; Training Loss 0.00473120740261158\n",
      "Episode 5785; Testing Loss 0.005845162457229728; Training Loss 0.00473119926589941\n",
      "Episode 5786; Testing Loss 0.005845184748376791; Training Loss 0.004731186020258995\n",
      "Episode 5787; Testing Loss 0.005845196977799591; Training Loss 0.004731177159860841\n",
      "Episode 5788; Testing Loss 0.00584524937490462; Training Loss 0.004731167389948853\n",
      "Episode 5789; Testing Loss 0.00584529005973127; Training Loss 0.0047311561311552085\n",
      "Episode 5790; Testing Loss 0.005845246825873484; Training Loss 0.004731148690933476\n",
      "Episode 5791; Testing Loss 0.005845235779258314; Training Loss 0.004731140645985289\n",
      "Episode 5792; Testing Loss 0.005845232941231563; Training Loss 0.004731128614578794\n",
      "Episode 5793; Testing Loss 0.005845176235070911; Training Loss 0.0047311147372875615\n",
      "Episode 5794; Testing Loss 0.005845062771305446; Training Loss 0.004731110449530557\n",
      "Episode 5795; Testing Loss 0.00584502212697679; Training Loss 0.004731099337623558\n",
      "Episode 5796; Testing Loss 0.005845128434267611; Training Loss 0.0047310883598793855\n",
      "Episode 5797; Testing Loss 0.005845203833742513; Training Loss 0.004731084635211741\n",
      "Episode 5798; Testing Loss 0.005845075956227668; Training Loss 0.0047310704859114035\n",
      "Episode 5799; Testing Loss 0.005844993468159245; Training Loss 0.004731059217564882\n",
      "Episode 5800; Testing Loss 0.005845170407026414; Training Loss 0.004731056827158802\n",
      "Episode 5801; Testing Loss 0.005845273967972943; Training Loss 0.00473104841362191\n",
      "Episode 5802; Testing Loss 0.005845119273538211; Training Loss 0.004731027800113173\n",
      "Episode 5803; Testing Loss 0.0058449212467929215; Training Loss 0.004731024838996987\n",
      "Episode 5804; Testing Loss 0.005844955349421108; Training Loss 0.00473102083368217\n",
      "Episode 5805; Testing Loss 0.0058450779523481645; Training Loss 0.004731008985712949\n",
      "Episode 5806; Testing Loss 0.0058451150796668344; Training Loss 0.0047309974408729204\n",
      "Episode 5807; Testing Loss 0.005845068799138685; Training Loss 0.004730983035540069\n",
      "Episode 5808; Testing Loss 0.005845059981208265; Training Loss 0.004730968167941619\n",
      "Episode 5809; Testing Loss 0.005845036571259135; Training Loss 0.004730967044471921\n",
      "Episode 5810; Testing Loss 0.0058449093900488835; Training Loss 0.0047309583714627585\n",
      "Episode 5811; Testing Loss 0.005844841156950788; Training Loss 0.004730942575522027\n",
      "Episode 5812; Testing Loss 0.005844911888733463; Training Loss 0.004730932508094917\n",
      "Episode 5813; Testing Loss 0.005845038918856938; Training Loss 0.004730927465708764\n",
      "Episode 5814; Testing Loss 0.005845043455149583; Training Loss 0.004730917825378583\n",
      "Episode 5815; Testing Loss 0.005844925719856603; Training Loss 0.004730905270480808\n",
      "Episode 5816; Testing Loss 0.005844866215787554; Training Loss 0.004730892163944634\n",
      "Episode 5817; Testing Loss 0.005844944656043742; Training Loss 0.004730879457435961\n",
      "Episode 5818; Testing Loss 0.005844925617019031; Training Loss 0.004730871980926429\n",
      "Episode 5819; Testing Loss 0.005844873701953685; Training Loss 0.004730858295003637\n",
      "Episode 5820; Testing Loss 0.005844883125312134; Training Loss 0.004730852252912059\n",
      "Episode 5821; Testing Loss 0.005844907437254575; Training Loss 0.0047308402581041735\n",
      "Episode 5822; Testing Loss 0.0058449413495515416; Training Loss 0.004730830251825344\n",
      "Episode 5823; Testing Loss 0.005844944536487269; Training Loss 0.004730821557099135\n",
      "Episode 5824; Testing Loss 0.005844896410557245; Training Loss 0.004730810225106724\n",
      "Episode 5825; Testing Loss 0.005844899475209961; Training Loss 0.004730803062040629\n",
      "Episode 5826; Testing Loss 0.005844869660577254; Training Loss 0.0047307905725034825\n",
      "Episode 5827; Testing Loss 0.005844769471235288; Training Loss 0.00473078366092773\n",
      "Episode 5828; Testing Loss 0.005844813426765967; Training Loss 0.004730778196293981\n",
      "Episode 5829; Testing Loss 0.005844855487824939; Training Loss 0.004730765861211154\n",
      "Episode 5830; Testing Loss 0.005844807777160509; Training Loss 0.0047307563598455536\n",
      "Episode 5831; Testing Loss 0.005844823712532879; Training Loss 0.004730744929202757\n",
      "Episode 5832; Testing Loss 0.005844865578765136; Training Loss 0.00473074096468931\n",
      "Episode 5833; Testing Loss 0.005844844053488698; Training Loss 0.004730729046557795\n",
      "Episode 5834; Testing Loss 0.0058447746619053565; Training Loss 0.004730714362189756\n",
      "Episode 5835; Testing Loss 0.0058446777308991255; Training Loss 0.0047307088221765475\n",
      "Episode 5836; Testing Loss 0.005844740162898477; Training Loss 0.004730699341099887\n",
      "Episode 5837; Testing Loss 0.005844862716148794; Training Loss 0.004730692964244698\n",
      "Episode 5838; Testing Loss 0.005844844708424221; Training Loss 0.004730682276618057\n",
      "Episode 5839; Testing Loss 0.005844717897277595; Training Loss 0.004730668408442348\n",
      "Episode 5840; Testing Loss 0.005844693272099831; Training Loss 0.004730657099159144\n",
      "Episode 5841; Testing Loss 0.005844761276357214; Training Loss 0.004730646102243389\n",
      "Episode 5842; Testing Loss 0.005844718302186495; Training Loss 0.004730638467187902\n",
      "Episode 5843; Testing Loss 0.005844614253886782; Training Loss 0.004730628536097704\n",
      "Episode 5844; Testing Loss 0.005844609723171473; Training Loss 0.004730616229271103\n",
      "Episode 5845; Testing Loss 0.005844682990472977; Training Loss 0.004730605119083569\n",
      "Episode 5846; Testing Loss 0.005844711461260354; Training Loss 0.004730598472373011\n",
      "Episode 5847; Testing Loss 0.005844624328531856; Training Loss 0.004730584223267614\n",
      "Episode 5848; Testing Loss 0.00584461009319994; Training Loss 0.004730576071922852\n",
      "Episode 5849; Testing Loss 0.005844659710404706; Training Loss 0.004730566840392734\n",
      "Episode 5850; Testing Loss 0.005844657397523465; Training Loss 0.004730557204550586\n",
      "Episode 5851; Testing Loss 0.005844594206267876; Training Loss 0.004730547363683434\n",
      "Episode 5852; Testing Loss 0.00584458801674626; Training Loss 0.004730536044154399\n",
      "Episode 5853; Testing Loss 0.005844593408420638; Training Loss 0.004730525129249415\n",
      "Episode 5854; Testing Loss 0.005844599790576048; Training Loss 0.004730516552527402\n",
      "Episode 5855; Testing Loss 0.00584455073087537; Training Loss 0.004730504803673606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5856; Testing Loss 0.005844512549921641; Training Loss 0.004730495530177298\n",
      "Episode 5857; Testing Loss 0.005844595055287689; Training Loss 0.004730485779119923\n",
      "Episode 5858; Testing Loss 0.005844610828993494; Training Loss 0.0047304770218396346\n",
      "Episode 5859; Testing Loss 0.005844545230777254; Training Loss 0.0047304666020077145\n",
      "Episode 5860; Testing Loss 0.005844488599736767; Training Loss 0.004730454919720558\n",
      "Episode 5861; Testing Loss 0.005844486558744941; Training Loss 0.00473044662552306\n",
      "Episode 5862; Testing Loss 0.005844511371315378; Training Loss 0.004730437475880593\n",
      "Episode 5863; Testing Loss 0.005844508180641581; Training Loss 0.004730426467417438\n",
      "Episode 5864; Testing Loss 0.005844456387345795; Training Loss 0.0047304162432716115\n",
      "Episode 5865; Testing Loss 0.005844381630137909; Training Loss 0.004730407407233051\n",
      "Episode 5866; Testing Loss 0.005844383790787206; Training Loss 0.004730396750677171\n",
      "Episode 5867; Testing Loss 0.005844482414666333; Training Loss 0.004730388145509812\n",
      "Episode 5868; Testing Loss 0.005844529410548222; Training Loss 0.004730379482928765\n",
      "Episode 5869; Testing Loss 0.005844418986385235; Training Loss 0.004730369032394113\n",
      "Episode 5870; Testing Loss 0.00584433980205008; Training Loss 0.004730356981653661\n",
      "Episode 5871; Testing Loss 0.005844357405963348; Training Loss 0.00473034971754316\n",
      "Episode 5872; Testing Loss 0.005844394816138038; Training Loss 0.004730337767062841\n",
      "Episode 5873; Testing Loss 0.00584439519813288; Training Loss 0.004730330025983635\n",
      "Episode 5874; Testing Loss 0.005844482473364788; Training Loss 0.004730321386084281\n",
      "Episode 5875; Testing Loss 0.005844522729139714; Training Loss 0.004730310112036456\n",
      "Episode 5876; Testing Loss 0.005844404976922011; Training Loss 0.004730299735204126\n",
      "Episode 5877; Testing Loss 0.005844239163587139; Training Loss 0.004730289220433532\n",
      "Episode 5878; Testing Loss 0.0058442093097395035; Training Loss 0.004730280893461029\n",
      "Episode 5879; Testing Loss 0.005844357465723803; Training Loss 0.0047302692101151055\n",
      "Episode 5880; Testing Loss 0.005844417024256488; Training Loss 0.004730260522350458\n",
      "Episode 5881; Testing Loss 0.005844327201559604; Training Loss 0.004730249573542942\n",
      "Episode 5882; Testing Loss 0.005844314648785372; Training Loss 0.004730238972006928\n",
      "Episode 5883; Testing Loss 0.005844400822700377; Training Loss 0.004730232576221405\n",
      "Episode 5884; Testing Loss 0.005844348970923901; Training Loss 0.0047302216623092055\n",
      "Episode 5885; Testing Loss 0.0058442534439291285; Training Loss 0.0047302131750332505\n",
      "Episode 5886; Testing Loss 0.00584434998163869; Training Loss 0.004730202454594948\n",
      "Episode 5887; Testing Loss 0.005844388759864456; Training Loss 0.004730196565069512\n",
      "Episode 5888; Testing Loss 0.005844210272497662; Training Loss 0.004730183009390711\n",
      "Episode 5889; Testing Loss 0.005844107147747559; Training Loss 0.004730173706100407\n",
      "Episode 5890; Testing Loss 0.005844217317715071; Training Loss 0.004730166162275273\n",
      "Episode 5891; Testing Loss 0.005844317540656955; Training Loss 0.004730153905445899\n",
      "Episode 5892; Testing Loss 0.005844251232507204; Training Loss 0.004730141524110279\n",
      "Episode 5893; Testing Loss 0.005844090946807416; Training Loss 0.004730132399215793\n",
      "Episode 5894; Testing Loss 0.005844091699526073; Training Loss 0.004730121436627618\n",
      "Episode 5895; Testing Loss 0.0058442430119692475; Training Loss 0.004730113276104652\n",
      "Episode 5896; Testing Loss 0.005844282240564493; Training Loss 0.004730105919668845\n",
      "Episode 5897; Testing Loss 0.0058441038952084514; Training Loss 0.004730093649291502\n",
      "Episode 5898; Testing Loss 0.005844094299844635; Training Loss 0.004730083210760357\n",
      "Episode 5899; Testing Loss 0.0058442544216961686; Training Loss 0.004730073291316064\n",
      "Episode 5900; Testing Loss 0.005844298112767325; Training Loss 0.004730063730299942\n",
      "Episode 5901; Testing Loss 0.005844152569134884; Training Loss 0.00473005200233477\n",
      "Episode 5902; Testing Loss 0.0058440119514396975; Training Loss 0.004730044508421976\n",
      "Episode 5903; Testing Loss 0.005844078431946935; Training Loss 0.004730033083293342\n",
      "Episode 5904; Testing Loss 0.005844165557651758; Training Loss 0.004730025525845418\n",
      "Episode 5905; Testing Loss 0.005844169966519642; Training Loss 0.004730015667779452\n",
      "Episode 5906; Testing Loss 0.005844085304961933; Training Loss 0.004730006116899207\n",
      "Episode 5907; Testing Loss 0.005844096495427989; Training Loss 0.004729995553433909\n",
      "Episode 5908; Testing Loss 0.005844124476463145; Training Loss 0.0047299837246680635\n",
      "Episode 5909; Testing Loss 0.0058441178036453606; Training Loss 0.004729976852299842\n",
      "Episode 5910; Testing Loss 0.005844053323557466; Training Loss 0.004729969417327207\n",
      "Episode 5911; Testing Loss 0.005843968168014694; Training Loss 0.0047299578029498315\n",
      "Episode 5912; Testing Loss 0.005844005992467511; Training Loss 0.004729947206483976\n",
      "Episode 5913; Testing Loss 0.005844097728608062; Training Loss 0.00472994159403834\n",
      "Episode 5914; Testing Loss 0.005844039687583358; Training Loss 0.004729930528201543\n",
      "Episode 5915; Testing Loss 0.005843924988311883; Training Loss 0.0047299147034293035\n",
      "Episode 5916; Testing Loss 0.005843918967481113; Training Loss 0.0047299069331800895\n",
      "Episode 5917; Testing Loss 0.005844011929740087; Training Loss 0.004729896535216715\n",
      "Episode 5918; Testing Loss 0.005843994367586783; Training Loss 0.004729886841289709\n",
      "Episode 5919; Testing Loss 0.005843974980462306; Training Loss 0.004729878393116456\n",
      "Episode 5920; Testing Loss 0.005844015451377513; Training Loss 0.0047298678743837985\n",
      "Episode 5921; Testing Loss 0.00584402629887371; Training Loss 0.0047298554343772415\n",
      "Episode 5922; Testing Loss 0.005843956842569541; Training Loss 0.004729850314798907\n",
      "Episode 5923; Testing Loss 0.005843842553612163; Training Loss 0.004729838190426788\n",
      "Episode 5924; Testing Loss 0.005843876813656874; Training Loss 0.004729829177038667\n",
      "Episode 5925; Testing Loss 0.005844061025210952; Training Loss 0.004729820206974064\n",
      "Episode 5926; Testing Loss 0.005844045909558368; Training Loss 0.004729808402483387\n",
      "Episode 5927; Testing Loss 0.0058438466801262615; Training Loss 0.004729800322809223\n",
      "Episode 5928; Testing Loss 0.0058437503738244774; Training Loss 0.004729789797189759\n",
      "Episode 5929; Testing Loss 0.005843866609715473; Training Loss 0.0047297789164041995\n",
      "Episode 5930; Testing Loss 0.005844003055143419; Training Loss 0.004729769446731235\n",
      "Episode 5931; Testing Loss 0.005843939322362516; Training Loss 0.004729758553222528\n",
      "Episode 5932; Testing Loss 0.00584382095659589; Training Loss 0.0047297507297354565\n",
      "Episode 5933; Testing Loss 0.005843882330707406; Training Loss 0.004729738613064826\n",
      "Episode 5934; Testing Loss 0.005843961929510831; Training Loss 0.004729730289782674\n",
      "Episode 5935; Testing Loss 0.005843863531080545; Training Loss 0.004729721913883591\n",
      "Episode 5936; Testing Loss 0.005843701874548513; Training Loss 0.004729711342810878\n",
      "Episode 5937; Testing Loss 0.005843782110040278; Training Loss 0.004729700437224963\n",
      "Episode 5938; Testing Loss 0.005843968611016354; Training Loss 0.004729693340206104\n",
      "Episode 5939; Testing Loss 0.005843872529039992; Training Loss 0.004729679961596319\n",
      "Episode 5940; Testing Loss 0.005843717883194851; Training Loss 0.004729674716713423\n",
      "Episode 5941; Testing Loss 0.005843801277960853; Training Loss 0.00472966320782749\n",
      "Episode 5942; Testing Loss 0.005843919385301966; Training Loss 0.004729653086296413\n",
      "Episode 5943; Testing Loss 0.005843836252583807; Training Loss 0.004729643723410724\n",
      "Episode 5944; Testing Loss 0.005843611642290038; Training Loss 0.004729632755169931\n",
      "Episode 5945; Testing Loss 0.0058436185514555705; Training Loss 0.004729623740387223\n",
      "Episode 5946; Testing Loss 0.005843822419942843; Training Loss 0.004729612488830586\n",
      "Episode 5947; Testing Loss 0.005843883276487434; Training Loss 0.004729604118885863\n",
      "Episode 5948; Testing Loss 0.005843725648639532; Training Loss 0.004729594796732303\n",
      "Episode 5949; Testing Loss 0.00584370172613087; Training Loss 0.004729584917293125\n",
      "Episode 5950; Testing Loss 0.0058437893491108854; Training Loss 0.004729572906398542\n",
      "Episode 5951; Testing Loss 0.005843761697445159; Training Loss 0.004729567792110247\n",
      "Episode 5952; Testing Loss 0.005843602582120173; Training Loss 0.004729557437140137\n",
      "Episode 5953; Testing Loss 0.005843568172405198; Training Loss 0.004729543902454227\n",
      "Episode 5954; Testing Loss 0.0058437641291476956; Training Loss 0.004729533822663385\n",
      "Episode 5955; Testing Loss 0.005843841916210391; Training Loss 0.004729524992127214\n",
      "Episode 5956; Testing Loss 0.005843703406741185; Training Loss 0.004729514331404521\n",
      "Episode 5957; Testing Loss 0.005843636983429113; Training Loss 0.0047295043871168506\n",
      "Episode 5958; Testing Loss 0.0058437169266958786; Training Loss 0.004729495504817004\n",
      "Episode 5959; Testing Loss 0.005843660106436921; Training Loss 0.0047294829133018666\n",
      "Episode 5960; Testing Loss 0.00584360571458568; Training Loss 0.004729476731577003\n",
      "Episode 5961; Testing Loss 0.005843670841596126; Training Loss 0.004729466372739288\n",
      "Episode 5962; Testing Loss 0.005843704795781362; Training Loss 0.004729454466815825\n",
      "Episode 5963; Testing Loss 0.005843612417379576; Training Loss 0.0047294491953015555\n",
      "Episode 5964; Testing Loss 0.0058434638244261785; Training Loss 0.0047294363882200985\n",
      "Episode 5965; Testing Loss 0.005843519079792767; Training Loss 0.004729428052261665\n",
      "Episode 5966; Testing Loss 0.005843706445267931; Training Loss 0.004729421778958604\n",
      "Episode 5967; Testing Loss 0.005843663831753991; Training Loss 0.004729410987673362\n",
      "Episode 5968; Testing Loss 0.005843502584350964; Training Loss 0.004729397325090191\n",
      "Episode 5969; Testing Loss 0.00584351360217567; Training Loss 0.00472938765550071\n",
      "Episode 5970; Testing Loss 0.005843556655262649; Training Loss 0.004729377429883104\n",
      "Episode 5971; Testing Loss 0.005843613383260026; Training Loss 0.004729365910371809\n",
      "Episode 5972; Testing Loss 0.005843650927539681; Training Loss 0.00472935903762218\n",
      "Episode 5973; Testing Loss 0.005843561595793763; Training Loss 0.00472934994514978\n",
      "Episode 5974; Testing Loss 0.0058435068077784934; Training Loss 0.004729338087318087\n",
      "Episode 5975; Testing Loss 0.005843517770957032; Training Loss 0.00472932680049577\n",
      "Episode 5976; Testing Loss 0.0058434709829562614; Training Loss 0.004729321839400671\n",
      "Episode 5977; Testing Loss 0.005843425353237459; Training Loss 0.0047293090250842785\n",
      "Episode 5978; Testing Loss 0.005843441942049504; Training Loss 0.00472930032687683\n",
      "Episode 5979; Testing Loss 0.005843455541443612; Training Loss 0.00472929032639078\n",
      "Episode 5980; Testing Loss 0.00584348465586389; Training Loss 0.004729280582414065\n",
      "Episode 5981; Testing Loss 0.0058435109579464215; Training Loss 0.00472926859705716\n",
      "Episode 5982; Testing Loss 0.005843471236801282; Training Loss 0.004729260332333461\n",
      "Episode 5983; Testing Loss 0.005843337107566025; Training Loss 0.004729250813006104\n",
      "Episode 5984; Testing Loss 0.005843339430898242; Training Loss 0.004729241101398206\n",
      "Episode 5985; Testing Loss 0.005843487430644213; Training Loss 0.0047292323964144714\n",
      "Episode 5986; Testing Loss 0.005843486805412261; Training Loss 0.004729221837091004\n",
      "Episode 5987; Testing Loss 0.005843345512308105; Training Loss 0.004729210630597233\n",
      "Episode 5988; Testing Loss 0.0058432791941528716; Training Loss 0.004729202585645345\n",
      "Episode 5989; Testing Loss 0.0058433662195332345; Training Loss 0.004729190758452209\n",
      "Episode 5990; Testing Loss 0.0058435008139883365; Training Loss 0.004729183250120866\n",
      "Episode 5991; Testing Loss 0.005843446780122666; Training Loss 0.004729171893160779\n",
      "Episode 5992; Testing Loss 0.0058432607722245225; Training Loss 0.0047291620740799525\n",
      "Episode 5993; Testing Loss 0.005843239876105876; Training Loss 0.004729153982762484\n",
      "Episode 5994; Testing Loss 0.005843332716072629; Training Loss 0.004729142419762515\n",
      "Episode 5995; Testing Loss 0.0058433792033871215; Training Loss 0.004729133558548862\n",
      "Episode 5996; Testing Loss 0.0058433357435597735; Training Loss 0.00472912634526616\n",
      "Episode 5997; Testing Loss 0.005843284811073575; Training Loss 0.004729112993083253\n",
      "Episode 5998; Testing Loss 0.005843326635253313; Training Loss 0.0047291073343145\n",
      "Episode 5999; Testing Loss 0.005843338400954295; Training Loss 0.004729098815298904\n",
      "Episode 6000; Testing Loss 0.005843303431347789; Training Loss 0.004729088144481903\n",
      "Episode 6001; Testing Loss 0.005843348145933526; Training Loss 0.004729076879188634\n",
      "Episode 6002; Testing Loss 0.00584333150163593; Training Loss 0.004729066813486929\n",
      "Episode 6003; Testing Loss 0.005843198383089018; Training Loss 0.004729055286611166\n",
      "Episode 6004; Testing Loss 0.005843122608456467; Training Loss 0.004729045711927882\n",
      "Episode 6005; Testing Loss 0.00584315700173804; Training Loss 0.00472903801517359\n",
      "Episode 6006; Testing Loss 0.005843274705093996; Training Loss 0.004729027688031072\n",
      "Episode 6007; Testing Loss 0.005843290098525948; Training Loss 0.004729017317984678\n",
      "Episode 6008; Testing Loss 0.005843219137285456; Training Loss 0.004729007029495836\n",
      "Episode 6009; Testing Loss 0.005843169710259249; Training Loss 0.004728998890496575\n",
      "Episode 6010; Testing Loss 0.005843144438933905; Training Loss 0.004728987808858468\n",
      "Episode 6011; Testing Loss 0.005843104515803274; Training Loss 0.004728981439836014\n",
      "Episode 6012; Testing Loss 0.005843100009200176; Training Loss 0.004728970824074506\n",
      "Episode 6013; Testing Loss 0.005843178233646048; Training Loss 0.004728959317604497\n",
      "Episode 6014; Testing Loss 0.00584320208395298; Training Loss 0.004728949679859346\n",
      "Episode 6015; Testing Loss 0.005843172621170938; Training Loss 0.004728939806999361\n",
      "Episode 6016; Testing Loss 0.0058431739722926155; Training Loss 0.004728933334742924\n",
      "Episode 6017; Testing Loss 0.005843186739486607; Training Loss 0.00472891837430414\n",
      "Episode 6018; Testing Loss 0.005843160308147248; Training Loss 0.0047289079824886695\n",
      "Episode 6019; Testing Loss 0.005843106208548123; Training Loss 0.004728898926171431\n",
      "Episode 6020; Testing Loss 0.005843048919636307; Training Loss 0.004728890623543937\n",
      "Episode 6021; Testing Loss 0.00584303218754224; Training Loss 0.00472888049460571\n",
      "Episode 6022; Testing Loss 0.005843061072016508; Training Loss 0.004728870680858\n",
      "Episode 6023; Testing Loss 0.005843078251253428; Training Loss 0.004728862465072517\n",
      "Episode 6024; Testing Loss 0.005842977868080038; Training Loss 0.004728850861935167\n",
      "Episode 6025; Testing Loss 0.005842903770499617; Training Loss 0.004728841943661372\n",
      "Episode 6026; Testing Loss 0.005843089842742511; Training Loss 0.004728831903586229\n",
      "Episode 6027; Testing Loss 0.005843266977994654; Training Loss 0.004728823407897585\n",
      "Episode 6028; Testing Loss 0.005843131089232873; Training Loss 0.004728810993691294\n",
      "Episode 6029; Testing Loss 0.0058429186244607564; Training Loss 0.004728802499598298\n",
      "Episode 6030; Testing Loss 0.005842957678304969; Training Loss 0.004728791873455594\n",
      "Episode 6031; Testing Loss 0.00584302072702628; Training Loss 0.004728781637143783\n",
      "Episode 6032; Testing Loss 0.0058429685303559254; Training Loss 0.004728772359139271\n",
      "Episode 6033; Testing Loss 0.005842909022593698; Training Loss 0.004728762292554322\n",
      "Episode 6034; Testing Loss 0.005842888852857492; Training Loss 0.004728753198302036\n",
      "Episode 6035; Testing Loss 0.005842958992563856; Training Loss 0.004728744055690421\n",
      "Episode 6036; Testing Loss 0.005842936470249756; Training Loss 0.004728732420910304\n",
      "Episode 6037; Testing Loss 0.005842971012143426; Training Loss 0.004728722342393497\n",
      "Episode 6038; Testing Loss 0.00584302175697; Training Loss 0.004728715271214723\n",
      "Episode 6039; Testing Loss 0.005842936812921719; Training Loss 0.00472870424361395\n",
      "Episode 6040; Testing Loss 0.0058429489622706735; Training Loss 0.004728694907850692\n",
      "Episode 6041; Testing Loss 0.005842982089326118; Training Loss 0.0047286866416108506\n",
      "Episode 6042; Testing Loss 0.005842845504481737; Training Loss 0.004728674440521798\n",
      "Episode 6043; Testing Loss 0.005842791684986934; Training Loss 0.004728665388754864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6044; Testing Loss 0.005842848746899056; Training Loss 0.004728656581898953\n",
      "Episode 6045; Testing Loss 0.005842832987114161; Training Loss 0.00472864509951198\n",
      "Episode 6046; Testing Loss 0.005842771451316849; Training Loss 0.004728637234236755\n",
      "Episode 6047; Testing Loss 0.005842913833190835; Training Loss 0.004728627338466159\n",
      "Episode 6048; Testing Loss 0.005843029754889533; Training Loss 0.004728616427780975\n",
      "Episode 6049; Testing Loss 0.005842928316672169; Training Loss 0.004728607051948836\n",
      "Episode 6050; Testing Loss 0.005842824768526621; Training Loss 0.004728599898919697\n",
      "Episode 6051; Testing Loss 0.005842853352610105; Training Loss 0.0047285867488916465\n",
      "Episode 6052; Testing Loss 0.005842920069286963; Training Loss 0.004728577535401064\n",
      "Episode 6053; Testing Loss 0.005842853770454565; Training Loss 0.004728569195706109\n",
      "Episode 6054; Testing Loss 0.005842728368279746; Training Loss 0.004728560665601738\n",
      "Episode 6055; Testing Loss 0.005842749408238721; Training Loss 0.004728549025405475\n",
      "Episode 6056; Testing Loss 0.005842825672018429; Training Loss 0.004728542438503008\n",
      "Episode 6057; Testing Loss 0.005842735779754349; Training Loss 0.004728529567430638\n",
      "Episode 6058; Testing Loss 0.005842615196427228; Training Loss 0.004728524454659257\n",
      "Episode 6059; Testing Loss 0.005842799474747306; Training Loss 0.004728514412495565\n",
      "Episode 6060; Testing Loss 0.005843014223389335; Training Loss 0.0047285044061296895\n",
      "Episode 6061; Testing Loss 0.005842925517911214; Training Loss 0.00472849276143741\n",
      "Episode 6062; Testing Loss 0.005842678457997311; Training Loss 0.004728484957523561\n",
      "Episode 6063; Testing Loss 0.005842663094673508; Training Loss 0.004728474809576778\n",
      "Episode 6064; Testing Loss 0.005842874153007852; Training Loss 0.004728460557270086\n",
      "Episode 6065; Testing Loss 0.0058429140832912466; Training Loss 0.004728457975393499\n",
      "Episode 6066; Testing Loss 0.0058426331574126895; Training Loss 0.004728446155311599\n",
      "Episode 6067; Testing Loss 0.0058425395642965955; Training Loss 0.004728436710668291\n",
      "Episode 6068; Testing Loss 0.005842740937131759; Training Loss 0.004728422657183173\n",
      "Episode 6069; Testing Loss 0.005842791366534264; Training Loss 0.00472841564582653\n",
      "Episode 6070; Testing Loss 0.0058426195638341775; Training Loss 0.004728405642906907\n",
      "Episode 6071; Testing Loss 0.005842613048699254; Training Loss 0.004728395064873501\n",
      "Episode 6072; Testing Loss 0.005842821662439414; Training Loss 0.004728385249207464\n",
      "Episode 6073; Testing Loss 0.005842922346270975; Training Loss 0.004728381555897317\n",
      "Episode 6074; Testing Loss 0.0058426758691248505; Training Loss 0.004728366328087116\n",
      "Episode 6075; Testing Loss 0.005842501025151131; Training Loss 0.004728355335338195\n",
      "Episode 6076; Testing Loss 0.00584258297027644; Training Loss 0.00472834498722535\n",
      "Episode 6077; Testing Loss 0.005842739692400858; Training Loss 0.004728340889272992\n",
      "Episode 6078; Testing Loss 0.005842654381251055; Training Loss 0.004728326260361699\n",
      "Episode 6079; Testing Loss 0.005842456120985044; Training Loss 0.004728317127159032\n",
      "Episode 6080; Testing Loss 0.005842503029664041; Training Loss 0.0047283090692796645\n",
      "Episode 6081; Testing Loss 0.005842708459146558; Training Loss 0.00472829855322132\n",
      "Episode 6082; Testing Loss 0.005842773401454648; Training Loss 0.004728287939819559\n",
      "Episode 6083; Testing Loss 0.005842589795073548; Training Loss 0.004728275761000308\n",
      "Episode 6084; Testing Loss 0.005842441837819522; Training Loss 0.004728272680107608\n",
      "Episode 6085; Testing Loss 0.005842510072549332; Training Loss 0.00472826004570838\n",
      "Episode 6086; Testing Loss 0.005842605163009302; Training Loss 0.004728249986234052\n",
      "Episode 6087; Testing Loss 0.0058424942574772905; Training Loss 0.004728238618384509\n",
      "Episode 6088; Testing Loss 0.005842433785765839; Training Loss 0.004728231653947372\n",
      "Episode 6089; Testing Loss 0.005842606604473159; Training Loss 0.004728218174781779\n",
      "Episode 6090; Testing Loss 0.0058427113919087845; Training Loss 0.004728212098339938\n",
      "Episode 6091; Testing Loss 0.005842516240954104; Training Loss 0.004728201416633241\n",
      "Episode 6092; Testing Loss 0.005842412407385236; Training Loss 0.004728193088490623\n",
      "Episode 6093; Testing Loss 0.005842569877312416; Training Loss 0.0047281800740926055\n",
      "Episode 6094; Testing Loss 0.005842679898269715; Training Loss 0.004728174760835372\n",
      "Episode 6095; Testing Loss 0.0058425079880728426; Training Loss 0.0047281642539959805\n",
      "Episode 6096; Testing Loss 0.005842332068730462; Training Loss 0.004728155000581465\n",
      "Episode 6097; Testing Loss 0.005842432868682089; Training Loss 0.004728141323507549\n",
      "Episode 6098; Testing Loss 0.005842523924362344; Training Loss 0.004728130705758623\n",
      "Episode 6099; Testing Loss 0.005842501077825489; Training Loss 0.0047281243050687\n",
      "Episode 6100; Testing Loss 0.005842398564545431; Training Loss 0.004728114220393809\n",
      "Episode 6101; Testing Loss 0.005842343436668099; Training Loss 0.004728103336488595\n",
      "Episode 6102; Testing Loss 0.005842485174339494; Training Loss 0.004728097092563091\n",
      "Episode 6103; Testing Loss 0.005842501816411845; Training Loss 0.004728089777738225\n",
      "Episode 6104; Testing Loss 0.005842370151038649; Training Loss 0.0047280781851452335\n",
      "Episode 6105; Testing Loss 0.005842254755827278; Training Loss 0.00472806611455887\n",
      "Episode 6106; Testing Loss 0.005842318998718891; Training Loss 0.0047280550509097175\n",
      "Episode 6107; Testing Loss 0.0058424199221924385; Training Loss 0.004728051238970118\n",
      "Episode 6108; Testing Loss 0.005842397387736886; Training Loss 0.004728039208883708\n",
      "Episode 6109; Testing Loss 0.005842300480084603; Training Loss 0.004728027039520103\n",
      "Episode 6110; Testing Loss 0.005842266174541305; Training Loss 0.0047280153358972695\n",
      "Episode 6111; Testing Loss 0.0058423611452142485; Training Loss 0.004728009373697916\n",
      "Episode 6112; Testing Loss 0.005842424806768847; Training Loss 0.004728001462664594\n",
      "Episode 6113; Testing Loss 0.00584234245442597; Training Loss 0.004727987360245479\n",
      "Episode 6114; Testing Loss 0.005842185831468199; Training Loss 0.004727977137008286\n",
      "Episode 6115; Testing Loss 0.005842158628323041; Training Loss 0.004727970673369898\n",
      "Episode 6116; Testing Loss 0.005842335973966316; Training Loss 0.004727957983666231\n",
      "Episode 6117; Testing Loss 0.005842436519853292; Training Loss 0.004727948500988244\n",
      "Episode 6118; Testing Loss 0.005842321907978514; Training Loss 0.004727940563007341\n",
      "Episode 6119; Testing Loss 0.005842251174630715; Training Loss 0.004727932666569132\n",
      "Episode 6120; Testing Loss 0.005842351000278553; Training Loss 0.0047279228882076444\n",
      "Episode 6121; Testing Loss 0.005842336300475872; Training Loss 0.004727909107070562\n",
      "Episode 6122; Testing Loss 0.00584226355365006; Training Loss 0.004727899462565022\n",
      "Episode 6123; Testing Loss 0.005842218469681294; Training Loss 0.004727890177289855\n",
      "Episode 6124; Testing Loss 0.005842195118774603; Training Loss 0.004727879543229721\n",
      "Episode 6125; Testing Loss 0.005842208930377472; Training Loss 0.004727870263613549\n",
      "Episode 6126; Testing Loss 0.005842266705173525; Training Loss 0.0047278608362963\n",
      "Episode 6127; Testing Loss 0.005842280726936787; Training Loss 0.004727850460269704\n",
      "Episode 6128; Testing Loss 0.005842204740941152; Training Loss 0.0047278426547254105\n",
      "Episode 6129; Testing Loss 0.005842095913849986; Training Loss 0.004727833346435646\n",
      "Episode 6130; Testing Loss 0.005842163319814437; Training Loss 0.00472782304369421\n",
      "Episode 6131; Testing Loss 0.005842353399078569; Training Loss 0.004727816102340435\n",
      "Episode 6132; Testing Loss 0.0058422624847406105; Training Loss 0.004727804720578605\n",
      "Episode 6133; Testing Loss 0.005842035451513213; Training Loss 0.004727794292764561\n",
      "Episode 6134; Testing Loss 0.005842003567765082; Training Loss 0.004727786346148612\n",
      "Episode 6135; Testing Loss 0.005842130549476151; Training Loss 0.004727775999513116\n",
      "Episode 6136; Testing Loss 0.005842205106767953; Training Loss 0.004727764383237381\n",
      "Episode 6137; Testing Loss 0.005842148562436018; Training Loss 0.004727756550894416\n",
      "Episode 6138; Testing Loss 0.005842091761652454; Training Loss 0.004727747845766782\n",
      "Episode 6139; Testing Loss 0.0058421171377465745; Training Loss 0.00472773811405546\n",
      "Episode 6140; Testing Loss 0.005842101961352213; Training Loss 0.004727725057317842\n",
      "Episode 6141; Testing Loss 0.005842106252474297; Training Loss 0.004727716309903457\n",
      "Episode 6142; Testing Loss 0.005842091442860791; Training Loss 0.00472771016692867\n",
      "Episode 6143; Testing Loss 0.005842035223938285; Training Loss 0.0047276978688265935\n",
      "Episode 6144; Testing Loss 0.005842017865438135; Training Loss 0.004727686175719765\n",
      "Episode 6145; Testing Loss 0.005842122779129691; Training Loss 0.0047276778978319126\n",
      "Episode 6146; Testing Loss 0.0058421246220168805; Training Loss 0.004727668171357799\n",
      "Episode 6147; Testing Loss 0.005842068571752499; Training Loss 0.004727656933929729\n",
      "Episode 6148; Testing Loss 0.005842032543206331; Training Loss 0.004727648986325579\n",
      "Episode 6149; Testing Loss 0.0058420222605702656; Training Loss 0.0047276395069554425\n",
      "Episode 6150; Testing Loss 0.005841999259788114; Training Loss 0.004727631399787269\n",
      "Episode 6151; Testing Loss 0.005841914084886955; Training Loss 0.004727620547276794\n",
      "Episode 6152; Testing Loss 0.005841893799491224; Training Loss 0.004727609543983228\n",
      "Episode 6153; Testing Loss 0.00584191221139841; Training Loss 0.004727602363335905\n",
      "Episode 6154; Testing Loss 0.0058419565532084865; Training Loss 0.004727592793659977\n",
      "Episode 6155; Testing Loss 0.005841998403537258; Training Loss 0.004727583114917292\n",
      "Episode 6156; Testing Loss 0.005842007470137868; Training Loss 0.004727571552507126\n",
      "Episode 6157; Testing Loss 0.005841980605398113; Training Loss 0.004727562423270446\n",
      "Episode 6158; Testing Loss 0.005841965460785895; Training Loss 0.004727555395811533\n",
      "Episode 6159; Testing Loss 0.005841973428206658; Training Loss 0.0047275416170464825\n",
      "Episode 6160; Testing Loss 0.005841969654867384; Training Loss 0.004727531943847977\n",
      "Episode 6161; Testing Loss 0.005841911889770866; Training Loss 0.004727522630474907\n",
      "Episode 6162; Testing Loss 0.005841871452165919; Training Loss 0.004727512713750269\n",
      "Episode 6163; Testing Loss 0.005841867334550959; Training Loss 0.004727503079641395\n",
      "Episode 6164; Testing Loss 0.005841861542374791; Training Loss 0.004727493920093421\n",
      "Episode 6165; Testing Loss 0.005841876371231337; Training Loss 0.004727484071647089\n",
      "Episode 6166; Testing Loss 0.005841892798187278; Training Loss 0.004727473213470917\n",
      "Episode 6167; Testing Loss 0.005841916181020473; Training Loss 0.004727463703960607\n",
      "Episode 6168; Testing Loss 0.0058419807547053235; Training Loss 0.004727453987091106\n",
      "Episode 6169; Testing Loss 0.005842004399486931; Training Loss 0.004727444341354127\n",
      "Episode 6170; Testing Loss 0.00584187970112079; Training Loss 0.0047274357411907645\n",
      "Episode 6171; Testing Loss 0.005841719161104568; Training Loss 0.004727427349377483\n",
      "Episode 6172; Testing Loss 0.005841842279578657; Training Loss 0.004727415626260899\n",
      "Episode 6173; Testing Loss 0.005842027670316584; Training Loss 0.004727407362766864\n",
      "Episode 6174; Testing Loss 0.005841908870075829; Training Loss 0.00472739727962564\n",
      "Episode 6175; Testing Loss 0.005841657185446944; Training Loss 0.004727391011275626\n",
      "Episode 6176; Testing Loss 0.005841792909419495; Training Loss 0.004727378941113402\n",
      "Episode 6177; Testing Loss 0.005842058715331027; Training Loss 0.004727370617576527\n",
      "Episode 6178; Testing Loss 0.005841994162455293; Training Loss 0.004727358616732205\n",
      "Episode 6179; Testing Loss 0.005841748734499656; Training Loss 0.004727350596225072\n",
      "Episode 6180; Testing Loss 0.005841665259134472; Training Loss 0.004727338656599225\n",
      "Episode 6181; Testing Loss 0.00584184380025658; Training Loss 0.004727331174575094\n",
      "Episode 6182; Testing Loss 0.005841873727989274; Training Loss 0.004727319432130651\n",
      "Episode 6183; Testing Loss 0.005841758632420215; Training Loss 0.004727309050911799\n",
      "Episode 6184; Testing Loss 0.005841665129270893; Training Loss 0.004727301710915135\n",
      "Episode 6185; Testing Loss 0.005841751533354963; Training Loss 0.004727289796003195\n",
      "Episode 6186; Testing Loss 0.005841885034248386; Training Loss 0.004727282081201189\n",
      "Episode 6187; Testing Loss 0.0058418538854785864; Training Loss 0.004727270618569371\n",
      "Episode 6188; Testing Loss 0.005841737361938108; Training Loss 0.004727262872062357\n",
      "Episode 6189; Testing Loss 0.005841675962443651; Training Loss 0.0047272532193472815\n",
      "Episode 6190; Testing Loss 0.005841803927860631; Training Loss 0.004727243002305171\n",
      "Episode 6191; Testing Loss 0.0058417937669236275; Training Loss 0.004727233498861701\n",
      "Episode 6192; Testing Loss 0.005841729649223645; Training Loss 0.0047272262427747805\n",
      "Episode 6193; Testing Loss 0.0058416504444364645; Training Loss 0.00472721255332604\n",
      "Episode 6194; Testing Loss 0.005841646678174256; Training Loss 0.004727207496423586\n",
      "Episode 6195; Testing Loss 0.005841668131740868; Training Loss 0.0047271985173653235\n",
      "Episode 6196; Testing Loss 0.005841681519006966; Training Loss 0.004727188585549691\n",
      "Episode 6197; Testing Loss 0.005841742588013483; Training Loss 0.004727177548897191\n",
      "Episode 6198; Testing Loss 0.005841755573763154; Training Loss 0.004727170323266197\n",
      "Episode 6199; Testing Loss 0.005841629926805317; Training Loss 0.004727158767559222\n",
      "Episode 6200; Testing Loss 0.005841504391405519; Training Loss 0.004727148214450676\n",
      "Episode 6201; Testing Loss 0.005841481727420117; Training Loss 0.004727140354364634\n",
      "Episode 6202; Testing Loss 0.0058415782129017406; Training Loss 0.004727128422663235\n",
      "Episode 6203; Testing Loss 0.005841667550688394; Training Loss 0.00472711870320441\n",
      "Episode 6204; Testing Loss 0.005841580792203439; Training Loss 0.00472710719502575\n",
      "Episode 6205; Testing Loss 0.005841489119545155; Training Loss 0.004727098167491\n",
      "Episode 6206; Testing Loss 0.005841517575167209; Training Loss 0.00472709158389746\n",
      "Episode 6207; Testing Loss 0.005841567946931558; Training Loss 0.004727079337023239\n",
      "Episode 6208; Testing Loss 0.005841635359303639; Training Loss 0.0047270698841213096\n",
      "Episode 6209; Testing Loss 0.005841653623071144; Training Loss 0.004727060332778315\n",
      "Episode 6210; Testing Loss 0.005841557915585961; Training Loss 0.004727049689212549\n",
      "Episode 6211; Testing Loss 0.005841450816207152; Training Loss 0.004727043336643231\n",
      "Episode 6212; Testing Loss 0.005841468066143308; Training Loss 0.004727031078060823\n",
      "Episode 6213; Testing Loss 0.0058416000446518; Training Loss 0.004727020614871535\n",
      "Episode 6214; Testing Loss 0.0058416264978253096; Training Loss 0.004727011490487396\n",
      "Episode 6215; Testing Loss 0.005841535882782738; Training Loss 0.004727001700204474\n",
      "Episode 6216; Testing Loss 0.005841503446901355; Training Loss 0.004726993016359423\n",
      "Episode 6217; Testing Loss 0.005841538804878781; Training Loss 0.004726982780820695\n",
      "Episode 6218; Testing Loss 0.0058415397235661; Training Loss 0.004726972401401435\n",
      "Episode 6219; Testing Loss 0.005841468885569179; Training Loss 0.004726962943844795\n",
      "Episode 6220; Testing Loss 0.005841430303664709; Training Loss 0.00472695396816534\n",
      "Episode 6221; Testing Loss 0.0058414616630704476; Training Loss 0.004726944803143141\n",
      "Episode 6222; Testing Loss 0.005841514128748379; Training Loss 0.004726935798947677\n",
      "Episode 6223; Testing Loss 0.0058415398008165; Training Loss 0.0047269245325980225\n",
      "Episode 6224; Testing Loss 0.00584146477656539; Training Loss 0.004726914802811185\n",
      "Episode 6225; Testing Loss 0.005841390470711119; Training Loss 0.00472690641046866\n",
      "Episode 6226; Testing Loss 0.005841457655436352; Training Loss 0.004726896209892453\n",
      "Episode 6227; Testing Loss 0.0058415314527462845; Training Loss 0.0047268887827302455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6228; Testing Loss 0.00584147268451575; Training Loss 0.004726880118894522\n",
      "Episode 6229; Testing Loss 0.005841305594930932; Training Loss 0.004726866731116544\n",
      "Episode 6230; Testing Loss 0.005841316383776803; Training Loss 0.004726857282479204\n",
      "Episode 6231; Testing Loss 0.005841506167148911; Training Loss 0.004726848492777885\n",
      "Episode 6232; Testing Loss 0.005841515115268304; Training Loss 0.004726839778812695\n",
      "Episode 6233; Testing Loss 0.005841345130666803; Training Loss 0.004726829461121169\n",
      "Episode 6234; Testing Loss 0.005841328040862791; Training Loss 0.004726821751370466\n",
      "Episode 6235; Testing Loss 0.005841484492267357; Training Loss 0.004726810958909322\n",
      "Episode 6236; Testing Loss 0.00584142734331722; Training Loss 0.0047268025364775035\n",
      "Episode 6237; Testing Loss 0.005841185910365996; Training Loss 0.004726790764184314\n",
      "Episode 6238; Testing Loss 0.0058411667463175295; Training Loss 0.0047267815892938995\n",
      "Episode 6239; Testing Loss 0.00584138829415225; Training Loss 0.004726770299770969\n",
      "Episode 6240; Testing Loss 0.005841527570661512; Training Loss 0.004726763441019867\n",
      "Episode 6241; Testing Loss 0.005841379110421429; Training Loss 0.004726750879264277\n",
      "Episode 6242; Testing Loss 0.005841247524444869; Training Loss 0.004726742397566352\n",
      "Episode 6243; Testing Loss 0.005841268467061884; Training Loss 0.0047267312031899975\n",
      "Episode 6244; Testing Loss 0.005841405840900703; Training Loss 0.004726722376271263\n",
      "Episode 6245; Testing Loss 0.005841425913162513; Training Loss 0.00472671287094613\n",
      "Episode 6246; Testing Loss 0.005841284765183959; Training Loss 0.004726702326762063\n",
      "Episode 6247; Testing Loss 0.005841195753861847; Training Loss 0.004726695334363507\n",
      "Episode 6248; Testing Loss 0.005841295524480108; Training Loss 0.004726684172359832\n",
      "Episode 6249; Testing Loss 0.005841435481690553; Training Loss 0.0047266738891483685\n",
      "Episode 6250; Testing Loss 0.005841411690656423; Training Loss 0.004726665782288435\n",
      "Episode 6251; Testing Loss 0.005841268363683461; Training Loss 0.004726655367712895\n",
      "Episode 6252; Testing Loss 0.005841208324663616; Training Loss 0.004726644694119575\n",
      "Episode 6253; Testing Loss 0.0058412027304737685; Training Loss 0.004726635323255089\n",
      "Episode 6254; Testing Loss 0.005841287175250267; Training Loss 0.004726625299546449\n",
      "Episode 6255; Testing Loss 0.005841278177372785; Training Loss 0.0047266156045242536\n",
      "Episode 6256; Testing Loss 0.0058411918033432885; Training Loss 0.0047266063725117605\n",
      "Episode 6257; Testing Loss 0.00584117413393418; Training Loss 0.004726597116508391\n",
      "Episode 6258; Testing Loss 0.00584123397746939; Training Loss 0.0047265893926687974\n",
      "Episode 6259; Testing Loss 0.005841335322049703; Training Loss 0.004726578809743957\n",
      "Episode 6260; Testing Loss 0.005841303084104153; Training Loss 0.004726570327419794\n",
      "Episode 6261; Testing Loss 0.005841182998463713; Training Loss 0.004726562040369489\n",
      "Episode 6262; Testing Loss 0.005841120302096716; Training Loss 0.004726549805987617\n",
      "Episode 6263; Testing Loss 0.005841144342554238; Training Loss 0.004726541635673262\n",
      "Episode 6264; Testing Loss 0.005841238998884876; Training Loss 0.004726533058189318\n",
      "Episode 6265; Testing Loss 0.005841244501977612; Training Loss 0.004726522019145175\n",
      "Episode 6266; Testing Loss 0.005841136291200618; Training Loss 0.004726510157204554\n",
      "Episode 6267; Testing Loss 0.005841044610388337; Training Loss 0.004726502631533616\n",
      "Episode 6268; Testing Loss 0.005841128482977019; Training Loss 0.004726490910882032\n",
      "Episode 6269; Testing Loss 0.005841319822265524; Training Loss 0.004726482112270685\n",
      "Episode 6270; Testing Loss 0.00584129998739814; Training Loss 0.004726471941743643\n",
      "Episode 6271; Testing Loss 0.00584114705493503; Training Loss 0.004726461802931411\n",
      "Episode 6272; Testing Loss 0.005841058550724536; Training Loss 0.004726452523704094\n",
      "Episode 6273; Testing Loss 0.005841121384861577; Training Loss 0.004726442663097085\n",
      "Episode 6274; Testing Loss 0.005841156348999415; Training Loss 0.0047264332834269385\n",
      "Episode 6275; Testing Loss 0.005841191263062464; Training Loss 0.004726423510297924\n",
      "Episode 6276; Testing Loss 0.005841123929504944; Training Loss 0.004726413598332148\n",
      "Episode 6277; Testing Loss 0.00584108315276413; Training Loss 0.004726407132313303\n",
      "Episode 6278; Testing Loss 0.005841033785833329; Training Loss 0.0047263954765099\n",
      "Episode 6279; Testing Loss 0.00584117531770681; Training Loss 0.004726385391930713\n",
      "Episode 6280; Testing Loss 0.0058412903808139046; Training Loss 0.0047263800899535885\n",
      "Episode 6281; Testing Loss 0.005841095901589201; Training Loss 0.004726365878850712\n",
      "Episode 6282; Testing Loss 0.005840975288923346; Training Loss 0.004726358727871283\n",
      "Episode 6283; Testing Loss 0.005841072938442092; Training Loss 0.004726348269265905\n",
      "Episode 6284; Testing Loss 0.005841167924913676; Training Loss 0.004726338918488217\n",
      "Episode 6285; Testing Loss 0.005841023054513793; Training Loss 0.004726328994286805\n",
      "Episode 6286; Testing Loss 0.005840920160313884; Training Loss 0.0047263220298553645\n",
      "Episode 6287; Testing Loss 0.0058410586105672995; Training Loss 0.0047263098737089365\n",
      "Episode 6288; Testing Loss 0.005841118334783865; Training Loss 0.004726301748811342\n",
      "Episode 6289; Testing Loss 0.005841035696524724; Training Loss 0.004726289694868988\n",
      "Episode 6290; Testing Loss 0.0058409430805130644; Training Loss 0.004726282437971017\n",
      "Episode 6291; Testing Loss 0.005840933453627303; Training Loss 0.004726274223268531\n",
      "Episode 6292; Testing Loss 0.00584106499724829; Training Loss 0.004726263411815892\n",
      "Episode 6293; Testing Loss 0.00584110272474952; Training Loss 0.004726251005522271\n",
      "Episode 6294; Testing Loss 0.00584098047554347; Training Loss 0.004726242412426315\n",
      "Episode 6295; Testing Loss 0.005840936014567711; Training Loss 0.004726231954528939\n",
      "Episode 6296; Testing Loss 0.005841053185902659; Training Loss 0.004726225348715096\n",
      "Episode 6297; Testing Loss 0.0058410705350205915; Training Loss 0.004726215313936714\n",
      "Episode 6298; Testing Loss 0.005840983416374762; Training Loss 0.004726204591763233\n",
      "Episode 6299; Testing Loss 0.005840974814947579; Training Loss 0.004726196284300161\n",
      "Episode 6300; Testing Loss 0.005840997452804827; Training Loss 0.0047261864539642874\n",
      "Episode 6301; Testing Loss 0.005840926509697333; Training Loss 0.004726174576811666\n",
      "Episode 6302; Testing Loss 0.005840836184028335; Training Loss 0.004726167780576191\n",
      "Episode 6303; Testing Loss 0.0058408362554021615; Training Loss 0.0047261590370796655\n",
      "Episode 6304; Testing Loss 0.005840932033975635; Training Loss 0.00472614510792706\n",
      "Episode 6305; Testing Loss 0.005840970387860534; Training Loss 0.004726137563844761\n",
      "Episode 6306; Testing Loss 0.005840982289591216; Training Loss 0.004726131071369026\n",
      "Episode 6307; Testing Loss 0.005840965879338305; Training Loss 0.00472611890448199\n",
      "Episode 6308; Testing Loss 0.005840967558409578; Training Loss 0.004726107752328343\n",
      "Episode 6309; Testing Loss 0.0058409010597706545; Training Loss 0.004726098136110608\n",
      "Episode 6310; Testing Loss 0.005840818522902971; Training Loss 0.004726088872289403\n",
      "Episode 6311; Testing Loss 0.005840787039823046; Training Loss 0.0047260788544154185\n",
      "Episode 6312; Testing Loss 0.005840858276796727; Training Loss 0.0047260705880684635\n",
      "Episode 6313; Testing Loss 0.005840967640785064; Training Loss 0.004726061127069076\n",
      "Episode 6314; Testing Loss 0.005840903799431784; Training Loss 0.0047260491761584944\n",
      "Episode 6315; Testing Loss 0.005840766758202086; Training Loss 0.004726043806625974\n",
      "Episode 6316; Testing Loss 0.005840790373820921; Training Loss 0.004726029879892463\n",
      "Episode 6317; Testing Loss 0.005840863702606671; Training Loss 0.004726022615816771\n",
      "Episode 6318; Testing Loss 0.00584093162708302; Training Loss 0.0047260160709666375\n",
      "Episode 6319; Testing Loss 0.005840942360406883; Training Loss 0.00472600607954843\n",
      "Episode 6320; Testing Loss 0.005840805591720454; Training Loss 0.004725994517738447\n",
      "Episode 6321; Testing Loss 0.005840800951783689; Training Loss 0.004725983535376198\n",
      "Episode 6322; Testing Loss 0.0058408205269561584; Training Loss 0.0047259792349458015\n",
      "Episode 6323; Testing Loss 0.0058407128077226; Training Loss 0.0047259685366409015\n",
      "Episode 6324; Testing Loss 0.005840601394964493; Training Loss 0.004725957278373193\n",
      "Episode 6325; Testing Loss 0.005840729376258407; Training Loss 0.004725948416478539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6326; Testing Loss 0.005840880114609135; Training Loss 0.0047259400976926835\n",
      "Episode 6327; Testing Loss 0.005840749635091395; Training Loss 0.004725927342746518\n",
      "Episode 6328; Testing Loss 0.00584060627697603; Training Loss 0.004725916851089449\n",
      "Episode 6329; Testing Loss 0.005840706726189026; Training Loss 0.004725909063698089\n",
      "Episode 6330; Testing Loss 0.005840734686546757; Training Loss 0.00472589576351092\n",
      "Episode 6331; Testing Loss 0.005840694189083592; Training Loss 0.00472588629063259\n",
      "Episode 6332; Testing Loss 0.005840642992107904; Training Loss 0.004725878047568814\n",
      "Episode 6333; Testing Loss 0.005840618953369753; Training Loss 0.004725867579710684\n",
      "Episode 6334; Testing Loss 0.005840728362322437; Training Loss 0.004725858213687985\n",
      "Episode 6335; Testing Loss 0.005840700630483623; Training Loss 0.004725848680102949\n",
      "Episode 6336; Testing Loss 0.005840719845939627; Training Loss 0.00472583957790347\n",
      "Episode 6337; Testing Loss 0.005840755414772721; Training Loss 0.004725829116468135\n",
      "Episode 6338; Testing Loss 0.005840717685611632; Training Loss 0.0047258196194598714\n",
      "Episode 6339; Testing Loss 0.005840648083797643; Training Loss 0.004725809349986362\n",
      "Episode 6340; Testing Loss 0.0058406301881637115; Training Loss 0.004725799799319841\n",
      "Episode 6341; Testing Loss 0.005840707511378051; Training Loss 0.004725790502533048\n",
      "Episode 6342; Testing Loss 0.005840663380717908; Training Loss 0.004725780781113712\n",
      "Episode 6343; Testing Loss 0.00584057084854324; Training Loss 0.004725771235328184\n",
      "Episode 6344; Testing Loss 0.005840591837345065; Training Loss 0.004725761555377893\n",
      "Episode 6345; Testing Loss 0.0058406073147108445; Training Loss 0.004725753040974307\n",
      "Episode 6346; Testing Loss 0.005840656763597449; Training Loss 0.0047257435064887434\n",
      "Episode 6347; Testing Loss 0.005840680228754638; Training Loss 0.0047257333489427325\n",
      "Episode 6348; Testing Loss 0.005840679408564808; Training Loss 0.004725727771130144\n",
      "Episode 6349; Testing Loss 0.005840534838968836; Training Loss 0.004725716785512397\n",
      "Episode 6350; Testing Loss 0.005840455978928801; Training Loss 0.004725708528449203\n",
      "Episode 6351; Testing Loss 0.005840628139712564; Training Loss 0.004725698238101628\n",
      "Episode 6352; Testing Loss 0.00584074196348081; Training Loss 0.004725689905998721\n",
      "Episode 6353; Testing Loss 0.005840589917188916; Training Loss 0.004725677633090728\n",
      "Episode 6354; Testing Loss 0.0058404218367679875; Training Loss 0.004725671199300744\n",
      "Episode 6355; Testing Loss 0.005840477869995869; Training Loss 0.004725660696379325\n",
      "Episode 6356; Testing Loss 0.005840630515650704; Training Loss 0.004725647546604079\n",
      "Episode 6357; Testing Loss 0.005840651969115774; Training Loss 0.00472564054239874\n",
      "Episode 6358; Testing Loss 0.00584058388109105; Training Loss 0.004725631534399939\n",
      "Episode 6359; Testing Loss 0.005840522679888519; Training Loss 0.004725621189084831\n",
      "Episode 6360; Testing Loss 0.005840545439195237; Training Loss 0.004725608856455253\n",
      "Episode 6361; Testing Loss 0.0058405750059381915; Training Loss 0.004725601324271398\n",
      "Episode 6362; Testing Loss 0.005840453266654942; Training Loss 0.004725591718873082\n",
      "Episode 6363; Testing Loss 0.005840389413768258; Training Loss 0.004725581021491557\n",
      "Episode 6364; Testing Loss 0.005840506369165901; Training Loss 0.00472557254062324\n",
      "Episode 6365; Testing Loss 0.005840596048761602; Training Loss 0.004725564688154192\n",
      "Episode 6366; Testing Loss 0.005840500848757478; Training Loss 0.004725554103768211\n",
      "Episode 6367; Testing Loss 0.005840449076588589; Training Loss 0.004725542103709176\n",
      "Episode 6368; Testing Loss 0.005840428833432524; Training Loss 0.004725535911746287\n",
      "Episode 6369; Testing Loss 0.00584036561911267; Training Loss 0.00472552386637751\n",
      "Episode 6370; Testing Loss 0.00584036238313334; Training Loss 0.00472551475779193\n",
      "Episode 6371; Testing Loss 0.005840396581934458; Training Loss 0.004725503585230221\n",
      "Episode 6372; Testing Loss 0.005840402681463466; Training Loss 0.004725493311425544\n",
      "Episode 6373; Testing Loss 0.0058403668917637735; Training Loss 0.00472548768591912\n",
      "Episode 6374; Testing Loss 0.00584042949222463; Training Loss 0.0047254779765407914\n",
      "Episode 6375; Testing Loss 0.005840500299593424; Training Loss 0.004725464858301781\n",
      "Episode 6376; Testing Loss 0.005840468691710948; Training Loss 0.0047254586080372534\n",
      "Episode 6377; Testing Loss 0.0058403095095498755; Training Loss 0.004725450295559348\n",
      "Episode 6378; Testing Loss 0.0058401972089299455; Training Loss 0.0047254411463034\n",
      "Episode 6379; Testing Loss 0.005840328587526436; Training Loss 0.004725431859007558\n",
      "Episode 6380; Testing Loss 0.0058405447028819285; Training Loss 0.004725424208726796\n",
      "Episode 6381; Testing Loss 0.005840491644665637; Training Loss 0.004725410429200769\n",
      "Episode 6382; Testing Loss 0.005840267320256004; Training Loss 0.004725401796753616\n",
      "Episode 6383; Testing Loss 0.005840196509730213; Training Loss 0.0047253914927802365\n",
      "Episode 6384; Testing Loss 0.005840351154440233; Training Loss 0.004725383870420932\n",
      "Episode 6385; Testing Loss 0.005840447815796718; Training Loss 0.004725375113406311\n",
      "Episode 6386; Testing Loss 0.005840327537352948; Training Loss 0.004725363119165855\n",
      "Episode 6387; Testing Loss 0.0058402737633548195; Training Loss 0.00472535422639746\n",
      "Episode 6388; Testing Loss 0.005840365533754143; Training Loss 0.0047253426638222545\n",
      "Episode 6389; Testing Loss 0.005840325951611345; Training Loss 0.004725330881394569\n",
      "Episode 6390; Testing Loss 0.0058402469417807796; Training Loss 0.004725322687279493\n",
      "Episode 6391; Testing Loss 0.005840225046645751; Training Loss 0.004725314473077816\n",
      "Episode 6392; Testing Loss 0.005840298397960922; Training Loss 0.004725301196815636\n",
      "Episode 6393; Testing Loss 0.0058403939212568045; Training Loss 0.004725294169784559\n",
      "Episode 6394; Testing Loss 0.005840393958316464; Training Loss 0.004725286304645059\n",
      "Episode 6395; Testing Loss 0.005840231445177203; Training Loss 0.004725275494267703\n",
      "Episode 6396; Testing Loss 0.005840175984243432; Training Loss 0.004725265033764252\n",
      "Episode 6397; Testing Loss 0.005840196191734481; Training Loss 0.004725255067924326\n",
      "Episode 6398; Testing Loss 0.005840187233316158; Training Loss 0.004725250353006064\n",
      "Episode 6399; Testing Loss 0.0058401378765491315; Training Loss 0.004725238014048042\n",
      "Episode 6400; Testing Loss 0.005840133990592623; Training Loss 0.004725225086617017\n",
      "Episode 6401; Testing Loss 0.005840240885498587; Training Loss 0.004725220072005784\n",
      "Episode 6402; Testing Loss 0.00584032396686918; Training Loss 0.004725211049654483\n",
      "Episode 6403; Testing Loss 0.005840289005449134; Training Loss 0.004725200516377147\n",
      "Episode 6404; Testing Loss 0.0058402314258292716; Training Loss 0.004725188681394287\n",
      "Episode 6405; Testing Loss 0.005840222462273306; Training Loss 0.004725177279043346\n",
      "Episode 6406; Testing Loss 0.0058401925471075755; Training Loss 0.004725169272474684\n",
      "Episode 6407; Testing Loss 0.005840116086168651; Training Loss 0.004725160539714052\n",
      "Episode 6408; Testing Loss 0.005840037968378251; Training Loss 0.004725149362972436\n",
      "Episode 6409; Testing Loss 0.005840051728236825; Training Loss 0.004725138931963865\n",
      "Episode 6410; Testing Loss 0.005840088951546791; Training Loss 0.004725132165286801\n",
      "Episode 6411; Testing Loss 0.005840210774679638; Training Loss 0.004725125300518462\n",
      "Episode 6412; Testing Loss 0.005840233322513425; Training Loss 0.004725112315371259\n",
      "Episode 6413; Testing Loss 0.005840119590188789; Training Loss 0.004725100625325408\n",
      "Episode 6414; Testing Loss 0.005840044615579005; Training Loss 0.004725093239690188\n",
      "Episode 6415; Testing Loss 0.005840098680536813; Training Loss 0.004725087246485673\n",
      "Episode 6416; Testing Loss 0.005840092946256655; Training Loss 0.00472507249073539\n",
      "Episode 6417; Testing Loss 0.0058400633389373395; Training Loss 0.004725062312949249\n",
      "Episode 6418; Testing Loss 0.0058400749076487355; Training Loss 0.0047250554980328135\n",
      "Episode 6419; Testing Loss 0.005840113087029803; Training Loss 0.00472504614629314\n",
      "Episode 6420; Testing Loss 0.005840084320190269; Training Loss 0.0047250360606489765\n",
      "Episode 6421; Testing Loss 0.0058400236587373; Training Loss 0.00472502277633397\n",
      "Episode 6422; Testing Loss 0.005840054789335017; Training Loss 0.00472501502222395\n",
      "Episode 6423; Testing Loss 0.005840069141955809; Training Loss 0.004725005931382984\n",
      "Episode 6424; Testing Loss 0.005840015836371174; Training Loss 0.0047249942403371934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6425; Testing Loss 0.005839991101724285; Training Loss 0.00472498693090805\n",
      "Episode 6426; Testing Loss 0.005840045317469313; Training Loss 0.004724978513023173\n",
      "Episode 6427; Testing Loss 0.00584006557488053; Training Loss 0.004724967777537969\n",
      "Episode 6428; Testing Loss 0.0058400027377710235; Training Loss 0.004724955451414861\n",
      "Episode 6429; Testing Loss 0.005839940717205307; Training Loss 0.004724946356139276\n",
      "Episode 6430; Testing Loss 0.005839987388360467; Training Loss 0.004724936429787981\n",
      "Episode 6431; Testing Loss 0.005840068064871724; Training Loss 0.004724927338283394\n",
      "Episode 6432; Testing Loss 0.005840090764959138; Training Loss 0.004724917055389087\n",
      "Episode 6433; Testing Loss 0.00584002933461114; Training Loss 0.004724907263400828\n",
      "Episode 6434; Testing Loss 0.00584000569420496; Training Loss 0.004724897511918302\n",
      "Episode 6435; Testing Loss 0.0058400327126511215; Training Loss 0.004724888246761278\n",
      "Episode 6436; Testing Loss 0.005839973336570239; Training Loss 0.004724878481172614\n",
      "Episode 6437; Testing Loss 0.005839940437438768; Training Loss 0.004724870744305652\n",
      "Episode 6438; Testing Loss 0.005839966193491057; Training Loss 0.004724859054037383\n",
      "Episode 6439; Testing Loss 0.005840057896826185; Training Loss 0.004724850166321275\n",
      "Episode 6440; Testing Loss 0.005840004947303397; Training Loss 0.0047248387118791774\n",
      "Episode 6441; Testing Loss 0.005839951345434404; Training Loss 0.0047248323615363\n",
      "Episode 6442; Testing Loss 0.0058399499469392584; Training Loss 0.004724820567634006\n",
      "Episode 6443; Testing Loss 0.005840019529834899; Training Loss 0.004724811831570347\n",
      "Episode 6444; Testing Loss 0.005839952253650483; Training Loss 0.0047248021458604\n",
      "Episode 6445; Testing Loss 0.005839883261815781; Training Loss 0.004724793381150932\n",
      "Episode 6446; Testing Loss 0.0058399375149381; Training Loss 0.004724782058785706\n",
      "Episode 6447; Testing Loss 0.005839965836633853; Training Loss 0.004724773278011804\n",
      "Episode 6448; Testing Loss 0.005839873332092939; Training Loss 0.004724763443819507\n",
      "Episode 6449; Testing Loss 0.005839864391534384; Training Loss 0.004724754334349691\n",
      "Episode 6450; Testing Loss 0.005840015677960244; Training Loss 0.004724743627055628\n",
      "Episode 6451; Testing Loss 0.005839994843374825; Training Loss 0.004724735464051154\n",
      "Episode 6452; Testing Loss 0.00583984913892341; Training Loss 0.004724728276523255\n",
      "Episode 6453; Testing Loss 0.0058398521634663815; Training Loss 0.0047247159070063\n",
      "Episode 6454; Testing Loss 0.0058399693638096686; Training Loss 0.004724707995254567\n",
      "Episode 6455; Testing Loss 0.005839897415039835; Training Loss 0.004724698577037743\n",
      "Episode 6456; Testing Loss 0.005839684067906868; Training Loss 0.004724689796686166\n",
      "Episode 6457; Testing Loss 0.005839714867990425; Training Loss 0.00472468243788073\n",
      "Episode 6458; Testing Loss 0.005839877125898334; Training Loss 0.004724669449587802\n",
      "Episode 6459; Testing Loss 0.005839892782695477; Training Loss 0.004724661529930365\n",
      "Episode 6460; Testing Loss 0.005839754777360744; Training Loss 0.004724652694566191\n",
      "Episode 6461; Testing Loss 0.0058397709095248775; Training Loss 0.004724641967263296\n",
      "Episode 6462; Testing Loss 0.005839949318486123; Training Loss 0.00472463022220123\n",
      "Episode 6463; Testing Loss 0.005839943013952755; Training Loss 0.004724623614689265\n",
      "Episode 6464; Testing Loss 0.0058397424112803095; Training Loss 0.004724611792405316\n",
      "Episode 6465; Testing Loss 0.005839631467020353; Training Loss 0.004724602841301594\n",
      "Episode 6466; Testing Loss 0.005839782490091484; Training Loss 0.004724593707659377\n",
      "Episode 6467; Testing Loss 0.005839897550201744; Training Loss 0.00472458892779482\n",
      "Episode 6468; Testing Loss 0.005839671503428454; Training Loss 0.004724573450089342\n",
      "Episode 6469; Testing Loss 0.005839555100042643; Training Loss 0.00472456652598344\n",
      "Episode 6470; Testing Loss 0.005839767410375028; Training Loss 0.004724558862620476\n",
      "Episode 6471; Testing Loss 0.005839896700306404; Training Loss 0.00472455058635897\n",
      "Episode 6472; Testing Loss 0.005839713920574506; Training Loss 0.004724533386622031\n",
      "Episode 6473; Testing Loss 0.005839644198425592; Training Loss 0.004724532065031639\n",
      "Episode 6474; Testing Loss 0.005839873547086784; Training Loss 0.0047245192446163185\n",
      "Episode 6475; Testing Loss 0.005840010037640928; Training Loss 0.004724513149491967\n",
      "Episode 6476; Testing Loss 0.0058397124914044895; Training Loss 0.004724494529989543\n",
      "Episode 6477; Testing Loss 0.0058395000182152965; Training Loss 0.0047244900564463\n",
      "Episode 6478; Testing Loss 0.005839639269011959; Training Loss 0.0047244786208914214\n",
      "Episode 6479; Testing Loss 0.00583982760748034; Training Loss 0.004724468811826944\n",
      "Episode 6480; Testing Loss 0.005839737314755027; Training Loss 0.0047244589488889465\n",
      "Episode 6481; Testing Loss 0.005839630040386143; Training Loss 0.004724452270475026\n",
      "Episode 6482; Testing Loss 0.005839733207967754; Training Loss 0.004724440518664962\n",
      "Episode 6483; Testing Loss 0.005839865308104049; Training Loss 0.004724430054929115\n",
      "Episode 6484; Testing Loss 0.00583978490084939; Training Loss 0.004724420950695208\n",
      "Episode 6485; Testing Loss 0.005839593444160514; Training Loss 0.004724413074487152\n",
      "Episode 6486; Testing Loss 0.005839644563368531; Training Loss 0.004724400567916987\n",
      "Episode 6487; Testing Loss 0.005839811325907623; Training Loss 0.00472439162814048\n",
      "Episode 6488; Testing Loss 0.0058398088932843815; Training Loss 0.004724382171413896\n",
      "Episode 6489; Testing Loss 0.005839673224772325; Training Loss 0.004724371634819792\n",
      "Episode 6490; Testing Loss 0.005839638443753618; Training Loss 0.004724361604562695\n",
      "Episode 6491; Testing Loss 0.005839694341660355; Training Loss 0.004724352130115907\n",
      "Episode 6492; Testing Loss 0.005839751706338095; Training Loss 0.00472434372321044\n",
      "Episode 6493; Testing Loss 0.005839718533449652; Training Loss 0.004724333014967695\n",
      "Episode 6494; Testing Loss 0.005839620276278946; Training Loss 0.0047243225883299555\n",
      "Episode 6495; Testing Loss 0.005839604099470924; Training Loss 0.004724312872338584\n",
      "Episode 6496; Testing Loss 0.005839636845990737; Training Loss 0.00472430364099767\n",
      "Episode 6497; Testing Loss 0.0058396192467203975; Training Loss 0.004724294855766873\n",
      "Episode 6498; Testing Loss 0.00583957731922974; Training Loss 0.004724284924510944\n",
      "Episode 6499; Testing Loss 0.005839569929377193; Training Loss 0.004724277718213547\n",
      "Episode 6500; Testing Loss 0.0058395749020363095; Training Loss 0.004724266893136849\n",
      "Episode 6501; Testing Loss 0.005839591867277028; Training Loss 0.004724256968782006\n",
      "Episode 6502; Testing Loss 0.005839660628810911; Training Loss 0.004724249163947318\n",
      "Episode 6503; Testing Loss 0.005839668334121205; Training Loss 0.004724240198218378\n",
      "Episode 6504; Testing Loss 0.005839598117197339; Training Loss 0.00472422833065181\n",
      "Episode 6505; Testing Loss 0.005839581139351429; Training Loss 0.004724221026789264\n",
      "Episode 6506; Testing Loss 0.005839521731808283; Training Loss 0.00472421035038227\n",
      "Episode 6507; Testing Loss 0.005839483470437182; Training Loss 0.004724198340055499\n",
      "Episode 6508; Testing Loss 0.005839552503659872; Training Loss 0.00472419109607577\n",
      "Episode 6509; Testing Loss 0.005839572693064056; Training Loss 0.004724179067351872\n",
      "Episode 6510; Testing Loss 0.005839518750466528; Training Loss 0.004724171782903584\n",
      "Episode 6511; Testing Loss 0.005839550101787543; Training Loss 0.004724161179994832\n",
      "Episode 6512; Testing Loss 0.005839599398772265; Training Loss 0.00472415077647236\n",
      "Episode 6513; Testing Loss 0.005839612956595659; Training Loss 0.004724144375794781\n",
      "Episode 6514; Testing Loss 0.00583953796579562; Training Loss 0.004724132347692669\n",
      "Episode 6515; Testing Loss 0.005839506189142311; Training Loss 0.004724122893336444\n",
      "Episode 6516; Testing Loss 0.005839523813242763; Training Loss 0.004724113432394813\n",
      "Episode 6517; Testing Loss 0.005839481648966375; Training Loss 0.004724104827653589\n",
      "Episode 6518; Testing Loss 0.005839388130978174; Training Loss 0.004724093816100124\n",
      "Episode 6519; Testing Loss 0.005839409695387273; Training Loss 0.004724085172955456\n",
      "Episode 6520; Testing Loss 0.005839538736162996; Training Loss 0.004724074553620705\n",
      "Episode 6521; Testing Loss 0.0058395512930755; Training Loss 0.004724069281774035\n",
      "Episode 6522; Testing Loss 0.0058394013832423555; Training Loss 0.004724056532542761\n",
      "Episode 6523; Testing Loss 0.005839352368731443; Training Loss 0.004724050722447767\n",
      "Episode 6524; Testing Loss 0.005839506323099191; Training Loss 0.004724040841287624\n",
      "Episode 6525; Testing Loss 0.005839592004699632; Training Loss 0.004724035412518817\n",
      "Episode 6526; Testing Loss 0.005839423702870484; Training Loss 0.00472402237507872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6527; Testing Loss 0.005839269590857667; Training Loss 0.004724010684538069\n",
      "Episode 6528; Testing Loss 0.00583924525265664; Training Loss 0.00472400246034747\n",
      "Episode 6529; Testing Loss 0.0058393407172883955; Training Loss 0.004723990605342512\n",
      "Episode 6530; Testing Loss 0.005839456844886508; Training Loss 0.00472398249080303\n",
      "Episode 6531; Testing Loss 0.005839441226175375; Training Loss 0.004723972727006324\n",
      "Episode 6532; Testing Loss 0.005839413377293716; Training Loss 0.0047239613143745625\n",
      "Episode 6533; Testing Loss 0.0058393539859575945; Training Loss 0.0047239541263491705\n",
      "Episode 6534; Testing Loss 0.005839333682394627; Training Loss 0.004723942446659731\n",
      "Episode 6535; Testing Loss 0.005839417294004837; Training Loss 0.004723932307538334\n",
      "Episode 6536; Testing Loss 0.005839505029920596; Training Loss 0.004723923762964203\n",
      "Episode 6537; Testing Loss 0.005839436299377878; Training Loss 0.004723914326706514\n",
      "Episode 6538; Testing Loss 0.005839308067695095; Training Loss 0.004723905323395341\n",
      "Episode 6539; Testing Loss 0.00583927613446655; Training Loss 0.004723895466757654\n",
      "Episode 6540; Testing Loss 0.005839424387766176; Training Loss 0.004723885997778711\n",
      "Episode 6541; Testing Loss 0.005839439713211461; Training Loss 0.004723876016570779\n",
      "Episode 6542; Testing Loss 0.005839281952237287; Training Loss 0.004723866269953567\n",
      "Episode 6543; Testing Loss 0.005839239973803979; Training Loss 0.004723857096607529\n",
      "Episode 6544; Testing Loss 0.0058393736962293625; Training Loss 0.004723846846188113\n",
      "Episode 6545; Testing Loss 0.005839407828576691; Training Loss 0.004723839062310218\n",
      "Episode 6546; Testing Loss 0.00583932263742483; Training Loss 0.004723828779593574\n",
      "Episode 6547; Testing Loss 0.005839257083176759; Training Loss 0.004723818705036962\n",
      "Episode 6548; Testing Loss 0.005839269962482791; Training Loss 0.004723810301738942\n",
      "Episode 6549; Testing Loss 0.005839306093088797; Training Loss 0.00472379918263931\n",
      "Episode 6550; Testing Loss 0.005839320066899983; Training Loss 0.004723789774898157\n",
      "Episode 6551; Testing Loss 0.00583927967662161; Training Loss 0.004723782367060548\n",
      "Episode 6552; Testing Loss 0.005839139049699692; Training Loss 0.004723771866377275\n",
      "Episode 6553; Testing Loss 0.005839286993901431; Training Loss 0.004723761279723138\n",
      "Episode 6554; Testing Loss 0.005839340996340682; Training Loss 0.004723752840729352\n",
      "Episode 6555; Testing Loss 0.0058393029926417424; Training Loss 0.004723742585158565\n",
      "Episode 6556; Testing Loss 0.005839261098625936; Training Loss 0.004723732075289651\n",
      "Episode 6557; Testing Loss 0.005839138545223777; Training Loss 0.004723724133953747\n",
      "Episode 6558; Testing Loss 0.0058391808336365385; Training Loss 0.004723715664428441\n",
      "Episode 6559; Testing Loss 0.005839154547158642; Training Loss 0.00472370580818977\n",
      "Episode 6560; Testing Loss 0.005839195358428883; Training Loss 0.004723696653748426\n",
      "Episode 6561; Testing Loss 0.005839234254221243; Training Loss 0.004723688643853422\n",
      "Episode 6562; Testing Loss 0.0058391591713577; Training Loss 0.004723677338037728\n",
      "Episode 6563; Testing Loss 0.00583909363318706; Training Loss 0.004723671063699768\n",
      "Episode 6564; Testing Loss 0.005839172301615851; Training Loss 0.004723660873019463\n",
      "Episode 6565; Testing Loss 0.0058393416729741335; Training Loss 0.004723651440644493\n",
      "Episode 6566; Testing Loss 0.005839205960180067; Training Loss 0.0047236399179759695\n",
      "Episode 6567; Testing Loss 0.0058389192750422985; Training Loss 0.004723630832500186\n",
      "Episode 6568; Testing Loss 0.005838921777562473; Training Loss 0.004723622917499292\n",
      "Episode 6569; Testing Loss 0.005839286683043979; Training Loss 0.004723617009715092\n",
      "Episode 6570; Testing Loss 0.005839312646552084; Training Loss 0.0047236036019952035\n",
      "Episode 6571; Testing Loss 0.005838990985861921; Training Loss 0.004723594940098456\n",
      "Episode 6572; Testing Loss 0.005838938585224008; Training Loss 0.004723587563489274\n",
      "Episode 6573; Testing Loss 0.0058392170393691415; Training Loss 0.004723576676698784\n",
      "Episode 6574; Testing Loss 0.005839316208479996; Training Loss 0.004723567493832802\n",
      "Episode 6575; Testing Loss 0.005839075736641869; Training Loss 0.004723551775127792\n",
      "Episode 6576; Testing Loss 0.005838855972951413; Training Loss 0.004723548958338142\n",
      "Episode 6577; Testing Loss 0.005838961811981827; Training Loss 0.004723533697357909\n",
      "Episode 6578; Testing Loss 0.0058391669443144355; Training Loss 0.004723527955115521\n",
      "Episode 6579; Testing Loss 0.005839161612783678; Training Loss 0.004723518730073462\n",
      "Episode 6580; Testing Loss 0.005838958182294367; Training Loss 0.004723504053775967\n",
      "Episode 6581; Testing Loss 0.00583885256452454; Training Loss 0.0047234988382642935\n",
      "Episode 6582; Testing Loss 0.005838905959740468; Training Loss 0.0047234887080086895\n",
      "Episode 6583; Testing Loss 0.005839106016191372; Training Loss 0.004723476674367307\n",
      "Episode 6584; Testing Loss 0.005839120424839429; Training Loss 0.0047234685413543925\n",
      "Episode 6585; Testing Loss 0.005838899896642243; Training Loss 0.004723457704883589\n",
      "Episode 6586; Testing Loss 0.005838820094747014; Training Loss 0.004723447985513756\n",
      "Episode 6587; Testing Loss 0.005838956397356418; Training Loss 0.004723436469898508\n",
      "Episode 6588; Testing Loss 0.005839115657161457; Training Loss 0.00472343048776111\n",
      "Episode 6589; Testing Loss 0.00583894374108919; Training Loss 0.0047234190740466035\n",
      "Episode 6590; Testing Loss 0.005838834952948719; Training Loss 0.0047234105969487196\n",
      "Episode 6591; Testing Loss 0.0058389487784915265; Training Loss 0.004723400720898237\n",
      "Episode 6592; Testing Loss 0.005838976085572433; Training Loss 0.004723389864002524\n",
      "Episode 6593; Testing Loss 0.005838897042183884; Training Loss 0.0047233796776088455\n",
      "Episode 6594; Testing Loss 0.00583892506170953; Training Loss 0.004723369992845305\n",
      "Episode 6595; Testing Loss 0.0058389978171818286; Training Loss 0.004723362286972313\n",
      "Episode 6596; Testing Loss 0.005838873491237173; Training Loss 0.004723350641049339\n",
      "Episode 6597; Testing Loss 0.005838853900752114; Training Loss 0.0047233407646974125\n",
      "Episode 6598; Testing Loss 0.005838957417937757; Training Loss 0.004723333284657699\n",
      "Episode 6599; Testing Loss 0.005838891787070212; Training Loss 0.00472332118023297\n",
      "Episode 6600; Testing Loss 0.005838768388369471; Training Loss 0.0047233138876723\n",
      "Episode 6601; Testing Loss 0.005838821472158803; Training Loss 0.004723304962174452\n",
      "Episode 6602; Testing Loss 0.005838894865375271; Training Loss 0.0047232928362076305\n",
      "Episode 6603; Testing Loss 0.005838872203588261; Training Loss 0.004723286045692707\n",
      "Episode 6604; Testing Loss 0.0058387843853547105; Training Loss 0.004723275250229898\n",
      "Episode 6605; Testing Loss 0.005838809604897569; Training Loss 0.004723264985765479\n",
      "Episode 6606; Testing Loss 0.005838819023884237; Training Loss 0.004723254708008724\n",
      "Episode 6607; Testing Loss 0.005838774247242624; Training Loss 0.004723245003080175\n",
      "Episode 6608; Testing Loss 0.00583875271923069; Training Loss 0.004723236047243869\n",
      "Episode 6609; Testing Loss 0.005838740975740482; Training Loss 0.00472322576937905\n",
      "Episode 6610; Testing Loss 0.005838715853444093; Training Loss 0.004723218458648851\n",
      "Episode 6611; Testing Loss 0.005838687611374646; Training Loss 0.004723207870371507\n",
      "Episode 6612; Testing Loss 0.005838737827698409; Training Loss 0.004723198132502633\n",
      "Episode 6613; Testing Loss 0.005838658426238815; Training Loss 0.0047231879811097275\n",
      "Episode 6614; Testing Loss 0.0058386142219535855; Training Loss 0.0047231801402341286\n",
      "Episode 6615; Testing Loss 0.005838747504075047; Training Loss 0.004723169554539626\n",
      "Episode 6616; Testing Loss 0.00583880988607968; Training Loss 0.0047231618884077484\n",
      "Episode 6617; Testing Loss 0.005838629847446807; Training Loss 0.004723149382865728\n",
      "Episode 6618; Testing Loss 0.005838579956952472; Training Loss 0.004723142684872618\n",
      "Episode 6619; Testing Loss 0.005838707509724674; Training Loss 0.004723131525774705\n",
      "Episode 6620; Testing Loss 0.005838775804448957; Training Loss 0.004723124612707077\n",
      "Episode 6621; Testing Loss 0.005838579781376776; Training Loss 0.0047231124262021515\n",
      "Episode 6622; Testing Loss 0.005838536364844545; Training Loss 0.004723104299179796\n",
      "Episode 6623; Testing Loss 0.005838648669314798; Training Loss 0.004723092385579183\n",
      "Episode 6624; Testing Loss 0.005838722002284279; Training Loss 0.004723086235269436\n",
      "Episode 6625; Testing Loss 0.005838715165642066; Training Loss 0.004723074465276985\n",
      "Episode 6626; Testing Loss 0.005838587810935453; Training Loss 0.004723066806606912\n",
      "Episode 6627; Testing Loss 0.005838527188515306; Training Loss 0.004723055819798211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6628; Testing Loss 0.005838590399726994; Training Loss 0.004723046788180639\n",
      "Episode 6629; Testing Loss 0.005838699601697264; Training Loss 0.004723040143731995\n",
      "Episode 6630; Testing Loss 0.00583861180718892; Training Loss 0.00472302897507826\n",
      "Episode 6631; Testing Loss 0.005838561346312648; Training Loss 0.00472302060318084\n",
      "Episode 6632; Testing Loss 0.005838574527114396; Training Loss 0.004723007519518934\n",
      "Episode 6633; Testing Loss 0.005838577500091635; Training Loss 0.004723003119383177\n",
      "Episode 6634; Testing Loss 0.005838480704688707; Training Loss 0.00472299487895032\n",
      "Episode 6635; Testing Loss 0.005838430394865006; Training Loss 0.004722982531100055\n",
      "Episode 6636; Testing Loss 0.005838552392324468; Training Loss 0.004722973833893562\n",
      "Episode 6637; Testing Loss 0.0058385296807266914; Training Loss 0.004722962873767398\n",
      "Episode 6638; Testing Loss 0.005838413404632384; Training Loss 0.004722953845420494\n",
      "Episode 6639; Testing Loss 0.005838394164619125; Training Loss 0.004722945083123713\n",
      "Episode 6640; Testing Loss 0.00583852284750356; Training Loss 0.004722933556389612\n",
      "Episode 6641; Testing Loss 0.005838616553981738; Training Loss 0.004722926410755933\n",
      "Episode 6642; Testing Loss 0.005838486857754042; Training Loss 0.004722916784349653\n",
      "Episode 6643; Testing Loss 0.005838344277464752; Training Loss 0.004722904430884179\n",
      "Episode 6644; Testing Loss 0.005838405544879336; Training Loss 0.004722897084759783\n",
      "Episode 6645; Testing Loss 0.0058384832518911575; Training Loss 0.0047228871711746155\n",
      "Episode 6646; Testing Loss 0.0058384358630344056; Training Loss 0.004722874728116407\n",
      "Episode 6647; Testing Loss 0.005838390548633052; Training Loss 0.00472286942920858\n",
      "Episode 6648; Testing Loss 0.005838488343340064; Training Loss 0.004722860395167737\n",
      "Episode 6649; Testing Loss 0.005838521975948096; Training Loss 0.004722848487963021\n",
      "Episode 6650; Testing Loss 0.005838396738115325; Training Loss 0.004722838030271638\n",
      "Episode 6651; Testing Loss 0.005838299046228547; Training Loss 0.004722830218769465\n",
      "Episode 6652; Testing Loss 0.00583835385608462; Training Loss 0.004722820537428987\n",
      "Episode 6653; Testing Loss 0.0058383902444606155; Training Loss 0.0047228088886894035\n",
      "Episode 6654; Testing Loss 0.005838426850382681; Training Loss 0.00472279864067622\n",
      "Episode 6655; Testing Loss 0.005838455568948163; Training Loss 0.004722789876008187\n",
      "Episode 6656; Testing Loss 0.0058384407721434675; Training Loss 0.004722778663298697\n",
      "Episode 6657; Testing Loss 0.005838389776802246; Training Loss 0.0047227698396351955\n",
      "Episode 6658; Testing Loss 0.005838263220682073; Training Loss 0.004722763677056662\n",
      "Episode 6659; Testing Loss 0.005838187520746585; Training Loss 0.004722753836684611\n",
      "Episode 6660; Testing Loss 0.005838250143033329; Training Loss 0.004722742998623\n",
      "Episode 6661; Testing Loss 0.0058383801568355044; Training Loss 0.004722732546123294\n",
      "Episode 6662; Testing Loss 0.0058384074003474075; Training Loss 0.004722725086009299\n",
      "Episode 6663; Testing Loss 0.005838261452226977; Training Loss 0.004722712731463362\n",
      "Episode 6664; Testing Loss 0.005838226739093594; Training Loss 0.004722705212518059\n",
      "Episode 6665; Testing Loss 0.005838386710034915; Training Loss 0.004722695837912011\n",
      "Episode 6666; Testing Loss 0.005838402652830912; Training Loss 0.004722686370999009\n",
      "Episode 6667; Testing Loss 0.00583826565648999; Training Loss 0.004722674085109186\n",
      "Episode 6668; Testing Loss 0.005838168915845428; Training Loss 0.004722668304169879\n",
      "Episode 6669; Testing Loss 0.005838167780530443; Training Loss 0.00472265729479337\n",
      "Episode 6670; Testing Loss 0.005838180892019736; Training Loss 0.004722646400819768\n",
      "Episode 6671; Testing Loss 0.00583830197460185; Training Loss 0.0047226394813572065\n",
      "Episode 6672; Testing Loss 0.005838392365883895; Training Loss 0.004722629582378082\n",
      "Episode 6673; Testing Loss 0.00583826280293296; Training Loss 0.004722614866822093\n",
      "Episode 6674; Testing Loss 0.005838108179520801; Training Loss 0.004722610180930408\n",
      "Episode 6675; Testing Loss 0.005838117440682843; Training Loss 0.004722599399869124\n",
      "Episode 6676; Testing Loss 0.005838207305781886; Training Loss 0.004722588393903441\n",
      "Episode 6677; Testing Loss 0.005838301697112811; Training Loss 0.0047225821419806\n",
      "Episode 6678; Testing Loss 0.005838255958907141; Training Loss 0.004722570158917684\n",
      "Episode 6679; Testing Loss 0.005838147976595178; Training Loss 0.004722559276063553\n",
      "Episode 6680; Testing Loss 0.005838096528091123; Training Loss 0.004722551473514143\n",
      "Episode 6681; Testing Loss 0.005838190653470149; Training Loss 0.004722539191406772\n",
      "Episode 6682; Testing Loss 0.005838278702493608; Training Loss 0.004722531556120642\n",
      "Episode 6683; Testing Loss 0.005838197235096857; Training Loss 0.0047225229915117935\n",
      "Episode 6684; Testing Loss 0.005838044570329888; Training Loss 0.00472251363505439\n",
      "Episode 6685; Testing Loss 0.005838052761311963; Training Loss 0.0047224999830873275\n",
      "Episode 6686; Testing Loss 0.005838114549377467; Training Loss 0.004722491354113133\n",
      "Episode 6687; Testing Loss 0.00583807247733762; Training Loss 0.0047224837832096096\n",
      "Episode 6688; Testing Loss 0.005838138582943612; Training Loss 0.004722474204567546\n",
      "Episode 6689; Testing Loss 0.005838198966631436; Training Loss 0.004722464944028499\n",
      "Episode 6690; Testing Loss 0.005838130062391855; Training Loss 0.004722455910245245\n",
      "Episode 6691; Testing Loss 0.005837933389552335; Training Loss 0.004722442643542911\n",
      "Episode 6692; Testing Loss 0.005837876304080579; Training Loss 0.004722439028443016\n",
      "Episode 6693; Testing Loss 0.00583805824875077; Training Loss 0.004722429378561711\n",
      "Episode 6694; Testing Loss 0.005838103745789212; Training Loss 0.0047224156188345875\n",
      "Episode 6695; Testing Loss 0.005837948227695849; Training Loss 0.00472240454541767\n",
      "Episode 6696; Testing Loss 0.005837911259354487; Training Loss 0.004722402780564347\n",
      "Episode 6697; Testing Loss 0.005837979086484481; Training Loss 0.00472239123742762\n",
      "Episode 6698; Testing Loss 0.005837989003970742; Training Loss 0.004722371253264023\n",
      "Episode 6699; Testing Loss 0.005837933991284064; Training Loss 0.0047223716934713215\n",
      "Episode 6700; Testing Loss 0.005837909306956331; Training Loss 0.004722368561620221\n",
      "Episode 6701; Testing Loss 0.005837966586791807; Training Loss 0.0047223569195609825\n",
      "Episode 6702; Testing Loss 0.005837951020892621; Training Loss 0.004722339185973909\n",
      "Episode 6703; Testing Loss 0.0058378774983331196; Training Loss 0.004722330688455383\n",
      "Episode 6704; Testing Loss 0.005837913737580431; Training Loss 0.004722323788681059\n",
      "Episode 6705; Testing Loss 0.005837908573254224; Training Loss 0.004722319654731463\n",
      "Episode 6706; Testing Loss 0.005837855638603902; Training Loss 0.004722303433430236\n",
      "Episode 6707; Testing Loss 0.005837796883710295; Training Loss 0.004722281436312006\n",
      "Episode 6708; Testing Loss 0.005837777889756335; Training Loss 0.0047222818521396965\n",
      "Episode 6709; Testing Loss 0.005837773000243092; Training Loss 0.004722276343135521\n",
      "Episode 6710; Testing Loss 0.00583777630373969; Training Loss 0.004722262610718869\n",
      "Episode 6711; Testing Loss 0.005837723384757837; Training Loss 0.004722242984493213\n",
      "Episode 6712; Testing Loss 0.0058376879069570895; Training Loss 0.004722241563239102\n",
      "Episode 6713; Testing Loss 0.005837770919481005; Training Loss 0.004722241358139121\n",
      "Episode 6714; Testing Loss 0.005837810331809811; Training Loss 0.004722230142944879\n",
      "Episode 6715; Testing Loss 0.005837743544421302; Training Loss 0.004722211647628485\n",
      "Episode 6716; Testing Loss 0.005837732694165178; Training Loss 0.004722197429930245\n",
      "Episode 6717; Testing Loss 0.005837776997468787; Training Loss 0.0047221944926826975\n",
      "Episode 6718; Testing Loss 0.005837736720067698; Training Loss 0.0047221883793110144\n",
      "Episode 6719; Testing Loss 0.005837657833587067; Training Loss 0.004722174054147711\n",
      "Episode 6720; Testing Loss 0.005837651716279828; Training Loss 0.004722155934277544\n",
      "Episode 6721; Testing Loss 0.005837632241715703; Training Loss 0.004722151800322867\n",
      "Episode 6722; Testing Loss 0.005837633484401826; Training Loss 0.004722146279189531\n",
      "Episode 6723; Testing Loss 0.005837698894417616; Training Loss 0.004722136650277879\n",
      "Episode 6724; Testing Loss 0.005837735131990818; Training Loss 0.004722122254676589\n",
      "Episode 6725; Testing Loss 0.005837617816450385; Training Loss 0.004722107027097325\n",
      "Episode 6726; Testing Loss 0.00583752020166871; Training Loss 0.004722104285363294\n",
      "Episode 6727; Testing Loss 0.0058375144657074945; Training Loss 0.004722098203814642\n",
      "Episode 6728; Testing Loss 0.005837563502884426; Training Loss 0.00472208711987371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6729; Testing Loss 0.0058375443912492; Training Loss 0.004722071355094457\n",
      "Episode 6730; Testing Loss 0.005837462540435922; Training Loss 0.004722059247118161\n",
      "Episode 6731; Testing Loss 0.005837545388010823; Training Loss 0.004722052510489757\n",
      "Episode 6732; Testing Loss 0.005837729875186534; Training Loss 0.0047220454182197655\n",
      "Episode 6733; Testing Loss 0.005837751083121569; Training Loss 0.004722033851527001\n",
      "Episode 6734; Testing Loss 0.005837567492735225; Training Loss 0.004722015716751458\n",
      "Episode 6735; Testing Loss 0.005837438112011165; Training Loss 0.004722013406485299\n",
      "Episode 6736; Testing Loss 0.005837388624143609; Training Loss 0.004722007410004437\n",
      "Episode 6737; Testing Loss 0.005837471704630116; Training Loss 0.004721994608524221\n",
      "Episode 6738; Testing Loss 0.005837503014337597; Training Loss 0.004721977707164853\n",
      "Episode 6739; Testing Loss 0.005837463266298512; Training Loss 0.004721972075463437\n",
      "Episode 6740; Testing Loss 0.005837478922418964; Training Loss 0.00472196402994147\n",
      "Episode 6741; Testing Loss 0.005837472081497749; Training Loss 0.004721953820919994\n",
      "Episode 6742; Testing Loss 0.005837434469138218; Training Loss 0.004721937027131601\n",
      "Episode 6743; Testing Loss 0.005837488733870414; Training Loss 0.004721935188573791\n",
      "Episode 6744; Testing Loss 0.0058375436378132264; Training Loss 0.004721929186108506\n",
      "Episode 6745; Testing Loss 0.005837482354383824; Training Loss 0.004721912687039825\n",
      "Episode 6746; Testing Loss 0.005837354803357554; Training Loss 0.00472189806530672\n",
      "Episode 6747; Testing Loss 0.005837286953697087; Training Loss 0.004721895231825553\n",
      "Episode 6748; Testing Loss 0.005837278327741347; Training Loss 0.004721882713250286\n",
      "Episode 6749; Testing Loss 0.005837337711839483; Training Loss 0.0047218719256690205\n",
      "Episode 6750; Testing Loss 0.005837382039510794; Training Loss 0.004721865407807808\n",
      "Episode 6751; Testing Loss 0.005837362566048203; Training Loss 0.004721852296033185\n",
      "Episode 6752; Testing Loss 0.0058373119425231415; Training Loss 0.004721841992444811\n",
      "Episode 6753; Testing Loss 0.00583737659894681; Training Loss 0.004721836310258754\n",
      "Episode 6754; Testing Loss 0.005837425758217061; Training Loss 0.00472182587464116\n",
      "Episode 6755; Testing Loss 0.005837360238501921; Training Loss 0.00472180796044853\n",
      "Episode 6756; Testing Loss 0.005837263583088362; Training Loss 0.00472180867111357\n",
      "Episode 6757; Testing Loss 0.00583720958209055; Training Loss 0.004721802219958872\n",
      "Episode 6758; Testing Loss 0.005837189991870225; Training Loss 0.004721788153523224\n",
      "Episode 6759; Testing Loss 0.005837187141688008; Training Loss 0.004721772354150491\n",
      "Episode 6760; Testing Loss 0.005837253079137159; Training Loss 0.004721767185147438\n",
      "Episode 6761; Testing Loss 0.005837213885779928; Training Loss 0.0047217562857794215\n",
      "Episode 6762; Testing Loss 0.005837161124318943; Training Loss 0.004721741363635614\n",
      "Episode 6763; Testing Loss 0.005837222691181376; Training Loss 0.004721735382481947\n",
      "Episode 6764; Testing Loss 0.005837348356913612; Training Loss 0.004721724081382682\n",
      "Episode 6765; Testing Loss 0.0058373584589082855; Training Loss 0.004721717412038505\n",
      "Episode 6766; Testing Loss 0.005837226598318502; Training Loss 0.004721705304768678\n",
      "Episode 6767; Testing Loss 0.005837098867411109; Training Loss 0.004721694668326369\n",
      "Episode 6768; Testing Loss 0.005837164669532644; Training Loss 0.004721687073422818\n",
      "Episode 6769; Testing Loss 0.005837194360764346; Training Loss 0.004721678609989351\n",
      "Episode 6770; Testing Loss 0.00583713521997786; Training Loss 0.0047216647421629695\n",
      "Episode 6771; Testing Loss 0.005837116230127071; Training Loss 0.00472165769211383\n",
      "Episode 6772; Testing Loss 0.005837217060600056; Training Loss 0.004721650665286304\n",
      "Episode 6773; Testing Loss 0.0058372710096970945; Training Loss 0.004721639719109387\n",
      "Episode 6774; Testing Loss 0.005837144966941291; Training Loss 0.004721625895299207\n",
      "Episode 6775; Testing Loss 0.005837083683966063; Training Loss 0.004721617603783672\n",
      "Episode 6776; Testing Loss 0.005837083489554439; Training Loss 0.004721610275884733\n",
      "Episode 6777; Testing Loss 0.005837028775561354; Training Loss 0.004721596383717002\n",
      "Episode 6778; Testing Loss 0.005837016922385736; Training Loss 0.004721591905407238\n",
      "Episode 6779; Testing Loss 0.005837090821710063; Training Loss 0.004721583552868072\n",
      "Episode 6780; Testing Loss 0.005837073377912834; Training Loss 0.004721575951680751\n",
      "Episode 6781; Testing Loss 0.005836929983754721; Training Loss 0.004721560332852174\n",
      "Episode 6782; Testing Loss 0.005836926099580184; Training Loss 0.004721550612757448\n",
      "Episode 6783; Testing Loss 0.005837129884868557; Training Loss 0.004721544437910014\n",
      "Episode 6784; Testing Loss 0.005837206143533509; Training Loss 0.004721531048547507\n",
      "Episode 6785; Testing Loss 0.005837064842691909; Training Loss 0.004721519810798145\n",
      "Episode 6786; Testing Loss 0.005836926392678979; Training Loss 0.004721516022430259\n",
      "Episode 6787; Testing Loss 0.005836987966611496; Training Loss 0.004721503641843071\n",
      "Episode 6788; Testing Loss 0.005837134031449446; Training Loss 0.004721490790763734\n",
      "Episode 6789; Testing Loss 0.005837154999090833; Training Loss 0.004721479024066981\n",
      "Episode 6790; Testing Loss 0.005836999910141508; Training Loss 0.004721470357988179\n",
      "Episode 6791; Testing Loss 0.005836886496779226; Training Loss 0.004721459090209409\n",
      "Episode 6792; Testing Loss 0.005837074303633884; Training Loss 0.004721447520918589\n",
      "Episode 6793; Testing Loss 0.005837113842682856; Training Loss 0.004721441821404602\n",
      "Episode 6794; Testing Loss 0.0058368569307712505; Training Loss 0.0047214285907830385\n",
      "Episode 6795; Testing Loss 0.005836775531282463; Training Loss 0.004721421302390407\n",
      "Episode 6796; Testing Loss 0.005836959342358813; Training Loss 0.004721408912680463\n",
      "Episode 6797; Testing Loss 0.005837070823063499; Training Loss 0.004721400973860656\n",
      "Episode 6798; Testing Loss 0.005836909296381063; Training Loss 0.004721391976103606\n",
      "Episode 6799; Testing Loss 0.005836792368337961; Training Loss 0.004721383927406129\n",
      "Episode 6800; Testing Loss 0.005836999327689837; Training Loss 0.004721369728564446\n",
      "Episode 6801; Testing Loss 0.005837138060550563; Training Loss 0.004721369537332753\n",
      "Episode 6802; Testing Loss 0.005836840135455336; Training Loss 0.004721352965932918\n",
      "Episode 6803; Testing Loss 0.005836628302793525; Training Loss 0.004721347417931005\n",
      "Episode 6804; Testing Loss 0.005836871363015574; Training Loss 0.004721334377363419\n",
      "Episode 6805; Testing Loss 0.00583709632321662; Training Loss 0.004721327626042822\n",
      "Episode 6806; Testing Loss 0.005836907123023737; Training Loss 0.004721311582349664\n",
      "Episode 6807; Testing Loss 0.00583669201146371; Training Loss 0.004721305393440655\n",
      "Episode 6808; Testing Loss 0.005836766427587364; Training Loss 0.004721293675219442\n",
      "Episode 6809; Testing Loss 0.005836827138881279; Training Loss 0.004721285687999192\n",
      "Episode 6810; Testing Loss 0.005836709809053275; Training Loss 0.004721277172013056\n",
      "Episode 6811; Testing Loss 0.005836622998813477; Training Loss 0.004721265138395039\n",
      "Episode 6812; Testing Loss 0.005836719927513963; Training Loss 0.004721253220140899\n",
      "Episode 6813; Testing Loss 0.005836764672995668; Training Loss 0.004721245806449258\n",
      "Episode 6814; Testing Loss 0.00583664113978999; Training Loss 0.00472123339517777\n",
      "Episode 6815; Testing Loss 0.005836638833137882; Training Loss 0.004721225415436025\n",
      "Episode 6816; Testing Loss 0.00583673888447607; Training Loss 0.004721215996141632\n",
      "Episode 6817; Testing Loss 0.005836739747815353; Training Loss 0.004721205171141807\n",
      "Episode 6818; Testing Loss 0.0058365652679540385; Training Loss 0.004721196680684526\n",
      "Episode 6819; Testing Loss 0.005836429325099159; Training Loss 0.004721187950039521\n",
      "Episode 6820; Testing Loss 0.00583658702698773; Training Loss 0.004721177636952307\n",
      "Episode 6821; Testing Loss 0.005836773030667769; Training Loss 0.004721172809971689\n",
      "Episode 6822; Testing Loss 0.005836625056568495; Training Loss 0.004721157816042426\n",
      "Episode 6823; Testing Loss 0.005836456915096216; Training Loss 0.004721152387007031\n",
      "Episode 6824; Testing Loss 0.005836472631862905; Training Loss 0.004721140293737566\n",
      "Episode 6825; Testing Loss 0.005836584971623512; Training Loss 0.004721130151841125\n",
      "Episode 6826; Testing Loss 0.0058364458405667195; Training Loss 0.004721124006550446\n",
      "Episode 6827; Testing Loss 0.005836311330422038; Training Loss 0.004721123376758923\n",
      "Episode 6828; Testing Loss 0.0058364645980593715; Training Loss 0.004721111334339963\n",
      "Episode 6829; Testing Loss 0.005836640928714863; Training Loss 0.004721097047943478\n",
      "Episode 6830; Testing Loss 0.005836496481394083; Training Loss 0.0047210809495343686\n",
      "Episode 6831; Testing Loss 0.005836235008238807; Training Loss 0.00472107659903302\n",
      "Episode 6832; Testing Loss 0.005836314182453064; Training Loss 0.0047210605250728715\n",
      "Episode 6833; Testing Loss 0.0058365715022608835; Training Loss 0.004721054297676528\n",
      "Episode 6834; Testing Loss 0.005836582489019106; Training Loss 0.004721048663612638\n",
      "Episode 6835; Testing Loss 0.005836353970903704; Training Loss 0.004721036720158554\n",
      "Episode 6836; Testing Loss 0.005836250066697422; Training Loss 0.004721026175314797\n",
      "Episode 6837; Testing Loss 0.005836306287536244; Training Loss 0.004721014776341577\n",
      "Episode 6838; Testing Loss 0.0058363091769664645; Training Loss 0.004721005417673357\n",
      "Episode 6839; Testing Loss 0.005836305021671664; Training Loss 0.004720992704557813\n",
      "Episode 6840; Testing Loss 0.005836305296504993; Training Loss 0.004720987342158886\n",
      "Episode 6841; Testing Loss 0.005836355784631703; Training Loss 0.004720982359408144\n",
      "Episode 6842; Testing Loss 0.005836333917846881; Training Loss 0.0047209710539199605\n",
      "Episode 6843; Testing Loss 0.0058363469890393435; Training Loss 0.004720955100669582\n",
      "Episode 6844; Testing Loss 0.005836476148801917; Training Loss 0.004720945718163768\n",
      "Episode 6845; Testing Loss 0.0058364779229479885; Training Loss 0.004720938400477269\n",
      "Episode 6846; Testing Loss 0.0058363161817028076; Training Loss 0.004720922950971242\n",
      "Episode 6847; Testing Loss 0.005836167696509201; Training Loss 0.004720919922352142\n",
      "Episode 6848; Testing Loss 0.005836245081577884; Training Loss 0.0047209124318388414\n",
      "Episode 6849; Testing Loss 0.005836348626670711; Training Loss 0.004720905152142201\n",
      "Episode 6850; Testing Loss 0.005836200966170758; Training Loss 0.004720889253845809\n",
      "Episode 6851; Testing Loss 0.005836076446261375; Training Loss 0.004720877671589327\n",
      "Episode 6852; Testing Loss 0.005836153802725528; Training Loss 0.004720868209772192\n",
      "Episode 6853; Testing Loss 0.0058362882875149374; Training Loss 0.004720855526296055\n",
      "Episode 6854; Testing Loss 0.0058363150008294866; Training Loss 0.004720846152448348\n",
      "Episode 6855; Testing Loss 0.005836252287054653; Training Loss 0.0047208368877970155\n",
      "Episode 6856; Testing Loss 0.005836230634128981; Training Loss 0.004720830003536247\n",
      "Episode 6857; Testing Loss 0.005836256165507795; Training Loss 0.0047208168941448555\n",
      "Episode 6858; Testing Loss 0.005836203151455936; Training Loss 0.004720810178793404\n",
      "Episode 6859; Testing Loss 0.005836100258695281; Training Loss 0.004720804464837215\n",
      "Episode 6860; Testing Loss 0.005836171764428094; Training Loss 0.0047207920778636414\n",
      "Episode 6861; Testing Loss 0.005836247806978374; Training Loss 0.004720780678518858\n",
      "Episode 6862; Testing Loss 0.0058360705604135485; Training Loss 0.0047207695931856825\n",
      "Episode 6863; Testing Loss 0.005836066369820383; Training Loss 0.004720759442543966\n",
      "Episode 6864; Testing Loss 0.005836217699747691; Training Loss 0.0047207502015492505\n",
      "Episode 6865; Testing Loss 0.0058363580571742825; Training Loss 0.004720741469930108\n",
      "Episode 6866; Testing Loss 0.005836244597890198; Training Loss 0.0047207327427373335\n",
      "Episode 6867; Testing Loss 0.005836028398951467; Training Loss 0.00472072064574837\n",
      "Episode 6868; Testing Loss 0.005835991615303046; Training Loss 0.0047207161402392775\n",
      "Episode 6869; Testing Loss 0.005836127607640433; Training Loss 0.004720706857224993\n",
      "Episode 6870; Testing Loss 0.005836139716709228; Training Loss 0.004720697241691549\n",
      "Episode 6871; Testing Loss 0.0058360242430356415; Training Loss 0.0047206851282804675\n",
      "Episode 6872; Testing Loss 0.005835946053786379; Training Loss 0.004720676903308303\n",
      "Episode 6873; Testing Loss 0.005836016775673927; Training Loss 0.004720663274804586\n",
      "Episode 6874; Testing Loss 0.005836087210318134; Training Loss 0.0047206556453253555\n",
      "Episode 6875; Testing Loss 0.005836163001705531; Training Loss 0.00472064684958567\n",
      "Episode 6876; Testing Loss 0.005836200454723483; Training Loss 0.0047206370214158905\n",
      "Episode 6877; Testing Loss 0.005836168408169822; Training Loss 0.004720624805928435\n",
      "Episode 6878; Testing Loss 0.005836043739356988; Training Loss 0.004720616798140238\n",
      "Episode 6879; Testing Loss 0.005835921079129184; Training Loss 0.004720605709599301\n",
      "Episode 6880; Testing Loss 0.005835974953280551; Training Loss 0.004720595328179542\n",
      "Episode 6881; Testing Loss 0.005836071780144831; Training Loss 0.004720590558588986\n",
      "Episode 6882; Testing Loss 0.005835935125558894; Training Loss 0.004720577702257339\n",
      "Episode 6883; Testing Loss 0.00583582042185316; Training Loss 0.0047205694790336955\n",
      "Episode 6884; Testing Loss 0.00583597984250991; Training Loss 0.00472055815806579\n",
      "Episode 6885; Testing Loss 0.005836108588004766; Training Loss 0.0047205495072591315\n",
      "Episode 6886; Testing Loss 0.005836004565542564; Training Loss 0.004720539185220877\n",
      "Episode 6887; Testing Loss 0.005835873712539341; Training Loss 0.004720532248472571\n",
      "Episode 6888; Testing Loss 0.005835950432728264; Training Loss 0.004720519758252092\n",
      "Episode 6889; Testing Loss 0.005836105899224855; Training Loss 0.004720511826216449\n",
      "Episode 6890; Testing Loss 0.005835971031668093; Training Loss 0.004720500817927103\n",
      "Episode 6891; Testing Loss 0.005835710087307771; Training Loss 0.004720492637627816\n",
      "Episode 6892; Testing Loss 0.005835745543951948; Training Loss 0.004720483560637026\n",
      "Episode 6893; Testing Loss 0.005836033269121112; Training Loss 0.004720470305648832\n",
      "Episode 6894; Testing Loss 0.005836108770534899; Training Loss 0.004720466047932883\n",
      "Episode 6895; Testing Loss 0.00583585397081727; Training Loss 0.004720453737929543\n",
      "Episode 6896; Testing Loss 0.0058357596575122255; Training Loss 0.004720444024216485\n",
      "Episode 6897; Testing Loss 0.0058359249428092405; Training Loss 0.004720432677585712\n",
      "Episode 6898; Testing Loss 0.0058359113070927515; Training Loss 0.004720425498922122\n",
      "Episode 6899; Testing Loss 0.005835777595914454; Training Loss 0.00472041254900953\n",
      "Episode 6900; Testing Loss 0.005835778264307764; Training Loss 0.004720402454813463\n",
      "Episode 6901; Testing Loss 0.005835866484544417; Training Loss 0.004720395228452462\n",
      "Episode 6902; Testing Loss 0.005835850476137896; Training Loss 0.004720381755112255\n",
      "Episode 6903; Testing Loss 0.005835853915195626; Training Loss 0.004720374279110065\n",
      "Episode 6904; Testing Loss 0.005835797424763047; Training Loss 0.004720362662628858\n",
      "Episode 6905; Testing Loss 0.0058357534545953995; Training Loss 0.00472035679659492\n",
      "Episode 6906; Testing Loss 0.005835775137811323; Training Loss 0.004720351100508111\n",
      "Episode 6907; Testing Loss 0.0058357410909407; Training Loss 0.004720337686352793\n",
      "Episode 6908; Testing Loss 0.005835757388535932; Training Loss 0.004720325564577372\n",
      "Episode 6909; Testing Loss 0.00583584344614137; Training Loss 0.004720319803476823\n",
      "Episode 6910; Testing Loss 0.0058358013168140445; Training Loss 0.004720310807420995\n",
      "Episode 6911; Testing Loss 0.005835668330545705; Training Loss 0.004720296294865491\n",
      "Episode 6912; Testing Loss 0.005835719391537647; Training Loss 0.004720288949251292\n",
      "Episode 6913; Testing Loss 0.005835824778320152; Training Loss 0.004720281862918701\n",
      "Episode 6914; Testing Loss 0.005835704571631387; Training Loss 0.004720269028094461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6915; Testing Loss 0.005835601867759303; Training Loss 0.00472025868901886\n",
      "Episode 6916; Testing Loss 0.005835645016547744; Training Loss 0.004720250917862556\n",
      "Episode 6917; Testing Loss 0.005835671949475495; Training Loss 0.0047202381774439734\n",
      "Episode 6918; Testing Loss 0.005835682720413691; Training Loss 0.0047202320017308355\n",
      "Episode 6919; Testing Loss 0.005835741888350039; Training Loss 0.0047202227365067875\n",
      "Episode 6920; Testing Loss 0.005835717827442263; Training Loss 0.0047202119001251925\n",
      "Episode 6921; Testing Loss 0.0058356119336826695; Training Loss 0.004720199855190509\n",
      "Episode 6922; Testing Loss 0.00583558848603273; Training Loss 0.004720192749826307\n",
      "Episode 6923; Testing Loss 0.005835597462382585; Training Loss 0.004720183080901671\n",
      "Episode 6924; Testing Loss 0.005835539406089215; Training Loss 0.004720172010383647\n",
      "Episode 6925; Testing Loss 0.005835605186505641; Training Loss 0.004720163970897581\n",
      "Episode 6926; Testing Loss 0.005835644644544534; Training Loss 0.004720152381863526\n",
      "Episode 6927; Testing Loss 0.005835610438017148; Training Loss 0.004720142368471496\n",
      "Episode 6928; Testing Loss 0.005835530400517731; Training Loss 0.004720134309028673\n",
      "Episode 6929; Testing Loss 0.0058355831709713596; Training Loss 0.004720124590990425\n",
      "Episode 6930; Testing Loss 0.005835632828529308; Training Loss 0.004720115334969288\n",
      "Episode 6931; Testing Loss 0.005835568353971935; Training Loss 0.004720106700279299\n",
      "Episode 6932; Testing Loss 0.005835496959832643; Training Loss 0.004720095528862534\n",
      "Episode 6933; Testing Loss 0.005835510728732002; Training Loss 0.004720088949680721\n",
      "Episode 6934; Testing Loss 0.0058354746660833744; Training Loss 0.004720081699955634\n",
      "Episode 6935; Testing Loss 0.0058354113555221885; Training Loss 0.004720067867317102\n",
      "Episode 6936; Testing Loss 0.005835500146709384; Training Loss 0.00472005765565613\n",
      "Episode 6937; Testing Loss 0.005835648789661231; Training Loss 0.0047200504552945615\n",
      "Episode 6938; Testing Loss 0.005835557538300713; Training Loss 0.004720039497685965\n",
      "Episode 6939; Testing Loss 0.005835431706708663; Training Loss 0.0047200283885794345\n",
      "Episode 6940; Testing Loss 0.005835455611476707; Training Loss 0.004720020183378864\n",
      "Episode 6941; Testing Loss 0.005835459103628849; Training Loss 0.004720007822739558\n",
      "Episode 6942; Testing Loss 0.005835424304563819; Training Loss 0.004720001928679424\n",
      "Episode 6943; Testing Loss 0.005835420539156986; Training Loss 0.004719996477180535\n",
      "Episode 6944; Testing Loss 0.005835535965849299; Training Loss 0.004719983494712458\n",
      "Episode 6945; Testing Loss 0.005835519313080288; Training Loss 0.004719970760075517\n",
      "Episode 6946; Testing Loss 0.005835370926863515; Training Loss 0.004719968324846321\n",
      "Episode 6947; Testing Loss 0.005835268904324063; Training Loss 0.004719956034886356\n",
      "Episode 6948; Testing Loss 0.0058353608914031695; Training Loss 0.004719942243758531\n",
      "Episode 6949; Testing Loss 0.005835510172778598; Training Loss 0.004719936157999315\n",
      "Episode 6950; Testing Loss 0.005835470101923512; Training Loss 0.004719925416691234\n",
      "Episode 6951; Testing Loss 0.005835334071624524; Training Loss 0.004719913377174301\n",
      "Episode 6952; Testing Loss 0.00583526160060027; Training Loss 0.004719904350968861\n",
      "Episode 6953; Testing Loss 0.005835383574957897; Training Loss 0.004719896140143313\n",
      "Episode 6954; Testing Loss 0.005835455281375873; Training Loss 0.00471988635508222\n",
      "Episode 6955; Testing Loss 0.005835399462147936; Training Loss 0.004719877583550747\n",
      "Episode 6956; Testing Loss 0.005835359803657129; Training Loss 0.004719865676896764\n",
      "Episode 6957; Testing Loss 0.005835336421831847; Training Loss 0.004719853667307093\n",
      "Episode 6958; Testing Loss 0.005835308970578784; Training Loss 0.004719845897694729\n",
      "Episode 6959; Testing Loss 0.0058352558781337455; Training Loss 0.004719835551836732\n",
      "Episode 6960; Testing Loss 0.005835317485644575; Training Loss 0.004719824894846696\n",
      "Episode 6961; Testing Loss 0.0058353699721465455; Training Loss 0.004719819023906912\n",
      "Episode 6962; Testing Loss 0.005835250230791498; Training Loss 0.004719809378737666\n",
      "Episode 6963; Testing Loss 0.005835205834308116; Training Loss 0.004719805962348291\n",
      "Episode 6964; Testing Loss 0.005835349396896814; Training Loss 0.004719794175821222\n",
      "Episode 6965; Testing Loss 0.005835409078606633; Training Loss 0.00471978484531104\n",
      "Episode 6966; Testing Loss 0.005835251792355114; Training Loss 0.004719773422190842\n",
      "Episode 6967; Testing Loss 0.005835096099731562; Training Loss 0.004719764599379005\n",
      "Episode 6968; Testing Loss 0.005835228476646801; Training Loss 0.004719752099610575\n",
      "Episode 6969; Testing Loss 0.005835486977489705; Training Loss 0.004719743224304706\n",
      "Episode 6970; Testing Loss 0.005835445211114279; Training Loss 0.004719733163810538\n",
      "Episode 6971; Testing Loss 0.005835142356896594; Training Loss 0.004719725713652193\n",
      "Episode 6972; Testing Loss 0.005835088328594129; Training Loss 0.004719717555240784\n",
      "Episode 6973; Testing Loss 0.005835319546436895; Training Loss 0.004719704265210831\n",
      "Episode 6974; Testing Loss 0.005835374763173614; Training Loss 0.004719697156332261\n",
      "Episode 6975; Testing Loss 0.005835196864674627; Training Loss 0.004719686154704627\n",
      "Episode 6976; Testing Loss 0.005835095562062052; Training Loss 0.004719678351576439\n",
      "Episode 6977; Testing Loss 0.005835250435403967; Training Loss 0.004719669589993639\n",
      "Episode 6978; Testing Loss 0.0058352523692848; Training Loss 0.004719658817415248\n",
      "Episode 6979; Testing Loss 0.005835078732192788; Training Loss 0.004719647190190963\n",
      "Episode 6980; Testing Loss 0.005835044989706368; Training Loss 0.004719638116660538\n",
      "Episode 6981; Testing Loss 0.0058351432964399855; Training Loss 0.004719628045737799\n",
      "Episode 6982; Testing Loss 0.005835194017169712; Training Loss 0.004719619432983521\n",
      "Episode 6983; Testing Loss 0.005835099467976821; Training Loss 0.004719607644566297\n",
      "Episode 6984; Testing Loss 0.005835077078338336; Training Loss 0.004719600311529463\n",
      "Episode 6985; Testing Loss 0.005835136778053519; Training Loss 0.00471958953878438\n",
      "Episode 6986; Testing Loss 0.005835152491390957; Training Loss 0.00471957913789652\n",
      "Episode 6987; Testing Loss 0.005835147418181968; Training Loss 0.004719570627666861\n",
      "Episode 6988; Testing Loss 0.005835114361438998; Training Loss 0.004719560577009943\n",
      "Episode 6989; Testing Loss 0.005835101881490188; Training Loss 0.004719549844252293\n",
      "Episode 6990; Testing Loss 0.005835051084853383; Training Loss 0.004719540158516994\n",
      "Episode 6991; Testing Loss 0.005835034329962532; Training Loss 0.0047195313718609855\n",
      "Episode 6992; Testing Loss 0.005835102944107826; Training Loss 0.004719520842294947\n",
      "Episode 6993; Testing Loss 0.005835120332295415; Training Loss 0.00471951294285965\n",
      "Episode 6994; Testing Loss 0.005835042472860934; Training Loss 0.004719504487685606\n",
      "Episode 6995; Testing Loss 0.005835056678430601; Training Loss 0.004719494833029697\n",
      "Episode 6996; Testing Loss 0.0058350667116820335; Training Loss 0.004719484525330112\n",
      "Episode 6997; Testing Loss 0.005834934069848445; Training Loss 0.004719475088525978\n",
      "Episode 6998; Testing Loss 0.005834894577386773; Training Loss 0.004719466058869314\n",
      "Episode 6999; Testing Loss 0.005834968137561179; Training Loss 0.004719457327817622\n",
      "Episode 7000; Testing Loss 0.005835128198275739; Training Loss 0.004719447206417331\n",
      "Episode 7001; Testing Loss 0.005835035487964893; Training Loss 0.004719440439942122\n",
      "Episode 7002; Testing Loss 0.005834827237887305; Training Loss 0.004719429618667511\n",
      "Episode 7003; Testing Loss 0.005834903471858728; Training Loss 0.004719420588873683\n",
      "Episode 7004; Testing Loss 0.005835165472060862; Training Loss 0.004719412843947503\n",
      "Episode 7005; Testing Loss 0.005835144481309534; Training Loss 0.004719402292283302\n",
      "Episode 7006; Testing Loss 0.005834855729442703; Training Loss 0.004719390655317839\n",
      "Episode 7007; Testing Loss 0.0058347363176367495; Training Loss 0.00471938321378372\n",
      "Episode 7008; Testing Loss 0.005834978994598874; Training Loss 0.004719369513738819\n",
      "Episode 7009; Testing Loss 0.005835085739945444; Training Loss 0.004719365817788428\n",
      "Episode 7010; Testing Loss 0.005834833130980122; Training Loss 0.0047193493747555715\n",
      "Episode 7011; Testing Loss 0.005834678829361236; Training Loss 0.0047193444955106805\n",
      "Episode 7012; Testing Loss 0.005834842807873272; Training Loss 0.004719331865004966\n",
      "Episode 7013; Testing Loss 0.005834987575617182; Training Loss 0.0047193262095282205\n",
      "Episode 7014; Testing Loss 0.005834930847037562; Training Loss 0.0047193140211573\n",
      "Episode 7015; Testing Loss 0.005834833084470345; Training Loss 0.004719306591385317\n",
      "Episode 7016; Testing Loss 0.005834924625715363; Training Loss 0.004719301498065423\n",
      "Episode 7017; Testing Loss 0.0058349444400543895; Training Loss 0.004719292440546514\n",
      "Episode 7018; Testing Loss 0.005834835616941753; Training Loss 0.00471927508701195\n",
      "Episode 7019; Testing Loss 0.005834784029443048; Training Loss 0.004719271146532009\n",
      "Episode 7020; Testing Loss 0.005834864526500622; Training Loss 0.0047192631391983555\n",
      "Episode 7021; Testing Loss 0.005834874821745785; Training Loss 0.004719247203041972\n",
      "Episode 7022; Testing Loss 0.005834820403806105; Training Loss 0.004719244089383397\n",
      "Episode 7023; Testing Loss 0.005834788037949136; Training Loss 0.004719237873904661\n",
      "Episode 7024; Testing Loss 0.005834826579299254; Training Loss 0.004719223750662069\n",
      "Episode 7025; Testing Loss 0.005834867639193881; Training Loss 0.004719214618415525\n",
      "Episode 7026; Testing Loss 0.005834879594604719; Training Loss 0.004719202410566731\n",
      "Episode 7027; Testing Loss 0.005834850384007738; Training Loss 0.004719193418705037\n",
      "Episode 7028; Testing Loss 0.005834782310738588; Training Loss 0.00471918513866194\n",
      "Episode 7029; Testing Loss 0.005834750649461144; Training Loss 0.004719172836603866\n",
      "Episode 7030; Testing Loss 0.005834790222704299; Training Loss 0.004719162785766095\n",
      "Episode 7031; Testing Loss 0.005834830253316593; Training Loss 0.004719154176484536\n",
      "Episode 7032; Testing Loss 0.005834811689330084; Training Loss 0.0047191457822739905\n",
      "Episode 7033; Testing Loss 0.005834722405973798; Training Loss 0.004719133902634151\n",
      "Episode 7034; Testing Loss 0.005834747336947036; Training Loss 0.004719126743911889\n",
      "Episode 7035; Testing Loss 0.0058347761923011965; Training Loss 0.004719119043945508\n",
      "Episode 7036; Testing Loss 0.005834763568591531; Training Loss 0.00471910754818336\n",
      "Episode 7037; Testing Loss 0.005834766943228525; Training Loss 0.0047190979043934675\n",
      "Episode 7038; Testing Loss 0.005834853327613059; Training Loss 0.004719089910800489\n",
      "Episode 7039; Testing Loss 0.005834771797829906; Training Loss 0.00471907704914898\n",
      "Episode 7040; Testing Loss 0.0058346393352266615; Training Loss 0.004719070494011841\n",
      "Episode 7041; Testing Loss 0.005834675082595523; Training Loss 0.004719061736951181\n",
      "Episode 7042; Testing Loss 0.005834780540421677; Training Loss 0.004719051425865246\n",
      "Episode 7043; Testing Loss 0.005834748474252157; Training Loss 0.00471903962632474\n",
      "Episode 7044; Testing Loss 0.005834632125845582; Training Loss 0.004719029684851549\n",
      "Episode 7045; Testing Loss 0.0058345937069187725; Training Loss 0.004719020133530432\n",
      "Episode 7046; Testing Loss 0.005834730672435874; Training Loss 0.004719010087177861\n",
      "Episode 7047; Testing Loss 0.005834765971189063; Training Loss 0.004719000911291221\n",
      "Episode 7048; Testing Loss 0.00583464431028433; Training Loss 0.0047189914161441995\n",
      "Episode 7049; Testing Loss 0.005834604561286243; Training Loss 0.004718981255649233\n",
      "Episode 7050; Testing Loss 0.005834699946989949; Training Loss 0.0047189716475754435\n",
      "Episode 7051; Testing Loss 0.005834732677787005; Training Loss 0.004718964476000819\n",
      "Episode 7052; Testing Loss 0.00583458620442574; Training Loss 0.004718954290403471\n",
      "Episode 7053; Testing Loss 0.005834602095636542; Training Loss 0.004718945138226403\n",
      "Episode 7054; Testing Loss 0.0058347677212124156; Training Loss 0.004718936167511397\n",
      "Episode 7055; Testing Loss 0.00583470487787723; Training Loss 0.004718924081026764\n",
      "Episode 7056; Testing Loss 0.005834520953940826; Training Loss 0.004718920577255624\n",
      "Episode 7057; Testing Loss 0.0058345952358669455; Training Loss 0.004718910425141087\n",
      "Episode 7058; Testing Loss 0.005834704010451991; Training Loss 0.004718902005034839\n",
      "Episode 7059; Testing Loss 0.005834606644529515; Training Loss 0.004718888096410374\n",
      "Episode 7060; Testing Loss 0.005834423777985901; Training Loss 0.004718879342229865\n",
      "Episode 7061; Testing Loss 0.005834468789842145; Training Loss 0.0047188701169637234\n",
      "Episode 7062; Testing Loss 0.0058347026290536605; Training Loss 0.004718866102393348\n",
      "Episode 7063; Testing Loss 0.005834749912519229; Training Loss 0.004718852828788866\n",
      "Episode 7064; Testing Loss 0.005834548119657601; Training Loss 0.004718843963967807\n",
      "Episode 7065; Testing Loss 0.005834391970734847; Training Loss 0.004718839885152496\n",
      "Episode 7066; Testing Loss 0.00583451563501135; Training Loss 0.004718826736193041\n",
      "Episode 7067; Testing Loss 0.00583479742055939; Training Loss 0.004718816818569327\n",
      "Episode 7068; Testing Loss 0.0058347150390270705; Training Loss 0.004718804521772309\n",
      "Episode 7069; Testing Loss 0.005834434367719108; Training Loss 0.004718794821972328\n",
      "Episode 7070; Testing Loss 0.005834355651072562; Training Loss 0.00471878637332508\n",
      "Episode 7071; Testing Loss 0.005834553334516369; Training Loss 0.004718775671326014\n",
      "Episode 7072; Testing Loss 0.005834647774321533; Training Loss 0.004718767552981071\n",
      "Episode 7073; Testing Loss 0.005834436884223134; Training Loss 0.004718755304821421\n",
      "Episode 7074; Testing Loss 0.005834290201543715; Training Loss 0.004718749933561018\n",
      "Episode 7075; Testing Loss 0.005834421912811676; Training Loss 0.004718735690533647\n",
      "Episode 7076; Testing Loss 0.005834600175929552; Training Loss 0.004718729956280662\n",
      "Episode 7077; Testing Loss 0.005834537580364176; Training Loss 0.004718717512898356\n",
      "Episode 7078; Testing Loss 0.005834419728583363; Training Loss 0.004718707857145176\n",
      "Episode 7079; Testing Loss 0.005834373324290133; Training Loss 0.004718698974197932\n",
      "Episode 7080; Testing Loss 0.005834399949819774; Training Loss 0.0047186885680798795\n",
      "Episode 7081; Testing Loss 0.00583450817840928; Training Loss 0.004718679079893319\n",
      "Episode 7082; Testing Loss 0.005834447887889407; Training Loss 0.004718670786877601\n",
      "Episode 7083; Testing Loss 0.0058344778139575145; Training Loss 0.004718661793518763\n",
      "Episode 7084; Testing Loss 0.005834492923514184; Training Loss 0.00471865219565206\n",
      "Episode 7085; Testing Loss 0.005834424146009752; Training Loss 0.004718643762358034\n",
      "Episode 7086; Testing Loss 0.005834365356993977; Training Loss 0.004718635873807594\n",
      "Episode 7087; Testing Loss 0.005834487576729324; Training Loss 0.004718625149736765\n",
      "Episode 7088; Testing Loss 0.005834490043456823; Training Loss 0.004718617539128227\n",
      "Episode 7089; Testing Loss 0.0058343384659494575; Training Loss 0.004718606301330008\n",
      "Episode 7090; Testing Loss 0.005834265956115441; Training Loss 0.004718598844898175\n",
      "Episode 7091; Testing Loss 0.005834465472125395; Training Loss 0.004718588850530625\n",
      "Episode 7092; Testing Loss 0.005834575856665704; Training Loss 0.004718580528198524\n",
      "Episode 7093; Testing Loss 0.005834440283241146; Training Loss 0.004718565982162288\n",
      "Episode 7094; Testing Loss 0.005834302352366167; Training Loss 0.004718562196510854\n",
      "Episode 7095; Testing Loss 0.005834345522617952; Training Loss 0.004718552509118614\n",
      "Episode 7096; Testing Loss 0.005834418271616007; Training Loss 0.004718538956082375\n",
      "Episode 7097; Testing Loss 0.005834344895655793; Training Loss 0.004718529213569355\n",
      "Episode 7098; Testing Loss 0.005834291860937111; Training Loss 0.0047185203828179105\n",
      "Episode 7099; Testing Loss 0.005834366031516714; Training Loss 0.0047185099425964576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7100; Testing Loss 0.005834462764895415; Training Loss 0.00471850176326998\n",
      "Episode 7101; Testing Loss 0.005834366545213481; Training Loss 0.004718490781333118\n",
      "Episode 7102; Testing Loss 0.005834227402422795; Training Loss 0.004718483573391885\n",
      "Episode 7103; Testing Loss 0.005834275160182323; Training Loss 0.0047184723191691005\n",
      "Episode 7104; Testing Loss 0.005834420753342249; Training Loss 0.0047184654463738\n",
      "Episode 7105; Testing Loss 0.005834288941769904; Training Loss 0.004718452468016779\n",
      "Episode 7106; Testing Loss 0.005834221080587632; Training Loss 0.004718444223663106\n",
      "Episode 7107; Testing Loss 0.0058343412274770474; Training Loss 0.004718438186124716\n",
      "Episode 7108; Testing Loss 0.0058343285343371715; Training Loss 0.004718427165378221\n",
      "Episode 7109; Testing Loss 0.005834186067164864; Training Loss 0.004718417462168235\n",
      "Episode 7110; Testing Loss 0.00583416602380394; Training Loss 0.004718411499774534\n",
      "Episode 7111; Testing Loss 0.005834296751274155; Training Loss 0.004718397997568531\n",
      "Episode 7112; Testing Loss 0.0058343531021211315; Training Loss 0.004718387912351455\n",
      "Episode 7113; Testing Loss 0.005834201965832546; Training Loss 0.004718379079150604\n",
      "Episode 7114; Testing Loss 0.005834185761919713; Training Loss 0.004718369825933386\n",
      "Episode 7115; Testing Loss 0.005834356060284659; Training Loss 0.0047183618145334215\n",
      "Episode 7116; Testing Loss 0.005834307770499978; Training Loss 0.0047183501927951415\n",
      "Episode 7117; Testing Loss 0.0058341268652770005; Training Loss 0.0047183415102239196\n",
      "Episode 7118; Testing Loss 0.005834112493852354; Training Loss 0.004718333565379053\n",
      "Episode 7119; Testing Loss 0.005834235177437414; Training Loss 0.004718321628826773\n",
      "Episode 7120; Testing Loss 0.005834335140145489; Training Loss 0.004718316084273042\n",
      "Episode 7121; Testing Loss 0.005834294820112019; Training Loss 0.004718305069808247\n",
      "Episode 7122; Testing Loss 0.005834239891852088; Training Loss 0.004718295122594872\n",
      "Episode 7123; Testing Loss 0.005834235685507658; Training Loss 0.004718286034328688\n",
      "Episode 7124; Testing Loss 0.005834113817414041; Training Loss 0.004718276943962137\n",
      "Episode 7125; Testing Loss 0.005833999666022088; Training Loss 0.004718266346412671\n",
      "Episode 7126; Testing Loss 0.005834129156870515; Training Loss 0.004718257519527393\n",
      "Episode 7127; Testing Loss 0.005834276311300144; Training Loss 0.004718249795412955\n",
      "Episode 7128; Testing Loss 0.005834164336348679; Training Loss 0.004718236889952771\n",
      "Episode 7129; Testing Loss 0.00583403671099812; Training Loss 0.004718228409858888\n",
      "Episode 7130; Testing Loss 0.005834079789287154; Training Loss 0.004718219553926573\n",
      "Episode 7131; Testing Loss 0.0058341368209599035; Training Loss 0.004718208850832462\n",
      "Episode 7132; Testing Loss 0.005834131437032623; Training Loss 0.004718201120857498\n",
      "Episode 7133; Testing Loss 0.0058341088480818645; Training Loss 0.004718191936668438\n",
      "Episode 7134; Testing Loss 0.005834100444464716; Training Loss 0.004718182340919251\n",
      "Episode 7135; Testing Loss 0.005834125415328616; Training Loss 0.004718173571505528\n",
      "Episode 7136; Testing Loss 0.005834118314824441; Training Loss 0.0047181648343203614\n",
      "Episode 7137; Testing Loss 0.005834084431429043; Training Loss 0.004718156400439933\n",
      "Episode 7138; Testing Loss 0.005834044411585262; Training Loss 0.004718147566380246\n",
      "Episode 7139; Testing Loss 0.005834106467733248; Training Loss 0.004718135864494547\n",
      "Episode 7140; Testing Loss 0.005834138669423526; Training Loss 0.004718126803571981\n",
      "Episode 7141; Testing Loss 0.005834036866375723; Training Loss 0.0047181188452671274\n",
      "Episode 7142; Testing Loss 0.0058338749432908675; Training Loss 0.004718111048872036\n",
      "Episode 7143; Testing Loss 0.005833854477193147; Training Loss 0.004718098389295654\n",
      "Episode 7144; Testing Loss 0.005834030229123513; Training Loss 0.004718091510178125\n",
      "Episode 7145; Testing Loss 0.005834157356100574; Training Loss 0.004718084066060357\n",
      "Episode 7146; Testing Loss 0.005834059454871393; Training Loss 0.0047180691242632855\n",
      "Episode 7147; Testing Loss 0.005833939708631716; Training Loss 0.00471806053475884\n",
      "Episode 7148; Testing Loss 0.005834008039529187; Training Loss 0.0047180558864016895\n",
      "Episode 7149; Testing Loss 0.0058340785593698555; Training Loss 0.0047180419015870724\n",
      "Episode 7150; Testing Loss 0.005834057862952627; Training Loss 0.004718035264342132\n",
      "Episode 7151; Testing Loss 0.005834068123983798; Training Loss 0.004718028188167007\n",
      "Episode 7152; Testing Loss 0.005834052394712524; Training Loss 0.004718014843999701\n",
      "Episode 7153; Testing Loss 0.005834006691736775; Training Loss 0.004718007523381427\n",
      "Episode 7154; Testing Loss 0.005833906509462286; Training Loss 0.004718000049482858\n",
      "Episode 7155; Testing Loss 0.005833866423216321; Training Loss 0.004717985654832691\n",
      "Episode 7156; Testing Loss 0.00583393647099448; Training Loss 0.0047179795128894815\n",
      "Episode 7157; Testing Loss 0.005834106635753376; Training Loss 0.004717974891456802\n",
      "Episode 7158; Testing Loss 0.005834100124870415; Training Loss 0.0047179639007693475\n",
      "Episode 7159; Testing Loss 0.005833939832260277; Training Loss 0.00471794908084781\n",
      "Episode 7160; Testing Loss 0.00583386428675278; Training Loss 0.004717940390165962\n",
      "Episode 7161; Testing Loss 0.005833834325278966; Training Loss 0.0047179322647882636\n",
      "Episode 7162; Testing Loss 0.0058338288045510795; Training Loss 0.004717920551328644\n",
      "Episode 7163; Testing Loss 0.005833846301161767; Training Loss 0.004717912212232772\n",
      "Episode 7164; Testing Loss 0.0058338905051260725; Training Loss 0.004717903582994953\n",
      "Episode 7165; Testing Loss 0.005833867834031434; Training Loss 0.004717892034830802\n",
      "Episode 7166; Testing Loss 0.005833780197873257; Training Loss 0.004717882571176279\n",
      "Episode 7167; Testing Loss 0.005833840693309358; Training Loss 0.004717873525195328\n",
      "Episode 7168; Testing Loss 0.005833991154939641; Training Loss 0.004717862821351273\n",
      "Episode 7169; Testing Loss 0.005834052562081256; Training Loss 0.0047178540747958115\n",
      "Episode 7170; Testing Loss 0.005833934448473198; Training Loss 0.004717846159717233\n",
      "Episode 7171; Testing Loss 0.005833824717575288; Training Loss 0.004717834157592099\n",
      "Episode 7172; Testing Loss 0.005833902989756045; Training Loss 0.00471782428483318\n",
      "Episode 7173; Testing Loss 0.005833892937832033; Training Loss 0.0047178174942140005\n",
      "Episode 7174; Testing Loss 0.00583377264626511; Training Loss 0.004717807449843316\n",
      "Episode 7175; Testing Loss 0.0058337800856007515; Training Loss 0.00471779775499733\n",
      "Episode 7176; Testing Loss 0.005833828908709549; Training Loss 0.0047177886193643555\n",
      "Episode 7177; Testing Loss 0.005833795245496772; Training Loss 0.004717777180710971\n",
      "Episode 7178; Testing Loss 0.005833777088333052; Training Loss 0.004717769034477701\n",
      "Episode 7179; Testing Loss 0.005833905903238273; Training Loss 0.004717757942892116\n",
      "Episode 7180; Testing Loss 0.005833909682252757; Training Loss 0.004717752257296183\n",
      "Episode 7181; Testing Loss 0.005833767751309895; Training Loss 0.004717742309830062\n",
      "Episode 7182; Testing Loss 0.005833728286893683; Training Loss 0.004717732319780368\n",
      "Episode 7183; Testing Loss 0.005833876837126353; Training Loss 0.004717722454230913\n",
      "Episode 7184; Testing Loss 0.005833869828083873; Training Loss 0.004717713099482673\n",
      "Episode 7185; Testing Loss 0.00583360753543271; Training Loss 0.004717705423169529\n",
      "Episode 7186; Testing Loss 0.005833573730366623; Training Loss 0.00471769773306986\n",
      "Episode 7187; Testing Loss 0.00583376557527556; Training Loss 0.004717685916417485\n",
      "Episode 7188; Testing Loss 0.005833815575739351; Training Loss 0.0047176795719009335\n",
      "Episode 7189; Testing Loss 0.005833672877375944; Training Loss 0.004717669685105643\n",
      "Episode 7190; Testing Loss 0.005833660359554301; Training Loss 0.004717656887923018\n",
      "Episode 7191; Testing Loss 0.005833806395217367; Training Loss 0.004717650454922197\n",
      "Episode 7192; Testing Loss 0.005833816827906741; Training Loss 0.004717641028631276\n",
      "Episode 7193; Testing Loss 0.005833673097943317; Training Loss 0.004717629775167315\n",
      "Episode 7194; Testing Loss 0.0058335922449225044; Training Loss 0.004717621651205699\n",
      "Episode 7195; Testing Loss 0.0058336866218231705; Training Loss 0.00471761287459416\n",
      "Episode 7196; Testing Loss 0.005833766840052244; Training Loss 0.004717604238140256\n",
      "Episode 7197; Testing Loss 0.0058336304490672316; Training Loss 0.0047175939324767486\n",
      "Episode 7198; Testing Loss 0.005833498337210713; Training Loss 0.004717583131088614\n",
      "Episode 7199; Testing Loss 0.005833553492896138; Training Loss 0.004717575074591184\n",
      "Episode 7200; Testing Loss 0.0058336535854009745; Training Loss 0.0047175644057443535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7201; Testing Loss 0.005833640859697343; Training Loss 0.004717552507978701\n",
      "Episode 7202; Testing Loss 0.005833664714326331; Training Loss 0.004717545875618951\n",
      "Episode 7203; Testing Loss 0.005833699074026082; Training Loss 0.004717535141120477\n",
      "Episode 7204; Testing Loss 0.005833732376894883; Training Loss 0.004717524335881901\n",
      "Episode 7205; Testing Loss 0.00583359867094428; Training Loss 0.004717518188112178\n",
      "Episode 7206; Testing Loss 0.005833554140322962; Training Loss 0.004717507342850154\n",
      "Episode 7207; Testing Loss 0.005833671287451166; Training Loss 0.00471749764849667\n",
      "Episode 7208; Testing Loss 0.005833633535593176; Training Loss 0.004717486398178376\n",
      "Episode 7209; Testing Loss 0.00583354009495152; Training Loss 0.004717480013943068\n",
      "Episode 7210; Testing Loss 0.005833637623303354; Training Loss 0.004717467465623748\n",
      "Episode 7211; Testing Loss 0.005833678574457757; Training Loss 0.004717459174741814\n",
      "Episode 7212; Testing Loss 0.00583356988403744; Training Loss 0.004717451965799863\n",
      "Episode 7213; Testing Loss 0.005833524719793664; Training Loss 0.004717441358693473\n",
      "Episode 7214; Testing Loss 0.005833607338987035; Training Loss 0.00471743071243197\n",
      "Episode 7215; Testing Loss 0.005833581538825348; Training Loss 0.004717420911247378\n",
      "Episode 7216; Testing Loss 0.005833581874609155; Training Loss 0.004717411574402051\n",
      "Episode 7217; Testing Loss 0.005833493004741005; Training Loss 0.00471740248803568\n",
      "Episode 7218; Testing Loss 0.005833470126943173; Training Loss 0.004717395076962019\n",
      "Episode 7219; Testing Loss 0.00583359588288905; Training Loss 0.004717385660383144\n",
      "Episode 7220; Testing Loss 0.005833639072829572; Training Loss 0.004717374934788589\n",
      "Episode 7221; Testing Loss 0.005833485785108037; Training Loss 0.00471736708564776\n",
      "Episode 7222; Testing Loss 0.0058333676505781; Training Loss 0.004717358233148888\n",
      "Episode 7223; Testing Loss 0.005833564442662091; Training Loss 0.004717347425724562\n",
      "Episode 7224; Testing Loss 0.005833624694171295; Training Loss 0.004717339151620589\n",
      "Episode 7225; Testing Loss 0.005833407365852132; Training Loss 0.0047173297409123845\n",
      "Episode 7226; Testing Loss 0.005833345794875207; Training Loss 0.00471732457109114\n",
      "Episode 7227; Testing Loss 0.005833484536162959; Training Loss 0.004717310075825576\n",
      "Episode 7228; Testing Loss 0.005833535275585324; Training Loss 0.0047173038126008425\n",
      "Episode 7229; Testing Loss 0.005833426759358876; Training Loss 0.004717293875674637\n",
      "Episode 7230; Testing Loss 0.0058334219780863065; Training Loss 0.004717286948347152\n",
      "Episode 7231; Testing Loss 0.00583357960091427; Training Loss 0.0047172739867537596\n",
      "Episode 7232; Testing Loss 0.005833588913172447; Training Loss 0.004717267956738222\n",
      "Episode 7233; Testing Loss 0.005833310375865627; Training Loss 0.004717257454322955\n",
      "Episode 7234; Testing Loss 0.005833107855660532; Training Loss 0.004717247557333032\n",
      "Episode 7235; Testing Loss 0.005833305576178184; Training Loss 0.004717235832437655\n",
      "Episode 7236; Testing Loss 0.005833585572792823; Training Loss 0.004717232765602941\n",
      "Episode 7237; Testing Loss 0.005833338244519459; Training Loss 0.004717215893368198\n",
      "Episode 7238; Testing Loss 0.00583312929550797; Training Loss 0.004717210561100986\n",
      "Episode 7239; Testing Loss 0.005833293690963722; Training Loss 0.004717197557578451\n",
      "Episode 7240; Testing Loss 0.005833494512771393; Training Loss 0.004717187611370302\n",
      "Episode 7241; Testing Loss 0.0058334554889735366; Training Loss 0.004717180797444795\n",
      "Episode 7242; Testing Loss 0.005833379043824298; Training Loss 0.004717172938614786\n",
      "Episode 7243; Testing Loss 0.005833384921030243; Training Loss 0.0047171606961972385\n",
      "Episode 7244; Testing Loss 0.005833403563139711; Training Loss 0.004717149784726217\n",
      "Episode 7245; Testing Loss 0.00583328236320038; Training Loss 0.004717144582926909\n",
      "Episode 7246; Testing Loss 0.005833119310827421; Training Loss 0.004717134523673418\n",
      "Episode 7247; Testing Loss 0.005833170197145526; Training Loss 0.004717123266953113\n",
      "Episode 7248; Testing Loss 0.005833410981774876; Training Loss 0.004717116770355435\n",
      "Episode 7249; Testing Loss 0.005833376788785891; Training Loss 0.004717103705133489\n",
      "Episode 7250; Testing Loss 0.005833157577295725; Training Loss 0.004717094322087364\n",
      "Episode 7251; Testing Loss 0.005833173013727008; Training Loss 0.004717085019443103\n",
      "Episode 7252; Testing Loss 0.005833382358318256; Training Loss 0.0047170759278814785\n",
      "Episode 7253; Testing Loss 0.0058333888086579385; Training Loss 0.004717066056814871\n",
      "Episode 7254; Testing Loss 0.005833219282892695; Training Loss 0.004717057176743189\n",
      "Episode 7255; Testing Loss 0.005833173047423516; Training Loss 0.004717045764693783\n",
      "Episode 7256; Testing Loss 0.0058332058234319955; Training Loss 0.004717036904231551\n",
      "Episode 7257; Testing Loss 0.00583314302906831; Training Loss 0.004717026957124554\n",
      "Episode 7258; Testing Loss 0.005833093302916326; Training Loss 0.004717019018067965\n",
      "Episode 7259; Testing Loss 0.0058331221267156645; Training Loss 0.004717008634684634\n",
      "Episode 7260; Testing Loss 0.005833209654427409; Training Loss 0.004716998638739202\n",
      "Episode 7261; Testing Loss 0.005833141666186549; Training Loss 0.004716989963637309\n",
      "Episode 7262; Testing Loss 0.005833131632160568; Training Loss 0.004716980708613597\n",
      "Episode 7263; Testing Loss 0.005833303564262764; Training Loss 0.004716974717527428\n",
      "Episode 7264; Testing Loss 0.00583324604476349; Training Loss 0.0047169636714956935\n",
      "Episode 7265; Testing Loss 0.005833060451872842; Training Loss 0.004716954887324509\n",
      "Episode 7266; Testing Loss 0.005833046696186299; Training Loss 0.004716944198224654\n",
      "Episode 7267; Testing Loss 0.005833248407637272; Training Loss 0.004716932615335593\n",
      "Episode 7268; Testing Loss 0.005833254604107859; Training Loss 0.004716930276413728\n",
      "Episode 7269; Testing Loss 0.005833021193708669; Training Loss 0.004716918670697188\n",
      "Episode 7270; Testing Loss 0.0058329541649202125; Training Loss 0.0047169056842594845\n",
      "Episode 7271; Testing Loss 0.005833125045450036; Training Loss 0.004716897413032254\n",
      "Episode 7272; Testing Loss 0.005833267391342351; Training Loss 0.00471689077885022\n",
      "Episode 7273; Testing Loss 0.005833123869517361; Training Loss 0.004716876938400583\n",
      "Episode 7274; Testing Loss 0.005832954978510432; Training Loss 0.004716870389257957\n",
      "Episode 7275; Testing Loss 0.005832932900323682; Training Loss 0.00471686330616824\n",
      "Episode 7276; Testing Loss 0.005833057679643346; Training Loss 0.004716847679160559\n",
      "Episode 7277; Testing Loss 0.00583315763717697; Training Loss 0.00471684175097346\n",
      "Episode 7278; Testing Loss 0.00583310188194697; Training Loss 0.004716832005338648\n",
      "Episode 7279; Testing Loss 0.005832989650953451; Training Loss 0.004716819126923214\n",
      "Episode 7280; Testing Loss 0.005832982921141826; Training Loss 0.004716812913549583\n",
      "Episode 7281; Testing Loss 0.005833054405601791; Training Loss 0.0047168033226755845\n",
      "Episode 7282; Testing Loss 0.005833009880532133; Training Loss 0.004716793953496185\n",
      "Episode 7283; Testing Loss 0.005833002965464726; Training Loss 0.004716789167237116\n",
      "Episode 7284; Testing Loss 0.005833081543588253; Training Loss 0.004716777704256219\n",
      "Episode 7285; Testing Loss 0.005833067430071484; Training Loss 0.004716764181275818\n",
      "Episode 7286; Testing Loss 0.005832897417378072; Training Loss 0.004716757509739988\n",
      "Episode 7287; Testing Loss 0.005832861128907627; Training Loss 0.004716748865456521\n",
      "Episode 7288; Testing Loss 0.005832983601837109; Training Loss 0.00471673597769933\n",
      "Episode 7289; Testing Loss 0.005832936044229607; Training Loss 0.0047167264209335355\n",
      "Episode 7290; Testing Loss 0.005832848077626734; Training Loss 0.0047167175545885015\n",
      "Episode 7291; Testing Loss 0.005832813483366492; Training Loss 0.004716707165065813\n",
      "Episode 7292; Testing Loss 0.005832847387349915; Training Loss 0.004716697521050953\n",
      "Episode 7293; Testing Loss 0.005832882624155505; Training Loss 0.004716687680643493\n",
      "Episode 7294; Testing Loss 0.00583292479109736; Training Loss 0.004716679110518544\n",
      "Episode 7295; Testing Loss 0.005832859975039814; Training Loss 0.004716668527059299\n",
      "Episode 7296; Testing Loss 0.0058328311824783294; Training Loss 0.004716658949423129\n",
      "Episode 7297; Testing Loss 0.005832788490855787; Training Loss 0.0047166495407371676\n",
      "Episode 7298; Testing Loss 0.005832758142505906; Training Loss 0.004716640365763945\n",
      "Episode 7299; Testing Loss 0.0058328045105565446; Training Loss 0.004716631501220487\n",
      "Episode 7300; Testing Loss 0.005832839573547186; Training Loss 0.004716622943551877\n",
      "Episode 7301; Testing Loss 0.005832773218015827; Training Loss 0.004716614625588102\n",
      "Episode 7302; Testing Loss 0.005832862695746311; Training Loss 0.004716606433852988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7303; Testing Loss 0.005832840673550584; Training Loss 0.0047165931269938744\n",
      "Episode 7304; Testing Loss 0.005832779723938398; Training Loss 0.004716582865003444\n",
      "Episode 7305; Testing Loss 0.005832735318921652; Training Loss 0.00471658018922172\n",
      "Episode 7306; Testing Loss 0.005832623222710112; Training Loss 0.004716567227546202\n",
      "Episode 7307; Testing Loss 0.005832619788413491; Training Loss 0.004716558007668701\n",
      "Episode 7308; Testing Loss 0.005832784179007117; Training Loss 0.0047165505130717204\n",
      "Episode 7309; Testing Loss 0.005832834465638216; Training Loss 0.004716538415818221\n",
      "Episode 7310; Testing Loss 0.0058326992135313924; Training Loss 0.00471652992146052\n",
      "Episode 7311; Testing Loss 0.0058325655618303805; Training Loss 0.00471652159054741\n",
      "Episode 7312; Testing Loss 0.005832610414593352; Training Loss 0.004716509435291093\n",
      "Episode 7313; Testing Loss 0.005832749343503209; Training Loss 0.004716500413325691\n",
      "Episode 7314; Testing Loss 0.005832707396404285; Training Loss 0.004716493503467109\n",
      "Episode 7315; Testing Loss 0.0058326273934373365; Training Loss 0.004716483249531954\n",
      "Episode 7316; Testing Loss 0.005832593453598268; Training Loss 0.004716473489938882\n",
      "Episode 7317; Testing Loss 0.0058326136100178205; Training Loss 0.004716463146339661\n",
      "Episode 7318; Testing Loss 0.005832671963572519; Training Loss 0.004716453652470312\n",
      "Episode 7319; Testing Loss 0.0058327295603717525; Training Loss 0.004716444076665281\n",
      "Episode 7320; Testing Loss 0.005832599952603831; Training Loss 0.004716435766889442\n",
      "Episode 7321; Testing Loss 0.005832498780101945; Training Loss 0.004716425835203945\n",
      "Episode 7322; Testing Loss 0.005832504182408007; Training Loss 0.0047164177013746126\n",
      "Episode 7323; Testing Loss 0.005832495934911202; Training Loss 0.004716405639513234\n",
      "Episode 7324; Testing Loss 0.0058325648635963725; Training Loss 0.004716395915155126\n",
      "Episode 7325; Testing Loss 0.005832638214511976; Training Loss 0.004716386855696605\n",
      "Episode 7326; Testing Loss 0.005832548836712284; Training Loss 0.004716377759783765\n",
      "Episode 7327; Testing Loss 0.005832471772752541; Training Loss 0.0047163684269791424\n",
      "Episode 7328; Testing Loss 0.00583255519814802; Training Loss 0.0047163585452020705\n",
      "Episode 7329; Testing Loss 0.0058325759585088335; Training Loss 0.004716350943348065\n",
      "Episode 7330; Testing Loss 0.005832407550453729; Training Loss 0.0047163404441645865\n",
      "Episode 7331; Testing Loss 0.005832406873443058; Training Loss 0.004716330464894513\n",
      "Episode 7332; Testing Loss 0.0058324880104120005; Training Loss 0.004716321158119083\n",
      "Episode 7333; Testing Loss 0.005832483805678684; Training Loss 0.00471631368366179\n",
      "Episode 7334; Testing Loss 0.005832521112407707; Training Loss 0.004716303171861627\n",
      "Episode 7335; Testing Loss 0.005832510685056043; Training Loss 0.004716292810538178\n",
      "Episode 7336; Testing Loss 0.0058324044521369045; Training Loss 0.004716284198113202\n",
      "Episode 7337; Testing Loss 0.005832372652365633; Training Loss 0.004716274497582552\n",
      "Episode 7338; Testing Loss 0.005832370685576832; Training Loss 0.004716266690003416\n",
      "Episode 7339; Testing Loss 0.005832497654903218; Training Loss 0.0047162556650848005\n",
      "Episode 7340; Testing Loss 0.005832428344547453; Training Loss 0.004716246857616857\n",
      "Episode 7341; Testing Loss 0.005832302986837035; Training Loss 0.0047162368408050015\n",
      "Episode 7342; Testing Loss 0.005832361915470443; Training Loss 0.0047162267985618065\n",
      "Episode 7343; Testing Loss 0.005832495521651026; Training Loss 0.004716221260732279\n",
      "Episode 7344; Testing Loss 0.005832382281885797; Training Loss 0.004716207766084391\n",
      "Episode 7345; Testing Loss 0.0058322602716589155; Training Loss 0.004716200807912333\n",
      "Episode 7346; Testing Loss 0.00583232519753545; Training Loss 0.004716190597428284\n",
      "Episode 7347; Testing Loss 0.005832384766879833; Training Loss 0.004716182740097985\n",
      "Episode 7348; Testing Loss 0.005832254726731105; Training Loss 0.004716172743274002\n",
      "Episode 7349; Testing Loss 0.005832282911237586; Training Loss 0.004716163247918036\n",
      "Episode 7350; Testing Loss 0.005832452121210092; Training Loss 0.004716152336371268\n",
      "Episode 7351; Testing Loss 0.005832399833267348; Training Loss 0.004716144354062905\n",
      "Episode 7352; Testing Loss 0.005832255378071025; Training Loss 0.004716133999103477\n",
      "Episode 7353; Testing Loss 0.005832266381815675; Training Loss 0.004716125008070663\n",
      "Episode 7354; Testing Loss 0.005832285029967501; Training Loss 0.0047161176453304025\n",
      "Episode 7355; Testing Loss 0.0058322486493323; Training Loss 0.004716106287386394\n",
      "Episode 7356; Testing Loss 0.005832218093370563; Training Loss 0.004716096812206585\n",
      "Episode 7357; Testing Loss 0.005832166053126765; Training Loss 0.004716086657626834\n",
      "Episode 7358; Testing Loss 0.005832226073242694; Training Loss 0.0047160774917386775\n",
      "Episode 7359; Testing Loss 0.005832209195603452; Training Loss 0.004716068451327946\n",
      "Episode 7360; Testing Loss 0.005832226451196538; Training Loss 0.00471605919908367\n",
      "Episode 7361; Testing Loss 0.005832242600101233; Training Loss 0.004716050430850003\n",
      "Episode 7362; Testing Loss 0.005832211304752024; Training Loss 0.004716040502104095\n",
      "Episode 7363; Testing Loss 0.0058322446228948; Training Loss 0.0047160329364374165\n",
      "Episode 7364; Testing Loss 0.0058322573261653035; Training Loss 0.004716021099138048\n",
      "Episode 7365; Testing Loss 0.005832161196420671; Training Loss 0.004716014552694383\n",
      "Episode 7366; Testing Loss 0.005832101179118412; Training Loss 0.004716004113338638\n",
      "Episode 7367; Testing Loss 0.0058322104193936795; Training Loss 0.004715994132601625\n",
      "Episode 7368; Testing Loss 0.005832218730749948; Training Loss 0.004715983932697839\n",
      "Episode 7369; Testing Loss 0.005832098613657404; Training Loss 0.004715974331973953\n",
      "Episode 7370; Testing Loss 0.005832099748527723; Training Loss 0.004715967485426229\n",
      "Episode 7371; Testing Loss 0.005832142677599991; Training Loss 0.004715956551987516\n",
      "Episode 7372; Testing Loss 0.005832091115546575; Training Loss 0.00471594926345662\n",
      "Episode 7373; Testing Loss 0.005832094217021602; Training Loss 0.004715939367459711\n",
      "Episode 7374; Testing Loss 0.005832150906240907; Training Loss 0.00471592836967378\n",
      "Episode 7375; Testing Loss 0.0058321015018601005; Training Loss 0.004715919587002352\n",
      "Episode 7376; Testing Loss 0.0058319913138138646; Training Loss 0.004715909679665075\n",
      "Episode 7377; Testing Loss 0.005832023405058268; Training Loss 0.004715899969643319\n",
      "Episode 7378; Testing Loss 0.005832128374776067; Training Loss 0.004715891345743494\n",
      "Episode 7379; Testing Loss 0.0058320992066374765; Training Loss 0.004715882840774315\n",
      "Episode 7380; Testing Loss 0.00583206510150955; Training Loss 0.00471587271356825\n",
      "Episode 7381; Testing Loss 0.005831964982262628; Training Loss 0.004715864864558444\n",
      "Episode 7382; Testing Loss 0.005831843722309844; Training Loss 0.004715856450512931\n",
      "Episode 7383; Testing Loss 0.005832005787253376; Training Loss 0.004715844827389412\n",
      "Episode 7384; Testing Loss 0.005832058710784019; Training Loss 0.00471583596430479\n",
      "Episode 7385; Testing Loss 0.005831914621753248; Training Loss 0.0047158251457140405\n",
      "Episode 7386; Testing Loss 0.005831871986338958; Training Loss 0.004715818502388573\n",
      "Episode 7387; Testing Loss 0.005831950788665789; Training Loss 0.004715807259882315\n",
      "Episode 7388; Testing Loss 0.005832018188310672; Training Loss 0.004715801637702017\n",
      "Episode 7389; Testing Loss 0.005832125780414168; Training Loss 0.004715794302704991\n",
      "Episode 7390; Testing Loss 0.005832029618352563; Training Loss 0.004715781420798834\n",
      "Episode 7391; Testing Loss 0.00583186018550206; Training Loss 0.0047157710953245014\n",
      "Episode 7392; Testing Loss 0.00583179618530395; Training Loss 0.004715763391643834\n",
      "Episode 7393; Testing Loss 0.005831902946901026; Training Loss 0.004715752142038382\n",
      "Episode 7394; Testing Loss 0.005832033777248041; Training Loss 0.004715745518443899\n",
      "Episode 7395; Testing Loss 0.005831954982328249; Training Loss 0.004715732316145756\n",
      "Episode 7396; Testing Loss 0.005831789577961111; Training Loss 0.0047157262893458875\n",
      "Episode 7397; Testing Loss 0.0058317334728967204; Training Loss 0.004715720062196872\n",
      "Episode 7398; Testing Loss 0.00583181321987505; Training Loss 0.004715704895146889\n",
      "Episode 7399; Testing Loss 0.005831931572962442; Training Loss 0.004715700457252988\n",
      "Episode 7400; Testing Loss 0.005831960843109357; Training Loss 0.004715692549306301\n",
      "Episode 7401; Testing Loss 0.005831847163847288; Training Loss 0.004715681755392445\n",
      "Episode 7402; Testing Loss 0.005831732517222909; Training Loss 0.004715669571355547\n",
      "Episode 7403; Testing Loss 0.005831833421382505; Training Loss 0.004715659454667091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7404; Testing Loss 0.005831908099090201; Training Loss 0.004715652197410575\n",
      "Episode 7405; Testing Loss 0.005831876605851012; Training Loss 0.004715639045030559\n",
      "Episode 7406; Testing Loss 0.005831777498949565; Training Loss 0.004715630458310352\n",
      "Episode 7407; Testing Loss 0.005831741371190146; Training Loss 0.004715621833222541\n",
      "Episode 7408; Testing Loss 0.005831846430491867; Training Loss 0.004715613122472027\n",
      "Episode 7409; Testing Loss 0.005831781155432506; Training Loss 0.0047156011799810455\n",
      "Episode 7410; Testing Loss 0.0058317014891492375; Training Loss 0.004715599528098044\n",
      "Episode 7411; Testing Loss 0.005831821589618873; Training Loss 0.004715586673652906\n",
      "Episode 7412; Testing Loss 0.005831959228214471; Training Loss 0.0047155775913090316\n",
      "Episode 7413; Testing Loss 0.005831784321291606; Training Loss 0.004715566178083771\n",
      "Episode 7414; Testing Loss 0.005831539643091615; Training Loss 0.004715560369119929\n",
      "Episode 7415; Testing Loss 0.005831595456199133; Training Loss 0.004715548655490708\n",
      "Episode 7416; Testing Loss 0.0058318806555405; Training Loss 0.004715538436267914\n",
      "Episode 7417; Testing Loss 0.005831940815217523; Training Loss 0.00471553097982597\n",
      "Episode 7418; Testing Loss 0.005831649130064152; Training Loss 0.0047155207627448065\n",
      "Episode 7419; Testing Loss 0.005831545529183526; Training Loss 0.00471551421158712\n",
      "Episode 7420; Testing Loss 0.0058318562312497215; Training Loss 0.004715501789127242\n",
      "Episode 7421; Testing Loss 0.00583191875614557; Training Loss 0.004715495443927021\n",
      "Episode 7422; Testing Loss 0.005831604344943772; Training Loss 0.0047154807777846095\n",
      "Episode 7423; Testing Loss 0.005831424979726297; Training Loss 0.004715475031099091\n",
      "Episode 7424; Testing Loss 0.0058316483529440785; Training Loss 0.004715462273861027\n",
      "Episode 7425; Testing Loss 0.0058317981562606555; Training Loss 0.004715455111487755\n",
      "Episode 7426; Testing Loss 0.005831675304349081; Training Loss 0.004715442678063648\n",
      "Episode 7427; Testing Loss 0.005831475748241026; Training Loss 0.004715434067364084\n",
      "Episode 7428; Testing Loss 0.005831495180034297; Training Loss 0.0047154242083779635\n",
      "Episode 7429; Testing Loss 0.0058316244538648; Training Loss 0.004715416005662906\n",
      "Episode 7430; Testing Loss 0.005831728564815059; Training Loss 0.0047154073101278865\n",
      "Episode 7431; Testing Loss 0.005831669725271164; Training Loss 0.004715395742351409\n",
      "Episode 7432; Testing Loss 0.005831512127597985; Training Loss 0.004715389469009246\n",
      "Episode 7433; Testing Loss 0.005831609891480653; Training Loss 0.0047153781841061514\n",
      "Episode 7434; Testing Loss 0.0058316569865646825; Training Loss 0.00471536845580201\n",
      "Episode 7435; Testing Loss 0.005831580586320637; Training Loss 0.004715360735437579\n",
      "Episode 7436; Testing Loss 0.005831537140657907; Training Loss 0.004715355950603381\n",
      "Episode 7437; Testing Loss 0.0058314783346133225; Training Loss 0.004715343856851628\n",
      "Episode 7438; Testing Loss 0.005831492108063413; Training Loss 0.004715332044277999\n",
      "Episode 7439; Testing Loss 0.005831604366788604; Training Loss 0.004715323300706408\n",
      "Episode 7440; Testing Loss 0.005831681406773304; Training Loss 0.004715312490078929\n",
      "Episode 7441; Testing Loss 0.0058315715667298035; Training Loss 0.0047153060680193565\n",
      "Episode 7442; Testing Loss 0.0058314279012369056; Training Loss 0.004715300473007855\n",
      "Episode 7443; Testing Loss 0.005831543910626951; Training Loss 0.004715285709847823\n",
      "Episode 7444; Testing Loss 0.005831591541024274; Training Loss 0.0047152759655224\n",
      "Episode 7445; Testing Loss 0.005831448897459151; Training Loss 0.004715269919297255\n",
      "Episode 7446; Testing Loss 0.0058313337710060665; Training Loss 0.004715259552256434\n",
      "Episode 7447; Testing Loss 0.005831411950768392; Training Loss 0.004715248809662272\n",
      "Episode 7448; Testing Loss 0.005831394859001186; Training Loss 0.004715239784325273\n",
      "Episode 7449; Testing Loss 0.005831388446085907; Training Loss 0.004715232923798494\n",
      "Episode 7450; Testing Loss 0.0058314356978924125; Training Loss 0.004715220222768819\n",
      "Episode 7451; Testing Loss 0.005831390475811401; Training Loss 0.004715209811281212\n",
      "Episode 7452; Testing Loss 0.005831305687511379; Training Loss 0.004715205949972541\n",
      "Episode 7453; Testing Loss 0.0058313875539501995; Training Loss 0.004715194731628402\n",
      "Episode 7454; Testing Loss 0.00583143180723072; Training Loss 0.004715182923825043\n",
      "Episode 7455; Testing Loss 0.005831290575393429; Training Loss 0.00471517544977406\n",
      "Episode 7456; Testing Loss 0.005831263391604495; Training Loss 0.004715167127231977\n",
      "Episode 7457; Testing Loss 0.005831345263401121; Training Loss 0.0047151554356691504\n",
      "Episode 7458; Testing Loss 0.005831423155211057; Training Loss 0.004715145310603497\n",
      "Episode 7459; Testing Loss 0.005831294028199167; Training Loss 0.00471513742017907\n",
      "Episode 7460; Testing Loss 0.005831287150783846; Training Loss 0.004715131066646579\n",
      "Episode 7461; Testing Loss 0.0058313450253837055; Training Loss 0.004715120471413068\n",
      "Episode 7462; Testing Loss 0.005831243934102531; Training Loss 0.004715107607835318\n",
      "Episode 7463; Testing Loss 0.005831181880325735; Training Loss 0.004715105723378929\n",
      "Episode 7464; Testing Loss 0.005831333008606896; Training Loss 0.004715096539539423\n",
      "Episode 7465; Testing Loss 0.0058314468999998694; Training Loss 0.004715083498611667\n",
      "Episode 7466; Testing Loss 0.005831316662888533; Training Loss 0.0047150709367159765\n",
      "Episode 7467; Testing Loss 0.005831184491958602; Training Loss 0.004715063922653127\n",
      "Episode 7468; Testing Loss 0.005831202098678111; Training Loss 0.004715055544685445\n",
      "Episode 7469; Testing Loss 0.005831288967678429; Training Loss 0.0047150455601413874\n",
      "Episode 7470; Testing Loss 0.005831186399791448; Training Loss 0.004715035158644324\n",
      "Episode 7471; Testing Loss 0.0058309597585291485; Training Loss 0.004715029184913\n",
      "Episode 7472; Testing Loss 0.005830991653273318; Training Loss 0.004715022091575708\n",
      "Episode 7473; Testing Loss 0.005831280201974225; Training Loss 0.004715010643842182\n",
      "Episode 7474; Testing Loss 0.005831289373701754; Training Loss 0.004714998840571301\n",
      "Episode 7475; Testing Loss 0.005831039117353779; Training Loss 0.004714988785981052\n",
      "Episode 7476; Testing Loss 0.005831065348320783; Training Loss 0.004714984100531748\n",
      "Episode 7477; Testing Loss 0.0058313509461921315; Training Loss 0.004714973724777143\n",
      "Episode 7478; Testing Loss 0.005831347520124112; Training Loss 0.004714960130688716\n",
      "Episode 7479; Testing Loss 0.005831091567847461; Training Loss 0.00471495723598175\n",
      "Episode 7480; Testing Loss 0.0058311141534705515; Training Loss 0.0047149484279536925\n",
      "Episode 7481; Testing Loss 0.00583131020828292; Training Loss 0.004714933967931552\n",
      "Episode 7482; Testing Loss 0.005831241461054202; Training Loss 0.004714921598546147\n",
      "Episode 7483; Testing Loss 0.005831008288780891; Training Loss 0.004714914681096511\n",
      "Episode 7484; Testing Loss 0.0058309720322293965; Training Loss 0.00471490806356993\n",
      "Episode 7485; Testing Loss 0.005831186552176518; Training Loss 0.004714893531108566\n",
      "Episode 7486; Testing Loss 0.005831192687693722; Training Loss 0.004714884394302941\n",
      "Episode 7487; Testing Loss 0.005831013937649379; Training Loss 0.004714878357487311\n",
      "Episode 7488; Testing Loss 0.0058309887428438885; Training Loss 0.004714871559741146\n",
      "Episode 7489; Testing Loss 0.005831113512392881; Training Loss 0.004714857706254307\n",
      "Episode 7490; Testing Loss 0.005831201284843434; Training Loss 0.004714845721336819\n",
      "Episode 7491; Testing Loss 0.005831173553811813; Training Loss 0.004714835222029109\n",
      "Episode 7492; Testing Loss 0.005831049603867776; Training Loss 0.004714828262292582\n",
      "Episode 7493; Testing Loss 0.005830979277205086; Training Loss 0.00471481833125921\n",
      "Episode 7494; Testing Loss 0.005830914834372674; Training Loss 0.004714808333013773\n",
      "Episode 7495; Testing Loss 0.005831040438030623; Training Loss 0.004714801642143642\n",
      "Episode 7496; Testing Loss 0.005831142122966311; Training Loss 0.004714793735640419\n",
      "Episode 7497; Testing Loss 0.005831085138866095; Training Loss 0.004714781426843325\n",
      "Episode 7498; Testing Loss 0.005830983859099954; Training Loss 0.0047147727567419416\n",
      "Episode 7499; Testing Loss 0.005831128931967515; Training Loss 0.004714761394118355\n",
      "Episode 7500; Testing Loss 0.005831249416637616; Training Loss 0.004714761268336678\n",
      "Episode 7501; Testing Loss 0.0058310020456649025; Training Loss 0.00471474444047858\n",
      "Episode 7502; Testing Loss 0.005830771314379062; Training Loss 0.004714740409752974\n",
      "Episode 7503; Testing Loss 0.005831009129200201; Training Loss 0.004714729292346403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7504; Testing Loss 0.0058313054809314485; Training Loss 0.004714723795065287\n",
      "Episode 7505; Testing Loss 0.005831056438132456; Training Loss 0.00471470684775448\n",
      "Episode 7506; Testing Loss 0.005830756597193455; Training Loss 0.004714701828022164\n",
      "Episode 7507; Testing Loss 0.005830948097167757; Training Loss 0.004714685268432931\n",
      "Episode 7508; Testing Loss 0.005831153147988853; Training Loss 0.004714685611232934\n",
      "Episode 7509; Testing Loss 0.005830964885634957; Training Loss 0.004714670616399416\n",
      "Episode 7510; Testing Loss 0.005830765848244501; Training Loss 0.00471466251076857\n",
      "Episode 7511; Testing Loss 0.005830890425690769; Training Loss 0.004714652331286154\n",
      "Episode 7512; Testing Loss 0.005831077107059604; Training Loss 0.00471464694456058\n",
      "Episode 7513; Testing Loss 0.005830995138720721; Training Loss 0.004714631995800049\n",
      "Episode 7514; Testing Loss 0.005830872087639196; Training Loss 0.004714623621562331\n",
      "Episode 7515; Testing Loss 0.005830961740301012; Training Loss 0.004714610991381743\n",
      "Episode 7516; Testing Loss 0.0058310486355824115; Training Loss 0.004714607071702498\n",
      "Episode 7517; Testing Loss 0.0058308284084872515; Training Loss 0.004714592770028942\n",
      "Episode 7518; Testing Loss 0.005830704217808567; Training Loss 0.004714589194820475\n",
      "Episode 7519; Testing Loss 0.005830935524584587; Training Loss 0.004714578412864005\n",
      "Episode 7520; Testing Loss 0.005831120161031345; Training Loss 0.004714573419662474\n",
      "Episode 7521; Testing Loss 0.005830887333542384; Training Loss 0.004714556612592755\n",
      "Episode 7522; Testing Loss 0.005830733454755546; Training Loss 0.004714549296802221\n",
      "Episode 7523; Testing Loss 0.005830928851476906; Training Loss 0.0047145370627455765\n",
      "Episode 7524; Testing Loss 0.005831091222909093; Training Loss 0.004714529198299347\n",
      "Episode 7525; Testing Loss 0.005830906346827268; Training Loss 0.004714519390000917\n",
      "Episode 7526; Testing Loss 0.0058307840631125615; Training Loss 0.004714513265783113\n",
      "Episode 7527; Testing Loss 0.005830898600157827; Training Loss 0.004714499412596228\n",
      "Episode 7528; Testing Loss 0.005831005651009286; Training Loss 0.00471449399080301\n",
      "Episode 7529; Testing Loss 0.0058308298785364025; Training Loss 0.004714482849921953\n",
      "Episode 7530; Testing Loss 0.0058306755025595005; Training Loss 0.004714473208611571\n",
      "Episode 7531; Testing Loss 0.005830816452179955; Training Loss 0.00471446143186771\n",
      "Episode 7532; Testing Loss 0.005831061701303427; Training Loss 0.0047144532304689854\n",
      "Episode 7533; Testing Loss 0.005830975848845735; Training Loss 0.004714443175324491\n",
      "Episode 7534; Testing Loss 0.0058306878145756025; Training Loss 0.0047144357366748\n",
      "Episode 7535; Testing Loss 0.005830797625474572; Training Loss 0.004714422446662316\n",
      "Episode 7536; Testing Loss 0.005831016742128284; Training Loss 0.004714414393664021\n",
      "Episode 7537; Testing Loss 0.00583094783219682; Training Loss 0.004714405983508888\n",
      "Episode 7538; Testing Loss 0.005830748763124453; Training Loss 0.004714395756952306\n",
      "Episode 7539; Testing Loss 0.005830824707796316; Training Loss 0.00471438766972387\n",
      "Episode 7540; Testing Loss 0.0058309058680106616; Training Loss 0.004714378218866976\n",
      "Episode 7541; Testing Loss 0.00583086001692444; Training Loss 0.004714366255263806\n",
      "Episode 7542; Testing Loss 0.0058308082413767014; Training Loss 0.004714359390493088\n",
      "Episode 7543; Testing Loss 0.005830783898436719; Training Loss 0.004714346655815133\n",
      "Episode 7544; Testing Loss 0.005830832719163066; Training Loss 0.004714337324733323\n",
      "Episode 7545; Testing Loss 0.005830765791615285; Training Loss 0.0047143286765024845\n",
      "Episode 7546; Testing Loss 0.005830793652829147; Training Loss 0.00471431930385147\n",
      "Episode 7547; Testing Loss 0.005830853833355937; Training Loss 0.004714309387623875\n",
      "Episode 7548; Testing Loss 0.005830789121791793; Training Loss 0.0047143011099471535\n",
      "Episode 7549; Testing Loss 0.00583081393326001; Training Loss 0.0047142915857005694\n",
      "Episode 7550; Testing Loss 0.0058308297528170926; Training Loss 0.004714281394861757\n",
      "Episode 7551; Testing Loss 0.005830706502031779; Training Loss 0.004714273722666871\n",
      "Episode 7552; Testing Loss 0.00583069393208213; Training Loss 0.0047142636189013085\n",
      "Episode 7553; Testing Loss 0.005830728720388134; Training Loss 0.004714254443568996\n",
      "Episode 7554; Testing Loss 0.005830685551556325; Training Loss 0.00471424671973525\n",
      "Episode 7555; Testing Loss 0.005830726658156648; Training Loss 0.004714234751798777\n",
      "Episode 7556; Testing Loss 0.005830792561978223; Training Loss 0.004714229044086888\n",
      "Episode 7557; Testing Loss 0.005830705789413217; Training Loss 0.0047142201243318416\n",
      "Episode 7558; Testing Loss 0.005830648790787721; Training Loss 0.004714212535241657\n",
      "Episode 7559; Testing Loss 0.005830805145947997; Training Loss 0.00471419968543099\n",
      "Episode 7560; Testing Loss 0.005830837600209053; Training Loss 0.004714193918499438\n",
      "Episode 7561; Testing Loss 0.005830642712936719; Training Loss 0.0047141834062546\n",
      "Episode 7562; Testing Loss 0.005830531469922837; Training Loss 0.004714169960831107\n",
      "Episode 7563; Testing Loss 0.0058306541150288885; Training Loss 0.004714160695190714\n",
      "Episode 7564; Testing Loss 0.0058307682496542305; Training Loss 0.004714153259262517\n",
      "Episode 7565; Testing Loss 0.005830641234641348; Training Loss 0.004714140924792962\n",
      "Episode 7566; Testing Loss 0.005830586523245907; Training Loss 0.004714134881663532\n",
      "Episode 7567; Testing Loss 0.005830704983192489; Training Loss 0.004714125587840437\n",
      "Episode 7568; Testing Loss 0.005830688964445226; Training Loss 0.004714112437178993\n",
      "Episode 7569; Testing Loss 0.0058305860117369174; Training Loss 0.0047141091281381765\n",
      "Episode 7570; Testing Loss 0.005830657695118395; Training Loss 0.004714098198730747\n",
      "Episode 7571; Testing Loss 0.005830714377790311; Training Loss 0.004714089418360316\n",
      "Episode 7572; Testing Loss 0.0058306135270760106; Training Loss 0.004714078327880937\n",
      "Episode 7573; Testing Loss 0.005830546481947054; Training Loss 0.004714070878012281\n",
      "Episode 7574; Testing Loss 0.005830718268330764; Training Loss 0.004714059161882628\n",
      "Episode 7575; Testing Loss 0.005830846550073267; Training Loss 0.004714052012562962\n",
      "Episode 7576; Testing Loss 0.00583063772890905; Training Loss 0.004714039099552562\n",
      "Episode 7577; Testing Loss 0.005830406891653711; Training Loss 0.004714035184920932\n",
      "Episode 7578; Testing Loss 0.005830580005822165; Training Loss 0.004714024441991537\n",
      "Episode 7579; Testing Loss 0.005830836639616281; Training Loss 0.004714021045439989\n",
      "Episode 7580; Testing Loss 0.005830667819812657; Training Loss 0.004714003329293725\n",
      "Episode 7581; Testing Loss 0.005830465676331085; Training Loss 0.004714000390340389\n",
      "Episode 7582; Testing Loss 0.00583068550517174; Training Loss 0.004713987023300057\n",
      "Episode 7583; Testing Loss 0.0058308764554697145; Training Loss 0.004713980687275924\n",
      "Episode 7584; Testing Loss 0.0058306427341155076; Training Loss 0.00471396350239868\n",
      "Episode 7585; Testing Loss 0.0058303959407865445; Training Loss 0.004713958259359785\n",
      "Episode 7586; Testing Loss 0.005830529769862126; Training Loss 0.004713949828404233\n",
      "Episode 7587; Testing Loss 0.005830674760611645; Training Loss 0.0047139390301771935\n",
      "Episode 7588; Testing Loss 0.005830629580345623; Training Loss 0.004713926770407289\n",
      "Episode 7589; Testing Loss 0.005830534849025156; Training Loss 0.004713920696561361\n",
      "Episode 7590; Testing Loss 0.0058305662575882785; Training Loss 0.0047139154094682015\n",
      "Episode 7591; Testing Loss 0.005830641095093535; Training Loss 0.004713902214751828\n",
      "Episode 7592; Testing Loss 0.0058306168273417665; Training Loss 0.004713890453219147\n",
      "Episode 7593; Testing Loss 0.005830616078334195; Training Loss 0.004713883326434001\n",
      "Episode 7594; Testing Loss 0.005830579496182894; Training Loss 0.004713872070995015\n",
      "Episode 7595; Testing Loss 0.005830546537463388; Training Loss 0.004713861118041401\n",
      "Episode 7596; Testing Loss 0.00583050308166428; Training Loss 0.004713855239319163\n",
      "Episode 7597; Testing Loss 0.00583047388265577; Training Loss 0.0047138451085176475\n",
      "Episode 7598; Testing Loss 0.005830511241088501; Training Loss 0.004713832253273059\n",
      "Episode 7599; Testing Loss 0.005830586185024506; Training Loss 0.004713825292069232\n",
      "Episode 7600; Testing Loss 0.005830569308337813; Training Loss 0.004713814758174208\n",
      "Episode 7601; Testing Loss 0.005830445859857381; Training Loss 0.004713805189580438\n",
      "Episode 7602; Testing Loss 0.0058304035939901805; Training Loss 0.004713796590320697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7603; Testing Loss 0.005830518628419091; Training Loss 0.004713786129476681\n",
      "Episode 7604; Testing Loss 0.005830590214695835; Training Loss 0.004713777044329146\n",
      "Episode 7605; Testing Loss 0.005830606101392565; Training Loss 0.004713768337287875\n",
      "Episode 7606; Testing Loss 0.005830588484820389; Training Loss 0.004713757045990662\n",
      "Episode 7607; Testing Loss 0.005830470231567418; Training Loss 0.00471374805288198\n",
      "Episode 7608; Testing Loss 0.005830396055545264; Training Loss 0.004713740389453278\n",
      "Episode 7609; Testing Loss 0.005830555929173232; Training Loss 0.004713729671873099\n",
      "Episode 7610; Testing Loss 0.005830579137850717; Training Loss 0.00471372118420041\n",
      "Episode 7611; Testing Loss 0.0058303966597393515; Training Loss 0.004713714826901142\n",
      "Episode 7612; Testing Loss 0.00583048793717606; Training Loss 0.004713704255215392\n",
      "Episode 7613; Testing Loss 0.005830660319953834; Training Loss 0.004713694625820916\n",
      "Episode 7614; Testing Loss 0.005830532664734035; Training Loss 0.004713684611342011\n",
      "Episode 7615; Testing Loss 0.005830326801284172; Training Loss 0.004713676167312626\n",
      "Episode 7616; Testing Loss 0.005830387719127156; Training Loss 0.004713666048303986\n",
      "Episode 7617; Testing Loss 0.005830502941864554; Training Loss 0.004713659348458618\n",
      "Episode 7618; Testing Loss 0.005830428826510488; Training Loss 0.004713648846082338\n",
      "Episode 7619; Testing Loss 0.005830354571564571; Training Loss 0.004713636921889084\n",
      "Episode 7620; Testing Loss 0.005830376178596225; Training Loss 0.004713631170941153\n",
      "Episode 7621; Testing Loss 0.0058303804923747465; Training Loss 0.004713619756803092\n",
      "Episode 7622; Testing Loss 0.005830420015725272; Training Loss 0.004713610085449197\n",
      "Episode 7623; Testing Loss 0.005830496343154586; Training Loss 0.004713603744180533\n",
      "Episode 7624; Testing Loss 0.0058303656074397674; Training Loss 0.004713593182899353\n",
      "Episode 7625; Testing Loss 0.005830348054260884; Training Loss 0.004713580478353448\n",
      "Episode 7626; Testing Loss 0.00583038278293333; Training Loss 0.004713572245811628\n",
      "Episode 7627; Testing Loss 0.0058303836829977735; Training Loss 0.004713562235634567\n",
      "Episode 7628; Testing Loss 0.005830359958629801; Training Loss 0.00471355342991986\n",
      "Episode 7629; Testing Loss 0.005830384527965844; Training Loss 0.00471354427525439\n",
      "Episode 7630; Testing Loss 0.005830289866187732; Training Loss 0.004713535216858505\n",
      "Episode 7631; Testing Loss 0.005830316118734551; Training Loss 0.004713525175587937\n",
      "Episode 7632; Testing Loss 0.005830333068721934; Training Loss 0.004713518041290775\n",
      "Episode 7633; Testing Loss 0.005830330586478174; Training Loss 0.0047135066928105\n",
      "Episode 7634; Testing Loss 0.005830353586657821; Training Loss 0.004713501524894586\n",
      "Episode 7635; Testing Loss 0.005830314174636619; Training Loss 0.004713490416488784\n",
      "Episode 7636; Testing Loss 0.005830270736318735; Training Loss 0.004713481633658273\n",
      "Episode 7637; Testing Loss 0.005830383901400442; Training Loss 0.0047134745192414975\n",
      "Episode 7638; Testing Loss 0.0058304280112603385; Training Loss 0.004713461690749698\n",
      "Episode 7639; Testing Loss 0.005830222180613184; Training Loss 0.00471345488132061\n",
      "Episode 7640; Testing Loss 0.00583012861498719; Training Loss 0.004713449839437848\n",
      "Episode 7641; Testing Loss 0.005830382956215168; Training Loss 0.004713437337528545\n",
      "Episode 7642; Testing Loss 0.00583044608458789; Training Loss 0.004713426522090266\n",
      "Episode 7643; Testing Loss 0.005830261406799242; Training Loss 0.0047134165517169835\n",
      "Episode 7644; Testing Loss 0.005830084048436917; Training Loss 0.004713407844429642\n",
      "Episode 7645; Testing Loss 0.005830087019177367; Training Loss 0.004713399441633888\n",
      "Episode 7646; Testing Loss 0.005830203147979246; Training Loss 0.004713389554840806\n",
      "Episode 7647; Testing Loss 0.005830255512578503; Training Loss 0.004713383406958544\n",
      "Episode 7648; Testing Loss 0.0058303054185834015; Training Loss 0.004713373675585805\n",
      "Episode 7649; Testing Loss 0.005830307854839507; Training Loss 0.004713361480413113\n",
      "Episode 7650; Testing Loss 0.00583024657707394; Training Loss 0.004713353723008483\n",
      "Episode 7651; Testing Loss 0.005830203771222547; Training Loss 0.004713346354202171\n",
      "Episode 7652; Testing Loss 0.005830187297245887; Training Loss 0.004713334849714774\n",
      "Episode 7653; Testing Loss 0.005830145056248365; Training Loss 0.004713323497245543\n",
      "Episode 7654; Testing Loss 0.005830134295150655; Training Loss 0.004713319045368713\n",
      "Episode 7655; Testing Loss 0.005830213269874475; Training Loss 0.004713309667645638\n",
      "Episode 7656; Testing Loss 0.005830220692582613; Training Loss 0.0047132999177314505\n",
      "Episode 7657; Testing Loss 0.0058300887075652735; Training Loss 0.004713288348788454\n",
      "Episode 7658; Testing Loss 0.0058300423845562685; Training Loss 0.004713280801882813\n",
      "Episode 7659; Testing Loss 0.005830219817707789; Training Loss 0.004713268909122031\n",
      "Episode 7660; Testing Loss 0.005830316815361564; Training Loss 0.004713262512386054\n",
      "Episode 7661; Testing Loss 0.005830200501393893; Training Loss 0.004713250379890924\n",
      "Episode 7662; Testing Loss 0.005830031550334139; Training Loss 0.004713239532572031\n",
      "Episode 7663; Testing Loss 0.005830016578855903; Training Loss 0.004713231980011139\n",
      "Episode 7664; Testing Loss 0.005830103491882698; Training Loss 0.004713220674908776\n",
      "Episode 7665; Testing Loss 0.005830127458699811; Training Loss 0.004713211734439199\n",
      "Episode 7666; Testing Loss 0.005830130470463006; Training Loss 0.004713205214503234\n",
      "Episode 7667; Testing Loss 0.005830159711463329; Training Loss 0.004713193183006785\n",
      "Episode 7668; Testing Loss 0.005830186755027197; Training Loss 0.004713186009701787\n",
      "Episode 7669; Testing Loss 0.005830071025828301; Training Loss 0.0047131769494641\n",
      "Episode 7670; Testing Loss 0.005829997111175184; Training Loss 0.004713166577784521\n",
      "Episode 7671; Testing Loss 0.005830127190253844; Training Loss 0.004713156163999325\n",
      "Episode 7672; Testing Loss 0.005830196949438679; Training Loss 0.004713150436950268\n",
      "Episode 7673; Testing Loss 0.005829995047733841; Training Loss 0.004713138782113005\n",
      "Episode 7674; Testing Loss 0.005829887329562931; Training Loss 0.004713130335883925\n",
      "Episode 7675; Testing Loss 0.005830047010834878; Training Loss 0.004713122438208296\n",
      "Episode 7676; Testing Loss 0.005830087283091477; Training Loss 0.004713110744002609\n",
      "Episode 7677; Testing Loss 0.005829978876074963; Training Loss 0.004713103407056088\n",
      "Episode 7678; Testing Loss 0.005829964165004562; Training Loss 0.0047130952739370185\n",
      "Episode 7679; Testing Loss 0.005830003303474269; Training Loss 0.004713082848711066\n",
      "Episode 7680; Testing Loss 0.005829974541502912; Training Loss 0.004713075747388789\n",
      "Episode 7681; Testing Loss 0.005829922684398826; Training Loss 0.00471306836083333\n",
      "Episode 7682; Testing Loss 0.005829963826176508; Training Loss 0.004713064289649026\n",
      "Episode 7683; Testing Loss 0.005830143502479524; Training Loss 0.004713055054196269\n",
      "Episode 7684; Testing Loss 0.005830105197798078; Training Loss 0.004713039940133582\n",
      "Episode 7685; Testing Loss 0.00582986640838409; Training Loss 0.004713027715416107\n",
      "Episode 7686; Testing Loss 0.005829711051864826; Training Loss 0.00471302180130997\n",
      "Episode 7687; Testing Loss 0.0058298807571411015; Training Loss 0.004713009715537014\n",
      "Episode 7688; Testing Loss 0.005830136564540169; Training Loss 0.0047130036229041505\n",
      "Episode 7689; Testing Loss 0.005830008096895274; Training Loss 0.0047129900892223815\n",
      "Episode 7690; Testing Loss 0.00582972689584814; Training Loss 0.004712987397998307\n",
      "Episode 7691; Testing Loss 0.005829825132264228; Training Loss 0.004712977164957304\n",
      "Episode 7692; Testing Loss 0.005830028924681428; Training Loss 0.004712964567445583\n",
      "Episode 7693; Testing Loss 0.0058299448087374695; Training Loss 0.004712956457032361\n",
      "Episode 7694; Testing Loss 0.0058298219632817085; Training Loss 0.004712953740034968\n",
      "Episode 7695; Testing Loss 0.005829820140594292; Training Loss 0.004712942744641336\n",
      "Episode 7696; Testing Loss 0.005829907576660225; Training Loss 0.004712925329741849\n",
      "Episode 7697; Testing Loss 0.0058298970680663136; Training Loss 0.004712921447066412\n",
      "Episode 7698; Testing Loss 0.005829812635161489; Training Loss 0.004712914271677918\n",
      "Episode 7699; Testing Loss 0.005829811166509207; Training Loss 0.004712905770859673\n",
      "Episode 7700; Testing Loss 0.00582999184847746; Training Loss 0.00471289595430898\n",
      "Episode 7701; Testing Loss 0.005829954999981081; Training Loss 0.004712881401186994\n",
      "Episode 7702; Testing Loss 0.005829710858088987; Training Loss 0.004712869850910096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7703; Testing Loss 0.005829638865801182; Training Loss 0.004712869386150139\n",
      "Episode 7704; Testing Loss 0.005829784747739958; Training Loss 0.004712861120575332\n",
      "Episode 7705; Testing Loss 0.005829862217257913; Training Loss 0.004712848173296339\n",
      "Episode 7706; Testing Loss 0.0058298149901079765; Training Loss 0.004712833177956644\n",
      "Episode 7707; Testing Loss 0.005829773566908171; Training Loss 0.004712826904753718\n",
      "Episode 7708; Testing Loss 0.005829769268482154; Training Loss 0.004712819604755397\n",
      "Episode 7709; Testing Loss 0.00582979949999384; Training Loss 0.004712809354199612\n",
      "Episode 7710; Testing Loss 0.005829915850205571; Training Loss 0.0047127982969294244\n",
      "Episode 7711; Testing Loss 0.005829825826573506; Training Loss 0.0047127871774013345\n",
      "Episode 7712; Testing Loss 0.005829631662500214; Training Loss 0.004712780306371056\n",
      "Episode 7713; Testing Loss 0.0058295250040953945; Training Loss 0.0047127713297903575\n",
      "Episode 7714; Testing Loss 0.005829721122798206; Training Loss 0.004712758920080862\n",
      "Episode 7715; Testing Loss 0.005829853775149704; Training Loss 0.004712754035279095\n",
      "Episode 7716; Testing Loss 0.005829685119798064; Training Loss 0.004712745717567601\n",
      "Episode 7717; Testing Loss 0.005829648217355566; Training Loss 0.004712733513857698\n",
      "Episode 7718; Testing Loss 0.005829712941144353; Training Loss 0.004712723858175085\n",
      "Episode 7719; Testing Loss 0.005829644503456912; Training Loss 0.00471271338575582\n",
      "Episode 7720; Testing Loss 0.005829522189343238; Training Loss 0.004712709014730122\n",
      "Episode 7721; Testing Loss 0.0058295603503075565; Training Loss 0.004712698709823985\n",
      "Episode 7722; Testing Loss 0.005829723541539837; Training Loss 0.00471268754796384\n",
      "Episode 7723; Testing Loss 0.0058296970255217505; Training Loss 0.004712678528575966\n",
      "Episode 7724; Testing Loss 0.005829486624595151; Training Loss 0.0047126691400872665\n",
      "Episode 7725; Testing Loss 0.005829432811238197; Training Loss 0.0047126595015398245\n",
      "Episode 7726; Testing Loss 0.005829679907455741; Training Loss 0.004712650247066729\n",
      "Episode 7727; Testing Loss 0.005829727571971235; Training Loss 0.004712641246923925\n",
      "Episode 7728; Testing Loss 0.005829524622654682; Training Loss 0.004712629665048141\n",
      "Episode 7729; Testing Loss 0.005829447291058169; Training Loss 0.004712622491511286\n",
      "Episode 7730; Testing Loss 0.00582969383728266; Training Loss 0.004712612530941213\n",
      "Episode 7731; Testing Loss 0.005829730125279472; Training Loss 0.004712604010686357\n",
      "Episode 7732; Testing Loss 0.00582951064988692; Training Loss 0.004712592283801381\n",
      "Episode 7733; Testing Loss 0.005829430708650764; Training Loss 0.004712584441915877\n",
      "Episode 7734; Testing Loss 0.0058296190113492396; Training Loss 0.004712574937117427\n",
      "Episode 7735; Testing Loss 0.005829614683262193; Training Loss 0.004712566317374475\n",
      "Episode 7736; Testing Loss 0.005829494252184326; Training Loss 0.00471255521480118\n",
      "Episode 7737; Testing Loss 0.005829471080706906; Training Loss 0.004712546329478333\n",
      "Episode 7738; Testing Loss 0.005829489283676075; Training Loss 0.004712536887630016\n",
      "Episode 7739; Testing Loss 0.005829516815943113; Training Loss 0.004712529405282926\n",
      "Episode 7740; Testing Loss 0.005829603742959429; Training Loss 0.0047125228194999065\n",
      "Episode 7741; Testing Loss 0.00582958502920734; Training Loss 0.0047125104226476335\n",
      "Episode 7742; Testing Loss 0.005829401105843726; Training Loss 0.004712500918023855\n",
      "Episode 7743; Testing Loss 0.005829328761672544; Training Loss 0.004712495962376854\n",
      "Episode 7744; Testing Loss 0.005829359762681153; Training Loss 0.00471248201466581\n",
      "Episode 7745; Testing Loss 0.0058293976327392265; Training Loss 0.004712473918098277\n",
      "Episode 7746; Testing Loss 0.005829495252617158; Training Loss 0.004712465724651991\n",
      "Episode 7747; Testing Loss 0.00582945204608888; Training Loss 0.004712456254981264\n",
      "Episode 7748; Testing Loss 0.005829331381255106; Training Loss 0.004712446017293124\n",
      "Episode 7749; Testing Loss 0.005829396286003091; Training Loss 0.004712437872014662\n",
      "Episode 7750; Testing Loss 0.0058295249498253; Training Loss 0.004712428079234842\n",
      "Episode 7751; Testing Loss 0.005829506200046301; Training Loss 0.004712416508251153\n",
      "Episode 7752; Testing Loss 0.0058293835117526295; Training Loss 0.004712410242838117\n",
      "Episode 7753; Testing Loss 0.005829391767913776; Training Loss 0.0047123996284142025\n",
      "Episode 7754; Testing Loss 0.005829410200275582; Training Loss 0.004712389438222599\n",
      "Episode 7755; Testing Loss 0.005829374776185141; Training Loss 0.004712379414320275\n",
      "Episode 7756; Testing Loss 0.005829367065100811; Training Loss 0.004712372126063872\n",
      "Episode 7757; Testing Loss 0.005829351201518199; Training Loss 0.004712361827093699\n",
      "Episode 7758; Testing Loss 0.005829368052533293; Training Loss 0.004712353472787287\n",
      "Episode 7759; Testing Loss 0.005829455690904905; Training Loss 0.004712344232331833\n",
      "Episode 7760; Testing Loss 0.005829456919958588; Training Loss 0.004712333306511766\n",
      "Episode 7761; Testing Loss 0.00582934940881518; Training Loss 0.004712328355215217\n",
      "Episode 7762; Testing Loss 0.005829236030484079; Training Loss 0.004712319002097132\n",
      "Episode 7763; Testing Loss 0.005829248756405933; Training Loss 0.0047123065424851274\n",
      "Episode 7764; Testing Loss 0.005829382417696194; Training Loss 0.004712298869780714\n",
      "Episode 7765; Testing Loss 0.005829489636983672; Training Loss 0.004712291399572543\n",
      "Episode 7766; Testing Loss 0.005829441512701056; Training Loss 0.0047122804156503675\n",
      "Episode 7767; Testing Loss 0.0058292617154582305; Training Loss 0.004712270091909644\n",
      "Episode 7768; Testing Loss 0.005829275742025572; Training Loss 0.0047122612459518645\n",
      "Episode 7769; Testing Loss 0.005829383121120495; Training Loss 0.004712251173267785\n",
      "Episode 7770; Testing Loss 0.00582938289197847; Training Loss 0.004712244030944748\n",
      "Episode 7771; Testing Loss 0.005829205792777619; Training Loss 0.004712236548692798\n",
      "Episode 7772; Testing Loss 0.005829181883984202; Training Loss 0.004712225316544435\n",
      "Episode 7773; Testing Loss 0.005829246817059557; Training Loss 0.004712212975906983\n",
      "Episode 7774; Testing Loss 0.0058292181245902415; Training Loss 0.004712207189963409\n",
      "Episode 7775; Testing Loss 0.0058292582095424966; Training Loss 0.00471219639658213\n",
      "Episode 7776; Testing Loss 0.005829307078619119; Training Loss 0.004712187772686981\n",
      "Episode 7777; Testing Loss 0.005829146338289447; Training Loss 0.004712176734563686\n",
      "Episode 7778; Testing Loss 0.005829044105056631; Training Loss 0.004712171901246101\n",
      "Episode 7779; Testing Loss 0.005829229341307094; Training Loss 0.004712160508785785\n",
      "Episode 7780; Testing Loss 0.005829347587811797; Training Loss 0.004712150504681137\n",
      "Episode 7781; Testing Loss 0.005829163943029581; Training Loss 0.004712140584170546\n",
      "Episode 7782; Testing Loss 0.005829004797086987; Training Loss 0.00471213366317633\n",
      "Episode 7783; Testing Loss 0.005829171527962293; Training Loss 0.0047121209899039125\n",
      "Episode 7784; Testing Loss 0.005829244546113582; Training Loss 0.0047121152982292904\n",
      "Episode 7785; Testing Loss 0.005829026685355601; Training Loss 0.0047121074693748525\n",
      "Episode 7786; Testing Loss 0.005829032540569022; Training Loss 0.0047120978159896935\n",
      "Episode 7787; Testing Loss 0.005829215844735813; Training Loss 0.004712087166246457\n",
      "Episode 7788; Testing Loss 0.005829166149791074; Training Loss 0.00471207754944159\n",
      "Episode 7789; Testing Loss 0.005828997413293813; Training Loss 0.004712071624594423\n",
      "Episode 7790; Testing Loss 0.005829098693730425; Training Loss 0.0047120616281944435\n",
      "Episode 7791; Testing Loss 0.005829290254969374; Training Loss 0.004712051675603033\n",
      "Episode 7792; Testing Loss 0.0058292098747273514; Training Loss 0.004712043529702799\n",
      "Episode 7793; Testing Loss 0.005828867452521836; Training Loss 0.004712033487560758\n",
      "Episode 7794; Testing Loss 0.005828782748240847; Training Loss 0.004712027104230729\n",
      "Episode 7795; Testing Loss 0.005829126671097958; Training Loss 0.004712012715520637\n",
      "Episode 7796; Testing Loss 0.005829229943807444; Training Loss 0.00471200671055355\n",
      "Episode 7797; Testing Loss 0.005828952860037252; Training Loss 0.004711994310743006\n",
      "Episode 7798; Testing Loss 0.005828850499665372; Training Loss 0.0047119864993789866\n",
      "Episode 7799; Testing Loss 0.005829068236917783; Training Loss 0.004711979420836849\n",
      "Episode 7800; Testing Loss 0.005829128678753909; Training Loss 0.0047119672354557504\n",
      "Episode 7801; Testing Loss 0.005828937847069863; Training Loss 0.0047119617677534595\n",
      "Episode 7802; Testing Loss 0.005828975585023978; Training Loss 0.004711952222644686\n",
      "Episode 7803; Testing Loss 0.005829112719412226; Training Loss 0.00471194002745296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7804; Testing Loss 0.005829084543990566; Training Loss 0.004711934597476081\n",
      "Episode 7805; Testing Loss 0.005828869919036499; Training Loss 0.004711921865592418\n",
      "Episode 7806; Testing Loss 0.005828789618068463; Training Loss 0.004711916439922089\n",
      "Episode 7807; Testing Loss 0.00582901714854003; Training Loss 0.004711908491965343\n",
      "Episode 7808; Testing Loss 0.005829082547683577; Training Loss 0.004711899463411613\n",
      "Episode 7809; Testing Loss 0.005828835875824898; Training Loss 0.004711884839738997\n",
      "Episode 7810; Testing Loss 0.00582865303117294; Training Loss 0.004711880643245951\n",
      "Episode 7811; Testing Loss 0.0058288745025114365; Training Loss 0.004711865961475705\n",
      "Episode 7812; Testing Loss 0.005829154361730589; Training Loss 0.004711861351674671\n",
      "Episode 7813; Testing Loss 0.005828999211233837; Training Loss 0.004711849895935715\n",
      "Episode 7814; Testing Loss 0.005828777613831862; Training Loss 0.004711844020340749\n",
      "Episode 7815; Testing Loss 0.005828880487379458; Training Loss 0.004711829695969937\n",
      "Episode 7816; Testing Loss 0.0058289932666742445; Training Loss 0.004711827630551496\n",
      "Episode 7817; Testing Loss 0.005828799840316696; Training Loss 0.004711813779789771\n",
      "Episode 7818; Testing Loss 0.005828621604840434; Training Loss 0.004711810081411674\n",
      "Episode 7819; Testing Loss 0.005828887624553677; Training Loss 0.004711795853001157\n",
      "Episode 7820; Testing Loss 0.005829075672882478; Training Loss 0.004711788463590952\n",
      "Episode 7821; Testing Loss 0.005828844694663326; Training Loss 0.004711774207950375\n",
      "Episode 7822; Testing Loss 0.005828579501346908; Training Loss 0.0047117704887092975\n",
      "Episode 7823; Testing Loss 0.005828780561375613; Training Loss 0.00471175582938457\n",
      "Episode 7824; Testing Loss 0.005828968385820423; Training Loss 0.00471174987024815\n",
      "Episode 7825; Testing Loss 0.005828834284858501; Training Loss 0.004711738520455189\n",
      "Episode 7826; Testing Loss 0.005828739545226203; Training Loss 0.004711730413397609\n",
      "Episode 7827; Testing Loss 0.005828900828303171; Training Loss 0.004711721611997518\n",
      "Episode 7828; Testing Loss 0.005828917863066721; Training Loss 0.004711714623165422\n",
      "Episode 7829; Testing Loss 0.005828709763576963; Training Loss 0.0047117020385505715\n",
      "Episode 7830; Testing Loss 0.005828700448819226; Training Loss 0.004711694028420434\n",
      "Episode 7831; Testing Loss 0.005828865662969145; Training Loss 0.004711683695824227\n",
      "Episode 7832; Testing Loss 0.005828821638549515; Training Loss 0.004711674993422452\n",
      "Episode 7833; Testing Loss 0.005828656533996663; Training Loss 0.0047116654381556275\n",
      "Episode 7834; Testing Loss 0.005828727360495058; Training Loss 0.004711656090097846\n",
      "Episode 7835; Testing Loss 0.005828898245797654; Training Loss 0.004711646544380598\n",
      "Episode 7836; Testing Loss 0.0058288504805966285; Training Loss 0.004711636321295175\n",
      "Episode 7837; Testing Loss 0.005828661056822824; Training Loss 0.004711628874890146\n",
      "Episode 7838; Testing Loss 0.005828696155741262; Training Loss 0.004711618872756162\n",
      "Episode 7839; Testing Loss 0.005828742453296517; Training Loss 0.004711609154146504\n",
      "Episode 7840; Testing Loss 0.005828679839292702; Training Loss 0.004711603367205374\n",
      "Episode 7841; Testing Loss 0.005828685617081906; Training Loss 0.0047115959749171035\n",
      "Episode 7842; Testing Loss 0.005828724538075001; Training Loss 0.004711582292228012\n",
      "Episode 7843; Testing Loss 0.00582873158527331; Training Loss 0.004711578198035584\n",
      "Episode 7844; Testing Loss 0.005828636932237227; Training Loss 0.004711570035249301\n",
      "Episode 7845; Testing Loss 0.005828624417077343; Training Loss 0.004711555111932265\n",
      "Episode 7846; Testing Loss 0.005828648462316517; Training Loss 0.004711550025339902\n",
      "Episode 7847; Testing Loss 0.005828701596349734; Training Loss 0.004711542407531194\n",
      "Episode 7848; Testing Loss 0.005828717103950373; Training Loss 0.004711531452263366\n",
      "Episode 7849; Testing Loss 0.005828654770887619; Training Loss 0.004711518560049566\n",
      "Episode 7850; Testing Loss 0.00582864132585952; Training Loss 0.00471151446018218\n",
      "Episode 7851; Testing Loss 0.0058286585762392885; Training Loss 0.004711506758187484\n",
      "Episode 7852; Testing Loss 0.005828637538654754; Training Loss 0.004711495605694252\n",
      "Episode 7853; Testing Loss 0.0058285279100404935; Training Loss 0.004711483506934831\n",
      "Episode 7854; Testing Loss 0.005828570147896049; Training Loss 0.004711477683957626\n",
      "Episode 7855; Testing Loss 0.005828708686917369; Training Loss 0.004711466640790529\n",
      "Episode 7856; Testing Loss 0.0058285957653801835; Training Loss 0.004711454850572173\n",
      "Episode 7857; Testing Loss 0.00582840161880082; Training Loss 0.004711448983860135\n",
      "Episode 7858; Testing Loss 0.005828387583344553; Training Loss 0.0047114397540473\n",
      "Episode 7859; Testing Loss 0.005828671458857767; Training Loss 0.004711429029132617\n",
      "Episode 7860; Testing Loss 0.005828697368786738; Training Loss 0.004711420573464863\n",
      "Episode 7861; Testing Loss 0.005828482249143269; Training Loss 0.004711411090646684\n",
      "Episode 7862; Testing Loss 0.005828455608476358; Training Loss 0.004711401246896057\n",
      "Episode 7863; Testing Loss 0.005828636225704755; Training Loss 0.004711391492549715\n",
      "Episode 7864; Testing Loss 0.0058286492876715714; Training Loss 0.004711382176929147\n",
      "Episode 7865; Testing Loss 0.005828472829529428; Training Loss 0.004711372466313024\n",
      "Episode 7866; Testing Loss 0.0058284283498701425; Training Loss 0.004711363916435133\n",
      "Episode 7867; Testing Loss 0.005828488464937546; Training Loss 0.004711353818558588\n",
      "Episode 7868; Testing Loss 0.005828499529793087; Training Loss 0.0047113469893558125\n",
      "Episode 7869; Testing Loss 0.005828419446325209; Training Loss 0.004711335194077161\n",
      "Episode 7870; Testing Loss 0.0058284279552739664; Training Loss 0.0047113261906931575\n",
      "Episode 7871; Testing Loss 0.005828564231787416; Training Loss 0.004711317383207384\n",
      "Episode 7872; Testing Loss 0.0058285449628804374; Training Loss 0.004711307565431459\n",
      "Episode 7873; Testing Loss 0.005828417699457319; Training Loss 0.004711299085229174\n",
      "Episode 7874; Testing Loss 0.0058283505053311835; Training Loss 0.0047112903108050466\n",
      "Episode 7875; Testing Loss 0.005828497175057567; Training Loss 0.004711282105922712\n",
      "Episode 7876; Testing Loss 0.005828452244211903; Training Loss 0.004711272874227917\n",
      "Episode 7877; Testing Loss 0.005828392994743825; Training Loss 0.004711262452888212\n",
      "Episode 7878; Testing Loss 0.005828342216200551; Training Loss 0.00471125556451148\n",
      "Episode 7879; Testing Loss 0.005828447028805339; Training Loss 0.004711249052232485\n",
      "Episode 7880; Testing Loss 0.005828513976559856; Training Loss 0.004711238491165211\n",
      "Episode 7881; Testing Loss 0.00582837230777081; Training Loss 0.0047112282318068\n",
      "Episode 7882; Testing Loss 0.005828386951609731; Training Loss 0.004711223493022981\n",
      "Episode 7883; Testing Loss 0.005828604914464416; Training Loss 0.004711211923265588\n",
      "Episode 7884; Testing Loss 0.0058285799571034195; Training Loss 0.004711200788509331\n",
      "Episode 7885; Testing Loss 0.005828213368413869; Training Loss 0.004711193748017199\n",
      "Episode 7886; Testing Loss 0.005828128017981046; Training Loss 0.004711182405965493\n",
      "Episode 7887; Testing Loss 0.005828352736871576; Training Loss 0.004711177995479356\n",
      "Episode 7888; Testing Loss 0.005828437876129891; Training Loss 0.004711171831300395\n",
      "Episode 7889; Testing Loss 0.005828244024743416; Training Loss 0.004711155711862684\n",
      "Episode 7890; Testing Loss 0.005828254168661726; Training Loss 0.004711148088168206\n",
      "Episode 7891; Testing Loss 0.005828497445385466; Training Loss 0.00471113966781767\n",
      "Episode 7892; Testing Loss 0.005828529342391122; Training Loss 0.0047111297781912975\n",
      "Episode 7893; Testing Loss 0.005828254911899116; Training Loss 0.00471111742270499\n",
      "Episode 7894; Testing Loss 0.005828123905830734; Training Loss 0.004711116288932092\n",
      "Episode 7895; Testing Loss 0.005828396747206966; Training Loss 0.004711102118669976\n",
      "Episode 7896; Testing Loss 0.00582852069195299; Training Loss 0.00471109464221997\n",
      "Episode 7897; Testing Loss 0.0058282368264711655; Training Loss 0.004711083722779691\n",
      "Episode 7898; Testing Loss 0.005827988206881219; Training Loss 0.004711077670537693\n",
      "Episode 7899; Testing Loss 0.005828250427052962; Training Loss 0.004711061502338234\n",
      "Episode 7900; Testing Loss 0.005828551170000251; Training Loss 0.004711062308877967\n",
      "Episode 7901; Testing Loss 0.005828409526926324; Training Loss 0.0047110479932405535\n",
      "Episode 7902; Testing Loss 0.005828137249441339; Training Loss 0.004711041793819137\n",
      "Episode 7903; Testing Loss 0.00582824417843451; Training Loss 0.004711025333445899\n",
      "Episode 7904; Testing Loss 0.005828393285643324; Training Loss 0.004711021921322694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7905; Testing Loss 0.005828192335021571; Training Loss 0.004711009389115904\n",
      "Episode 7906; Testing Loss 0.005827987567804745; Training Loss 0.004711005102014281\n",
      "Episode 7907; Testing Loss 0.005828194169216005; Training Loss 0.00471099282424659\n",
      "Episode 7908; Testing Loss 0.005828448137378908; Training Loss 0.004710987671742632\n",
      "Episode 7909; Testing Loss 0.005828261681068187; Training Loss 0.004710970641384741\n",
      "Episode 7910; Testing Loss 0.005828005774921195; Training Loss 0.00471096729862005\n",
      "Episode 7911; Testing Loss 0.005828135981427534; Training Loss 0.0047109547062366\n",
      "Episode 7912; Testing Loss 0.0058283288275169445; Training Loss 0.00471094696486464\n",
      "Episode 7913; Testing Loss 0.005828234749702147; Training Loss 0.00471093422225945\n",
      "Episode 7914; Testing Loss 0.005828014476733606; Training Loss 0.004710929895594149\n",
      "Episode 7915; Testing Loss 0.005828069116181672; Training Loss 0.004710919904993975\n",
      "Episode 7916; Testing Loss 0.005828276462173733; Training Loss 0.004710910972532305\n",
      "Episode 7917; Testing Loss 0.005828154650683322; Training Loss 0.004710899405865124\n",
      "Episode 7918; Testing Loss 0.00582794608494981; Training Loss 0.00471089207252755\n",
      "Episode 7919; Testing Loss 0.0058280963634753085; Training Loss 0.004710880713435705\n",
      "Episode 7920; Testing Loss 0.005828321113258378; Training Loss 0.004710874260456428\n",
      "Episode 7921; Testing Loss 0.005828186490664356; Training Loss 0.00471086162916143\n",
      "Episode 7922; Testing Loss 0.005827964115103102; Training Loss 0.004710856860632184\n",
      "Episode 7923; Testing Loss 0.005828125104100269; Training Loss 0.004710845430602546\n",
      "Episode 7924; Testing Loss 0.005828331562704719; Training Loss 0.00471083870759968\n",
      "Episode 7925; Testing Loss 0.005828113891464227; Training Loss 0.004710825355871726\n",
      "Episode 7926; Testing Loss 0.005827824680256475; Training Loss 0.004710819546515439\n",
      "Episode 7927; Testing Loss 0.00582790689713439; Training Loss 0.004710807557030605\n",
      "Episode 7928; Testing Loss 0.00582812719525435; Training Loss 0.004710799740445192\n",
      "Episode 7929; Testing Loss 0.005828160632124215; Training Loss 0.004710788166899425\n",
      "Episode 7930; Testing Loss 0.005828007126406254; Training Loss 0.004710778202560635\n",
      "Episode 7931; Testing Loss 0.005827872511173865; Training Loss 0.004710770316491747\n",
      "Episode 7932; Testing Loss 0.005827999947279336; Training Loss 0.004710759843986278\n",
      "Episode 7933; Testing Loss 0.0058281150475083445; Training Loss 0.004710752524117259\n",
      "Episode 7934; Testing Loss 0.005827974693957352; Training Loss 0.004710741570154021\n",
      "Episode 7935; Testing Loss 0.0058278900097231195; Training Loss 0.004710735970905801\n",
      "Episode 7936; Testing Loss 0.005827870333352273; Training Loss 0.004710723105818182\n",
      "Episode 7937; Testing Loss 0.005827957496251994; Training Loss 0.0047107151257336\n",
      "Episode 7938; Testing Loss 0.005827972518600349; Training Loss 0.004710706738030728\n",
      "Episode 7939; Testing Loss 0.005827844787759003; Training Loss 0.004710697835309446\n",
      "Episode 7940; Testing Loss 0.005827707815065273; Training Loss 0.004710689828967233\n",
      "Episode 7941; Testing Loss 0.005827817451084944; Training Loss 0.004710676983141497\n",
      "Episode 7942; Testing Loss 0.005828059867848172; Training Loss 0.00471067195081365\n",
      "Episode 7943; Testing Loss 0.005827945097249058; Training Loss 0.004710660601532971\n",
      "Episode 7944; Testing Loss 0.005827775065924686; Training Loss 0.004710653554144735\n",
      "Episode 7945; Testing Loss 0.005827839080698319; Training Loss 0.004710641003811078\n",
      "Episode 7946; Testing Loss 0.005827928985410838; Training Loss 0.0047106342543642935\n",
      "Episode 7947; Testing Loss 0.005827816466626839; Training Loss 0.00471062378456996\n",
      "Episode 7948; Testing Loss 0.005827725671055255; Training Loss 0.004710612729513944\n",
      "Episode 7949; Testing Loss 0.005827806475692344; Training Loss 0.004710606606497071\n",
      "Episode 7950; Testing Loss 0.0058278677760113225; Training Loss 0.0047105966585836885\n",
      "Episode 7951; Testing Loss 0.005827795868006388; Training Loss 0.0047105871592097535\n",
      "Episode 7952; Testing Loss 0.005827848348972941; Training Loss 0.004710577444606828\n",
      "Episode 7953; Testing Loss 0.005827917773662978; Training Loss 0.004710570357919078\n",
      "Episode 7954; Testing Loss 0.0058277804749636685; Training Loss 0.004710558142147605\n",
      "Episode 7955; Testing Loss 0.005827708926192644; Training Loss 0.004710552060426804\n",
      "Episode 7956; Testing Loss 0.0058278828322425; Training Loss 0.0047105422161916695\n",
      "Episode 7957; Testing Loss 0.005827985869934755; Training Loss 0.0047105328153838775\n",
      "Episode 7958; Testing Loss 0.005827820285009016; Training Loss 0.004710522857340087\n",
      "Episode 7959; Testing Loss 0.00582757572886589; Training Loss 0.0047105168917252905\n",
      "Episode 7960; Testing Loss 0.0058276919146353166; Training Loss 0.004710503919212517\n",
      "Episode 7961; Testing Loss 0.0058278351604220515; Training Loss 0.004710497158784976\n",
      "Episode 7962; Testing Loss 0.00582769842558691; Training Loss 0.004710485930912697\n",
      "Episode 7963; Testing Loss 0.00582754576855733; Training Loss 0.00471047721228125\n",
      "Episode 7964; Testing Loss 0.0058276184777579084; Training Loss 0.004710469496238413\n",
      "Episode 7965; Testing Loss 0.005827738944833848; Training Loss 0.004710457644444979\n",
      "Episode 7966; Testing Loss 0.00582777535192474; Training Loss 0.004710450944702222\n",
      "Episode 7967; Testing Loss 0.005827681605747831; Training Loss 0.004710442876757193\n",
      "Episode 7968; Testing Loss 0.005827525418418822; Training Loss 0.004710430690889466\n",
      "Episode 7969; Testing Loss 0.005827545963391177; Training Loss 0.004710428328596429\n",
      "Episode 7970; Testing Loss 0.0058276637560441494; Training Loss 0.004710421301497745\n",
      "Episode 7971; Testing Loss 0.005827648881219497; Training Loss 0.004710404825316352\n",
      "Episode 7972; Testing Loss 0.005827533994934798; Training Loss 0.004710401944534763\n",
      "Episode 7973; Testing Loss 0.0058275186968160765; Training Loss 0.0047103964929070775\n",
      "Episode 7974; Testing Loss 0.005827631601353626; Training Loss 0.004710382178693173\n",
      "Episode 7975; Testing Loss 0.005827722817577179; Training Loss 0.0047103740312552135\n",
      "Episode 7976; Testing Loss 0.005827669263889484; Training Loss 0.00471036174537774\n",
      "Episode 7977; Testing Loss 0.005827547874225454; Training Loss 0.0047103514062454485\n",
      "Episode 7978; Testing Loss 0.005827441902519344; Training Loss 0.004710345599397491\n",
      "Episode 7979; Testing Loss 0.0058274077934288805; Training Loss 0.004710335030699853\n",
      "Episode 7980; Testing Loss 0.005827487299040435; Training Loss 0.004710323934877175\n",
      "Episode 7981; Testing Loss 0.005827544507042115; Training Loss 0.0047103190369984105\n",
      "Episode 7982; Testing Loss 0.005827627277674417; Training Loss 0.004710310754793555\n",
      "Episode 7983; Testing Loss 0.0058275565408935585; Training Loss 0.00471029843979635\n",
      "Episode 7984; Testing Loss 0.0058274739429974055; Training Loss 0.0047102884808672375\n",
      "Episode 7985; Testing Loss 0.005827421158304399; Training Loss 0.004710280730070224\n",
      "Episode 7986; Testing Loss 0.005827452014470851; Training Loss 0.004710273942563403\n",
      "Episode 7987; Testing Loss 0.005827505834966649; Training Loss 0.004710261323584743\n",
      "Episode 7988; Testing Loss 0.00582748603812776; Training Loss 0.004710251750420577\n",
      "Episode 7989; Testing Loss 0.005827489689019362; Training Loss 0.004710243357377551\n",
      "Episode 7990; Testing Loss 0.00582744868923274; Training Loss 0.0047102338711430755\n",
      "Episode 7991; Testing Loss 0.005827352815199268; Training Loss 0.004710222317877146\n",
      "Episode 7992; Testing Loss 0.005827326011057642; Training Loss 0.004710217637698507\n",
      "Episode 7993; Testing Loss 0.005827254950897867; Training Loss 0.004710207877166117\n",
      "Episode 7994; Testing Loss 0.0058272816945514685; Training Loss 0.004710195748732958\n",
      "Episode 7995; Testing Loss 0.005827482587305575; Training Loss 0.004710186245432603\n",
      "Episode 7996; Testing Loss 0.005827557669222786; Training Loss 0.004710177919667646\n",
      "Episode 7997; Testing Loss 0.005827487933513597; Training Loss 0.004710166604301456\n",
      "Episode 7998; Testing Loss 0.0058273638585994774; Training Loss 0.00471015987825001\n",
      "Episode 7999; Testing Loss 0.005827314297390375; Training Loss 0.004710153459808194\n",
      "Episode 8000; Testing Loss 0.005827371711175607; Training Loss 0.004710143094998088\n",
      "Episode 8001; Testing Loss 0.005827402984844598; Training Loss 0.004710133968185068\n",
      "Episode 8002; Testing Loss 0.005827422479550036; Training Loss 0.004710125056029831\n",
      "Episode 8003; Testing Loss 0.005827352621916178; Training Loss 0.004710115409944605\n",
      "Episode 8004; Testing Loss 0.005827235190593514; Training Loss 0.004710106920066413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8005; Testing Loss 0.005827221912637318; Training Loss 0.00471009897935018\n",
      "Episode 8006; Testing Loss 0.005827346567764249; Training Loss 0.004710088742567117\n",
      "Episode 8007; Testing Loss 0.005827440635038088; Training Loss 0.004710081799806245\n",
      "Episode 8008; Testing Loss 0.005827383029802989; Training Loss 0.004710069625777287\n",
      "Episode 8009; Testing Loss 0.005827240525134425; Training Loss 0.004710060181911909\n",
      "Episode 8010; Testing Loss 0.005827242552937838; Training Loss 0.004710051829302172\n",
      "Episode 8011; Testing Loss 0.005827278110740821; Training Loss 0.004710044011925092\n",
      "Episode 8012; Testing Loss 0.005827223749315422; Training Loss 0.004710031672940296\n",
      "Episode 8013; Testing Loss 0.005827194877307463; Training Loss 0.0047100231490417345\n",
      "Episode 8014; Testing Loss 0.005827297009019256; Training Loss 0.0047100142844064006\n",
      "Episode 8015; Testing Loss 0.005827300853682801; Training Loss 0.004710004629305103\n",
      "Episode 8016; Testing Loss 0.0058271586932404475; Training Loss 0.0047099950783913\n",
      "Episode 8017; Testing Loss 0.005827109194219393; Training Loss 0.00470998643376361\n",
      "Episode 8018; Testing Loss 0.005827230911954739; Training Loss 0.004709977638178123\n",
      "Episode 8019; Testing Loss 0.005827253315110732; Training Loss 0.004709966921719901\n",
      "Episode 8020; Testing Loss 0.005827264390170057; Training Loss 0.0047099582025836905\n",
      "Episode 8021; Testing Loss 0.00582724606929629; Training Loss 0.004709948482726635\n",
      "Episode 8022; Testing Loss 0.00582717961449716; Training Loss 0.004709940531804921\n",
      "Episode 8023; Testing Loss 0.005827150712186102; Training Loss 0.004709931686031026\n",
      "Episode 8024; Testing Loss 0.005827298050179695; Training Loss 0.004709921333958709\n",
      "Episode 8025; Testing Loss 0.005827231457667763; Training Loss 0.004709913270616833\n",
      "Episode 8026; Testing Loss 0.005827096137319373; Training Loss 0.004709904590512023\n",
      "Episode 8027; Testing Loss 0.005827125487124893; Training Loss 0.004709894613248111\n",
      "Episode 8028; Testing Loss 0.00582728177378938; Training Loss 0.004709887258727555\n",
      "Episode 8029; Testing Loss 0.005827266499091194; Training Loss 0.004709879563759029\n",
      "Episode 8030; Testing Loss 0.005827111651162769; Training Loss 0.004709869316490687\n",
      "Episode 8031; Testing Loss 0.005827075639850241; Training Loss 0.00470985721124568\n",
      "Episode 8032; Testing Loss 0.00582714570091898; Training Loss 0.0047098565784793765\n",
      "Episode 8033; Testing Loss 0.005827074410065426; Training Loss 0.004709846941155086\n",
      "Episode 8034; Testing Loss 0.005827001875460995; Training Loss 0.00470983696292665\n",
      "Episode 8035; Testing Loss 0.005827091851900337; Training Loss 0.004709824786932118\n",
      "Episode 8036; Testing Loss 0.005827222062495582; Training Loss 0.004709816431377558\n",
      "Episode 8037; Testing Loss 0.00582717013732892; Training Loss 0.004709806880752646\n",
      "Episode 8038; Testing Loss 0.005826997071757505; Training Loss 0.004709794721241237\n",
      "Episode 8039; Testing Loss 0.005826958548338876; Training Loss 0.004709789492291697\n",
      "Episode 8040; Testing Loss 0.005827074325693424; Training Loss 0.004709778128377047\n",
      "Episode 8041; Testing Loss 0.005827156980843058; Training Loss 0.004709770633429262\n",
      "Episode 8042; Testing Loss 0.0058271336794191945; Training Loss 0.00470976311149645\n",
      "Episode 8043; Testing Loss 0.0058270223001262245; Training Loss 0.004709753800326281\n",
      "Episode 8044; Testing Loss 0.005827061274599068; Training Loss 0.004709740891071543\n",
      "Episode 8045; Testing Loss 0.00582706206857112; Training Loss 0.004709735896351352\n",
      "Episode 8046; Testing Loss 0.005826916241583407; Training Loss 0.004709727072744035\n",
      "Episode 8047; Testing Loss 0.005826842321650151; Training Loss 0.004709714161601388\n",
      "Episode 8048; Testing Loss 0.005827031860054831; Training Loss 0.004709703519161172\n",
      "Episode 8049; Testing Loss 0.005827208300803898; Training Loss 0.004709697973150697\n",
      "Episode 8050; Testing Loss 0.0058270083399546545; Training Loss 0.004709687036422121\n",
      "Episode 8051; Testing Loss 0.005826861469943752; Training Loss 0.004709678983449444\n",
      "Episode 8052; Testing Loss 0.005826924725280763; Training Loss 0.004709670123740138\n",
      "Episode 8053; Testing Loss 0.0058269426887900865; Training Loss 0.004709659654983923\n",
      "Episode 8054; Testing Loss 0.005826895865062432; Training Loss 0.004709650817053059\n",
      "Episode 8055; Testing Loss 0.005826927770196965; Training Loss 0.004709642950874591\n",
      "Episode 8056; Testing Loss 0.005826899274034223; Training Loss 0.00470963149394633\n",
      "Episode 8057; Testing Loss 0.005826981203326538; Training Loss 0.004709622465507783\n",
      "Episode 8058; Testing Loss 0.005827094430958149; Training Loss 0.004709614355138641\n",
      "Episode 8059; Testing Loss 0.005827003312745139; Training Loss 0.004709606013213235\n",
      "Episode 8060; Testing Loss 0.005826894603663608; Training Loss 0.0047095979432652665\n",
      "Episode 8061; Testing Loss 0.005826958739775992; Training Loss 0.004709587250174234\n",
      "Episode 8062; Testing Loss 0.005826946477633314; Training Loss 0.004709578619684534\n",
      "Episode 8063; Testing Loss 0.005826726302064081; Training Loss 0.004709570937816268\n",
      "Episode 8064; Testing Loss 0.005826789181059007; Training Loss 0.00470956097416957\n",
      "Episode 8065; Testing Loss 0.005826907748690157; Training Loss 0.004709551737611531\n",
      "Episode 8066; Testing Loss 0.005826869492078314; Training Loss 0.00470954202031572\n",
      "Episode 8067; Testing Loss 0.0058268935810164985; Training Loss 0.004709533586571523\n",
      "Episode 8068; Testing Loss 0.005826964747140148; Training Loss 0.004709524503956895\n",
      "Episode 8069; Testing Loss 0.005826945528637684; Training Loss 0.004709518277025833\n",
      "Episode 8070; Testing Loss 0.005826805656660865; Training Loss 0.004709510703727081\n",
      "Episode 8071; Testing Loss 0.005826771425883934; Training Loss 0.004709499272525812\n",
      "Episode 8072; Testing Loss 0.005826897104754719; Training Loss 0.004709492408793731\n",
      "Episode 8073; Testing Loss 0.005827013687893259; Training Loss 0.004709485947158641\n",
      "Episode 8074; Testing Loss 0.005826906028188896; Training Loss 0.004709473608413881\n",
      "Episode 8075; Testing Loss 0.005826806306392885; Training Loss 0.004709463245670859\n",
      "Episode 8076; Testing Loss 0.0058269106786086; Training Loss 0.004709454575182298\n",
      "Episode 8077; Testing Loss 0.005826954769437415; Training Loss 0.004709446748742793\n",
      "Episode 8078; Testing Loss 0.005826825427966592; Training Loss 0.004709432031012025\n",
      "Episode 8079; Testing Loss 0.005826750098735509; Training Loss 0.004709425255004823\n",
      "Episode 8080; Testing Loss 0.005826712121405011; Training Loss 0.0047094158076546044\n",
      "Episode 8081; Testing Loss 0.005826716227104482; Training Loss 0.004709407685668573\n",
      "Episode 8082; Testing Loss 0.005826731147330268; Training Loss 0.004709398125417936\n",
      "Episode 8083; Testing Loss 0.005826810852339602; Training Loss 0.004709389595745762\n",
      "Episode 8084; Testing Loss 0.005826781218028456; Training Loss 0.004709378332403771\n",
      "Episode 8085; Testing Loss 0.005826796231010039; Training Loss 0.004709370380353976\n",
      "Episode 8086; Testing Loss 0.005826906425945999; Training Loss 0.0047093625662419725\n",
      "Episode 8087; Testing Loss 0.005826875491509909; Training Loss 0.0047093511257009025\n",
      "Episode 8088; Testing Loss 0.005826773098483091; Training Loss 0.0047093414579565404\n",
      "Episode 8089; Testing Loss 0.0058267187046431535; Training Loss 0.00470933244168989\n",
      "Episode 8090; Testing Loss 0.005826745955609774; Training Loss 0.00470932435201291\n",
      "Episode 8091; Testing Loss 0.005826691670420112; Training Loss 0.004709317802774371\n",
      "Episode 8092; Testing Loss 0.005826733241147611; Training Loss 0.004709310333503534\n",
      "Episode 8093; Testing Loss 0.005826810167096357; Training Loss 0.004709298941544077\n",
      "Episode 8094; Testing Loss 0.00582674684344023; Training Loss 0.00470929371058855\n",
      "Episode 8095; Testing Loss 0.005826598631709074; Training Loss 0.004709287497335267\n",
      "Episode 8096; Testing Loss 0.005826619719640994; Training Loss 0.004709271369551855\n",
      "Episode 8097; Testing Loss 0.005826760798309957; Training Loss 0.004709267080371859\n",
      "Episode 8098; Testing Loss 0.005826799254216802; Training Loss 0.004709263300102241\n",
      "Episode 8099; Testing Loss 0.005826664453511959; Training Loss 0.004709253063322193\n",
      "Episode 8100; Testing Loss 0.005826599007400339; Training Loss 0.004709241714148709\n",
      "Episode 8101; Testing Loss 0.0058266426384263464; Training Loss 0.0047092295897915165\n",
      "Episode 8102; Testing Loss 0.005826686309409079; Training Loss 0.004709221066862599\n",
      "Episode 8103; Testing Loss 0.005826634882242377; Training Loss 0.004709214796325568\n",
      "Episode 8104; Testing Loss 0.005826561822285177; Training Loss 0.004709201792068843\n",
      "Episode 8105; Testing Loss 0.005826526637984297; Training Loss 0.004709191858143906\n",
      "Episode 8106; Testing Loss 0.005826638137003045; Training Loss 0.004709187746897314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8107; Testing Loss 0.005826678527111095; Training Loss 0.00470917367583957\n",
      "Episode 8108; Testing Loss 0.005826635496086237; Training Loss 0.004709163560661802\n",
      "Episode 8109; Testing Loss 0.0058266139283556; Training Loss 0.00470916172462464\n",
      "Episode 8110; Testing Loss 0.005826663897654122; Training Loss 0.004709150219360155\n",
      "Episode 8111; Testing Loss 0.005826707482076557; Training Loss 0.004709137678383092\n",
      "Episode 8112; Testing Loss 0.005826694019295825; Training Loss 0.004709131700362395\n",
      "Episode 8113; Testing Loss 0.005826580752068602; Training Loss 0.004709122099534663\n",
      "Episode 8114; Testing Loss 0.00582651453002844; Training Loss 0.0047091083590908535\n",
      "Episode 8115; Testing Loss 0.005826572970423579; Training Loss 0.004709104716629752\n",
      "Episode 8116; Testing Loss 0.00582656552284744; Training Loss 0.004709096916685404\n",
      "Episode 8117; Testing Loss 0.005826492137350198; Training Loss 0.0047090832136019\n",
      "Episode 8118; Testing Loss 0.005826489097446791; Training Loss 0.004709077131146934\n",
      "Episode 8119; Testing Loss 0.005826565476532733; Training Loss 0.00470907269727574\n",
      "Episode 8120; Testing Loss 0.005826576146279402; Training Loss 0.004709059174186033\n",
      "Episode 8121; Testing Loss 0.005826555430808681; Training Loss 0.004709045555732261\n",
      "Episode 8122; Testing Loss 0.005826485613122037; Training Loss 0.004709038371602249\n",
      "Episode 8123; Testing Loss 0.005826482464531095; Training Loss 0.004709029123299204\n",
      "Episode 8124; Testing Loss 0.005826602946635768; Training Loss 0.004709019582304692\n",
      "Episode 8125; Testing Loss 0.005826621916515658; Training Loss 0.004709012402531191\n",
      "Episode 8126; Testing Loss 0.005826480957334294; Training Loss 0.004709002265514901\n",
      "Episode 8127; Testing Loss 0.005826375608588849; Training Loss 0.00470899188674614\n",
      "Episode 8128; Testing Loss 0.005826391893862588; Training Loss 0.0047089855818225725\n",
      "Episode 8129; Testing Loss 0.005826415660386907; Training Loss 0.004708972886245345\n",
      "Episode 8130; Testing Loss 0.005826502874393441; Training Loss 0.004708964824191264\n",
      "Episode 8131; Testing Loss 0.005826553571818226; Training Loss 0.004708955834186411\n",
      "Episode 8132; Testing Loss 0.005826488085144435; Training Loss 0.00470894903077797\n",
      "Episode 8133; Testing Loss 0.005826324070082991; Training Loss 0.00470893891603372\n",
      "Episode 8134; Testing Loss 0.00582634014007286; Training Loss 0.004708931025518544\n",
      "Episode 8135; Testing Loss 0.005826530109022206; Training Loss 0.0047089237803394305\n",
      "Episode 8136; Testing Loss 0.005826472396324834; Training Loss 0.0047089131086912785\n",
      "Episode 8137; Testing Loss 0.005826251470604557; Training Loss 0.004708903826242848\n",
      "Episode 8138; Testing Loss 0.0058261572499960146; Training Loss 0.004708895550030912\n",
      "Episode 8139; Testing Loss 0.005826341722034483; Training Loss 0.004708884813368233\n",
      "Episode 8140; Testing Loss 0.005826608954580539; Training Loss 0.004708879085320727\n",
      "Episode 8141; Testing Loss 0.005826542475121567; Training Loss 0.0047088672588947895\n",
      "Episode 8142; Testing Loss 0.005826339046424552; Training Loss 0.004708862528694648\n",
      "Episode 8143; Testing Loss 0.005826301441259339; Training Loss 0.004708853364196847\n",
      "Episode 8144; Testing Loss 0.005826475765718663; Training Loss 0.004708838457051867\n",
      "Episode 8145; Testing Loss 0.005826578867506084; Training Loss 0.004708833074275746\n",
      "Episode 8146; Testing Loss 0.0058263516641514565; Training Loss 0.004708820568393701\n",
      "Episode 8147; Testing Loss 0.0058261518394456145; Training Loss 0.004708815128538489\n",
      "Episode 8148; Testing Loss 0.005826260708424525; Training Loss 0.004708804814826171\n",
      "Episode 8149; Testing Loss 0.005826369661494752; Training Loss 0.004708795004301825\n",
      "Episode 8150; Testing Loss 0.0058263651498159826; Training Loss 0.004708785236327333\n",
      "Episode 8151; Testing Loss 0.005826301584162154; Training Loss 0.004708775202659773\n",
      "Episode 8152; Testing Loss 0.005826248695116099; Training Loss 0.004708767281471112\n",
      "Episode 8153; Testing Loss 0.005826418032929442; Training Loss 0.004708758440700939\n",
      "Episode 8154; Testing Loss 0.005826471132746004; Training Loss 0.004708749923936383\n",
      "Episode 8155; Testing Loss 0.005826397676472757; Training Loss 0.004708741335365653\n",
      "Episode 8156; Testing Loss 0.005826332058394188; Training Loss 0.004708730189334161\n",
      "Episode 8157; Testing Loss 0.005826285421811392; Training Loss 0.0047087229057369805\n",
      "Episode 8158; Testing Loss 0.00582619356419002; Training Loss 0.004708713474773742\n",
      "Episode 8159; Testing Loss 0.0058262638181956275; Training Loss 0.004708705227275831\n",
      "Episode 8160; Testing Loss 0.005826415927670664; Training Loss 0.004708695687671141\n",
      "Episode 8161; Testing Loss 0.005826369533283065; Training Loss 0.004708684664037625\n",
      "Episode 8162; Testing Loss 0.00582622140542757; Training Loss 0.004708678746882252\n",
      "Episode 8163; Testing Loss 0.005826298954616869; Training Loss 0.004708668143888178\n",
      "Episode 8164; Testing Loss 0.005826341139050536; Training Loss 0.004708657063057343\n",
      "Episode 8165; Testing Loss 0.005826229891869458; Training Loss 0.004708654832069274\n",
      "Episode 8166; Testing Loss 0.0058262651031922436; Training Loss 0.004708646120209306\n",
      "Episode 8167; Testing Loss 0.005826330961340637; Training Loss 0.004708634447410681\n",
      "Episode 8168; Testing Loss 0.0058263402152678835; Training Loss 0.0047086233894166555\n",
      "Episode 8169; Testing Loss 0.005826185721740789; Training Loss 0.004708614853028122\n",
      "Episode 8170; Testing Loss 0.005826241286713282; Training Loss 0.00470860583108142\n",
      "Episode 8171; Testing Loss 0.005826429812184777; Training Loss 0.0047086008100824545\n",
      "Episode 8172; Testing Loss 0.005826306425126008; Training Loss 0.004708587135168293\n",
      "Episode 8173; Testing Loss 0.005826089509743794; Training Loss 0.004708579651571264\n",
      "Episode 8174; Testing Loss 0.005826041657855372; Training Loss 0.004708570477332105\n",
      "Episode 8175; Testing Loss 0.005826145412938703; Training Loss 0.0047085587900368665\n",
      "Episode 8176; Testing Loss 0.005826260560695941; Training Loss 0.004708554468369197\n",
      "Episode 8177; Testing Loss 0.005826326959712175; Training Loss 0.00470854647741863\n",
      "Episode 8178; Testing Loss 0.005826300671536368; Training Loss 0.004708536417916748\n",
      "Episode 8179; Testing Loss 0.0058262069592675625; Training Loss 0.0047085254239779845\n",
      "Episode 8180; Testing Loss 0.00582616092564709; Training Loss 0.004708516946803446\n",
      "Episode 8181; Testing Loss 0.005826190580448233; Training Loss 0.004708509256079181\n",
      "Episode 8182; Testing Loss 0.005826251571962904; Training Loss 0.004708500718717774\n",
      "Episode 8183; Testing Loss 0.005826200292453833; Training Loss 0.00470849154652276\n",
      "Episode 8184; Testing Loss 0.005826084372300797; Training Loss 0.004708479400036928\n",
      "Episode 8185; Testing Loss 0.005826008852411454; Training Loss 0.004708471262594053\n",
      "Episode 8186; Testing Loss 0.005826124979530973; Training Loss 0.004708461773722807\n",
      "Episode 8187; Testing Loss 0.005826307437954495; Training Loss 0.0047084538795167975\n",
      "Episode 8188; Testing Loss 0.005826264591606426; Training Loss 0.004708447097419389\n",
      "Episode 8189; Testing Loss 0.005826006609740587; Training Loss 0.004708436428585821\n",
      "Episode 8190; Testing Loss 0.005826001329565121; Training Loss 0.004708429674608466\n",
      "Episode 8191; Testing Loss 0.005826276911765006; Training Loss 0.00470842034271886\n",
      "Episode 8192; Testing Loss 0.005826256758643999; Training Loss 0.004708410579482753\n",
      "Episode 8193; Testing Loss 0.005825944953182548; Training Loss 0.004708399654720855\n",
      "Episode 8194; Testing Loss 0.005825878100408284; Training Loss 0.004708393706597014\n",
      "Episode 8195; Testing Loss 0.005826016011115007; Training Loss 0.0047083797178164325\n",
      "Episode 8196; Testing Loss 0.005826210820107289; Training Loss 0.004708373952421209\n",
      "Episode 8197; Testing Loss 0.0058261840008356994; Training Loss 0.004708363801154597\n",
      "Episode 8198; Testing Loss 0.005826003558920337; Training Loss 0.004708357666991663\n",
      "Episode 8199; Testing Loss 0.0058259603341457656; Training Loss 0.0047083470158562615\n",
      "Episode 8200; Testing Loss 0.005826025611045948; Training Loss 0.0047083369759194135\n",
      "Episode 8201; Testing Loss 0.005826121066982954; Training Loss 0.0047083288619640925\n",
      "Episode 8202; Testing Loss 0.005826097037012941; Training Loss 0.0047083183728462465\n",
      "Episode 8203; Testing Loss 0.005825969946668483; Training Loss 0.004708312716862047\n",
      "Episode 8204; Testing Loss 0.005825871893611197; Training Loss 0.0047083042014317384\n",
      "Episode 8205; Testing Loss 0.005825969393998381; Training Loss 0.004708290834207639\n",
      "Episode 8206; Testing Loss 0.005826173203437295; Training Loss 0.0047082828542039995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8207; Testing Loss 0.005826152302575412; Training Loss 0.004708273711066238\n",
      "Episode 8208; Testing Loss 0.005825946889086804; Training Loss 0.004708264862948051\n",
      "Episode 8209; Testing Loss 0.005825902225211511; Training Loss 0.00470825658775357\n",
      "Episode 8210; Testing Loss 0.00582609234384564; Training Loss 0.004708247361627531\n",
      "Episode 8211; Testing Loss 0.005826052390792957; Training Loss 0.004708240979097059\n",
      "Episode 8212; Testing Loss 0.00582579881307953; Training Loss 0.004708231096465336\n",
      "Episode 8213; Testing Loss 0.005825812072008522; Training Loss 0.004708220598775781\n",
      "Episode 8214; Testing Loss 0.0058260554954408655; Training Loss 0.004708213391004391\n",
      "Episode 8215; Testing Loss 0.00582602932103506; Training Loss 0.004708200728906821\n",
      "Episode 8216; Testing Loss 0.0058259273736159575; Training Loss 0.0047081926522072\n",
      "Episode 8217; Testing Loss 0.005826013315077499; Training Loss 0.004708184989807573\n",
      "Episode 8218; Testing Loss 0.005826045394603581; Training Loss 0.004708174045100353\n",
      "Episode 8219; Testing Loss 0.005825940275547509; Training Loss 0.00470816953350033\n",
      "Episode 8220; Testing Loss 0.005825945412302398; Training Loss 0.0047081590162686236\n",
      "Episode 8221; Testing Loss 0.005826020751346965; Training Loss 0.004708149299625451\n",
      "Episode 8222; Testing Loss 0.0058260256148808505; Training Loss 0.004708143122492573\n",
      "Episode 8223; Testing Loss 0.005825885177336666; Training Loss 0.004708133753568545\n",
      "Episode 8224; Testing Loss 0.00582586773623034; Training Loss 0.004708121027127084\n",
      "Episode 8225; Testing Loss 0.005826014945786317; Training Loss 0.004708112239992122\n",
      "Episode 8226; Testing Loss 0.005825984523874294; Training Loss 0.00470810541245962\n",
      "Episode 8227; Testing Loss 0.0058257359931162215; Training Loss 0.004708094040317789\n",
      "Episode 8228; Testing Loss 0.0058257176450605104; Training Loss 0.004708085565745804\n",
      "Episode 8229; Testing Loss 0.005825928091609461; Training Loss 0.004708075868622908\n",
      "Episode 8230; Testing Loss 0.005825929533032603; Training Loss 0.0047080670148212204\n",
      "Episode 8231; Testing Loss 0.0058258748626023845; Training Loss 0.0047080579581806215\n",
      "Episode 8232; Testing Loss 0.005825902543042506; Training Loss 0.0047080487493756295\n",
      "Episode 8233; Testing Loss 0.0058258151211881435; Training Loss 0.0047080384887481464\n",
      "Episode 8234; Testing Loss 0.005825735815336618; Training Loss 0.0047080325265431554\n",
      "Episode 8235; Testing Loss 0.005825736218471304; Training Loss 0.004708020556855078\n",
      "Episode 8236; Testing Loss 0.005825790893635704; Training Loss 0.004708015327682458\n",
      "Episode 8237; Testing Loss 0.00582572194675713; Training Loss 0.004708004756007031\n",
      "Episode 8238; Testing Loss 0.005825672523274221; Training Loss 0.004707997746501389\n",
      "Episode 8239; Testing Loss 0.005825859848135815; Training Loss 0.004707987569255505\n",
      "Episode 8240; Testing Loss 0.005825981761536787; Training Loss 0.004707982984930557\n",
      "Episode 8241; Testing Loss 0.005825735884240826; Training Loss 0.004707970803864419\n",
      "Episode 8242; Testing Loss 0.0058254541356179725; Training Loss 0.004707965838315511\n",
      "Episode 8243; Testing Loss 0.005825635604541406; Training Loss 0.004707951319633275\n",
      "Episode 8244; Testing Loss 0.005826017349751297; Training Loss 0.004707947614828146\n",
      "Episode 8245; Testing Loss 0.005825886999371121; Training Loss 0.004707932491494742\n",
      "Episode 8246; Testing Loss 0.0058255393082026355; Training Loss 0.004707928445112819\n",
      "Episode 8247; Testing Loss 0.00582554620093536; Training Loss 0.004707916280059863\n",
      "Episode 8248; Testing Loss 0.005825813023498872; Training Loss 0.004707905906042836\n",
      "Episode 8249; Testing Loss 0.005825840941351544; Training Loss 0.00470789979603365\n",
      "Episode 8250; Testing Loss 0.005825541042699861; Training Loss 0.004707887940547133\n",
      "Episode 8251; Testing Loss 0.0058254214543105665; Training Loss 0.004707883493424397\n",
      "Episode 8252; Testing Loss 0.005825614233215176; Training Loss 0.004707869350093961\n",
      "Episode 8253; Testing Loss 0.005825813522194055; Training Loss 0.004707864888182997\n",
      "Episode 8254; Testing Loss 0.005825763093640654; Training Loss 0.004707854816026238\n",
      "Episode 8255; Testing Loss 0.0058256472455483765; Training Loss 0.004707843751058592\n",
      "Episode 8256; Testing Loss 0.005825576868976545; Training Loss 0.004707832257520594\n",
      "Episode 8257; Testing Loss 0.005825622211826384; Training Loss 0.00470782669282712\n",
      "Episode 8258; Testing Loss 0.005825646598930671; Training Loss 0.004707815224067839\n",
      "Episode 8259; Testing Loss 0.005825622182311385; Training Loss 0.004707808492393503\n",
      "Episode 8260; Testing Loss 0.005825704475846389; Training Loss 0.004707800590824052\n",
      "Episode 8261; Testing Loss 0.0058257576176718295; Training Loss 0.004707790560298166\n",
      "Episode 8262; Testing Loss 0.0058256861651982865; Training Loss 0.004707777939432952\n",
      "Episode 8263; Testing Loss 0.005825573115281427; Training Loss 0.004707774393811997\n",
      "Episode 8264; Testing Loss 0.005825438747187629; Training Loss 0.004707768077486108\n",
      "Episode 8265; Testing Loss 0.00582553423807091; Training Loss 0.004707756729646422\n",
      "Episode 8266; Testing Loss 0.005825702770020259; Training Loss 0.00470774523196605\n",
      "Episode 8267; Testing Loss 0.0058257233773169464; Training Loss 0.004707736369944192\n",
      "Episode 8268; Testing Loss 0.005825602447601851; Training Loss 0.004707726318571062\n",
      "Episode 8269; Testing Loss 0.005825528713300331; Training Loss 0.004707717389300147\n",
      "Episode 8270; Testing Loss 0.005825549045143139; Training Loss 0.004707706926114882\n",
      "Episode 8271; Testing Loss 0.0058255918905780755; Training Loss 0.004707698484084077\n",
      "Episode 8272; Testing Loss 0.005825594675414359; Training Loss 0.004707690933042494\n",
      "Episode 8273; Testing Loss 0.005825519056096463; Training Loss 0.004707681101137057\n",
      "Episode 8274; Testing Loss 0.005825477270129861; Training Loss 0.004707670482018321\n",
      "Episode 8275; Testing Loss 0.005825469029368682; Training Loss 0.004707664502719648\n",
      "Episode 8276; Testing Loss 0.005825460874782183; Training Loss 0.004707652905152064\n",
      "Episode 8277; Testing Loss 0.005825520778150552; Training Loss 0.004707644842753388\n",
      "Episode 8278; Testing Loss 0.0058256266907680005; Training Loss 0.004707635010362773\n",
      "Episode 8279; Testing Loss 0.005825631789864019; Training Loss 0.004707624812071486\n",
      "Episode 8280; Testing Loss 0.00582546831199423; Training Loss 0.004707617180724092\n",
      "Episode 8281; Testing Loss 0.00582536168548331; Training Loss 0.004707608220765183\n",
      "Episode 8282; Testing Loss 0.0058255146295187734; Training Loss 0.0047075968262786675\n",
      "Episode 8283; Testing Loss 0.005825661786688857; Training Loss 0.004707592295825283\n",
      "Episode 8284; Testing Loss 0.00582553376948453; Training Loss 0.004707579721338006\n",
      "Episode 8285; Testing Loss 0.005825338195278274; Training Loss 0.004707574109047727\n",
      "Episode 8286; Testing Loss 0.005825398613319922; Training Loss 0.004707563296330254\n",
      "Episode 8287; Testing Loss 0.0058256011981110915; Training Loss 0.004707555930623113\n",
      "Episode 8288; Testing Loss 0.00582561447107419; Training Loss 0.004707543568774556\n",
      "Episode 8289; Testing Loss 0.005825366942936752; Training Loss 0.004707532590397842\n",
      "Episode 8290; Testing Loss 0.005825271360178158; Training Loss 0.004707527012320554\n",
      "Episode 8291; Testing Loss 0.005825472313702955; Training Loss 0.00470751450518455\n",
      "Episode 8292; Testing Loss 0.00582558060867529; Training Loss 0.004707506225755607\n",
      "Episode 8293; Testing Loss 0.005825490719266873; Training Loss 0.0047074981902315936\n",
      "Episode 8294; Testing Loss 0.005825294532312436; Training Loss 0.00470749262627416\n",
      "Episode 8295; Testing Loss 0.005825461826088974; Training Loss 0.004707478279918967\n",
      "Episode 8296; Testing Loss 0.005825595333125677; Training Loss 0.0047074714354563155\n",
      "Episode 8297; Testing Loss 0.005825404074892818; Training Loss 0.004707460400398453\n",
      "Episode 8298; Testing Loss 0.00582525691013; Training Loss 0.004707451290993856\n",
      "Episode 8299; Testing Loss 0.005825345634924381; Training Loss 0.004707442275996202\n",
      "Episode 8300; Testing Loss 0.00582557153691598; Training Loss 0.0047074340090998154\n",
      "Episode 8301; Testing Loss 0.005825483865571215; Training Loss 0.00470742662273024\n",
      "Episode 8302; Testing Loss 0.005825263724369818; Training Loss 0.004707416211197196\n",
      "Episode 8303; Testing Loss 0.005825322382725015; Training Loss 0.004707408190004024\n",
      "Episode 8304; Testing Loss 0.005825635683685029; Training Loss 0.004707399490599503\n",
      "Episode 8305; Testing Loss 0.005825622636246318; Training Loss 0.004707388740560464\n",
      "Episode 8306; Testing Loss 0.0058253005131979575; Training Loss 0.0047073801974582465\n",
      "Episode 8307; Testing Loss 0.005825123450837063; Training Loss 0.0047073743560328615\n",
      "Episode 8308; Testing Loss 0.005825379098832213; Training Loss 0.0047073595306432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8309; Testing Loss 0.005825633247680463; Training Loss 0.004707356028933632\n",
      "Episode 8310; Testing Loss 0.0058254296337140726; Training Loss 0.004707340528622243\n",
      "Episode 8311; Testing Loss 0.005825097735127924; Training Loss 0.004707340292717761\n",
      "Episode 8312; Testing Loss 0.005825316027675102; Training Loss 0.00470732348361942\n",
      "Episode 8313; Testing Loss 0.00582561387035665; Training Loss 0.004707320302115898\n",
      "Episode 8314; Testing Loss 0.005825421950064003; Training Loss 0.004707305352700081\n",
      "Episode 8315; Testing Loss 0.005825150072102932; Training Loss 0.004707298193741732\n",
      "Episode 8316; Testing Loss 0.005825211971998578; Training Loss 0.0047072870321792975\n",
      "Episode 8317; Testing Loss 0.005825351232542911; Training Loss 0.004707277308230293\n",
      "Episode 8318; Testing Loss 0.005825347076382743; Training Loss 0.004707267915429413\n",
      "Episode 8319; Testing Loss 0.005825284994885623; Training Loss 0.004707257702375533\n",
      "Episode 8320; Testing Loss 0.0058252516339954; Training Loss 0.00470724960210156\n",
      "Episode 8321; Testing Loss 0.005825218625932853; Training Loss 0.00470723954549195\n",
      "Episode 8322; Testing Loss 0.0058252935063134855; Training Loss 0.004707231242355158\n",
      "Episode 8323; Testing Loss 0.0058253088565120665; Training Loss 0.004707222264729604\n",
      "Episode 8324; Testing Loss 0.005825273122625512; Training Loss 0.00470721094300968\n",
      "Episode 8325; Testing Loss 0.005825168205433819; Training Loss 0.004707205656433467\n",
      "Episode 8326; Testing Loss 0.005825294401522604; Training Loss 0.004707195600331123\n",
      "Episode 8327; Testing Loss 0.0058253472369692; Training Loss 0.004707187571065363\n",
      "Episode 8328; Testing Loss 0.005825165766586617; Training Loss 0.004707176548617363\n",
      "Episode 8329; Testing Loss 0.0058250667084633195; Training Loss 0.004707174055884771\n",
      "Episode 8330; Testing Loss 0.005825322040333436; Training Loss 0.004707160329513072\n",
      "Episode 8331; Testing Loss 0.005825441972647084; Training Loss 0.004707151144986846\n",
      "Episode 8332; Testing Loss 0.005825175220735249; Training Loss 0.00470714136036464\n",
      "Episode 8333; Testing Loss 0.005824976782979386; Training Loss 0.004707136228868966\n",
      "Episode 8334; Testing Loss 0.00582527727094422; Training Loss 0.004707121045551625\n",
      "Episode 8335; Testing Loss 0.005825454224450968; Training Loss 0.004707121482349875\n",
      "Episode 8336; Testing Loss 0.005825137813264659; Training Loss 0.004707103095956988\n",
      "Episode 8337; Testing Loss 0.005824916088719991; Training Loss 0.0047071066348395585\n",
      "Episode 8338; Testing Loss 0.005825261560249885; Training Loss 0.004707088492110941\n",
      "Episode 8339; Testing Loss 0.005825612857358357; Training Loss 0.0047070866974713525\n",
      "Episode 8340; Testing Loss 0.005825336164306396; Training Loss 0.004707067446457151\n",
      "Episode 8341; Testing Loss 0.005824972595088166; Training Loss 0.004707067227126274\n",
      "Episode 8342; Testing Loss 0.00582504017867198; Training Loss 0.0047070510303429715\n",
      "Episode 8343; Testing Loss 0.005825335920573861; Training Loss 0.004707043208940888\n",
      "Episode 8344; Testing Loss 0.005825283023732474; Training Loss 0.0047070386253089515\n",
      "Episode 8345; Testing Loss 0.005824971409322517; Training Loss 0.004707024025663352\n",
      "Episode 8346; Testing Loss 0.005824902569053729; Training Loss 0.0047070148875425705\n",
      "Episode 8347; Testing Loss 0.005825098599514865; Training Loss 0.004707005415909919\n",
      "Episode 8348; Testing Loss 0.005825224680750814; Training Loss 0.004706993468635329\n",
      "Episode 8349; Testing Loss 0.005825163075784648; Training Loss 0.0047069852277548336\n",
      "Episode 8350; Testing Loss 0.005825147316724315; Training Loss 0.004706978127858846\n",
      "Episode 8351; Testing Loss 0.005825206960870988; Training Loss 0.004706966515798026\n",
      "Episode 8352; Testing Loss 0.005825173402365544; Training Loss 0.004706959897579069\n",
      "Episode 8353; Testing Loss 0.005824984804547264; Training Loss 0.004706949803433357\n",
      "Episode 8354; Testing Loss 0.005824847106278772; Training Loss 0.004706940975450368\n",
      "Episode 8355; Testing Loss 0.005825026095583892; Training Loss 0.004706933479062211\n",
      "Episode 8356; Testing Loss 0.00582525070138135; Training Loss 0.004706925375178641\n",
      "Episode 8357; Testing Loss 0.005825097325202224; Training Loss 0.004706911201043765\n",
      "Episode 8358; Testing Loss 0.005824883064829544; Training Loss 0.004706905082656527\n",
      "Episode 8359; Testing Loss 0.0058249784518036645; Training Loss 0.004706895959284673\n",
      "Episode 8360; Testing Loss 0.005825184083672051; Training Loss 0.004706883666890753\n",
      "Episode 8361; Testing Loss 0.005825178502039426; Training Loss 0.004706878388095477\n",
      "Episode 8362; Testing Loss 0.005824983584248471; Training Loss 0.004706868997204172\n",
      "Episode 8363; Testing Loss 0.005824879303804851; Training Loss 0.004706857337516638\n",
      "Episode 8364; Testing Loss 0.005825006571344817; Training Loss 0.004706853785518017\n",
      "Episode 8365; Testing Loss 0.00582502219684506; Training Loss 0.0047068470162360135\n",
      "Episode 8366; Testing Loss 0.005824909279656505; Training Loss 0.004706831975771289\n",
      "Episode 8367; Testing Loss 0.005824913087030646; Training Loss 0.004706821765199569\n",
      "Episode 8368; Testing Loss 0.005825144239168534; Training Loss 0.004706815286246028\n",
      "Episode 8369; Testing Loss 0.005825136266140665; Training Loss 0.004706805353736905\n",
      "Episode 8370; Testing Loss 0.00582495811544062; Training Loss 0.004706793709071571\n",
      "Episode 8371; Testing Loss 0.005824900077825308; Training Loss 0.004706788794911471\n",
      "Episode 8372; Testing Loss 0.005824933537067786; Training Loss 0.004706780184020892\n",
      "Episode 8373; Testing Loss 0.005824955214867298; Training Loss 0.004706766991137141\n",
      "Episode 8374; Testing Loss 0.00582497107984895; Training Loss 0.004706759393695256\n",
      "Episode 8375; Testing Loss 0.005824999187941353; Training Loss 0.00470675285996842\n",
      "Episode 8376; Testing Loss 0.005825011409854215; Training Loss 0.004706741141130828\n",
      "Episode 8377; Testing Loss 0.005825014026008347; Training Loss 0.004706728752557046\n",
      "Episode 8378; Testing Loss 0.00582486383934563; Training Loss 0.004706723483862821\n",
      "Episode 8379; Testing Loss 0.005824816112705912; Training Loss 0.00470671540631111\n",
      "Episode 8380; Testing Loss 0.005824951924831255; Training Loss 0.004706703442725935\n",
      "Episode 8381; Testing Loss 0.005825103808927961; Training Loss 0.004706695942201694\n",
      "Episode 8382; Testing Loss 0.005824984566359232; Training Loss 0.004706684750777127\n",
      "Episode 8383; Testing Loss 0.005824818020274328; Training Loss 0.004706675277547344\n",
      "Episode 8384; Testing Loss 0.005824803018356899; Training Loss 0.004706669154922056\n",
      "Episode 8385; Testing Loss 0.005824793247129381; Training Loss 0.004706658510700265\n",
      "Episode 8386; Testing Loss 0.005824807433215112; Training Loss 0.004706649260613491\n",
      "Episode 8387; Testing Loss 0.005824903920146868; Training Loss 0.004706640148523915\n",
      "Episode 8388; Testing Loss 0.00582493083521393; Training Loss 0.004706629030265031\n",
      "Episode 8389; Testing Loss 0.005824867226781049; Training Loss 0.004706621162370573\n",
      "Episode 8390; Testing Loss 0.005824795267543546; Training Loss 0.004706612210377301\n",
      "Episode 8391; Testing Loss 0.005824914821293332; Training Loss 0.004706602458581661\n",
      "Episode 8392; Testing Loss 0.005825040027328733; Training Loss 0.004706594794366063\n",
      "Episode 8393; Testing Loss 0.0058248636518290625; Training Loss 0.0047065831083845985\n",
      "Episode 8394; Testing Loss 0.005824598827354998; Training Loss 0.004706577331337338\n",
      "Episode 8395; Testing Loss 0.005824597516684951; Training Loss 0.004706566644114601\n",
      "Episode 8396; Testing Loss 0.005824851157127761; Training Loss 0.004706557326348545\n",
      "Episode 8397; Testing Loss 0.005824894068172719; Training Loss 0.004706549347034049\n",
      "Episode 8398; Testing Loss 0.005824781054625219; Training Loss 0.004706540060346368\n",
      "Episode 8399; Testing Loss 0.00582475094588479; Training Loss 0.004706534775615083\n",
      "Episode 8400; Testing Loss 0.00582475532950359; Training Loss 0.0047065235172862995\n",
      "Episode 8401; Testing Loss 0.005824746106072478; Training Loss 0.004706509869570392\n",
      "Episode 8402; Testing Loss 0.005824742355696002; Training Loss 0.0047065022461084995\n",
      "Episode 8403; Testing Loss 0.005824698004071328; Training Loss 0.00470649376069328\n",
      "Episode 8404; Testing Loss 0.005824566638036232; Training Loss 0.004706485300939946\n",
      "Episode 8405; Testing Loss 0.00582467067731584; Training Loss 0.004706473800349909\n",
      "Episode 8406; Testing Loss 0.0058247918883319256; Training Loss 0.004706470981284435\n",
      "Episode 8407; Testing Loss 0.005824672872500376; Training Loss 0.00470645900674907\n",
      "Episode 8408; Testing Loss 0.005824548812085311; Training Loss 0.00470645088851607\n",
      "Episode 8409; Testing Loss 0.005824743437866583; Training Loss 0.004706439057041926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8410; Testing Loss 0.00582488322746276; Training Loss 0.00470643662505689\n",
      "Episode 8411; Testing Loss 0.005824583460463028; Training Loss 0.004706418958736012\n",
      "Episode 8412; Testing Loss 0.005824455805399165; Training Loss 0.004706415389303414\n",
      "Episode 8413; Testing Loss 0.005824737984848115; Training Loss 0.004706405382694457\n",
      "Episode 8414; Testing Loss 0.00582481652155543; Training Loss 0.004706395261265733\n",
      "Episode 8415; Testing Loss 0.0058246105531742; Training Loss 0.004706384867445154\n",
      "Episode 8416; Testing Loss 0.005824582665385433; Training Loss 0.0047063745149854555\n",
      "Episode 8417; Testing Loss 0.005824718383196513; Training Loss 0.004706367730407136\n",
      "Episode 8418; Testing Loss 0.005824685955089304; Training Loss 0.004706357524864909\n",
      "Episode 8419; Testing Loss 0.005824550826381574; Training Loss 0.004706347961735323\n",
      "Episode 8420; Testing Loss 0.005824593670345999; Training Loss 0.004706339212004299\n",
      "Episode 8421; Testing Loss 0.00582473148383867; Training Loss 0.004706329664254791\n",
      "Episode 8422; Testing Loss 0.0058246547735247714; Training Loss 0.004706320460656373\n",
      "Episode 8423; Testing Loss 0.005824465464675525; Training Loss 0.004706313595819513\n",
      "Episode 8424; Testing Loss 0.005824607286504628; Training Loss 0.004706300229845528\n",
      "Episode 8425; Testing Loss 0.0058247071844640085; Training Loss 0.004706293583904578\n",
      "Episode 8426; Testing Loss 0.00582459086985758; Training Loss 0.004706285324916593\n",
      "Episode 8427; Testing Loss 0.005824547012238432; Training Loss 0.004706274953350954\n",
      "Episode 8428; Testing Loss 0.00582457190050474; Training Loss 0.00470626784991905\n",
      "Episode 8429; Testing Loss 0.0058244425428156405; Training Loss 0.004706256666837204\n",
      "Episode 8430; Testing Loss 0.005824416739960477; Training Loss 0.004706250723400137\n",
      "Episode 8431; Testing Loss 0.005824648330705938; Training Loss 0.004706239524769676\n",
      "Episode 8432; Testing Loss 0.0058246914612200065; Training Loss 0.004706229731863443\n",
      "Episode 8433; Testing Loss 0.005824473986935497; Training Loss 0.004706220570523644\n",
      "Episode 8434; Testing Loss 0.0058243361678187375; Training Loss 0.004706212424256471\n",
      "Episode 8435; Testing Loss 0.005824576244323998; Training Loss 0.004706203694548092\n",
      "Episode 8436; Testing Loss 0.0058245786172668535; Training Loss 0.004706193459349153\n",
      "Episode 8437; Testing Loss 0.005824393445114781; Training Loss 0.004706183814976095\n",
      "Episode 8438; Testing Loss 0.005824280458717987; Training Loss 0.004706176609001564\n",
      "Episode 8439; Testing Loss 0.005824492900085889; Training Loss 0.004706168718947978\n",
      "Episode 8440; Testing Loss 0.005824557171561503; Training Loss 0.00470615908696549\n",
      "Episode 8441; Testing Loss 0.0058243998412790396; Training Loss 0.004706152372780456\n",
      "Episode 8442; Testing Loss 0.005824467422000905; Training Loss 0.0047061418983741984\n",
      "Episode 8443; Testing Loss 0.00582463856919263; Training Loss 0.004706131047221398\n",
      "Episode 8444; Testing Loss 0.0058244763116629536; Training Loss 0.004706122666534368\n",
      "Episode 8445; Testing Loss 0.005824172489564866; Training Loss 0.004706116648891554\n",
      "Episode 8446; Testing Loss 0.0058243143229346; Training Loss 0.004706102989841423\n",
      "Episode 8447; Testing Loss 0.005824535655120779; Training Loss 0.004706097123885047\n",
      "Episode 8448; Testing Loss 0.005824404060776651; Training Loss 0.00470608534315939\n",
      "Episode 8449; Testing Loss 0.005824227008612555; Training Loss 0.004706078893266579\n",
      "Episode 8450; Testing Loss 0.005824377246434364; Training Loss 0.004706065936851737\n",
      "Episode 8451; Testing Loss 0.005824427719685421; Training Loss 0.004706056865672571\n",
      "Episode 8452; Testing Loss 0.005824342847161864; Training Loss 0.004706050536645107\n",
      "Episode 8453; Testing Loss 0.005824407529016636; Training Loss 0.00470604020346594\n",
      "Episode 8454; Testing Loss 0.005824403075716618; Training Loss 0.004706029311415575\n",
      "Episode 8455; Testing Loss 0.005824302595883196; Training Loss 0.004706020430675747\n",
      "Episode 8456; Testing Loss 0.005824219272291749; Training Loss 0.0047060122770929596\n",
      "Episode 8457; Testing Loss 0.005824334033292649; Training Loss 0.004706002105671054\n",
      "Episode 8458; Testing Loss 0.005824517856539164; Training Loss 0.004705995141781947\n",
      "Episode 8459; Testing Loss 0.005824414495803488; Training Loss 0.004705984100141398\n",
      "Episode 8460; Testing Loss 0.0058241702926708815; Training Loss 0.004705980068097484\n",
      "Episode 8461; Testing Loss 0.005824290726602535; Training Loss 0.004705966956300035\n",
      "Episode 8462; Testing Loss 0.005824498118598564; Training Loss 0.004705962951922523\n",
      "Episode 8463; Testing Loss 0.00582430603796835; Training Loss 0.0047059491466386934\n",
      "Episode 8464; Testing Loss 0.005824063911270624; Training Loss 0.004705945345188375\n",
      "Episode 8465; Testing Loss 0.005824231688432099; Training Loss 0.004705933377961995\n",
      "Episode 8466; Testing Loss 0.005824493720678009; Training Loss 0.004705925673935825\n",
      "Episode 8467; Testing Loss 0.005824362803015879; Training Loss 0.0047059156394700895\n",
      "Episode 8468; Testing Loss 0.005823998776768427; Training Loss 0.00470590940791514\n",
      "Episode 8469; Testing Loss 0.005824057278673206; Training Loss 0.004705895485356758\n",
      "Episode 8470; Testing Loss 0.005824419669110396; Training Loss 0.004705887985892371\n",
      "Episode 8471; Testing Loss 0.005824471229779393; Training Loss 0.004705879598912637\n",
      "Episode 8472; Testing Loss 0.005824158318020965; Training Loss 0.0047058677567998275\n",
      "Episode 8473; Testing Loss 0.005823971411220903; Training Loss 0.004705863634736289\n",
      "Episode 8474; Testing Loss 0.005824268506992974; Training Loss 0.004705848480278042\n",
      "Episode 8475; Testing Loss 0.005824421909429471; Training Loss 0.004705843672111036\n",
      "Episode 8476; Testing Loss 0.005824226386849123; Training Loss 0.004705833064518523\n",
      "Episode 8477; Testing Loss 0.005824068286199979; Training Loss 0.004705823986857701\n",
      "Episode 8478; Testing Loss 0.005824194463054352; Training Loss 0.004705815587977953\n",
      "Episode 8479; Testing Loss 0.005824234215341534; Training Loss 0.0047058080551778386\n",
      "Episode 8480; Testing Loss 0.005824105741719609; Training Loss 0.004705793755178347\n",
      "Episode 8481; Testing Loss 0.005824044930343877; Training Loss 0.004705791019516853\n",
      "Episode 8482; Testing Loss 0.005824234002728534; Training Loss 0.0047057834651285865\n",
      "Episode 8483; Testing Loss 0.00582433881365019; Training Loss 0.004705773406246831\n",
      "Episode 8484; Testing Loss 0.005824189304441913; Training Loss 0.0047057582506382645\n",
      "Episode 8485; Testing Loss 0.00582405745660838; Training Loss 0.004705754890045158\n",
      "Episode 8486; Testing Loss 0.005824091669632182; Training Loss 0.004705747253971926\n",
      "Episode 8487; Testing Loss 0.005824187443584309; Training Loss 0.004705734940621763\n",
      "Episode 8488; Testing Loss 0.005824099048433677; Training Loss 0.004705722176094567\n",
      "Episode 8489; Testing Loss 0.005824076660367196; Training Loss 0.004705715781667815\n",
      "Episode 8490; Testing Loss 0.005824134870326962; Training Loss 0.0047057038838745026\n",
      "Episode 8491; Testing Loss 0.005824086930801732; Training Loss 0.004705694633663842\n",
      "Episode 8492; Testing Loss 0.00582402095662091; Training Loss 0.004705685806940138\n",
      "Episode 8493; Testing Loss 0.005824058298208463; Training Loss 0.004705676945379634\n",
      "Episode 8494; Testing Loss 0.005824137109114884; Training Loss 0.004705667599761403\n",
      "Episode 8495; Testing Loss 0.00582411873270896; Training Loss 0.004705658114102224\n",
      "Episode 8496; Testing Loss 0.005824040407496223; Training Loss 0.004705649332455335\n",
      "Episode 8497; Testing Loss 0.0058240689664859555; Training Loss 0.004705639773759164\n",
      "Episode 8498; Testing Loss 0.005824050552807662; Training Loss 0.004705630047985036\n",
      "Episode 8499; Testing Loss 0.005824065513662612; Training Loss 0.0047056219932177625\n",
      "Episode 8500; Testing Loss 0.005824107447952927; Training Loss 0.0047056122688185235\n",
      "Episode 8501; Testing Loss 0.005823981794801167; Training Loss 0.004705604545058558\n",
      "Episode 8502; Testing Loss 0.005824017193283684; Training Loss 0.004705593977915265\n",
      "Episode 8503; Testing Loss 0.005824057911148455; Training Loss 0.004705589684390601\n",
      "Episode 8504; Testing Loss 0.0058238884479823415; Training Loss 0.004705580834916905\n",
      "Episode 8505; Testing Loss 0.005823795986640142; Training Loss 0.00470557152574191\n",
      "Episode 8506; Testing Loss 0.00582406850705315; Training Loss 0.004705559715654913\n",
      "Episode 8507; Testing Loss 0.005824272280600363; Training Loss 0.004705556146494509\n",
      "Episode 8508; Testing Loss 0.005823990190226086; Training Loss 0.0047055408001487236\n",
      "Episode 8509; Testing Loss 0.005823794182048016; Training Loss 0.004705542694545378\n",
      "Episode 8510; Testing Loss 0.005824109966829503; Training Loss 0.00470552819618932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8511; Testing Loss 0.005824233632196798; Training Loss 0.0047055198016875835\n",
      "Episode 8512; Testing Loss 0.005823942395539454; Training Loss 0.0047055060295184215\n",
      "Episode 8513; Testing Loss 0.005823652374727565; Training Loss 0.004705500988412817\n",
      "Episode 8514; Testing Loss 0.005823823850454592; Training Loss 0.004705486825936752\n",
      "Episode 8515; Testing Loss 0.0058241619081075; Training Loss 0.004705485169113767\n",
      "Episode 8516; Testing Loss 0.00582398280719455; Training Loss 0.004705469277554729\n",
      "Episode 8517; Testing Loss 0.005823685181298256; Training Loss 0.004705463746349605\n",
      "Episode 8518; Testing Loss 0.005823810074838784; Training Loss 0.004705453807095289\n",
      "Episode 8519; Testing Loss 0.00582403813451576; Training Loss 0.004705444712868765\n",
      "Episode 8520; Testing Loss 0.0058239652347613306; Training Loss 0.004705434970828851\n",
      "Episode 8521; Testing Loss 0.005823875830170125; Training Loss 0.004705428763162044\n",
      "Episode 8522; Testing Loss 0.005823917317332843; Training Loss 0.004705415593132312\n",
      "Episode 8523; Testing Loss 0.0058239755615776545; Training Loss 0.004705410607612522\n",
      "Episode 8524; Testing Loss 0.005823855511944393; Training Loss 0.004705404257402297\n",
      "Episode 8525; Testing Loss 0.005823713178399869; Training Loss 0.004705388912716708\n",
      "Episode 8526; Testing Loss 0.00582376365232807; Training Loss 0.004705380043665894\n",
      "Episode 8527; Testing Loss 0.005823924861349393; Training Loss 0.004705372711700033\n",
      "Episode 8528; Testing Loss 0.005823896982125266; Training Loss 0.004705361354629303\n",
      "Episode 8529; Testing Loss 0.005823714417791022; Training Loss 0.004705353188006034\n",
      "Episode 8530; Testing Loss 0.005823699256271943; Training Loss 0.004705344460767656\n",
      "Episode 8531; Testing Loss 0.005823926191590108; Training Loss 0.004705334815160775\n",
      "Episode 8532; Testing Loss 0.005823994548515937; Training Loss 0.004705325895870997\n",
      "Episode 8533; Testing Loss 0.005823853552050405; Training Loss 0.004705316626500396\n",
      "Episode 8534; Testing Loss 0.005823733270233496; Training Loss 0.004705308994688515\n",
      "Episode 8535; Testing Loss 0.005823712304026344; Training Loss 0.004705298089921217\n",
      "Episode 8536; Testing Loss 0.005823818401170121; Training Loss 0.004705289164970195\n",
      "Episode 8537; Testing Loss 0.005823812412846621; Training Loss 0.004705279690441873\n",
      "Episode 8538; Testing Loss 0.00582366315216443; Training Loss 0.004705270709389206\n",
      "Episode 8539; Testing Loss 0.00582375675143496; Training Loss 0.004705263979797153\n",
      "Episode 8540; Testing Loss 0.00582377957851784; Training Loss 0.004705254319281976\n",
      "Episode 8541; Testing Loss 0.0058236893536234005; Training Loss 0.004705246301382213\n",
      "Episode 8542; Testing Loss 0.00582375023392291; Training Loss 0.0047052383855281914\n",
      "Episode 8543; Testing Loss 0.0058237952302668036; Training Loss 0.004705225846418224\n",
      "Episode 8544; Testing Loss 0.0058236700363474446; Training Loss 0.004705218066683986\n",
      "Episode 8545; Testing Loss 0.005823558796448575; Training Loss 0.004705208484841692\n",
      "Episode 8546; Testing Loss 0.005823744024433632; Training Loss 0.004705199267245865\n",
      "Episode 8547; Testing Loss 0.0058238518336395425; Training Loss 0.0047051923610774826\n",
      "Episode 8548; Testing Loss 0.00582362256679523; Training Loss 0.004705181933045386\n",
      "Episode 8549; Testing Loss 0.0058235513915938395; Training Loss 0.004705172904380744\n",
      "Episode 8550; Testing Loss 0.005823724140060244; Training Loss 0.004705166669987071\n",
      "Episode 8551; Testing Loss 0.005823659247103309; Training Loss 0.004705155697230811\n",
      "Episode 8552; Testing Loss 0.0058234892542620995; Training Loss 0.004705148726874841\n",
      "Episode 8553; Testing Loss 0.005823673532208673; Training Loss 0.0047051360636708745\n",
      "Episode 8554; Testing Loss 0.0058238875444257925; Training Loss 0.004705132644973241\n",
      "Episode 8555; Testing Loss 0.005823657905965392; Training Loss 0.00470511946209447\n",
      "Episode 8556; Testing Loss 0.005823463015212148; Training Loss 0.004705118077285514\n",
      "Episode 8557; Testing Loss 0.005823687149933568; Training Loss 0.004705102238224985\n",
      "Episode 8558; Testing Loss 0.005823862685066773; Training Loss 0.004705096882510148\n",
      "Episode 8559; Testing Loss 0.005823595188787919; Training Loss 0.00470508721555428\n",
      "Episode 8560; Testing Loss 0.005823246645319408; Training Loss 0.004705082211797353\n",
      "Episode 8561; Testing Loss 0.005823373121710366; Training Loss 0.004705065610024608\n",
      "Episode 8562; Testing Loss 0.005823756617316725; Training Loss 0.0047050636142305135\n",
      "Episode 8563; Testing Loss 0.005823707900019258; Training Loss 0.004705049500081773\n",
      "Episode 8564; Testing Loss 0.005823404309711866; Training Loss 0.00470503991082956\n",
      "Episode 8565; Testing Loss 0.005823391112558028; Training Loss 0.004705034287962598\n",
      "Episode 8566; Testing Loss 0.005823773145256159; Training Loss 0.00470502437250602\n",
      "Episode 8567; Testing Loss 0.005823818665270669; Training Loss 0.004705016857130249\n",
      "Episode 8568; Testing Loss 0.005823484635617145; Training Loss 0.004705003945926942\n",
      "Episode 8569; Testing Loss 0.005823380054499667; Training Loss 0.004704994048918234\n",
      "Episode 8570; Testing Loss 0.00582358304008379; Training Loss 0.004704986208953588\n",
      "Episode 8571; Testing Loss 0.005823586435505876; Training Loss 0.004704975025851545\n",
      "Episode 8572; Testing Loss 0.005823406356399574; Training Loss 0.004704970085561299\n",
      "Episode 8573; Testing Loss 0.005823458184723049; Training Loss 0.004704961421972131\n",
      "Episode 8574; Testing Loss 0.0058236554467356975; Training Loss 0.004704950962582564\n",
      "Episode 8575; Testing Loss 0.0058236157922916774; Training Loss 0.00470494109167341\n",
      "Episode 8576; Testing Loss 0.00582336054881397; Training Loss 0.004704933176916509\n",
      "Episode 8577; Testing Loss 0.00582333056793153; Training Loss 0.0047049232276591794\n",
      "Episode 8578; Testing Loss 0.005823550923280255; Training Loss 0.004704913488391113\n",
      "Episode 8579; Testing Loss 0.005823583724430064; Training Loss 0.004704905131470875\n",
      "Episode 8580; Testing Loss 0.005823298002151948; Training Loss 0.004704895618337586\n",
      "Episode 8581; Testing Loss 0.005823295110325688; Training Loss 0.004704885317987592\n",
      "Episode 8582; Testing Loss 0.005823463367642422; Training Loss 0.004704876407169406\n",
      "Episode 8583; Testing Loss 0.005823534990289713; Training Loss 0.004704865834348704\n",
      "Episode 8584; Testing Loss 0.005823530691802571; Training Loss 0.004704859426106738\n",
      "Episode 8585; Testing Loss 0.005823512554483248; Training Loss 0.004704848242735582\n",
      "Episode 8586; Testing Loss 0.005823427781055552; Training Loss 0.004704838581596229\n",
      "Episode 8587; Testing Loss 0.005823362718481435; Training Loss 0.004704832072183776\n",
      "Episode 8588; Testing Loss 0.005823527273984903; Training Loss 0.004704821296946518\n",
      "Episode 8589; Testing Loss 0.005823549831348844; Training Loss 0.004704816496098605\n",
      "Episode 8590; Testing Loss 0.0058233064192424085; Training Loss 0.004704805161971956\n",
      "Episode 8591; Testing Loss 0.005823254840039717; Training Loss 0.004704798439427188\n",
      "Episode 8592; Testing Loss 0.005823546157381452; Training Loss 0.004704788373786457\n",
      "Episode 8593; Testing Loss 0.005823547389670552; Training Loss 0.004704779411228706\n",
      "Episode 8594; Testing Loss 0.005823182875132842; Training Loss 0.0047047722763878495\n",
      "Episode 8595; Testing Loss 0.005823190050382215; Training Loss 0.004704759887619077\n",
      "Episode 8596; Testing Loss 0.005823477995016638; Training Loss 0.004704754417470144\n",
      "Episode 8597; Testing Loss 0.005823433750534948; Training Loss 0.004704742792335045\n",
      "Episode 8598; Testing Loss 0.005823163473279049; Training Loss 0.004704739117961101\n",
      "Episode 8599; Testing Loss 0.005823298622878898; Training Loss 0.004704726953294418\n",
      "Episode 8600; Testing Loss 0.005823533866527542; Training Loss 0.004704719629616256\n",
      "Episode 8601; Testing Loss 0.005823386215146933; Training Loss 0.004704705678892179\n",
      "Episode 8602; Testing Loss 0.005823013889223088; Training Loss 0.004704699162177081\n",
      "Episode 8603; Testing Loss 0.005823054987655837; Training Loss 0.00470468866635129\n",
      "Episode 8604; Testing Loss 0.005823428460064272; Training Loss 0.004704680394766493\n",
      "Episode 8605; Testing Loss 0.005823439108474842; Training Loss 0.00470467161701757\n",
      "Episode 8606; Testing Loss 0.005823188625317129; Training Loss 0.004704663103364389\n",
      "Episode 8607; Testing Loss 0.005823168575526257; Training Loss 0.0047046505813807815\n",
      "Episode 8608; Testing Loss 0.005823369477974683; Training Loss 0.0047046486569970606\n",
      "Episode 8609; Testing Loss 0.0058232567767507425; Training Loss 0.004704639639056286\n",
      "Episode 8610; Testing Loss 0.005823013430850673; Training Loss 0.004704627815557404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8611; Testing Loss 0.005823058948176162; Training Loss 0.004704617133926456\n",
      "Episode 8612; Testing Loss 0.005823423513979174; Training Loss 0.004704610564518147\n",
      "Episode 8613; Testing Loss 0.005823447088175161; Training Loss 0.0047046005408368586\n",
      "Episode 8614; Testing Loss 0.00582315544138289; Training Loss 0.004704588731111101\n",
      "Episode 8615; Testing Loss 0.005822974706114015; Training Loss 0.004704582714204124\n",
      "Episode 8616; Testing Loss 0.005823196639686779; Training Loss 0.0047045704941104625\n",
      "Episode 8617; Testing Loss 0.005823300423499656; Training Loss 0.00470456279225606\n",
      "Episode 8618; Testing Loss 0.005823159181794963; Training Loss 0.0047045533039021675\n",
      "Episode 8619; Testing Loss 0.005823034574441379; Training Loss 0.004704543038006044\n",
      "Episode 8620; Testing Loss 0.005823064771425497; Training Loss 0.004704534876316884\n",
      "Episode 8621; Testing Loss 0.0058230948711113645; Training Loss 0.004704524990047437\n",
      "Episode 8622; Testing Loss 0.005823116094075148; Training Loss 0.004704515731020103\n",
      "Episode 8623; Testing Loss 0.005823132949251752; Training Loss 0.004704506757247741\n",
      "Episode 8624; Testing Loss 0.0058231317747439405; Training Loss 0.004704497089454981\n",
      "Episode 8625; Testing Loss 0.005823040389514348; Training Loss 0.004704488305436347\n",
      "Episode 8626; Testing Loss 0.005823018649633115; Training Loss 0.004704480103595593\n",
      "Episode 8627; Testing Loss 0.0058230383956967115; Training Loss 0.004704470275333151\n",
      "Episode 8628; Testing Loss 0.005823139809765902; Training Loss 0.004704461198763985\n",
      "Episode 8629; Testing Loss 0.0058231530769427; Training Loss 0.0047044532997544105\n",
      "Episode 8630; Testing Loss 0.005822971301274675; Training Loss 0.004704443398769533\n",
      "Episode 8631; Testing Loss 0.005822993071100646; Training Loss 0.004704434065334151\n",
      "Episode 8632; Testing Loss 0.005823142417947132; Training Loss 0.00470442493010332\n",
      "Episode 8633; Testing Loss 0.005823148101257251; Training Loss 0.0047044165011461215\n",
      "Episode 8634; Testing Loss 0.005823043764239142; Training Loss 0.004704407306408491\n",
      "Episode 8635; Testing Loss 0.005822948374328641; Training Loss 0.004704399890615402\n",
      "Episode 8636; Testing Loss 0.00582313280646294; Training Loss 0.0047043908876065015\n",
      "Episode 8637; Testing Loss 0.005823069932345197; Training Loss 0.004704380670262874\n",
      "Episode 8638; Testing Loss 0.0058229479937614995; Training Loss 0.004704371368616725\n",
      "Episode 8639; Testing Loss 0.005823057851264992; Training Loss 0.004704366252558647\n",
      "Episode 8640; Testing Loss 0.005823002182586988; Training Loss 0.0047043565269865315\n",
      "Episode 8641; Testing Loss 0.005822882025332455; Training Loss 0.004704344629241107\n",
      "Episode 8642; Testing Loss 0.005823012851869093; Training Loss 0.004704334422155284\n",
      "Episode 8643; Testing Loss 0.005823106823828471; Training Loss 0.004704329313432647\n",
      "Episode 8644; Testing Loss 0.005822950104697733; Training Loss 0.004704318140868354\n",
      "Episode 8645; Testing Loss 0.005822872514075519; Training Loss 0.004704314066756718\n",
      "Episode 8646; Testing Loss 0.005823066145966415; Training Loss 0.004704303506254955\n",
      "Episode 8647; Testing Loss 0.005823110783810197; Training Loss 0.004704293095316428\n",
      "Episode 8648; Testing Loss 0.005822881458128627; Training Loss 0.004704284221711029\n",
      "Episode 8649; Testing Loss 0.00582269847327957; Training Loss 0.004704278640825285\n",
      "Episode 8650; Testing Loss 0.005822830622570141; Training Loss 0.00470426310722717\n",
      "Episode 8651; Testing Loss 0.005823056520664766; Training Loss 0.004704260543373285\n",
      "Episode 8652; Testing Loss 0.005823093669015382; Training Loss 0.004704254931569035\n",
      "Episode 8653; Testing Loss 0.005822886420918792; Training Loss 0.0047042414768416465\n",
      "Episode 8654; Testing Loss 0.0058228183655799496; Training Loss 0.004704228383587081\n",
      "Episode 8655; Testing Loss 0.005822958360142349; Training Loss 0.0047042269543933445\n",
      "Episode 8656; Testing Loss 0.005822956630544258; Training Loss 0.004704221669976949\n",
      "Episode 8657; Testing Loss 0.005822800609809157; Training Loss 0.004704209682130085\n",
      "Episode 8658; Testing Loss 0.0058227785965177985; Training Loss 0.004704193812865066\n",
      "Episode 8659; Testing Loss 0.005822892292468578; Training Loss 0.004704188980986105\n",
      "Episode 8660; Testing Loss 0.005822981003815663; Training Loss 0.004704188304838325\n",
      "Episode 8661; Testing Loss 0.0058228965188295255; Training Loss 0.004704175830507073\n",
      "Episode 8662; Testing Loss 0.005822853167230697; Training Loss 0.004704163014386775\n",
      "Episode 8663; Testing Loss 0.005822982521539599; Training Loss 0.004704149646813122\n",
      "Episode 8664; Testing Loss 0.005822995430890016; Training Loss 0.004704147359645516\n",
      "Episode 8665; Testing Loss 0.005822816097207366; Training Loss 0.004704137310380305\n",
      "Episode 8666; Testing Loss 0.005822717631378671; Training Loss 0.004704124769557588\n",
      "Episode 8667; Testing Loss 0.005822854744043391; Training Loss 0.004704112541895531\n",
      "Episode 8668; Testing Loss 0.005822940479037211; Training Loss 0.004704107237984073\n",
      "Episode 8669; Testing Loss 0.005822855256936199; Training Loss 0.004704098422271464\n",
      "Episode 8670; Testing Loss 0.00582275536494408; Training Loss 0.004704085325276559\n",
      "Episode 8671; Testing Loss 0.005822805800292579; Training Loss 0.004704079659361646\n",
      "Episode 8672; Testing Loss 0.005822797809023813; Training Loss 0.00470407352317978\n",
      "Episode 8673; Testing Loss 0.005822704685678126; Training Loss 0.004704061115975\n",
      "Episode 8674; Testing Loss 0.005822644286712773; Training Loss 0.004704050840365149\n",
      "Episode 8675; Testing Loss 0.005822786144797266; Training Loss 0.004704048907230952\n",
      "Episode 8676; Testing Loss 0.005822894990052495; Training Loss 0.004704040913631642\n",
      "Episode 8677; Testing Loss 0.005822783035292478; Training Loss 0.004704025857461625\n",
      "Episode 8678; Testing Loss 0.005822619965320166; Training Loss 0.004704017859280702\n",
      "Episode 8679; Testing Loss 0.005822601542440507; Training Loss 0.004704010676173223\n",
      "Episode 8680; Testing Loss 0.005822714057076141; Training Loss 0.004704000589305717\n",
      "Episode 8681; Testing Loss 0.005822828504493851; Training Loss 0.004703990448996754\n",
      "Episode 8682; Testing Loss 0.005822834889091341; Training Loss 0.004703981215014727\n",
      "Episode 8683; Testing Loss 0.005822718959945606; Training Loss 0.004703973749673862\n",
      "Episode 8684; Testing Loss 0.005822729047440834; Training Loss 0.00470396253262854\n",
      "Episode 8685; Testing Loss 0.005822745795836342; Training Loss 0.004703955596721528\n",
      "Episode 8686; Testing Loss 0.005822611018359263; Training Loss 0.0047039467405524575\n",
      "Episode 8687; Testing Loss 0.005822503708093553; Training Loss 0.004703935427562246\n",
      "Episode 8688; Testing Loss 0.005822643464309211; Training Loss 0.004703925032140756\n",
      "Episode 8689; Testing Loss 0.005822802728546419; Training Loss 0.004703918061957644\n",
      "Episode 8690; Testing Loss 0.005822748013761897; Training Loss 0.004703906296990738\n",
      "Episode 8691; Testing Loss 0.005822546604673219; Training Loss 0.004703896549074853\n",
      "Episode 8692; Testing Loss 0.005822566775031104; Training Loss 0.004703888932461737\n",
      "Episode 8693; Testing Loss 0.005822628288912452; Training Loss 0.0047038776625782566\n",
      "Episode 8694; Testing Loss 0.005822660944068935; Training Loss 0.004703869351026099\n",
      "Episode 8695; Testing Loss 0.005822540853177811; Training Loss 0.004703859826314357\n",
      "Episode 8696; Testing Loss 0.005822479007467871; Training Loss 0.0047038544278892504\n",
      "Episode 8697; Testing Loss 0.005822655855615654; Training Loss 0.0047038429105149444\n",
      "Episode 8698; Testing Loss 0.005822741254496154; Training Loss 0.004703837485898635\n",
      "Episode 8699; Testing Loss 0.005822535082097355; Training Loss 0.004703825842508132\n",
      "Episode 8700; Testing Loss 0.005822394216326647; Training Loss 0.004703820266590762\n",
      "Episode 8701; Testing Loss 0.005822601110412048; Training Loss 0.004703809533492494\n",
      "Episode 8702; Testing Loss 0.005822735687062959; Training Loss 0.004703801053319991\n",
      "Episode 8703; Testing Loss 0.005822508587855367; Training Loss 0.004703793104854687\n",
      "Episode 8704; Testing Loss 0.005822305949941234; Training Loss 0.004703787490296777\n",
      "Episode 8705; Testing Loss 0.005822445396607944; Training Loss 0.00470377254603104\n",
      "Episode 8706; Testing Loss 0.005822727365758911; Training Loss 0.004703766265584331\n",
      "Episode 8707; Testing Loss 0.005822773631228683; Training Loss 0.004703759633059354\n",
      "Episode 8708; Testing Loss 0.005822513249775996; Training Loss 0.00470374636914693\n",
      "Episode 8709; Testing Loss 0.005822433381973627; Training Loss 0.004703738097083611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8710; Testing Loss 0.005822505165885095; Training Loss 0.004703729802038409\n",
      "Episode 8711; Testing Loss 0.0058225435426454; Training Loss 0.004703718080983149\n",
      "Episode 8712; Testing Loss 0.0058224414157243614; Training Loss 0.0047037136035225436\n",
      "Episode 8713; Testing Loss 0.0058224246364889515; Training Loss 0.004703707299312797\n",
      "Episode 8714; Testing Loss 0.005822500887299186; Training Loss 0.004703697258113032\n",
      "Episode 8715; Testing Loss 0.00582250793738154; Training Loss 0.004703684327391331\n",
      "Episode 8716; Testing Loss 0.005822396231215183; Training Loss 0.004703673937701294\n",
      "Episode 8717; Testing Loss 0.005822309373620607; Training Loss 0.004703667089977146\n",
      "Episode 8718; Testing Loss 0.005822361454025418; Training Loss 0.004703654971050727\n",
      "Episode 8719; Testing Loss 0.005822476096463289; Training Loss 0.004703647954146725\n",
      "Episode 8720; Testing Loss 0.00582250901777163; Training Loss 0.004703640660211479\n",
      "Episode 8721; Testing Loss 0.005822394594089741; Training Loss 0.004703628411736333\n",
      "Episode 8722; Testing Loss 0.005822300540173825; Training Loss 0.004703619330977359\n",
      "Episode 8723; Testing Loss 0.005822324030292383; Training Loss 0.004703611407137036\n",
      "Episode 8724; Testing Loss 0.00582239040929716; Training Loss 0.004703600766080643\n",
      "Episode 8725; Testing Loss 0.005822433438943055; Training Loss 0.00470359240830642\n",
      "Episode 8726; Testing Loss 0.00582237353798917; Training Loss 0.0047035825040373895\n",
      "Episode 8727; Testing Loss 0.0058223332344475055; Training Loss 0.004703574064675769\n",
      "Episode 8728; Testing Loss 0.0058223071177943096; Training Loss 0.0047035646946094675\n",
      "Episode 8729; Testing Loss 0.005822350387736168; Training Loss 0.004703555835804955\n",
      "Episode 8730; Testing Loss 0.005822391003690382; Training Loss 0.004703546760064582\n",
      "Episode 8731; Testing Loss 0.005822381888172219; Training Loss 0.0047035373990851085\n",
      "Episode 8732; Testing Loss 0.005822344654353948; Training Loss 0.004703528067384435\n",
      "Episode 8733; Testing Loss 0.005822325185192849; Training Loss 0.004703519788293791\n",
      "Episode 8734; Testing Loss 0.005822367043005318; Training Loss 0.004703510568329302\n",
      "Episode 8735; Testing Loss 0.005822312945806102; Training Loss 0.0047035013231133856\n",
      "Episode 8736; Testing Loss 0.005822314345126985; Training Loss 0.004703492074325687\n",
      "Episode 8737; Testing Loss 0.0058222254515114015; Training Loss 0.004703484618202101\n",
      "Episode 8738; Testing Loss 0.0058222910543218095; Training Loss 0.004703473609653201\n",
      "Episode 8739; Testing Loss 0.005822335833051388; Training Loss 0.004703466593183837\n",
      "Episode 8740; Testing Loss 0.005822264733777089; Training Loss 0.0047034581305434135\n",
      "Episode 8741; Testing Loss 0.005822273843339933; Training Loss 0.004703448521876603\n",
      "Episode 8742; Testing Loss 0.00582226324092434; Training Loss 0.0047034404551241065\n",
      "Episode 8743; Testing Loss 0.0058221419665089565; Training Loss 0.004703431354972191\n",
      "Episode 8744; Testing Loss 0.005822105427671858; Training Loss 0.004703425826007286\n",
      "Episode 8745; Testing Loss 0.005822300900138842; Training Loss 0.004703416440948957\n",
      "Episode 8746; Testing Loss 0.0058224273384715934; Training Loss 0.00470340504247223\n",
      "Episode 8747; Testing Loss 0.00582225343489576; Training Loss 0.004703399595434659\n",
      "Episode 8748; Testing Loss 0.005822075993969554; Training Loss 0.004703393931266693\n",
      "Episode 8749; Testing Loss 0.005822151536157302; Training Loss 0.004703378943940428\n",
      "Episode 8750; Testing Loss 0.005822291945901828; Training Loss 0.00470337234696855\n",
      "Episode 8751; Testing Loss 0.005822335785743679; Training Loss 0.004703366871355527\n",
      "Episode 8752; Testing Loss 0.0058222093072356675; Training Loss 0.004703354149365634\n",
      "Episode 8753; Testing Loss 0.0058221276873428075; Training Loss 0.004703340961619746\n",
      "Episode 8754; Testing Loss 0.005822109876434613; Training Loss 0.0047033378672186495\n",
      "Episode 8755; Testing Loss 0.005822170565299732; Training Loss 0.00470332934241823\n",
      "Episode 8756; Testing Loss 0.0058222118140670775; Training Loss 0.004703315378007619\n",
      "Episode 8757; Testing Loss 0.005822137392142088; Training Loss 0.004703310651084814\n",
      "Episode 8758; Testing Loss 0.005822165325277719; Training Loss 0.004703307967101901\n",
      "Episode 8759; Testing Loss 0.005822237513256201; Training Loss 0.0047032946710623075\n",
      "Episode 8760; Testing Loss 0.005822169911270659; Training Loss 0.004703283255828863\n",
      "Episode 8761; Testing Loss 0.005822145568608496; Training Loss 0.0047032766610905575\n",
      "Episode 8762; Testing Loss 0.005822119495938756; Training Loss 0.004703269274148402\n",
      "Episode 8763; Testing Loss 0.005822129538154451; Training Loss 0.004703254092959116\n",
      "Episode 8764; Testing Loss 0.005822118899074601; Training Loss 0.004703245006122071\n",
      "Episode 8765; Testing Loss 0.005822165211620758; Training Loss 0.004703244637070727\n",
      "Episode 8766; Testing Loss 0.0058221610897965445; Training Loss 0.004703232277673368\n",
      "Episode 8767; Testing Loss 0.005822059319812442; Training Loss 0.004703216524696347\n",
      "Episode 8768; Testing Loss 0.005822034394659593; Training Loss 0.004703212232855838\n",
      "Episode 8769; Testing Loss 0.00582207676544141; Training Loss 0.004703205052131306\n",
      "Episode 8770; Testing Loss 0.005822144797467978; Training Loss 0.004703192617603846\n",
      "Episode 8771; Testing Loss 0.005822123000855333; Training Loss 0.004703182048999232\n",
      "Episode 8772; Testing Loss 0.005822031769315938; Training Loss 0.004703174419657314\n",
      "Episode 8773; Testing Loss 0.005822025706237062; Training Loss 0.004703167955373871\n",
      "Episode 8774; Testing Loss 0.005821990442026716; Training Loss 0.004703157287959647\n",
      "Episode 8775; Testing Loss 0.005821926649560362; Training Loss 0.004703150005356636\n",
      "Episode 8776; Testing Loss 0.0058219275931396305; Training Loss 0.0047031425669820884\n",
      "Episode 8777; Testing Loss 0.005822037521420899; Training Loss 0.004703131749429525\n",
      "Episode 8778; Testing Loss 0.00582213049690064; Training Loss 0.004703124987480046\n",
      "Episode 8779; Testing Loss 0.005822114354714108; Training Loss 0.004703113837352989\n",
      "Episode 8780; Testing Loss 0.005822016206208852; Training Loss 0.004703102441101331\n",
      "Episode 8781; Testing Loss 0.005821952921197056; Training Loss 0.004703093552909607\n",
      "Episode 8782; Testing Loss 0.005821865797149983; Training Loss 0.004703086566481201\n",
      "Episode 8783; Testing Loss 0.005821821028687435; Training Loss 0.0047030749433777715\n",
      "Episode 8784; Testing Loss 0.0058219482705940215; Training Loss 0.00470306655184702\n",
      "Episode 8785; Testing Loss 0.005822039938111551; Training Loss 0.004703058442384649\n",
      "Episode 8786; Testing Loss 0.005821919563360854; Training Loss 0.004703047207271271\n",
      "Episode 8787; Testing Loss 0.005821796250366086; Training Loss 0.0047030396761801145\n",
      "Episode 8788; Testing Loss 0.005821935728731843; Training Loss 0.004703029688082246\n",
      "Episode 8789; Testing Loss 0.0058220625782370574; Training Loss 0.00470302076732788\n",
      "Episode 8790; Testing Loss 0.00582203038784213; Training Loss 0.004703011612900055\n",
      "Episode 8791; Testing Loss 0.005821873101682908; Training Loss 0.004703001500979677\n",
      "Episode 8792; Testing Loss 0.005821758560439646; Training Loss 0.004702995183084158\n",
      "Episode 8793; Testing Loss 0.005821783054928721; Training Loss 0.0047029842363413695\n",
      "Episode 8794; Testing Loss 0.005821917598785238; Training Loss 0.004702978081038131\n",
      "Episode 8795; Testing Loss 0.00582198505927289; Training Loss 0.004702970206431104\n",
      "Episode 8796; Testing Loss 0.0058218647399938565; Training Loss 0.0047029568218618185\n",
      "Episode 8797; Testing Loss 0.005821784611605163; Training Loss 0.004702953488992094\n",
      "Episode 8798; Testing Loss 0.005821842047028319; Training Loss 0.00470294835247254\n",
      "Episode 8799; Testing Loss 0.005821978947171307; Training Loss 0.0047029359833971095\n",
      "Episode 8800; Testing Loss 0.005821962112271511; Training Loss 0.004702923943059037\n",
      "Episode 8801; Testing Loss 0.005821889845260456; Training Loss 0.004702917756445241\n",
      "Episode 8802; Testing Loss 0.005821845697107493; Training Loss 0.004702906277926286\n",
      "Episode 8803; Testing Loss 0.0058218174487956135; Training Loss 0.004702895136218831\n",
      "Episode 8804; Testing Loss 0.005821794646540956; Training Loss 0.004702892013878132\n",
      "Episode 8805; Testing Loss 0.005821730157294727; Training Loss 0.004702880726998476\n",
      "Episode 8806; Testing Loss 0.00582174622155175; Training Loss 0.004702867726150619\n",
      "Episode 8807; Testing Loss 0.005821830582142791; Training Loss 0.004702863088623705\n",
      "Episode 8808; Testing Loss 0.00582188595282625; Training Loss 0.004702855672133055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8809; Testing Loss 0.005821852586508779; Training Loss 0.0047028414116466\n",
      "Episode 8810; Testing Loss 0.005821793184414009; Training Loss 0.004702837840772896\n",
      "Episode 8811; Testing Loss 0.005821700350088699; Training Loss 0.0047028302495190615\n",
      "Episode 8812; Testing Loss 0.00582165933577785; Training Loss 0.004702818884622042\n",
      "Episode 8813; Testing Loss 0.005821680207455584; Training Loss 0.00470280745576294\n",
      "Episode 8814; Testing Loss 0.005821838689128981; Training Loss 0.004702800964166689\n",
      "Episode 8815; Testing Loss 0.005821853709226595; Training Loss 0.00470279041508196\n",
      "Episode 8816; Testing Loss 0.005821626051890881; Training Loss 0.004702780721398313\n",
      "Episode 8817; Testing Loss 0.005821506794990638; Training Loss 0.004702774075144023\n",
      "Episode 8818; Testing Loss 0.005821641705513456; Training Loss 0.0047027612720856584\n",
      "Episode 8819; Testing Loss 0.005821864068553049; Training Loss 0.004702755233110057\n",
      "Episode 8820; Testing Loss 0.005821762730474103; Training Loss 0.004702744548693148\n",
      "Episode 8821; Testing Loss 0.005821567985639547; Training Loss 0.004702735417384555\n",
      "Episode 8822; Testing Loss 0.005821578201840106; Training Loss 0.004702728516348017\n",
      "Episode 8823; Testing Loss 0.005821618680067611; Training Loss 0.004702718688255998\n",
      "Episode 8824; Testing Loss 0.005821591304280105; Training Loss 0.004702708383184485\n",
      "Episode 8825; Testing Loss 0.005821620715071046; Training Loss 0.004702702811757738\n",
      "Episode 8826; Testing Loss 0.0058216752440823066; Training Loss 0.004702692804026585\n",
      "Episode 8827; Testing Loss 0.005821645565348469; Training Loss 0.00470268061202911\n",
      "Episode 8828; Testing Loss 0.005821557794738787; Training Loss 0.004702672248311016\n",
      "Episode 8829; Testing Loss 0.005821583958974235; Training Loss 0.004702665799258917\n",
      "Episode 8830; Testing Loss 0.005821650543427713; Training Loss 0.004702654036902962\n",
      "Episode 8831; Testing Loss 0.005821648412339793; Training Loss 0.00470264438041365\n",
      "Episode 8832; Testing Loss 0.005821549799107832; Training Loss 0.004702638739237177\n",
      "Episode 8833; Testing Loss 0.005821605771735957; Training Loss 0.004702628372115141\n",
      "Episode 8834; Testing Loss 0.005821701928627837; Training Loss 0.004702620318987134\n",
      "Episode 8835; Testing Loss 0.005821570788203488; Training Loss 0.0047026103545457715\n",
      "Episode 8836; Testing Loss 0.005821449916713226; Training Loss 0.004702602041832985\n",
      "Episode 8837; Testing Loss 0.005821641565685202; Training Loss 0.004702590760892429\n",
      "Episode 8838; Testing Loss 0.005821722443771252; Training Loss 0.004702583931592546\n",
      "Episode 8839; Testing Loss 0.005821451704511; Training Loss 0.004702575955634006\n",
      "Episode 8840; Testing Loss 0.0058214166603205286; Training Loss 0.004702567385429444\n",
      "Episode 8841; Testing Loss 0.005821624850643649; Training Loss 0.0047025561987936474\n",
      "Episode 8842; Testing Loss 0.005821609959559482; Training Loss 0.0047025464614898925\n",
      "Episode 8843; Testing Loss 0.005821454629862314; Training Loss 0.004702537808720439\n",
      "Episode 8844; Testing Loss 0.0058214697192396; Training Loss 0.00470252890384741\n",
      "Episode 8845; Testing Loss 0.0058215095972910975; Training Loss 0.004702519284209932\n",
      "Episode 8846; Testing Loss 0.005821465584466912; Training Loss 0.004702512621214772\n",
      "Episode 8847; Testing Loss 0.005821439373385217; Training Loss 0.004702502076245599\n",
      "Episode 8848; Testing Loss 0.0058214432821117375; Training Loss 0.004702495434950904\n",
      "Episode 8849; Testing Loss 0.005821350367893412; Training Loss 0.004702486715047162\n",
      "Episode 8850; Testing Loss 0.005821326330742325; Training Loss 0.004702474917000149\n",
      "Episode 8851; Testing Loss 0.0058214893303935545; Training Loss 0.004702467530354961\n",
      "Episode 8852; Testing Loss 0.005821662599838363; Training Loss 0.004702459809434628\n",
      "Episode 8853; Testing Loss 0.0058215026407243495; Training Loss 0.004702449318506385\n",
      "Episode 8854; Testing Loss 0.005821210106999518; Training Loss 0.004702442016586898\n",
      "Episode 8855; Testing Loss 0.005821284827801859; Training Loss 0.004702433734667659\n",
      "Episode 8856; Testing Loss 0.0058216378380906535; Training Loss 0.004702425951322916\n",
      "Episode 8857; Testing Loss 0.005821603066457103; Training Loss 0.004702415312120656\n",
      "Episode 8858; Testing Loss 0.005821211195986547; Training Loss 0.004702408007687822\n",
      "Episode 8859; Testing Loss 0.005821267154109657; Training Loss 0.004702396986511848\n",
      "Episode 8860; Testing Loss 0.005821480163842087; Training Loss 0.004702387560386313\n",
      "Episode 8861; Testing Loss 0.005821446287091638; Training Loss 0.004702377143983477\n",
      "Episode 8862; Testing Loss 0.00582128287435076; Training Loss 0.004702368664829326\n",
      "Episode 8863; Testing Loss 0.005821270510560078; Training Loss 0.0047023596192174635\n",
      "Episode 8864; Testing Loss 0.005821419735721165; Training Loss 0.004702350505976268\n",
      "Episode 8865; Testing Loss 0.005821411236880286; Training Loss 0.004702342342657216\n",
      "Episode 8866; Testing Loss 0.00582137485136651; Training Loss 0.00470233525104866\n",
      "Episode 8867; Testing Loss 0.005821359706969457; Training Loss 0.004702325457688938\n",
      "Episode 8868; Testing Loss 0.00582132444195094; Training Loss 0.004702316266993998\n",
      "Episode 8869; Testing Loss 0.005821299177484356; Training Loss 0.004702306193219985\n",
      "Episode 8870; Testing Loss 0.005821374417353408; Training Loss 0.004702299275230029\n",
      "Episode 8871; Testing Loss 0.005821421711714213; Training Loss 0.004702290315069333\n",
      "Episode 8872; Testing Loss 0.005821347743133925; Training Loss 0.004702280258113733\n",
      "Episode 8873; Testing Loss 0.0058212174061565355; Training Loss 0.004702272052456239\n",
      "Episode 8874; Testing Loss 0.005821225528239913; Training Loss 0.004702260913109558\n",
      "Episode 8875; Testing Loss 0.00582134327200526; Training Loss 0.004702253219732408\n",
      "Episode 8876; Testing Loss 0.005821374691214245; Training Loss 0.004702245650268647\n",
      "Episode 8877; Testing Loss 0.005821100453400274; Training Loss 0.004702235894732135\n",
      "Episode 8878; Testing Loss 0.005821058365890012; Training Loss 0.004702226462151471\n",
      "Episode 8879; Testing Loss 0.005821223948499687; Training Loss 0.004702215843541023\n",
      "Episode 8880; Testing Loss 0.0058213105971430005; Training Loss 0.004702208128980833\n",
      "Episode 8881; Testing Loss 0.0058212348874218806; Training Loss 0.004702199005957232\n",
      "Episode 8882; Testing Loss 0.005821118339485408; Training Loss 0.004702192436978194\n",
      "Episode 8883; Testing Loss 0.005821214420267438; Training Loss 0.004702180650066002\n",
      "Episode 8884; Testing Loss 0.0058212846733104; Training Loss 0.004702175362850512\n",
      "Episode 8885; Testing Loss 0.005821095004023445; Training Loss 0.004702164667455877\n",
      "Episode 8886; Testing Loss 0.005821003695349429; Training Loss 0.004702157574497619\n",
      "Episode 8887; Testing Loss 0.005821249414940402; Training Loss 0.004702145334673667\n",
      "Episode 8888; Testing Loss 0.005821402545502407; Training Loss 0.00470214246059735\n",
      "Episode 8889; Testing Loss 0.005821137476520179; Training Loss 0.004702128651396488\n",
      "Episode 8890; Testing Loss 0.005820941291501647; Training Loss 0.004702125973331033\n",
      "Episode 8891; Testing Loss 0.005821178648353784; Training Loss 0.004702112389743828\n",
      "Episode 8892; Testing Loss 0.005821394913830142; Training Loss 0.0047021064319584995\n",
      "Episode 8893; Testing Loss 0.0058211364090579; Training Loss 0.004702092016744678\n",
      "Episode 8894; Testing Loss 0.00582082794777838; Training Loss 0.004702090433789011\n",
      "Episode 8895; Testing Loss 0.005821077086208442; Training Loss 0.0047020741191021575\n",
      "Episode 8896; Testing Loss 0.005821331275490981; Training Loss 0.004702072378106783\n",
      "Episode 8897; Testing Loss 0.005821118588076161; Training Loss 0.004702056097311568\n",
      "Episode 8898; Testing Loss 0.005820891856704963; Training Loss 0.004702053031172106\n",
      "Episode 8899; Testing Loss 0.005821080061513548; Training Loss 0.00470203774209214\n",
      "Episode 8900; Testing Loss 0.0058212631292330575; Training Loss 0.004702036846151897\n",
      "Episode 8901; Testing Loss 0.005821042080422151; Training Loss 0.004702024503684635\n",
      "Episode 8902; Testing Loss 0.005820847515362372; Training Loss 0.00470201608856461\n",
      "Episode 8903; Testing Loss 0.0058210853781379265; Training Loss 0.004702004253428106\n",
      "Episode 8904; Testing Loss 0.005821328364441355; Training Loss 0.0047020013832218275\n",
      "Episode 8905; Testing Loss 0.005821121846803773; Training Loss 0.004701984791948213\n",
      "Episode 8906; Testing Loss 0.00582083098362947; Training Loss 0.004701983016841594\n",
      "Episode 8907; Testing Loss 0.005820969443994254; Training Loss 0.004701968589405439\n",
      "Episode 8908; Testing Loss 0.005821239176175298; Training Loss 0.00470196275238197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8909; Testing Loss 0.005821196950541006; Training Loss 0.004701956060116197\n",
      "Episode 8910; Testing Loss 0.0058209024029127285; Training Loss 0.00470194304351616\n",
      "Episode 8911; Testing Loss 0.005820903956102212; Training Loss 0.0047019362388774\n",
      "Episode 8912; Testing Loss 0.0058210554672110835; Training Loss 0.004701928435448486\n",
      "Episode 8913; Testing Loss 0.005821102153852985; Training Loss 0.004701916926787193\n",
      "Episode 8914; Testing Loss 0.005820996338734183; Training Loss 0.004701907719625642\n",
      "Episode 8915; Testing Loss 0.0058210924483166285; Training Loss 0.004701899758814549\n",
      "Episode 8916; Testing Loss 0.005821224701884425; Training Loss 0.00470188992070281\n",
      "Episode 8917; Testing Loss 0.005821133769040155; Training Loss 0.004701881219474227\n",
      "Episode 8918; Testing Loss 0.00582088219536045; Training Loss 0.004701872870576451\n",
      "Episode 8919; Testing Loss 0.005820845482497907; Training Loss 0.004701863713270584\n",
      "Episode 8920; Testing Loss 0.005821125934056022; Training Loss 0.004701855290436381\n",
      "Episode 8921; Testing Loss 0.005821203988513568; Training Loss 0.0047018479536063565\n",
      "Episode 8922; Testing Loss 0.005820952981939775; Training Loss 0.004701835385546675\n",
      "Episode 8923; Testing Loss 0.005820794687165316; Training Loss 0.004701829605210199\n",
      "Episode 8924; Testing Loss 0.005820987796947938; Training Loss 0.00470182048407818\n",
      "Episode 8925; Testing Loss 0.005821055467157695; Training Loss 0.004701810533289079\n",
      "Episode 8926; Testing Loss 0.005820888654694908; Training Loss 0.004701799941301402\n",
      "Episode 8927; Testing Loss 0.0058208940705827585; Training Loss 0.00470179234176247\n",
      "Episode 8928; Testing Loss 0.005821033375925946; Training Loss 0.004701781366016291\n",
      "Episode 8929; Testing Loss 0.005821006351198504; Training Loss 0.004701772245264708\n",
      "Episode 8930; Testing Loss 0.005820887019115599; Training Loss 0.004701764678335897\n",
      "Episode 8931; Testing Loss 0.005820814054376713; Training Loss 0.004701754365276347\n",
      "Episode 8932; Testing Loss 0.0058208801640191035; Training Loss 0.004701745232458156\n",
      "Episode 8933; Testing Loss 0.005820911535960804; Training Loss 0.004701737764135713\n",
      "Episode 8934; Testing Loss 0.005820767594836282; Training Loss 0.004701728012364376\n",
      "Episode 8935; Testing Loss 0.005820769145501805; Training Loss 0.004701720143055449\n",
      "Episode 8936; Testing Loss 0.005820926163383601; Training Loss 0.004701711539208032\n",
      "Episode 8937; Testing Loss 0.005820876015203124; Training Loss 0.004701700765416981\n",
      "Episode 8938; Testing Loss 0.005820834580785064; Training Loss 0.004701694217243922\n",
      "Episode 8939; Testing Loss 0.005820838935550158; Training Loss 0.00470168446776857\n",
      "Episode 8940; Testing Loss 0.005820781935516437; Training Loss 0.00470167427701405\n",
      "Episode 8941; Testing Loss 0.005820721317796163; Training Loss 0.004701668890404169\n",
      "Episode 8942; Testing Loss 0.0058208645924168415; Training Loss 0.004701658761123511\n",
      "Episode 8943; Testing Loss 0.005820939898036261; Training Loss 0.004701649309398262\n",
      "Episode 8944; Testing Loss 0.0058207078837336345; Training Loss 0.004701637675670711\n",
      "Episode 8945; Testing Loss 0.005820699851040596; Training Loss 0.004701628788338239\n",
      "Episode 8946; Testing Loss 0.005820926208814436; Training Loss 0.004701623670115011\n",
      "Episode 8947; Testing Loss 0.005820872721854734; Training Loss 0.004701612777644726\n",
      "Episode 8948; Testing Loss 0.005820619777083442; Training Loss 0.004701606972297953\n",
      "Episode 8949; Testing Loss 0.005820704056226338; Training Loss 0.004701596023764943\n",
      "Episode 8950; Testing Loss 0.005820879627566514; Training Loss 0.004701585160823293\n",
      "Episode 8951; Testing Loss 0.005820813787479432; Training Loss 0.0047015809905408285\n",
      "Episode 8952; Testing Loss 0.005820614711882571; Training Loss 0.004701573438248675\n",
      "Episode 8953; Testing Loss 0.005820616849192701; Training Loss 0.004701558519912649\n",
      "Episode 8954; Testing Loss 0.00582080926783282; Training Loss 0.004701553626585605\n",
      "Episode 8955; Testing Loss 0.0058208387946566825; Training Loss 0.0047015475245722246\n",
      "Episode 8956; Testing Loss 0.0058206486310781345; Training Loss 0.004701533690656216\n",
      "Episode 8957; Testing Loss 0.005820602225447745; Training Loss 0.004701521972621381\n",
      "Episode 8958; Testing Loss 0.005820671399527217; Training Loss 0.004701515283851638\n",
      "Episode 8959; Testing Loss 0.005820723900612586; Training Loss 0.004701502753957012\n",
      "Episode 8960; Testing Loss 0.005820650043844221; Training Loss 0.004701494593384121\n",
      "Episode 8961; Testing Loss 0.005820602291399922; Training Loss 0.004701485630738074\n",
      "Episode 8962; Testing Loss 0.005820602958120981; Training Loss 0.004701477206613165\n",
      "Episode 8963; Testing Loss 0.005820502009333147; Training Loss 0.00470146813495807\n",
      "Episode 8964; Testing Loss 0.005820419808135793; Training Loss 0.004701459909982481\n",
      "Episode 8965; Testing Loss 0.005820613669347262; Training Loss 0.004701450201925462\n",
      "Episode 8966; Testing Loss 0.00582069664899975; Training Loss 0.004701442863246906\n",
      "Episode 8967; Testing Loss 0.0058205803134459095; Training Loss 0.004701431832343787\n",
      "Episode 8968; Testing Loss 0.005820522664718254; Training Loss 0.004701424181405196\n",
      "Episode 8969; Testing Loss 0.005820591487425053; Training Loss 0.004701414971143328\n",
      "Episode 8970; Testing Loss 0.005820575145618847; Training Loss 0.004701404610074455\n",
      "Episode 8971; Testing Loss 0.005820538016066759; Training Loss 0.004701396545213703\n",
      "Episode 8972; Testing Loss 0.00582050764994214; Training Loss 0.004701386231041693\n",
      "Episode 8973; Testing Loss 0.005820435706374763; Training Loss 0.004701377968898037\n",
      "Episode 8974; Testing Loss 0.00582040161742907; Training Loss 0.004701368667545532\n",
      "Episode 8975; Testing Loss 0.005820526549848712; Training Loss 0.004701359399606144\n",
      "Episode 8976; Testing Loss 0.005820617424843921; Training Loss 0.0047013504368949985\n",
      "Episode 8977; Testing Loss 0.005820533538188737; Training Loss 0.004701340892224533\n",
      "Episode 8978; Testing Loss 0.0058203642285367466; Training Loss 0.004701333610843706\n",
      "Episode 8979; Testing Loss 0.005820450790916042; Training Loss 0.004701322194867076\n",
      "Episode 8980; Testing Loss 0.005820633057386564; Training Loss 0.004701316928350193\n",
      "Episode 8981; Testing Loss 0.005820477181978334; Training Loss 0.004701303850288294\n",
      "Episode 8982; Testing Loss 0.005820265241549942; Training Loss 0.0047012993610542715\n",
      "Episode 8983; Testing Loss 0.005820423013558083; Training Loss 0.00470128428875886\n",
      "Episode 8984; Testing Loss 0.005820583133961945; Training Loss 0.004701280268886522\n",
      "Episode 8985; Testing Loss 0.0058204413923578805; Training Loss 0.00470126912040534\n",
      "Episode 8986; Testing Loss 0.005820280668020896; Training Loss 0.004701261143542802\n",
      "Episode 8987; Testing Loss 0.005820403746374154; Training Loss 0.004701248784862843\n",
      "Episode 8988; Testing Loss 0.005820509691438108; Training Loss 0.004701247092518974\n",
      "Episode 8989; Testing Loss 0.005820308008096104; Training Loss 0.004701235684619684\n",
      "Episode 8990; Testing Loss 0.005820151603135294; Training Loss 0.004701226976713902\n",
      "Episode 8991; Testing Loss 0.005820394694201617; Training Loss 0.0047012142431791405\n",
      "Episode 8992; Testing Loss 0.005820627458826289; Training Loss 0.004701207387923427\n",
      "Episode 8993; Testing Loss 0.005820432600010308; Training Loss 0.004701195718337882\n",
      "Episode 8994; Testing Loss 0.005820170578786282; Training Loss 0.004701194557611713\n",
      "Episode 8995; Testing Loss 0.005820333187709102; Training Loss 0.004701177403206718\n",
      "Episode 8996; Testing Loss 0.005820574417716816; Training Loss 0.004701176856554148\n",
      "Episode 8997; Testing Loss 0.005820331124388076; Training Loss 0.004701164171716159\n",
      "Episode 8998; Testing Loss 0.005820019203774595; Training Loss 0.004701159005120422\n",
      "Episode 8999; Testing Loss 0.005820225789609891; Training Loss 0.004701141160530029\n",
      "Episode 9000; Testing Loss 0.005820489781311373; Training Loss 0.004701138298226583\n",
      "Episode 9001; Testing Loss 0.0058202848253532165; Training Loss 0.004701124760578331\n",
      "Episode 9002; Testing Loss 0.005820021590316031; Training Loss 0.004701118831961731\n",
      "Episode 9003; Testing Loss 0.005820235174654051; Training Loss 0.004701104756176053\n",
      "Episode 9004; Testing Loss 0.005820463540822516; Training Loss 0.004701098362795268\n",
      "Episode 9005; Testing Loss 0.005820306189667018; Training Loss 0.004701088837367666\n",
      "Episode 9006; Testing Loss 0.005820169125083347; Training Loss 0.004701083132580228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9007; Testing Loss 0.005820307960422023; Training Loss 0.004701067376187373\n",
      "Episode 9008; Testing Loss 0.005820319343825364; Training Loss 0.0047010611171469305\n",
      "Episode 9009; Testing Loss 0.005820133352860163; Training Loss 0.004701050172980611\n",
      "Episode 9010; Testing Loss 0.0058200729197494; Training Loss 0.004701040661239652\n",
      "Episode 9011; Testing Loss 0.005820210095830332; Training Loss 0.00470103200752556\n",
      "Episode 9012; Testing Loss 0.005820240490766638; Training Loss 0.0047010228733916385\n",
      "Episode 9013; Testing Loss 0.005820063395982905; Training Loss 0.0047010150351165176\n",
      "Episode 9014; Testing Loss 0.005820142695445906; Training Loss 0.004701005010886878\n",
      "Episode 9015; Testing Loss 0.0058202421417891765; Training Loss 0.004700994884291884\n",
      "Episode 9016; Testing Loss 0.0058201856590185804; Training Loss 0.004700986284160764\n",
      "Episode 9017; Testing Loss 0.005820130743933504; Training Loss 0.0047009765005934494\n",
      "Episode 9018; Testing Loss 0.005820016936423169; Training Loss 0.0047009684835458655\n",
      "Episode 9019; Testing Loss 0.005819995000556039; Training Loss 0.004700959311209447\n",
      "Episode 9020; Testing Loss 0.005820156749751946; Training Loss 0.004700951612542595\n",
      "Episode 9021; Testing Loss 0.005820204074949947; Training Loss 0.004700941052680634\n",
      "Episode 9022; Testing Loss 0.005820040445430891; Training Loss 0.00470093394468803\n",
      "Episode 9023; Testing Loss 0.005819903927364557; Training Loss 0.004700926198042106\n",
      "Episode 9024; Testing Loss 0.005820050515931543; Training Loss 0.004700912838477652\n",
      "Episode 9025; Testing Loss 0.005820207882614769; Training Loss 0.0047009070892274655\n",
      "Episode 9026; Testing Loss 0.005820136721137873; Training Loss 0.004700895768388114\n",
      "Episode 9027; Testing Loss 0.005819977034571984; Training Loss 0.004700888155594129\n",
      "Episode 9028; Testing Loss 0.00581993111243044; Training Loss 0.004700880282251742\n",
      "Episode 9029; Testing Loss 0.00582006629901328; Training Loss 0.004700868075812868\n",
      "Episode 9030; Testing Loss 0.005820129990803134; Training Loss 0.004700862226575432\n",
      "Episode 9031; Testing Loss 0.0058200046247859; Training Loss 0.004700852123455406\n",
      "Episode 9032; Testing Loss 0.0058199103811411405; Training Loss 0.004700840951088936\n",
      "Episode 9033; Testing Loss 0.0058199214314461865; Training Loss 0.004700832466300256\n",
      "Episode 9034; Testing Loss 0.005819967377917802; Training Loss 0.00470082219455433\n",
      "Episode 9035; Testing Loss 0.0058199861048205005; Training Loss 0.004700813175042021\n",
      "Episode 9036; Testing Loss 0.005819949541541236; Training Loss 0.004700804516429676\n",
      "Episode 9037; Testing Loss 0.00581985693101967; Training Loss 0.004700795892372311\n",
      "Episode 9038; Testing Loss 0.005819944559049361; Training Loss 0.004700786511378585\n",
      "Episode 9039; Testing Loss 0.005820061058386364; Training Loss 0.004700777791854892\n",
      "Episode 9040; Testing Loss 0.005819945212490993; Training Loss 0.004700768211550223\n",
      "Episode 9041; Testing Loss 0.005819841224629421; Training Loss 0.004700760403677168\n",
      "Episode 9042; Testing Loss 0.005819949168481793; Training Loss 0.004700750607536608\n",
      "Episode 9043; Testing Loss 0.005819888007536225; Training Loss 0.004700740974136372\n",
      "Episode 9044; Testing Loss 0.005819777201380912; Training Loss 0.004700733002869054\n",
      "Episode 9045; Testing Loss 0.005819843139417512; Training Loss 0.00470072248754932\n",
      "Episode 9046; Testing Loss 0.005819845024963877; Training Loss 0.004700713104127549\n",
      "Episode 9047; Testing Loss 0.0058198250183976626; Training Loss 0.004700705284467187\n",
      "Episode 9048; Testing Loss 0.00581989441069682; Training Loss 0.004700695140938784\n",
      "Episode 9049; Testing Loss 0.005819840303753794; Training Loss 0.004700687234148833\n",
      "Episode 9050; Testing Loss 0.005819904033844734; Training Loss 0.004700680049682306\n",
      "Episode 9051; Testing Loss 0.0058198540106059815; Training Loss 0.004700669338870651\n",
      "Episode 9052; Testing Loss 0.00581982567281484; Training Loss 0.004700658624843954\n",
      "Episode 9053; Testing Loss 0.00581977656723544; Training Loss 0.0047006534655305\n",
      "Episode 9054; Testing Loss 0.005819712240472337; Training Loss 0.004700644135953664\n",
      "Episode 9055; Testing Loss 0.005819845491719747; Training Loss 0.0047006327480357026\n",
      "Episode 9056; Testing Loss 0.005819938610011364; Training Loss 0.004700628154379173\n",
      "Episode 9057; Testing Loss 0.005819757811927799; Training Loss 0.004700617041271139\n",
      "Episode 9058; Testing Loss 0.005819748928101368; Training Loss 0.0047006048370289944\n",
      "Episode 9059; Testing Loss 0.005819717496463705; Training Loss 0.0047005966997294875\n",
      "Episode 9060; Testing Loss 0.005819689694033052; Training Loss 0.00470059009346205\n",
      "Episode 9061; Testing Loss 0.005819612755551703; Training Loss 0.004700578165764435\n",
      "Episode 9062; Testing Loss 0.005819617521229329; Training Loss 0.004700569865383864\n",
      "Episode 9063; Testing Loss 0.00581969245346564; Training Loss 0.004700561885938845\n",
      "Episode 9064; Testing Loss 0.005819796420586937; Training Loss 0.004700552118352419\n",
      "Episode 9065; Testing Loss 0.005819685694669072; Training Loss 0.0047005422705292\n",
      "Episode 9066; Testing Loss 0.0058195326684137355; Training Loss 0.004700536595627468\n",
      "Episode 9067; Testing Loss 0.005819714302911998; Training Loss 0.004700523854530402\n",
      "Episode 9068; Testing Loss 0.00581986691735985; Training Loss 0.004700519630657461\n",
      "Episode 9069; Testing Loss 0.005819707740238297; Training Loss 0.004700509598842206\n",
      "Episode 9070; Testing Loss 0.005819541989705217; Training Loss 0.004700502904209624\n",
      "Episode 9071; Testing Loss 0.005819700914717707; Training Loss 0.004700488603960473\n",
      "Episode 9072; Testing Loss 0.0058197902201754526; Training Loss 0.004700483845836582\n",
      "Episode 9073; Testing Loss 0.005819490889434209; Training Loss 0.004700470774016661\n",
      "Episode 9074; Testing Loss 0.005819307538282755; Training Loss 0.004700469897692869\n",
      "Episode 9075; Testing Loss 0.005819618798173855; Training Loss 0.004700453510840961\n",
      "Episode 9076; Testing Loss 0.0058198779228766494; Training Loss 0.004700451797118603\n",
      "Episode 9077; Testing Loss 0.005819601356746995; Training Loss 0.004700433580664689\n",
      "Episode 9078; Testing Loss 0.005819306411538886; Training Loss 0.004700435584860047\n",
      "Episode 9079; Testing Loss 0.005819563612470614; Training Loss 0.004700417747900883\n",
      "Episode 9080; Testing Loss 0.005819868181232458; Training Loss 0.0047004155644700145\n",
      "Episode 9081; Testing Loss 0.0058195823596241805; Training Loss 0.00470039841413589\n",
      "Episode 9082; Testing Loss 0.005819269766903085; Training Loss 0.004700394747154479\n",
      "Episode 9083; Testing Loss 0.005819400486882833; Training Loss 0.004700381025971475\n",
      "Episode 9084; Testing Loss 0.00581972887963627; Training Loss 0.0047003730597667236\n",
      "Episode 9085; Testing Loss 0.005819720097164483; Training Loss 0.004700364696478512\n",
      "Episode 9086; Testing Loss 0.005819488749986; Training Loss 0.004700354370609939\n",
      "Episode 9087; Testing Loss 0.005819423460825991; Training Loss 0.004700346276839535\n",
      "Episode 9088; Testing Loss 0.005819571323914791; Training Loss 0.00470033463824592\n",
      "Episode 9089; Testing Loss 0.005819599961635536; Training Loss 0.004700326417130823\n",
      "Episode 9090; Testing Loss 0.005819431923742297; Training Loss 0.0047003173640250894\n",
      "Episode 9091; Testing Loss 0.005819382076099956; Training Loss 0.004700307962614775\n",
      "Episode 9092; Testing Loss 0.005819380612473255; Training Loss 0.004700298580433017\n",
      "Episode 9093; Testing Loss 0.0058194179185170015; Training Loss 0.004700290282490052\n",
      "Episode 9094; Testing Loss 0.005819424587843084; Training Loss 0.004700280348144121\n",
      "Episode 9095; Testing Loss 0.00581934323989667; Training Loss 0.004700275752617263\n",
      "Episode 9096; Testing Loss 0.005819482389721926; Training Loss 0.004700265043517036\n",
      "Episode 9097; Testing Loss 0.005819519441704696; Training Loss 0.00470025337252889\n",
      "Episode 9098; Testing Loss 0.005819488728709672; Training Loss 0.0047002441154329055\n",
      "Episode 9099; Testing Loss 0.005819345697768986; Training Loss 0.004700240829379437\n",
      "Episode 9100; Testing Loss 0.005819404906221439; Training Loss 0.00470022869063957\n",
      "Episode 9101; Testing Loss 0.005819440559381585; Training Loss 0.004700218619328469\n",
      "Episode 9102; Testing Loss 0.005819378602441555; Training Loss 0.004700212305076785\n",
      "Episode 9103; Testing Loss 0.0058193800445051165; Training Loss 0.004700204077087775\n",
      "Episode 9104; Testing Loss 0.005819325619964261; Training Loss 0.004700192620620526\n",
      "Episode 9105; Testing Loss 0.005819347582455708; Training Loss 0.004700181674437899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9106; Testing Loss 0.005819295890780744; Training Loss 0.004700175253594559\n",
      "Episode 9107; Testing Loss 0.005819299440669344; Training Loss 0.004700162939794016\n",
      "Episode 9108; Testing Loss 0.005819299193188718; Training Loss 0.004700153732433216\n",
      "Episode 9109; Testing Loss 0.005819375965989719; Training Loss 0.004700147940804287\n",
      "Episode 9110; Testing Loss 0.005819314378941108; Training Loss 0.004700137984379584\n",
      "Episode 9111; Testing Loss 0.00581933015776317; Training Loss 0.00470012665549215\n",
      "Episode 9112; Testing Loss 0.005819255448828416; Training Loss 0.004700117773055488\n",
      "Episode 9113; Testing Loss 0.005819201833578213; Training Loss 0.004700113044080933\n",
      "Episode 9114; Testing Loss 0.005819348958476088; Training Loss 0.0047000999728290405\n",
      "Episode 9115; Testing Loss 0.005819372592855538; Training Loss 0.004700091374175218\n",
      "Episode 9116; Testing Loss 0.005819241409125809; Training Loss 0.00470008207848768\n",
      "Episode 9117; Testing Loss 0.005819226123068966; Training Loss 0.004700073258425754\n",
      "Episode 9118; Testing Loss 0.005819296076276975; Training Loss 0.004700065062384017\n",
      "Episode 9119; Testing Loss 0.0058193633218344394; Training Loss 0.0047000554257449435\n",
      "Episode 9120; Testing Loss 0.005819290263263547; Training Loss 0.0047000497565930855\n",
      "Episode 9121; Testing Loss 0.005819076557688732; Training Loss 0.004700040300127527\n",
      "Episode 9122; Testing Loss 0.005819041978949475; Training Loss 0.0047000305410255135\n",
      "Episode 9123; Testing Loss 0.005819295559407508; Training Loss 0.004700021511148726\n",
      "Episode 9124; Testing Loss 0.005819279289166934; Training Loss 0.0047000115310597\n",
      "Episode 9125; Testing Loss 0.005819077504696157; Training Loss 0.004700008022371163\n",
      "Episode 9126; Testing Loss 0.0058191890207761506; Training Loss 0.004699996486557796\n",
      "Episode 9127; Testing Loss 0.005819453055489618; Training Loss 0.004699990603713662\n",
      "Episode 9128; Testing Loss 0.005819272711489954; Training Loss 0.004699977506700988\n",
      "Episode 9129; Testing Loss 0.005818950536711536; Training Loss 0.0046999712030501885\n",
      "Episode 9130; Testing Loss 0.005819056091319017; Training Loss 0.00469996065151478\n",
      "Episode 9131; Testing Loss 0.005819286488333518; Training Loss 0.004699952451765689\n",
      "Episode 9132; Testing Loss 0.005819175979176637; Training Loss 0.004699940150018932\n",
      "Episode 9133; Testing Loss 0.005818932992495178; Training Loss 0.004699935941644918\n",
      "Episode 9134; Testing Loss 0.005819104616509626; Training Loss 0.004699922294706983\n",
      "Episode 9135; Testing Loss 0.005819324180839264; Training Loss 0.004699916950288852\n",
      "Episode 9136; Testing Loss 0.005819132564507008; Training Loss 0.004699903693638588\n",
      "Episode 9137; Testing Loss 0.005818914731143566; Training Loss 0.004699896895935982\n",
      "Episode 9138; Testing Loss 0.005819043812274079; Training Loss 0.004699885290093737\n",
      "Episode 9139; Testing Loss 0.0058191824093407835; Training Loss 0.004699878455136918\n",
      "Episode 9140; Testing Loss 0.0058190666168762345; Training Loss 0.004699868461910625\n",
      "Episode 9141; Testing Loss 0.005818947685413855; Training Loss 0.0046998588116680595\n",
      "Episode 9142; Testing Loss 0.005818946835092166; Training Loss 0.004699851913020486\n",
      "Episode 9143; Testing Loss 0.005819038693669752; Training Loss 0.0046998408412301075\n",
      "Episode 9144; Testing Loss 0.005819094288132341; Training Loss 0.0046998329385223095\n",
      "Episode 9145; Testing Loss 0.005819107486920082; Training Loss 0.00469982376375038\n",
      "Episode 9146; Testing Loss 0.005819127594940185; Training Loss 0.0046998135901917025\n",
      "Episode 9147; Testing Loss 0.005819035843330356; Training Loss 0.004699807637523796\n",
      "Episode 9148; Testing Loss 0.005818924184028241; Training Loss 0.0046997969819111375\n",
      "Episode 9149; Testing Loss 0.005818943245528731; Training Loss 0.004699787664485113\n",
      "Episode 9150; Testing Loss 0.005819132519844626; Training Loss 0.004699779010652792\n",
      "Episode 9151; Testing Loss 0.005819127268825859; Training Loss 0.0046997687015547344\n",
      "Episode 9152; Testing Loss 0.0058189023800157445; Training Loss 0.0046997627095439675\n",
      "Episode 9153; Testing Loss 0.005818821299989669; Training Loss 0.004699753398968622\n",
      "Episode 9154; Testing Loss 0.00581908524991455; Training Loss 0.0046997412722098895\n",
      "Episode 9155; Testing Loss 0.005819136871752441; Training Loss 0.004699736462382787\n",
      "Episode 9156; Testing Loss 0.005818966082102616; Training Loss 0.004699724763226379\n",
      "Episode 9157; Testing Loss 0.005818762640209936; Training Loss 0.00469971611148938\n",
      "Episode 9158; Testing Loss 0.0058189081578672456; Training Loss 0.004699707199508677\n",
      "Episode 9159; Testing Loss 0.005819009902655213; Training Loss 0.004699701063325456\n",
      "Episode 9160; Testing Loss 0.00581889376656612; Training Loss 0.004699690006597829\n",
      "Episode 9161; Testing Loss 0.005818859904544082; Training Loss 0.004699681566957388\n",
      "Episode 9162; Testing Loss 0.005818924424071067; Training Loss 0.0046996701485079164\n",
      "Episode 9163; Testing Loss 0.005818922243687741; Training Loss 0.00469966471185789\n",
      "Episode 9164; Testing Loss 0.005818795812407204; Training Loss 0.004699655572840137\n",
      "Episode 9165; Testing Loss 0.005818729230012936; Training Loss 0.0046996447133399845\n",
      "Episode 9166; Testing Loss 0.005818865481253177; Training Loss 0.004699636706582903\n",
      "Episode 9167; Testing Loss 0.005819053023711792; Training Loss 0.004699629714230787\n",
      "Episode 9168; Testing Loss 0.0058190057472780445; Training Loss 0.004699617020587206\n",
      "Episode 9169; Testing Loss 0.005818732873368333; Training Loss 0.004699607622024616\n",
      "Episode 9170; Testing Loss 0.00581866079679099; Training Loss 0.004699601310459851\n",
      "Episode 9171; Testing Loss 0.005818748275101095; Training Loss 0.004699590278707749\n",
      "Episode 9172; Testing Loss 0.005818844841974142; Training Loss 0.004699582175448389\n",
      "Episode 9173; Testing Loss 0.0058188047558360165; Training Loss 0.004699575075750875\n",
      "Episode 9174; Testing Loss 0.005818733606844708; Training Loss 0.004699564116466144\n",
      "Episode 9175; Testing Loss 0.005818686325088554; Training Loss 0.0046995547999139605\n",
      "Episode 9176; Testing Loss 0.005818679281630977; Training Loss 0.004699545353963587\n",
      "Episode 9177; Testing Loss 0.005818767068997399; Training Loss 0.004699537737454835\n",
      "Episode 9178; Testing Loss 0.005818821916056335; Training Loss 0.004699526083747581\n",
      "Episode 9179; Testing Loss 0.005818762068903126; Training Loss 0.0046995164861484305\n",
      "Episode 9180; Testing Loss 0.00581870141068592; Training Loss 0.004699508214425817\n",
      "Episode 9181; Testing Loss 0.005818721002622756; Training Loss 0.004699499112075038\n",
      "Episode 9182; Testing Loss 0.005818819080914723; Training Loss 0.0046994910194887605\n",
      "Episode 9183; Testing Loss 0.005818712868601649; Training Loss 0.0046994799170847696\n",
      "Episode 9184; Testing Loss 0.005818702713307157; Training Loss 0.0046994724779509615\n",
      "Episode 9185; Testing Loss 0.0058187381441010405; Training Loss 0.00469946186526354\n",
      "Episode 9186; Testing Loss 0.005818780319526345; Training Loss 0.004699454368758849\n",
      "Episode 9187; Testing Loss 0.005818659687726283; Training Loss 0.004699445886601772\n",
      "Episode 9188; Testing Loss 0.005818681115202505; Training Loss 0.004699435399979448\n",
      "Episode 9189; Testing Loss 0.005818692298728782; Training Loss 0.00469942820280362\n",
      "Episode 9190; Testing Loss 0.005818579096874823; Training Loss 0.004699417545900433\n",
      "Episode 9191; Testing Loss 0.005818649217847866; Training Loss 0.004699409814298459\n",
      "Episode 9192; Testing Loss 0.005818650795512752; Training Loss 0.004699400172967259\n",
      "Episode 9193; Testing Loss 0.005818643357272928; Training Loss 0.004699390455203024\n",
      "Episode 9194; Testing Loss 0.005818608522997238; Training Loss 0.004699381936397017\n",
      "Episode 9195; Testing Loss 0.005818588958289721; Training Loss 0.004699374085490513\n",
      "Episode 9196; Testing Loss 0.00581869237945487; Training Loss 0.004699364424941502\n",
      "Episode 9197; Testing Loss 0.005818639789771127; Training Loss 0.0046993552020344045\n",
      "Episode 9198; Testing Loss 0.0058184884981055994; Training Loss 0.004699348322105459\n",
      "Episode 9199; Testing Loss 0.005818607582794025; Training Loss 0.004699336327107723\n",
      "Episode 9200; Testing Loss 0.005818715401523946; Training Loss 0.00469932905115089\n",
      "Episode 9201; Testing Loss 0.0058185684405117856; Training Loss 0.004699319865676945\n",
      "Episode 9202; Testing Loss 0.005818539212264348; Training Loss 0.004699309472532077\n",
      "Episode 9203; Testing Loss 0.005818542986729204; Training Loss 0.0046993006295080395\n",
      "Episode 9204; Testing Loss 0.005818532614021803; Training Loss 0.004699292362614623\n",
      "Episode 9205; Testing Loss 0.0058185038821315874; Training Loss 0.00469928365196581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9206; Testing Loss 0.005818609897244125; Training Loss 0.0046992736365301085\n",
      "Episode 9207; Testing Loss 0.005818609911704178; Training Loss 0.004699265598171666\n",
      "Episode 9208; Testing Loss 0.005818501540631022; Training Loss 0.0046992579831172535\n",
      "Episode 9209; Testing Loss 0.005818577879632346; Training Loss 0.004699246437193006\n",
      "Episode 9210; Testing Loss 0.005818588490912778; Training Loss 0.004699241434266753\n",
      "Episode 9211; Testing Loss 0.005818411918570239; Training Loss 0.004699231705325363\n",
      "Episode 9212; Testing Loss 0.005818344224412771; Training Loss 0.004699220730407103\n",
      "Episode 9213; Testing Loss 0.005818553593698421; Training Loss 0.004699210793372088\n",
      "Episode 9214; Testing Loss 0.005818714626137556; Training Loss 0.004699203840942187\n",
      "Episode 9215; Testing Loss 0.005818475621642928; Training Loss 0.0046991940933029655\n",
      "Episode 9216; Testing Loss 0.005818400950509512; Training Loss 0.00469918454754262\n",
      "Episode 9217; Testing Loss 0.00581856318923845; Training Loss 0.004699177651057828\n",
      "Episode 9218; Testing Loss 0.00581854572976593; Training Loss 0.004699169186826371\n",
      "Episode 9219; Testing Loss 0.005818353528753175; Training Loss 0.00469915686082141\n",
      "Episode 9220; Testing Loss 0.005818321940919708; Training Loss 0.00469915457393167\n",
      "Episode 9221; Testing Loss 0.005818607860883702; Training Loss 0.004699144556722337\n",
      "Episode 9222; Testing Loss 0.005818742269901028; Training Loss 0.004699136258005978\n",
      "Episode 9223; Testing Loss 0.005818449402416229; Training Loss 0.004699120320639259\n",
      "Episode 9224; Testing Loss 0.005818261471593857; Training Loss 0.004699119945617724\n",
      "Episode 9225; Testing Loss 0.005818403820757961; Training Loss 0.004699105215812375\n",
      "Episode 9226; Testing Loss 0.005818556330322915; Training Loss 0.004699096028504778\n",
      "Episode 9227; Testing Loss 0.005818393817455514; Training Loss 0.004699089713198654\n",
      "Episode 9228; Testing Loss 0.0058183167122060255; Training Loss 0.004699086628764252\n",
      "Episode 9229; Testing Loss 0.005818484713799682; Training Loss 0.00469907454822423\n",
      "Episode 9230; Testing Loss 0.005818610601720145; Training Loss 0.004699058858909519\n",
      "Episode 9231; Testing Loss 0.0058185001140390845; Training Loss 0.0046990511058974185\n",
      "Episode 9232; Testing Loss 0.0058183303860269465; Training Loss 0.004699049215587741\n",
      "Episode 9233; Testing Loss 0.00581835821229748; Training Loss 0.004699033748237674\n",
      "Episode 9234; Testing Loss 0.005818502901403633; Training Loss 0.004699021663274281\n",
      "Episode 9235; Testing Loss 0.005818587857663791; Training Loss 0.004699018235940235\n",
      "Episode 9236; Testing Loss 0.005818386401513809; Training Loss 0.004699005957764128\n",
      "Episode 9237; Testing Loss 0.005818242435915405; Training Loss 0.004698994180501159\n",
      "Episode 9238; Testing Loss 0.0058182712570473815; Training Loss 0.004698992305324822\n",
      "Episode 9239; Testing Loss 0.005818251514364835; Training Loss 0.004698984207242302\n",
      "Episode 9240; Testing Loss 0.005818195846380279; Training Loss 0.004698968043581192\n",
      "Episode 9241; Testing Loss 0.005818225290597503; Training Loss 0.004698959663032132\n",
      "Episode 9242; Testing Loss 0.005818405121393052; Training Loss 0.0046989554776690895\n",
      "Episode 9243; Testing Loss 0.005818507647827189; Training Loss 0.0046989460629293265\n",
      "Episode 9244; Testing Loss 0.0058184079086573155; Training Loss 0.00469893283862276\n",
      "Episode 9245; Testing Loss 0.005818255269302014; Training Loss 0.004698924566110245\n",
      "Episode 9246; Testing Loss 0.005818306149857626; Training Loss 0.004698919635220365\n",
      "Episode 9247; Testing Loss 0.005818390557872937; Training Loss 0.004698910727319283\n",
      "Episode 9248; Testing Loss 0.0058182999573187025; Training Loss 0.0046988968674357015\n",
      "Episode 9249; Testing Loss 0.005818110873608257; Training Loss 0.004698886632804951\n",
      "Episode 9250; Testing Loss 0.005818149677386494; Training Loss 0.004698882077896761\n",
      "Episode 9251; Testing Loss 0.0058183271560042025; Training Loss 0.004698874598827427\n",
      "Episode 9252; Testing Loss 0.0058183048341739685; Training Loss 0.004698860854977273\n",
      "Episode 9253; Testing Loss 0.005818126523821416; Training Loss 0.004698850790469393\n",
      "Episode 9254; Testing Loss 0.005818089242970435; Training Loss 0.004698846435625435\n",
      "Episode 9255; Testing Loss 0.005818174608661506; Training Loss 0.004698837722890622\n",
      "Episode 9256; Testing Loss 0.005818231438725141; Training Loss 0.0046988248485263905\n",
      "Episode 9257; Testing Loss 0.005818204327210822; Training Loss 0.004698813593518765\n",
      "Episode 9258; Testing Loss 0.005818141171878851; Training Loss 0.004698810171214931\n",
      "Episode 9259; Testing Loss 0.005818214915648266; Training Loss 0.004698803706660211\n",
      "Episode 9260; Testing Loss 0.005818273993985299; Training Loss 0.0046987905696132684\n",
      "Episode 9261; Testing Loss 0.0058181843777390455; Training Loss 0.004698776283514444\n",
      "Episode 9262; Testing Loss 0.0058180091560135865; Training Loss 0.0046987710639850465\n",
      "Episode 9263; Testing Loss 0.005818004389975529; Training Loss 0.004698763363655062\n",
      "Episode 9264; Testing Loss 0.00581816012967433; Training Loss 0.004698752767101156\n",
      "Episode 9265; Testing Loss 0.005818184533239372; Training Loss 0.0046987396912837485\n",
      "Episode 9266; Testing Loss 0.005818087576966145; Training Loss 0.0046987369338593165\n",
      "Episode 9267; Testing Loss 0.005818115533048215; Training Loss 0.004698728306063908\n",
      "Episode 9268; Testing Loss 0.005818202788065763; Training Loss 0.004698714157369023\n",
      "Episode 9269; Testing Loss 0.005818160171541394; Training Loss 0.004698706271157425\n",
      "Episode 9270; Testing Loss 0.0058179386930193285; Training Loss 0.0046986998496549555\n",
      "Episode 9271; Testing Loss 0.005817794518144047; Training Loss 0.0046986933347670165\n",
      "Episode 9272; Testing Loss 0.005817878775897779; Training Loss 0.004698679613577504\n",
      "Episode 9273; Testing Loss 0.005818023592779517; Training Loss 0.004698668602152728\n",
      "Episode 9274; Testing Loss 0.005818026154856313; Training Loss 0.0046986622609747185\n",
      "Episode 9275; Testing Loss 0.005817939346030911; Training Loss 0.004698652022013118\n",
      "Episode 9276; Testing Loss 0.005817940363949303; Training Loss 0.004698641881485855\n",
      "Episode 9277; Testing Loss 0.005817941594519123; Training Loss 0.004698631395106261\n",
      "Episode 9278; Testing Loss 0.0058179649213830585; Training Loss 0.004698624823528418\n",
      "Episode 9279; Testing Loss 0.005817977594474148; Training Loss 0.004698617689349917\n",
      "Episode 9280; Testing Loss 0.005817935974150574; Training Loss 0.00469860762507242\n",
      "Episode 9281; Testing Loss 0.005817862356829363; Training Loss 0.004698596127452513\n",
      "Episode 9282; Testing Loss 0.005817901181697385; Training Loss 0.004698587165692298\n",
      "Episode 9283; Testing Loss 0.005817857418466075; Training Loss 0.004698578599845344\n",
      "Episode 9284; Testing Loss 0.005817784341487006; Training Loss 0.00469856723761089\n",
      "Episode 9285; Testing Loss 0.005817805314826198; Training Loss 0.004698559356690029\n",
      "Episode 9286; Testing Loss 0.005817976781379906; Training Loss 0.004698550685692361\n",
      "Episode 9287; Testing Loss 0.005817939457692453; Training Loss 0.004698540179764306\n",
      "Episode 9288; Testing Loss 0.005817724754654733; Training Loss 0.004698532964049819\n",
      "Episode 9289; Testing Loss 0.0058177479799787395; Training Loss 0.004698523140329469\n",
      "Episode 9290; Testing Loss 0.005817832923268794; Training Loss 0.004698514027758207\n",
      "Episode 9291; Testing Loss 0.005817784670572836; Training Loss 0.0046985027208676896\n",
      "Episode 9292; Testing Loss 0.00581771286616052; Training Loss 0.004698495742672525\n",
      "Episode 9293; Testing Loss 0.005817825408529831; Training Loss 0.004698484770799823\n",
      "Episode 9294; Testing Loss 0.005817773043430894; Training Loss 0.004698476150482508\n",
      "Episode 9295; Testing Loss 0.005817741838143398; Training Loss 0.004698467721469044\n",
      "Episode 9296; Testing Loss 0.005817663669031375; Training Loss 0.004698457942229901\n",
      "Episode 9297; Testing Loss 0.00581773356581119; Training Loss 0.0046984520722222565\n",
      "Episode 9298; Testing Loss 0.005817717337092841; Training Loss 0.004698440847811914\n",
      "Episode 9299; Testing Loss 0.005817630534840374; Training Loss 0.0046984356751285684\n",
      "Episode 9300; Testing Loss 0.005817722523600432; Training Loss 0.004698428023981572\n",
      "Episode 9301; Testing Loss 0.0058177777168712305; Training Loss 0.004698414479825398\n",
      "Episode 9302; Testing Loss 0.005817664678374955; Training Loss 0.004698408219953914\n",
      "Episode 9303; Testing Loss 0.005817588600548931; Training Loss 0.004698402387261264\n",
      "Episode 9304; Testing Loss 0.005817628677289248; Training Loss 0.004698388881932413\n",
      "Episode 9305; Testing Loss 0.005817655908834917; Training Loss 0.004698378502561941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9306; Testing Loss 0.005817677861046243; Training Loss 0.004698371751987211\n",
      "Episode 9307; Testing Loss 0.005817602356218468; Training Loss 0.004698358954399852\n",
      "Episode 9308; Testing Loss 0.005817590611591143; Training Loss 0.004698352598842505\n",
      "Episode 9309; Testing Loss 0.005817562665708081; Training Loss 0.004698345168886161\n",
      "Episode 9310; Testing Loss 0.005817559422027974; Training Loss 0.004698334528218407\n",
      "Episode 9311; Testing Loss 0.005817658377638418; Training Loss 0.004698323390175437\n",
      "Episode 9312; Testing Loss 0.00581772656706099; Training Loss 0.0046983155831993835\n",
      "Episode 9313; Testing Loss 0.005817648888469248; Training Loss 0.004698307496640048\n",
      "Episode 9314; Testing Loss 0.00581745678697323; Training Loss 0.004698296380240458\n",
      "Episode 9315; Testing Loss 0.005817418860293603; Training Loss 0.004698287420864201\n",
      "Episode 9316; Testing Loss 0.005817581135490791; Training Loss 0.00469827866935623\n",
      "Episode 9317; Testing Loss 0.005817571579238518; Training Loss 0.004698269495145973\n",
      "Episode 9318; Testing Loss 0.005817401456309097; Training Loss 0.004698261686145634\n",
      "Episode 9319; Testing Loss 0.005817437927359265; Training Loss 0.004698250858069968\n",
      "Episode 9320; Testing Loss 0.005817530450302709; Training Loss 0.004698243590262628\n",
      "Episode 9321; Testing Loss 0.005817447536846488; Training Loss 0.00469823204282103\n",
      "Episode 9322; Testing Loss 0.0058174057859664355; Training Loss 0.004698224398192774\n",
      "Episode 9323; Testing Loss 0.005817583438786007; Training Loss 0.004698213965547735\n",
      "Episode 9324; Testing Loss 0.005817637643633976; Training Loss 0.0046982060252693586\n",
      "Episode 9325; Testing Loss 0.005817519512676818; Training Loss 0.004698196706619451\n",
      "Episode 9326; Testing Loss 0.0058173780449433206; Training Loss 0.004698187792361264\n",
      "Episode 9327; Testing Loss 0.005817364733303803; Training Loss 0.004698177110602911\n",
      "Episode 9328; Testing Loss 0.005817444879819737; Training Loss 0.004698172826622751\n",
      "Episode 9329; Testing Loss 0.005817515154302665; Training Loss 0.004698163584614377\n",
      "Episode 9330; Testing Loss 0.005817440917565821; Training Loss 0.004698150408669824\n",
      "Episode 9331; Testing Loss 0.005817299763817854; Training Loss 0.004698144932415481\n",
      "Episode 9332; Testing Loss 0.005817292631744955; Training Loss 0.004698135618214477\n",
      "Episode 9333; Testing Loss 0.005817406788021429; Training Loss 0.004698125038339491\n",
      "Episode 9334; Testing Loss 0.005817521275832361; Training Loss 0.004698117541514692\n",
      "Episode 9335; Testing Loss 0.005817438957994364; Training Loss 0.004698106938545071\n",
      "Episode 9336; Testing Loss 0.00581732011887532; Training Loss 0.004698101457165974\n",
      "Episode 9337; Testing Loss 0.005817291693859686; Training Loss 0.004698093402999206\n",
      "Episode 9338; Testing Loss 0.005817298684050742; Training Loss 0.004698082742214895\n",
      "Episode 9339; Testing Loss 0.005817269303263166; Training Loss 0.004698070562154072\n",
      "Episode 9340; Testing Loss 0.00581732360167622; Training Loss 0.004698062378345489\n",
      "Episode 9341; Testing Loss 0.005817368095314706; Training Loss 0.004698052253083128\n",
      "Episode 9342; Testing Loss 0.005817208811506378; Training Loss 0.00469804307301708\n",
      "Episode 9343; Testing Loss 0.00581723653566414; Training Loss 0.0046980334839270745\n",
      "Episode 9344; Testing Loss 0.005817339379892022; Training Loss 0.004698027775246503\n",
      "Episode 9345; Testing Loss 0.005817216378891954; Training Loss 0.004698016114680601\n",
      "Episode 9346; Testing Loss 0.005817069506021071; Training Loss 0.00469800873929751\n",
      "Episode 9347; Testing Loss 0.0058171645916925115; Training Loss 0.00469799809725358\n",
      "Episode 9348; Testing Loss 0.005817218042550392; Training Loss 0.004697989296214079\n",
      "Episode 9349; Testing Loss 0.005817165016381858; Training Loss 0.004697981014450429\n",
      "Episode 9350; Testing Loss 0.005817172182683345; Training Loss 0.004697970286472524\n",
      "Episode 9351; Testing Loss 0.005817184632977356; Training Loss 0.00469796295118021\n",
      "Episode 9352; Testing Loss 0.005817147821387293; Training Loss 0.004697952387581384\n",
      "Episode 9353; Testing Loss 0.005817196055188118; Training Loss 0.004697944018034789\n",
      "Episode 9354; Testing Loss 0.005817155646733907; Training Loss 0.004697936076660115\n",
      "Episode 9355; Testing Loss 0.005817189699312117; Training Loss 0.004697927887452182\n",
      "Episode 9356; Testing Loss 0.005817141426262179; Training Loss 0.004697918609925032\n",
      "Episode 9357; Testing Loss 0.005817157848259213; Training Loss 0.004697907759384012\n",
      "Episode 9358; Testing Loss 0.005817099831096704; Training Loss 0.004697899405639234\n",
      "Episode 9359; Testing Loss 0.005817142116961148; Training Loss 0.004697889646444433\n",
      "Episode 9360; Testing Loss 0.005817184261792921; Training Loss 0.0046978820227182045\n",
      "Episode 9361; Testing Loss 0.0058172304745501115; Training Loss 0.0046978733065683115\n",
      "Episode 9362; Testing Loss 0.005817034948988138; Training Loss 0.004697862874854498\n",
      "Episode 9363; Testing Loss 0.005816981815888213; Training Loss 0.004697854936979766\n",
      "Episode 9364; Testing Loss 0.005817148865296507; Training Loss 0.004697847899417234\n",
      "Episode 9365; Testing Loss 0.005817091967370725; Training Loss 0.004697836162930414\n",
      "Episode 9366; Testing Loss 0.00581698901063016; Training Loss 0.004697832242963723\n",
      "Episode 9367; Testing Loss 0.005817151868135755; Training Loss 0.004697821609551145\n",
      "Episode 9368; Testing Loss 0.005817211474097565; Training Loss 0.004697810729684579\n",
      "Episode 9369; Testing Loss 0.0058170132820187575; Training Loss 0.004697802230290024\n",
      "Episode 9370; Testing Loss 0.005816863393093117; Training Loss 0.004697793678231776\n",
      "Episode 9371; Testing Loss 0.0058169996904758985; Training Loss 0.004697782586273485\n",
      "Episode 9372; Testing Loss 0.005817174236806109; Training Loss 0.004697776036035182\n",
      "Episode 9373; Testing Loss 0.005817076975278138; Training Loss 0.0046977645969375585\n",
      "Episode 9374; Testing Loss 0.005816843537449978; Training Loss 0.004697757818666594\n",
      "Episode 9375; Testing Loss 0.005816870093722748; Training Loss 0.004697748107896305\n",
      "Episode 9376; Testing Loss 0.005817084020646741; Training Loss 0.00469773766495108\n",
      "Episode 9377; Testing Loss 0.005817113186577471; Training Loss 0.004697730059751514\n",
      "Episode 9378; Testing Loss 0.005816996949045439; Training Loss 0.0046977221793835355\n",
      "Episode 9379; Testing Loss 0.005816900342909908; Training Loss 0.004697711225476555\n",
      "Episode 9380; Testing Loss 0.005816927214309965; Training Loss 0.004697705939902717\n",
      "Episode 9381; Testing Loss 0.0058170515352984255; Training Loss 0.004697696994766027\n",
      "Episode 9382; Testing Loss 0.00581699471380567; Training Loss 0.004697684806123592\n",
      "Episode 9383; Testing Loss 0.0058168266249127515; Training Loss 0.004697681384482183\n",
      "Episode 9384; Testing Loss 0.0058167071875722615; Training Loss 0.004697673964223677\n",
      "Episode 9385; Testing Loss 0.005816766224040486; Training Loss 0.00469766123055289\n",
      "Episode 9386; Testing Loss 0.005816907387560132; Training Loss 0.004697650609939495\n",
      "Episode 9387; Testing Loss 0.005816946097633778; Training Loss 0.004697645533906294\n",
      "Episode 9388; Testing Loss 0.005816969311943942; Training Loss 0.00469763708946262\n",
      "Episode 9389; Testing Loss 0.005816983166389768; Training Loss 0.004697622552522157\n",
      "Episode 9390; Testing Loss 0.005816910805335825; Training Loss 0.004697618375055648\n",
      "Episode 9391; Testing Loss 0.005816772328831698; Training Loss 0.004697615013608082\n",
      "Episode 9392; Testing Loss 0.005816766251942661; Training Loss 0.004697601463594061\n",
      "Episode 9393; Testing Loss 0.0058168890831138175; Training Loss 0.004697586455604646\n",
      "Episode 9394; Testing Loss 0.0058169391172496355; Training Loss 0.0046975831120912356\n",
      "Episode 9395; Testing Loss 0.005816968461825122; Training Loss 0.004697578966657893\n",
      "Episode 9396; Testing Loss 0.005816934980375651; Training Loss 0.00469756731519494\n",
      "Episode 9397; Testing Loss 0.005816884231248434; Training Loss 0.00469755307810636\n",
      "Episode 9398; Testing Loss 0.005816845422030624; Training Loss 0.00469754764847422\n",
      "Episode 9399; Testing Loss 0.005816747368663801; Training Loss 0.004697540578413232\n",
      "Episode 9400; Testing Loss 0.005816692611413695; Training Loss 0.0046975304370859925\n",
      "Episode 9401; Testing Loss 0.005816697619041333; Training Loss 0.0046975182657422905\n",
      "Episode 9402; Testing Loss 0.005816802439755923; Training Loss 0.004697508967800066\n",
      "Episode 9403; Testing Loss 0.005816839207417108; Training Loss 0.004697500254074268\n",
      "Episode 9404; Testing Loss 0.005816720111516887; Training Loss 0.0046974890605437835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9405; Testing Loss 0.005816600790938762; Training Loss 0.004697482718193629\n",
      "Episode 9406; Testing Loss 0.005816608398203421; Training Loss 0.004697471971920097\n",
      "Episode 9407; Testing Loss 0.0058167871466324106; Training Loss 0.004697462406818672\n",
      "Episode 9408; Testing Loss 0.005816890048669093; Training Loss 0.004697455823337369\n",
      "Episode 9409; Testing Loss 0.005816819842697845; Training Loss 0.004697444877589821\n",
      "Episode 9410; Testing Loss 0.005816674785331448; Training Loss 0.004697435052750762\n",
      "Episode 9411; Testing Loss 0.005816685974633545; Training Loss 0.004697428483653348\n",
      "Episode 9412; Testing Loss 0.005816779124069529; Training Loss 0.004697418374743437\n",
      "Episode 9413; Testing Loss 0.005816730296201691; Training Loss 0.004697407684375358\n",
      "Episode 9414; Testing Loss 0.005816640377629158; Training Loss 0.004697402560947656\n",
      "Episode 9415; Testing Loss 0.005816707097671893; Training Loss 0.004697394879159396\n",
      "Episode 9416; Testing Loss 0.005816711391062381; Training Loss 0.004697385694219875\n",
      "Episode 9417; Testing Loss 0.005816721402084631; Training Loss 0.004697372401364318\n",
      "Episode 9418; Testing Loss 0.005816691882984676; Training Loss 0.004697365512946421\n",
      "Episode 9419; Testing Loss 0.005816612226283283; Training Loss 0.004697359928180163\n",
      "Episode 9420; Testing Loss 0.005816567176314555; Training Loss 0.0046973471314169365\n",
      "Episode 9421; Testing Loss 0.005816640914917084; Training Loss 0.004697336840703016\n",
      "Episode 9422; Testing Loss 0.005816751122706675; Training Loss 0.0046973332463687845\n",
      "Episode 9423; Testing Loss 0.005816583568163887; Training Loss 0.0046973215433911975\n",
      "Episode 9424; Testing Loss 0.005816489872662412; Training Loss 0.004697311580069559\n",
      "Episode 9425; Testing Loss 0.00581668543988016; Training Loss 0.004697299834126704\n",
      "Episode 9426; Testing Loss 0.005816777285974192; Training Loss 0.004697297564296865\n",
      "Episode 9427; Testing Loss 0.005816591033908727; Training Loss 0.004697285987722467\n",
      "Episode 9428; Testing Loss 0.005816523734060482; Training Loss 0.004697275466762875\n",
      "Episode 9429; Testing Loss 0.00581669776840672; Training Loss 0.004697271584596397\n",
      "Episode 9430; Testing Loss 0.0058166458655063225; Training Loss 0.004697264048348085\n",
      "Episode 9431; Testing Loss 0.005816437225800774; Training Loss 0.004697251542696051\n",
      "Episode 9432; Testing Loss 0.0058165270696736915; Training Loss 0.00469723927321579\n",
      "Episode 9433; Testing Loss 0.005816693181207233; Training Loss 0.004697230665407369\n",
      "Episode 9434; Testing Loss 0.005816639758743427; Training Loss 0.004697221242194105\n",
      "Episode 9435; Testing Loss 0.005816434409817235; Training Loss 0.004697211726926433\n",
      "Episode 9436; Testing Loss 0.005816411254725339; Training Loss 0.00469720346982681\n",
      "Episode 9437; Testing Loss 0.005816516134541167; Training Loss 0.004697194971005\n",
      "Episode 9438; Testing Loss 0.005816637403109009; Training Loss 0.004697186377361056\n",
      "Episode 9439; Testing Loss 0.0058164925011521314; Training Loss 0.0046971757847673904\n",
      "Episode 9440; Testing Loss 0.005816335415101564; Training Loss 0.00469717059186216\n",
      "Episode 9441; Testing Loss 0.005816489763250371; Training Loss 0.004697156992473189\n",
      "Episode 9442; Testing Loss 0.0058165679486188865; Training Loss 0.004697149728610619\n",
      "Episode 9443; Testing Loss 0.0058165443943948465; Training Loss 0.004697139883639112\n",
      "Episode 9444; Testing Loss 0.0058163836066423774; Training Loss 0.004697131999285974\n",
      "Episode 9445; Testing Loss 0.005816386662309747; Training Loss 0.004697121406347894\n",
      "Episode 9446; Testing Loss 0.005816483668695506; Training Loss 0.0046971173374358455\n",
      "Episode 9447; Testing Loss 0.00581638164869114; Training Loss 0.004697107297924985\n",
      "Episode 9448; Testing Loss 0.005816234501276076; Training Loss 0.004697096725499994\n",
      "Episode 9449; Testing Loss 0.0058163738196629795; Training Loss 0.004697087413616406\n",
      "Episode 9450; Testing Loss 0.005816601568494258; Training Loss 0.004697079587141123\n",
      "Episode 9451; Testing Loss 0.005816446445698831; Training Loss 0.004697068310708055\n",
      "Episode 9452; Testing Loss 0.005816331184477704; Training Loss 0.004697059589119424\n",
      "Episode 9453; Testing Loss 0.005816421569078209; Training Loss 0.004697053160250991\n",
      "Episode 9454; Testing Loss 0.005816450500867226; Training Loss 0.0046970430436700916\n",
      "Episode 9455; Testing Loss 0.005816383661759325; Training Loss 0.004697036960709436\n",
      "Episode 9456; Testing Loss 0.005816448161440702; Training Loss 0.0046970282762882195\n",
      "Episode 9457; Testing Loss 0.005816521613449986; Training Loss 0.004697015805910561\n",
      "Episode 9458; Testing Loss 0.0058164155258663254; Training Loss 0.0046970093903630755\n",
      "Episode 9459; Testing Loss 0.005816230685087375; Training Loss 0.0046970035627081986\n",
      "Episode 9460; Testing Loss 0.005816193878847122; Training Loss 0.00469698929008774\n",
      "Episode 9461; Testing Loss 0.005816344781609976; Training Loss 0.004696983121872855\n",
      "Episode 9462; Testing Loss 0.005816422792277888; Training Loss 0.004696976620683187\n",
      "Episode 9463; Testing Loss 0.005816268239487712; Training Loss 0.004696963040712129\n",
      "Episode 9464; Testing Loss 0.0058161104993760355; Training Loss 0.004696956284003034\n",
      "Episode 9465; Testing Loss 0.005816205822349626; Training Loss 0.004696946771979902\n",
      "Episode 9466; Testing Loss 0.005816349239456624; Training Loss 0.004696936203167044\n",
      "Episode 9467; Testing Loss 0.005816365748834643; Training Loss 0.00469692806953453\n",
      "Episode 9468; Testing Loss 0.005816244926208185; Training Loss 0.004696919542572311\n",
      "Episode 9469; Testing Loss 0.005816311590019199; Training Loss 0.0046969078479708075\n",
      "Episode 9470; Testing Loss 0.005816423537628424; Training Loss 0.004696900491470479\n",
      "Episode 9471; Testing Loss 0.005816339909227814; Training Loss 0.004696891211278188\n",
      "Episode 9472; Testing Loss 0.005816266861180792; Training Loss 0.004696880778162618\n",
      "Episode 9473; Testing Loss 0.005816286290547655; Training Loss 0.004696872067499077\n",
      "Episode 9474; Testing Loss 0.00581625702640055; Training Loss 0.004696863636919424\n",
      "Episode 9475; Testing Loss 0.005816195843907721; Training Loss 0.004696855066736489\n",
      "Episode 9476; Testing Loss 0.0058160939800699345; Training Loss 0.004696845537203772\n",
      "Episode 9477; Testing Loss 0.0058161313304943354; Training Loss 0.0046968362397225615\n",
      "Episode 9478; Testing Loss 0.005816233614157998; Training Loss 0.004696829957312356\n",
      "Episode 9479; Testing Loss 0.005816102924710377; Training Loss 0.004696818575951389\n",
      "Episode 9480; Testing Loss 0.005816072111828866; Training Loss 0.004696811476770763\n",
      "Episode 9481; Testing Loss 0.0058162445613123386; Training Loss 0.004696802887250073\n",
      "Episode 9482; Testing Loss 0.005816229447255012; Training Loss 0.004696793479109878\n",
      "Episode 9483; Testing Loss 0.005816104542079064; Training Loss 0.00469678668234692\n",
      "Episode 9484; Testing Loss 0.005816223852394745; Training Loss 0.004696778117034757\n",
      "Episode 9485; Testing Loss 0.005816267405660674; Training Loss 0.004696767035392089\n",
      "Episode 9486; Testing Loss 0.005816059332778951; Training Loss 0.004696756046201377\n",
      "Episode 9487; Testing Loss 0.005815932024705452; Training Loss 0.004696754181265387\n",
      "Episode 9488; Testing Loss 0.00581617207072532; Training Loss 0.004696741977900516\n",
      "Episode 9489; Testing Loss 0.005816336400638948; Training Loss 0.004696733994694734\n",
      "Episode 9490; Testing Loss 0.005816060336509746; Training Loss 0.0046967204387073\n",
      "Episode 9491; Testing Loss 0.005815932362501746; Training Loss 0.00469671496822595\n",
      "Episode 9492; Testing Loss 0.005816091073573558; Training Loss 0.004696703798907272\n",
      "Episode 9493; Testing Loss 0.005816260076869618; Training Loss 0.004696696443330138\n",
      "Episode 9494; Testing Loss 0.005816280178125215; Training Loss 0.004696689102174896\n",
      "Episode 9495; Testing Loss 0.005816130031707663; Training Loss 0.004696676487785465\n",
      "Episode 9496; Testing Loss 0.005815970383629183; Training Loss 0.004696669950392369\n",
      "Episode 9497; Testing Loss 0.00581595042600838; Training Loss 0.004696660150327431\n",
      "Episode 9498; Testing Loss 0.005816014455399584; Training Loss 0.004696651453442239\n",
      "Episode 9499; Testing Loss 0.005816064489382533; Training Loss 0.004696643220825804\n",
      "Episode 9500; Testing Loss 0.005816081465844784; Training Loss 0.004696632714500251\n",
      "Episode 9501; Testing Loss 0.005816043167228491; Training Loss 0.004696623808878788\n",
      "Episode 9502; Testing Loss 0.00581598778969207; Training Loss 0.00469661571078139\n",
      "Episode 9503; Testing Loss 0.005816039259849161; Training Loss 0.004696605207127151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9504; Testing Loss 0.005816183110858729; Training Loss 0.0046965959677675905\n",
      "Episode 9505; Testing Loss 0.005816153946828212; Training Loss 0.004696587741057474\n",
      "Episode 9506; Testing Loss 0.005816072515549534; Training Loss 0.00469657867559404\n",
      "Episode 9507; Testing Loss 0.005816051289132168; Training Loss 0.004696569867239244\n",
      "Episode 9508; Testing Loss 0.005816031447196744; Training Loss 0.0046965595823886454\n",
      "Episode 9509; Testing Loss 0.005815984716073276; Training Loss 0.004696551536266477\n",
      "Episode 9510; Testing Loss 0.005815942138499075; Training Loss 0.004696545597042749\n",
      "Episode 9511; Testing Loss 0.0058160671536301375; Training Loss 0.004696534934172785\n",
      "Episode 9512; Testing Loss 0.005816024005411972; Training Loss 0.00469652646283848\n",
      "Episode 9513; Testing Loss 0.005815870878958695; Training Loss 0.004696519120113143\n",
      "Episode 9514; Testing Loss 0.005815910873346367; Training Loss 0.00469650754827075\n",
      "Episode 9515; Testing Loss 0.005816071514228726; Training Loss 0.004696501309151053\n",
      "Episode 9516; Testing Loss 0.005815977543490223; Training Loss 0.004696490614973396\n",
      "Episode 9517; Testing Loss 0.005815810137368653; Training Loss 0.0046964823664472554\n",
      "Episode 9518; Testing Loss 0.005815939374410504; Training Loss 0.004696471070712588\n",
      "Episode 9519; Testing Loss 0.005816024336461539; Training Loss 0.004696463849293915\n",
      "Episode 9520; Testing Loss 0.005815874538973715; Training Loss 0.004696453184685967\n",
      "Episode 9521; Testing Loss 0.0058158003295218155; Training Loss 0.004696444335102647\n",
      "Episode 9522; Testing Loss 0.005815845979538953; Training Loss 0.004696434746344571\n",
      "Episode 9523; Testing Loss 0.005815987812270648; Training Loss 0.004696428043879463\n",
      "Episode 9524; Testing Loss 0.005815921051443779; Training Loss 0.004696417095464818\n",
      "Episode 9525; Testing Loss 0.005815851779752574; Training Loss 0.00469640763815192\n",
      "Episode 9526; Testing Loss 0.005815827987589684; Training Loss 0.004696399418339612\n",
      "Episode 9527; Testing Loss 0.005815807516391127; Training Loss 0.004696391850888919\n",
      "Episode 9528; Testing Loss 0.00581591266012559; Training Loss 0.004696381504333492\n",
      "Episode 9529; Testing Loss 0.005815938476671322; Training Loss 0.004696374320979861\n",
      "Episode 9530; Testing Loss 0.005815813401809321; Training Loss 0.004696364125582712\n",
      "Episode 9531; Testing Loss 0.005815858401507683; Training Loss 0.004696354937758852\n",
      "Episode 9532; Testing Loss 0.0058159767108993165; Training Loss 0.00469634756220768\n",
      "Episode 9533; Testing Loss 0.005815905441632999; Training Loss 0.004696337033069153\n",
      "Episode 9534; Testing Loss 0.005815771428277152; Training Loss 0.004696330616481943\n",
      "Episode 9535; Testing Loss 0.005815846516044609; Training Loss 0.00469631996457945\n",
      "Episode 9536; Testing Loss 0.005815850551617604; Training Loss 0.0046963126135077075\n",
      "Episode 9537; Testing Loss 0.005815677786941481; Training Loss 0.004696301386909772\n",
      "Episode 9538; Testing Loss 0.00581570119391667; Training Loss 0.004696294010355449\n",
      "Episode 9539; Testing Loss 0.00581586504176931; Training Loss 0.00469628531332389\n",
      "Episode 9540; Testing Loss 0.0058158141736680425; Training Loss 0.004696274623527249\n",
      "Episode 9541; Testing Loss 0.005815673508604544; Training Loss 0.0046962706214856145\n",
      "Episode 9542; Testing Loss 0.00581576055284432; Training Loss 0.004696260384888835\n",
      "Episode 9543; Testing Loss 0.00581589875733963; Training Loss 0.0046962496261107505\n",
      "Episode 9544; Testing Loss 0.005815752147294575; Training Loss 0.0046962417291093895\n",
      "Episode 9545; Testing Loss 0.005815575567780183; Training Loss 0.004696233317727365\n",
      "Episode 9546; Testing Loss 0.005815674252864174; Training Loss 0.004696224705482358\n",
      "Episode 9547; Testing Loss 0.005815951630887847; Training Loss 0.004696216553202997\n",
      "Episode 9548; Testing Loss 0.0058159462997169715; Training Loss 0.004696207611753383\n",
      "Episode 9549; Testing Loss 0.005815657337054329; Training Loss 0.004696202038431564\n",
      "Episode 9550; Testing Loss 0.005815533023005889; Training Loss 0.004696193625029903\n",
      "Episode 9551; Testing Loss 0.005815686828200817; Training Loss 0.004696183732944303\n",
      "Episode 9552; Testing Loss 0.0058157186886094074; Training Loss 0.004696171692817961\n",
      "Episode 9553; Testing Loss 0.0058156593817800745; Training Loss 0.004696163654661723\n",
      "Episode 9554; Testing Loss 0.005815692198820769; Training Loss 0.004696154986335355\n",
      "Episode 9555; Testing Loss 0.005815799535066762; Training Loss 0.004696146141791066\n",
      "Episode 9556; Testing Loss 0.005815724485016459; Training Loss 0.004696135689102533\n",
      "Episode 9557; Testing Loss 0.005815597477877004; Training Loss 0.004696127911107282\n",
      "Episode 9558; Testing Loss 0.005815702018138825; Training Loss 0.004696118024987991\n",
      "Episode 9559; Testing Loss 0.005815775110821685; Training Loss 0.004696106750286296\n",
      "Episode 9560; Testing Loss 0.005815721533135619; Training Loss 0.004696097859303057\n",
      "Episode 9561; Testing Loss 0.0058156725353138514; Training Loss 0.004696089995222988\n",
      "Episode 9562; Testing Loss 0.005815599021617591; Training Loss 0.004696080434296424\n",
      "Episode 9563; Testing Loss 0.005815625941023051; Training Loss 0.004696071580399336\n",
      "Episode 9564; Testing Loss 0.005815643552404816; Training Loss 0.004696063429783119\n",
      "Episode 9565; Testing Loss 0.005815622021582319; Training Loss 0.004696052704909779\n",
      "Episode 9566; Testing Loss 0.005815650232488038; Training Loss 0.004696045070772116\n",
      "Episode 9567; Testing Loss 0.005815808126140422; Training Loss 0.0046960364009387395\n",
      "Episode 9568; Testing Loss 0.005815711209014379; Training Loss 0.004696027894434663\n",
      "Episode 9569; Testing Loss 0.005815623112142509; Training Loss 0.004696018616243431\n",
      "Episode 9570; Testing Loss 0.005815634973437554; Training Loss 0.004696011972559624\n",
      "Episode 9571; Testing Loss 0.005815596730431938; Training Loss 0.004696002932124965\n",
      "Episode 9572; Testing Loss 0.005815523934277511; Training Loss 0.004695991396077178\n",
      "Episode 9573; Testing Loss 0.0058155833734620495; Training Loss 0.004695984882855985\n",
      "Episode 9574; Testing Loss 0.005815680535035613; Training Loss 0.00469597464034007\n",
      "Episode 9575; Testing Loss 0.005815614450674037; Training Loss 0.0046959677836502485\n",
      "Episode 9576; Testing Loss 0.005815426759069545; Training Loss 0.004695960543600033\n",
      "Episode 9577; Testing Loss 0.005815408097837099; Training Loss 0.004695947671590544\n",
      "Episode 9578; Testing Loss 0.005815567819981326; Training Loss 0.004695940893820055\n",
      "Episode 9579; Testing Loss 0.005815648796463826; Training Loss 0.004695933864244257\n",
      "Episode 9580; Testing Loss 0.005815467589133893; Training Loss 0.004695921192782139\n",
      "Episode 9581; Testing Loss 0.0058153792306187314; Training Loss 0.004695917305435415\n",
      "Episode 9582; Testing Loss 0.0058154467239975105; Training Loss 0.004695909478843564\n",
      "Episode 9583; Testing Loss 0.005815548151407051; Training Loss 0.004695895713752608\n",
      "Episode 9584; Testing Loss 0.005815492644699912; Training Loss 0.004695887890232539\n",
      "Episode 9585; Testing Loss 0.005815519721642973; Training Loss 0.004695885625246529\n",
      "Episode 9586; Testing Loss 0.0058155260115271105; Training Loss 0.004695873233149457\n",
      "Episode 9587; Testing Loss 0.005815476707085999; Training Loss 0.004695859260096\n",
      "Episode 9588; Testing Loss 0.005815439914766201; Training Loss 0.00469585212244649\n",
      "Episode 9589; Testing Loss 0.005815456095935463; Training Loss 0.004695842737187241\n",
      "Episode 9590; Testing Loss 0.005815567000807874; Training Loss 0.0046958328824901335\n",
      "Episode 9591; Testing Loss 0.005815531243910294; Training Loss 0.004695824051624288\n",
      "Episode 9592; Testing Loss 0.005815486819381157; Training Loss 0.004695815397919446\n",
      "Episode 9593; Testing Loss 0.005815464302715286; Training Loss 0.0046958058007228335\n",
      "Episode 9594; Testing Loss 0.00581548733195514; Training Loss 0.004695796850225887\n",
      "Episode 9595; Testing Loss 0.005815392394708185; Training Loss 0.004695790274782793\n",
      "Episode 9596; Testing Loss 0.005815301905561775; Training Loss 0.004695780126078821\n",
      "Episode 9597; Testing Loss 0.005815330025841078; Training Loss 0.004695771941589298\n",
      "Episode 9598; Testing Loss 0.005815507304099949; Training Loss 0.00469576286316461\n",
      "Episode 9599; Testing Loss 0.005815539580424477; Training Loss 0.004695754681213597\n",
      "Episode 9600; Testing Loss 0.005815245521857775; Training Loss 0.004695745169630631\n",
      "Episode 9601; Testing Loss 0.005815230817029992; Training Loss 0.004695736652155352\n",
      "Episode 9602; Testing Loss 0.005815399689291359; Training Loss 0.004695726466863844\n",
      "Episode 9603; Testing Loss 0.005815441535147187; Training Loss 0.004695718374824699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9604; Testing Loss 0.005815294289417388; Training Loss 0.004695708970659114\n",
      "Episode 9605; Testing Loss 0.005815256949213033; Training Loss 0.004695699655329774\n",
      "Episode 9606; Testing Loss 0.005815382824070722; Training Loss 0.004695694051487134\n",
      "Episode 9607; Testing Loss 0.005815349226777282; Training Loss 0.004695686745435709\n",
      "Episode 9608; Testing Loss 0.005815197901719698; Training Loss 0.00469567389910101\n",
      "Episode 9609; Testing Loss 0.005815283506753214; Training Loss 0.004695663770791057\n",
      "Episode 9610; Testing Loss 0.005815407053458207; Training Loss 0.004695661291173357\n",
      "Episode 9611; Testing Loss 0.005815244147677575; Training Loss 0.004695650570957418\n",
      "Episode 9612; Testing Loss 0.005815117467169472; Training Loss 0.004695641050047085\n",
      "Episode 9613; Testing Loss 0.005815346214572375; Training Loss 0.004695629000513797\n",
      "Episode 9614; Testing Loss 0.005815435039319173; Training Loss 0.004695622036462359\n",
      "Episode 9615; Testing Loss 0.005815367422012732; Training Loss 0.00469561194828798\n",
      "Episode 9616; Testing Loss 0.005815273518019948; Training Loss 0.004695606596321266\n",
      "Episode 9617; Testing Loss 0.005815165026687984; Training Loss 0.004695596714166929\n",
      "Episode 9618; Testing Loss 0.005815194388990312; Training Loss 0.004695584882861816\n",
      "Episode 9619; Testing Loss 0.005815285483667678; Training Loss 0.004695578951175059\n",
      "Episode 9620; Testing Loss 0.0058152605895244605; Training Loss 0.004695568837390273\n",
      "Episode 9621; Testing Loss 0.005815089041378494; Training Loss 0.0046955596760670576\n",
      "Episode 9622; Testing Loss 0.005814984910979424; Training Loss 0.004695551609931477\n",
      "Episode 9623; Testing Loss 0.00581517347377598; Training Loss 0.004695542006489772\n",
      "Episode 9624; Testing Loss 0.005815333757330259; Training Loss 0.00469553450116169\n",
      "Episode 9625; Testing Loss 0.0058151349925283004; Training Loss 0.004695524431795642\n",
      "Episode 9626; Testing Loss 0.005815084988916838; Training Loss 0.0046955157529785406\n",
      "Episode 9627; Testing Loss 0.005815300153501607; Training Loss 0.004695506945636216\n",
      "Episode 9628; Testing Loss 0.005815309334188966; Training Loss 0.00469549707267976\n",
      "Episode 9629; Testing Loss 0.00581514348245244; Training Loss 0.004695489999607023\n",
      "Episode 9630; Testing Loss 0.005815131455212666; Training Loss 0.004695478813186908\n",
      "Episode 9631; Testing Loss 0.005815200038650753; Training Loss 0.0046954707558904156\n",
      "Episode 9632; Testing Loss 0.00581517368611918; Training Loss 0.004695460936147093\n",
      "Episode 9633; Testing Loss 0.0058151314084315215; Training Loss 0.0046954524522121645\n",
      "Episode 9634; Testing Loss 0.0058151770272972325; Training Loss 0.004695443573773209\n",
      "Episode 9635; Testing Loss 0.0058151713367479005; Training Loss 0.004695436983809458\n",
      "Episode 9636; Testing Loss 0.005815048349950308; Training Loss 0.004695426487412454\n",
      "Episode 9637; Testing Loss 0.005815047310061551; Training Loss 0.004695420357866096\n",
      "Episode 9638; Testing Loss 0.005815242748203099; Training Loss 0.004695411694928232\n",
      "Episode 9639; Testing Loss 0.0058152553814331326; Training Loss 0.00469539993959428\n",
      "Episode 9640; Testing Loss 0.005815036465034573; Training Loss 0.004695391523590043\n",
      "Episode 9641; Testing Loss 0.005814985168382724; Training Loss 0.004695382633033894\n",
      "Episode 9642; Testing Loss 0.005815156515944926; Training Loss 0.004695375005266289\n",
      "Episode 9643; Testing Loss 0.005815254832258348; Training Loss 0.004695365115531738\n",
      "Episode 9644; Testing Loss 0.00581505767108592; Training Loss 0.004695354694089991\n",
      "Episode 9645; Testing Loss 0.005814981607680578; Training Loss 0.004695346931641535\n",
      "Episode 9646; Testing Loss 0.005815103444845368; Training Loss 0.004695338073660717\n",
      "Episode 9647; Testing Loss 0.005815138953131704; Training Loss 0.004695328589608249\n",
      "Episode 9648; Testing Loss 0.005815093940309806; Training Loss 0.004695318883865755\n",
      "Episode 9649; Testing Loss 0.005815098620701123; Training Loss 0.0046953113518581\n",
      "Episode 9650; Testing Loss 0.0058150222271226145; Training Loss 0.004695303813079278\n",
      "Episode 9651; Testing Loss 0.005815008288888622; Training Loss 0.004695293428538729\n",
      "Episode 9652; Testing Loss 0.005815127215609533; Training Loss 0.00469528567272475\n",
      "Episode 9653; Testing Loss 0.005815175842650378; Training Loss 0.00469527672775281\n",
      "Episode 9654; Testing Loss 0.005814979366433674; Training Loss 0.0046952659210056594\n",
      "Episode 9655; Testing Loss 0.005814863600909969; Training Loss 0.004695259871262998\n",
      "Episode 9656; Testing Loss 0.00581491730158141; Training Loss 0.004695248516183571\n",
      "Episode 9657; Testing Loss 0.005815059137872602; Training Loss 0.004695240024671637\n",
      "Episode 9658; Testing Loss 0.0058150878783028515; Training Loss 0.00469523094522389\n",
      "Episode 9659; Testing Loss 0.005814916275536108; Training Loss 0.004695221145703643\n",
      "Episode 9660; Testing Loss 0.005814822867196712; Training Loss 0.0046952150141650415\n",
      "Episode 9661; Testing Loss 0.005815025580443551; Training Loss 0.004695205206625353\n",
      "Episode 9662; Testing Loss 0.005815059170040835; Training Loss 0.004695195430490821\n",
      "Episode 9663; Testing Loss 0.005815015163947216; Training Loss 0.004695187834768701\n",
      "Episode 9664; Testing Loss 0.005814894477633978; Training Loss 0.004695179022178716\n",
      "Episode 9665; Testing Loss 0.005814908054103984; Training Loss 0.004695171669728715\n",
      "Episode 9666; Testing Loss 0.005815080606142188; Training Loss 0.0046951619834617805\n",
      "Episode 9667; Testing Loss 0.0058150514184168185; Training Loss 0.004695151719951071\n",
      "Episode 9668; Testing Loss 0.0058148051306228495; Training Loss 0.004695144253914859\n",
      "Episode 9669; Testing Loss 0.005814826884991966; Training Loss 0.004695134519330862\n",
      "Episode 9670; Testing Loss 0.005814927629046891; Training Loss 0.004695125421808812\n",
      "Episode 9671; Testing Loss 0.00581490883963831; Training Loss 0.004695116908080545\n",
      "Episode 9672; Testing Loss 0.005814790003777599; Training Loss 0.004695108388961874\n",
      "Episode 9673; Testing Loss 0.005814755968541011; Training Loss 0.00469509875618818\n",
      "Episode 9674; Testing Loss 0.0058147736735449755; Training Loss 0.004695089302214894\n",
      "Episode 9675; Testing Loss 0.005814828834686386; Training Loss 0.004695081341038793\n",
      "Episode 9676; Testing Loss 0.005814859972313816; Training Loss 0.004695070947567082\n",
      "Episode 9677; Testing Loss 0.005814855468849588; Training Loss 0.004695064024016663\n",
      "Episode 9678; Testing Loss 0.005814785930051678; Training Loss 0.004695055819118032\n",
      "Episode 9679; Testing Loss 0.005814842916517046; Training Loss 0.004695045484576888\n",
      "Episode 9680; Testing Loss 0.00581487931502713; Training Loss 0.004695036261699084\n",
      "Episode 9681; Testing Loss 0.005814774893615814; Training Loss 0.004695027006194893\n",
      "Episode 9682; Testing Loss 0.005814700981606711; Training Loss 0.004695021589204924\n",
      "Episode 9683; Testing Loss 0.005814788766115559; Training Loss 0.004695010384373747\n",
      "Episode 9684; Testing Loss 0.005814810603826985; Training Loss 0.004695002870750674\n",
      "Episode 9685; Testing Loss 0.005814891178659713; Training Loss 0.004694996503045159\n",
      "Episode 9686; Testing Loss 0.0058149302742574885; Training Loss 0.004694992669249174\n",
      "Episode 9687; Testing Loss 0.00581477006630788; Training Loss 0.0046949817887327865\n",
      "Episode 9688; Testing Loss 0.005814703103826963; Training Loss 0.004694966736135166\n",
      "Episode 9689; Testing Loss 0.005814829736799446; Training Loss 0.004694960649363035\n",
      "Episode 9690; Testing Loss 0.005814958578284829; Training Loss 0.004694956164129088\n",
      "Episode 9691; Testing Loss 0.005814864311899843; Training Loss 0.004694941613220637\n",
      "Episode 9692; Testing Loss 0.005814728701492356; Training Loss 0.004694931912564095\n",
      "Episode 9693; Testing Loss 0.005814638491061167; Training Loss 0.0046949265128521395\n",
      "Episode 9694; Testing Loss 0.005814725239086708; Training Loss 0.004694914437445889\n",
      "Episode 9695; Testing Loss 0.005814841008743314; Training Loss 0.004694906289200392\n",
      "Episode 9696; Testing Loss 0.005814799425974432; Training Loss 0.004694895959482879\n",
      "Episode 9697; Testing Loss 0.005814585043724939; Training Loss 0.004694891331702824\n",
      "Episode 9698; Testing Loss 0.00581451952724932; Training Loss 0.004694881249773569\n",
      "Episode 9699; Testing Loss 0.005814757022820779; Training Loss 0.004694868847980102\n",
      "Episode 9700; Testing Loss 0.005814970333418773; Training Loss 0.004694864769529861\n",
      "Episode 9701; Testing Loss 0.005814830277057799; Training Loss 0.004694856026912261\n",
      "Episode 9702; Testing Loss 0.005814555083649033; Training Loss 0.0046948489438023155\n",
      "Episode 9703; Testing Loss 0.0058146771852868396; Training Loss 0.004694833471416491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9704; Testing Loss 0.005814840880651236; Training Loss 0.004694827351428302\n",
      "Episode 9705; Testing Loss 0.00581470251177838; Training Loss 0.004694817319352858\n",
      "Episode 9706; Testing Loss 0.005814517514353539; Training Loss 0.004694811710796639\n",
      "Episode 9707; Testing Loss 0.00581451222377652; Training Loss 0.004694800554009288\n",
      "Episode 9708; Testing Loss 0.005814698355082653; Training Loss 0.004694792512601773\n",
      "Episode 9709; Testing Loss 0.005814838190965165; Training Loss 0.004694784178608867\n",
      "Episode 9710; Testing Loss 0.00581468904613139; Training Loss 0.004694776230314793\n",
      "Episode 9711; Testing Loss 0.005814454790419493; Training Loss 0.0046947694177174905\n",
      "Episode 9712; Testing Loss 0.005814581001458897; Training Loss 0.00469475561565177\n",
      "Episode 9713; Testing Loss 0.005814914035947295; Training Loss 0.004694750666848988\n",
      "Episode 9714; Testing Loss 0.005814781058185826; Training Loss 0.00469473955142865\n",
      "Episode 9715; Testing Loss 0.005814494099420046; Training Loss 0.004694731272585093\n",
      "Episode 9716; Testing Loss 0.005814512625726527; Training Loss 0.0046947221063089436\n",
      "Episode 9717; Testing Loss 0.005814608555944303; Training Loss 0.004694716314550824\n",
      "Episode 9718; Testing Loss 0.0058145193680378855; Training Loss 0.004694703301511129\n",
      "Episode 9719; Testing Loss 0.00581443997276264; Training Loss 0.004694699701675254\n",
      "Episode 9720; Testing Loss 0.0058145855877482526; Training Loss 0.004694690436263502\n",
      "Episode 9721; Testing Loss 0.005814734895449659; Training Loss 0.004694679962279347\n",
      "Episode 9722; Testing Loss 0.00581463620011203; Training Loss 0.0046946712051030935\n",
      "Episode 9723; Testing Loss 0.005814459136505805; Training Loss 0.004694665054095151\n",
      "Episode 9724; Testing Loss 0.005814517482998463; Training Loss 0.004694653930427132\n",
      "Episode 9725; Testing Loss 0.005814675426438026; Training Loss 0.004694641902861122\n",
      "Episode 9726; Testing Loss 0.005814702978795598; Training Loss 0.004694639090151885\n",
      "Episode 9727; Testing Loss 0.0058145997965752035; Training Loss 0.004694632901335455\n",
      "Episode 9728; Testing Loss 0.0058145177898855215; Training Loss 0.004694619379880748\n",
      "Episode 9729; Testing Loss 0.005814563715971813; Training Loss 0.004694607972108416\n",
      "Episode 9730; Testing Loss 0.005814471665141072; Training Loss 0.004694601280775112\n",
      "Episode 9731; Testing Loss 0.005814406476832233; Training Loss 0.004694593343443575\n",
      "Episode 9732; Testing Loss 0.005814526190773834; Training Loss 0.004694581937863813\n",
      "Episode 9733; Testing Loss 0.005814694328483144; Training Loss 0.0046945767506571074\n",
      "Episode 9734; Testing Loss 0.0058146761757987666; Training Loss 0.004694568863020051\n",
      "Episode 9735; Testing Loss 0.005814564517519294; Training Loss 0.004694557670468011\n",
      "Episode 9736; Testing Loss 0.005814522906112501; Training Loss 0.004694548856754023\n",
      "Episode 9737; Testing Loss 0.005814535118325315; Training Loss 0.004694539011026524\n",
      "Episode 9738; Testing Loss 0.005814560752996244; Training Loss 0.004694528467217229\n",
      "Episode 9739; Testing Loss 0.005814568278134731; Training Loss 0.0046945233601228984\n",
      "Episode 9740; Testing Loss 0.005814474806971655; Training Loss 0.004694515896789826\n",
      "Episode 9741; Testing Loss 0.005814492641262044; Training Loss 0.00469450450818144\n",
      "Episode 9742; Testing Loss 0.005814541086819482; Training Loss 0.004694494373212509\n",
      "Episode 9743; Testing Loss 0.005814503689830926; Training Loss 0.00469448959849695\n",
      "Episode 9744; Testing Loss 0.00581432314411588; Training Loss 0.004694479462100747\n",
      "Episode 9745; Testing Loss 0.005814279246262842; Training Loss 0.004694468196675648\n",
      "Episode 9746; Testing Loss 0.005814466146750329; Training Loss 0.004694458573632525\n",
      "Episode 9747; Testing Loss 0.005814566149612862; Training Loss 0.004694452162809436\n",
      "Episode 9748; Testing Loss 0.0058144523038366185; Training Loss 0.004694439880264045\n",
      "Episode 9749; Testing Loss 0.005814314050154002; Training Loss 0.004694433328697985\n",
      "Episode 9750; Testing Loss 0.005814382496906564; Training Loss 0.0046944243753891685\n",
      "Episode 9751; Testing Loss 0.005814502573614048; Training Loss 0.004694414381546137\n",
      "Episode 9752; Testing Loss 0.005814476342923793; Training Loss 0.004694406056565493\n",
      "Episode 9753; Testing Loss 0.005814449662667823; Training Loss 0.004694399368949409\n",
      "Episode 9754; Testing Loss 0.005814540129946224; Training Loss 0.004694387302442625\n",
      "Episode 9755; Testing Loss 0.005814531840099651; Training Loss 0.0046943789013701685\n",
      "Episode 9756; Testing Loss 0.005814346544776954; Training Loss 0.004694370969347365\n",
      "Episode 9757; Testing Loss 0.005814223617632146; Training Loss 0.004694362154159126\n",
      "Episode 9758; Testing Loss 0.005814378433196079; Training Loss 0.004694351768223348\n",
      "Episode 9759; Testing Loss 0.005814514872648381; Training Loss 0.004694344784537687\n",
      "Episode 9760; Testing Loss 0.005814340881312125; Training Loss 0.004694333888078766\n",
      "Episode 9761; Testing Loss 0.005814236592439379; Training Loss 0.004694328410414842\n",
      "Episode 9762; Testing Loss 0.00581441752794713; Training Loss 0.004694317489074429\n",
      "Episode 9763; Testing Loss 0.00581445750443693; Training Loss 0.00469430809346024\n",
      "Episode 9764; Testing Loss 0.005814315731322553; Training Loss 0.004694302624431476\n",
      "Episode 9765; Testing Loss 0.005814389704530662; Training Loss 0.004694293287161791\n",
      "Episode 9766; Testing Loss 0.005814562473395891; Training Loss 0.00469428401365883\n",
      "Episode 9767; Testing Loss 0.0058143988562825355; Training Loss 0.004694273010693689\n",
      "Episode 9768; Testing Loss 0.0058142299569550904; Training Loss 0.004694267791432175\n",
      "Episode 9769; Testing Loss 0.005814373590043515; Training Loss 0.004694255804468411\n",
      "Episode 9770; Testing Loss 0.005814522073679344; Training Loss 0.004694252787813403\n",
      "Episode 9771; Testing Loss 0.005814264967670567; Training Loss 0.004694239654708343\n",
      "Episode 9772; Testing Loss 0.005814042167126073; Training Loss 0.00469423459873957\n",
      "Episode 9773; Testing Loss 0.005814269230918346; Training Loss 0.0046942201837305506\n",
      "Episode 9774; Testing Loss 0.005814490527252751; Training Loss 0.0046942175224060035\n",
      "Episode 9775; Testing Loss 0.005814251870357651; Training Loss 0.0046942031279534665\n",
      "Episode 9776; Testing Loss 0.005814108966358973; Training Loss 0.004694195322461746\n",
      "Episode 9777; Testing Loss 0.005814258557880125; Training Loss 0.004694186016277735\n",
      "Episode 9778; Testing Loss 0.005814331448172553; Training Loss 0.004694176207909558\n",
      "Episode 9779; Testing Loss 0.0058142072149721695; Training Loss 0.004694167779311364\n",
      "Episode 9780; Testing Loss 0.005814207925879254; Training Loss 0.00469415878522551\n",
      "Episode 9781; Testing Loss 0.005814220764646294; Training Loss 0.004694149292650513\n",
      "Episode 9782; Testing Loss 0.005814270890621493; Training Loss 0.004694140441747493\n",
      "Episode 9783; Testing Loss 0.005814186075656958; Training Loss 0.004694132646216998\n",
      "Episode 9784; Testing Loss 0.005814199799676534; Training Loss 0.004694122761703935\n",
      "Episode 9785; Testing Loss 0.0058142058997110784; Training Loss 0.0046941162521553\n",
      "Episode 9786; Testing Loss 0.005814153500627406; Training Loss 0.004694106141025581\n",
      "Episode 9787; Testing Loss 0.0058141913634511665; Training Loss 0.004694098690663523\n",
      "Episode 9788; Testing Loss 0.005814314614747016; Training Loss 0.004694090889736151\n",
      "Episode 9789; Testing Loss 0.005814279544386055; Training Loss 0.004694081076118542\n",
      "Episode 9790; Testing Loss 0.005814060187269934; Training Loss 0.004694072451123302\n",
      "Episode 9791; Testing Loss 0.005814076628670192; Training Loss 0.004694062957475874\n",
      "Episode 9792; Testing Loss 0.005814280574685461; Training Loss 0.004694053579292372\n",
      "Episode 9793; Testing Loss 0.00581428559395002; Training Loss 0.0046940453542268674\n",
      "Episode 9794; Testing Loss 0.00581409475909812; Training Loss 0.004694036668362353\n",
      "Episode 9795; Testing Loss 0.005814067996051032; Training Loss 0.004694026441975274\n",
      "Episode 9796; Testing Loss 0.005814118070555519; Training Loss 0.004694017940215776\n",
      "Episode 9797; Testing Loss 0.0058140774818427185; Training Loss 0.004694011629825828\n",
      "Episode 9798; Testing Loss 0.00581411072830893; Training Loss 0.0046940026142919256\n",
      "Episode 9799; Testing Loss 0.005814156283778766; Training Loss 0.00469399483530484\n",
      "Episode 9800; Testing Loss 0.005814015616479552; Training Loss 0.004693986481852126\n",
      "Episode 9801; Testing Loss 0.005813972167527816; Training Loss 0.004693975923472913\n",
      "Episode 9802; Testing Loss 0.0058141257089117555; Training Loss 0.004693966444492086\n",
      "Episode 9803; Testing Loss 0.005814243130359582; Training Loss 0.0046939575382982926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9804; Testing Loss 0.005814101822840818; Training Loss 0.004693948422762739\n",
      "Episode 9805; Testing Loss 0.005813932571263012; Training Loss 0.004693942906258564\n",
      "Episode 9806; Testing Loss 0.005814037638435686; Training Loss 0.004693931489750955\n",
      "Episode 9807; Testing Loss 0.005814251455571531; Training Loss 0.004693924648648342\n",
      "Episode 9808; Testing Loss 0.005814191874541071; Training Loss 0.00469391305830204\n",
      "Episode 9809; Testing Loss 0.005813918676037582; Training Loss 0.004693905638416066\n",
      "Episode 9810; Testing Loss 0.005813949752940585; Training Loss 0.004693898956788789\n",
      "Episode 9811; Testing Loss 0.005814131525832163; Training Loss 0.004693888359827322\n",
      "Episode 9812; Testing Loss 0.005814134048456448; Training Loss 0.0046938818197688565\n",
      "Episode 9813; Testing Loss 0.005814042828861067; Training Loss 0.004693875523334077\n",
      "Episode 9814; Testing Loss 0.005813967855872467; Training Loss 0.00469386181465534\n",
      "Episode 9815; Testing Loss 0.005814062144603337; Training Loss 0.004693853792095672\n",
      "Episode 9816; Testing Loss 0.00581401375183543; Training Loss 0.004693846131276502\n",
      "Episode 9817; Testing Loss 0.0058139266070835255; Training Loss 0.0046938344978160775\n",
      "Episode 9818; Testing Loss 0.005813997532639473; Training Loss 0.004693827067054415\n",
      "Episode 9819; Testing Loss 0.005814120514763096; Training Loss 0.004693820284622793\n",
      "Episode 9820; Testing Loss 0.005814137920773544; Training Loss 0.004693807913657393\n",
      "Episode 9821; Testing Loss 0.005813993026251741; Training Loss 0.004693799426397012\n",
      "Episode 9822; Testing Loss 0.005813892655951994; Training Loss 0.004693791358848881\n",
      "Episode 9823; Testing Loss 0.005814031198719829; Training Loss 0.0046937831453589774\n",
      "Episode 9824; Testing Loss 0.005814105265426224; Training Loss 0.004693772438324049\n",
      "Episode 9825; Testing Loss 0.00581399209282301; Training Loss 0.004693763452055137\n",
      "Episode 9826; Testing Loss 0.0058139341007821714; Training Loss 0.004693755436012782\n",
      "Episode 9827; Testing Loss 0.005814031512818273; Training Loss 0.004693747863646521\n",
      "Episode 9828; Testing Loss 0.005814047617526166; Training Loss 0.004693738494183739\n",
      "Episode 9829; Testing Loss 0.00581391278271541; Training Loss 0.004693727858640183\n",
      "Episode 9830; Testing Loss 0.00581394471479856; Training Loss 0.004693721023330809\n",
      "Episode 9831; Testing Loss 0.005814043811505947; Training Loss 0.004693712089944116\n",
      "Episode 9832; Testing Loss 0.00581400708895372; Training Loss 0.004693704090987379\n",
      "Episode 9833; Testing Loss 0.005813840719302771; Training Loss 0.004693695581979609\n",
      "Episode 9834; Testing Loss 0.005813844702530683; Training Loss 0.004693686783830578\n",
      "Episode 9835; Testing Loss 0.005814114944778172; Training Loss 0.0046936775914401445\n",
      "Episode 9836; Testing Loss 0.005814076061296362; Training Loss 0.004693668321800315\n",
      "Episode 9837; Testing Loss 0.005813752617036403; Training Loss 0.00469365996580527\n",
      "Episode 9838; Testing Loss 0.005813720049892354; Training Loss 0.004693653873705674\n",
      "Episode 9839; Testing Loss 0.005813942998654641; Training Loss 0.004693642804020754\n",
      "Episode 9840; Testing Loss 0.005813987956849907; Training Loss 0.004693632105851175\n",
      "Episode 9841; Testing Loss 0.005813881380166938; Training Loss 0.004693628092107674\n",
      "Episode 9842; Testing Loss 0.005813926852794296; Training Loss 0.004693617844368165\n",
      "Episode 9843; Testing Loss 0.005813941191072682; Training Loss 0.004693608972630079\n",
      "Episode 9844; Testing Loss 0.005813958362248644; Training Loss 0.004693597602911812\n",
      "Episode 9845; Testing Loss 0.005813849526190897; Training Loss 0.004693588804413971\n",
      "Episode 9846; Testing Loss 0.005813817046687488; Training Loss 0.004693582644120896\n",
      "Episode 9847; Testing Loss 0.005813788269735754; Training Loss 0.0046935760540284316\n",
      "Episode 9848; Testing Loss 0.005813698632353866; Training Loss 0.0046935659044601554\n",
      "Episode 9849; Testing Loss 0.0058137145380659895; Training Loss 0.00469355773102787\n",
      "Episode 9850; Testing Loss 0.005813804624969571; Training Loss 0.004693551345013727\n",
      "Episode 9851; Testing Loss 0.0058139346521585405; Training Loss 0.004693540801436269\n",
      "Episode 9852; Testing Loss 0.00581391642635809; Training Loss 0.00469352704118437\n",
      "Episode 9853; Testing Loss 0.005813781678854886; Training Loss 0.004693521059985097\n",
      "Episode 9854; Testing Loss 0.005813680101832668; Training Loss 0.004693513616568278\n",
      "Episode 9855; Testing Loss 0.005813743100058627; Training Loss 0.0046935023756110306\n",
      "Episode 9856; Testing Loss 0.00581387283979543; Training Loss 0.004693493926267395\n",
      "Episode 9857; Testing Loss 0.005813810040308276; Training Loss 0.004693483254893268\n",
      "Episode 9858; Testing Loss 0.005813671906008976; Training Loss 0.0046934764656001255\n",
      "Episode 9859; Testing Loss 0.005813611153801461; Training Loss 0.004693469368645778\n",
      "Episode 9860; Testing Loss 0.005813729895622235; Training Loss 0.0046934574111844015\n",
      "Episode 9861; Testing Loss 0.005813859353351948; Training Loss 0.004693451675187371\n",
      "Episode 9862; Testing Loss 0.00581375002425995; Training Loss 0.004693440766165753\n",
      "Episode 9863; Testing Loss 0.005813613438314026; Training Loss 0.004693433039367294\n",
      "Episode 9864; Testing Loss 0.0058135860503818046; Training Loss 0.0046934239529701\n",
      "Episode 9865; Testing Loss 0.005813719542863895; Training Loss 0.004693413572039073\n",
      "Episode 9866; Testing Loss 0.00581388420220356; Training Loss 0.004693407985321326\n",
      "Episode 9867; Testing Loss 0.005813771156054798; Training Loss 0.004693395962528198\n",
      "Episode 9868; Testing Loss 0.005813660984232525; Training Loss 0.004693388014252252\n",
      "Episode 9869; Testing Loss 0.0058136722931437295; Training Loss 0.004693378369959615\n",
      "Episode 9870; Testing Loss 0.005813761215314976; Training Loss 0.00469337100243183\n",
      "Episode 9871; Testing Loss 0.005813765799979377; Training Loss 0.004693361905771312\n",
      "Episode 9872; Testing Loss 0.00581360190420531; Training Loss 0.004693353435695753\n",
      "Episode 9873; Testing Loss 0.005813527431854846; Training Loss 0.00469334509363642\n",
      "Episode 9874; Testing Loss 0.005813770584857554; Training Loss 0.004693335685609673\n",
      "Episode 9875; Testing Loss 0.005813760425888999; Training Loss 0.0046933277492026675\n",
      "Episode 9876; Testing Loss 0.005813556824143722; Training Loss 0.00469332040774878\n",
      "Episode 9877; Testing Loss 0.00581360408024198; Training Loss 0.004693308232238864\n",
      "Episode 9878; Testing Loss 0.0058137011190188366; Training Loss 0.004693301233216326\n",
      "Episode 9879; Testing Loss 0.005813615406015648; Training Loss 0.004693291606584213\n",
      "Episode 9880; Testing Loss 0.005813599236622605; Training Loss 0.004693281664383334\n",
      "Episode 9881; Testing Loss 0.0058136690983662; Training Loss 0.004693275812039416\n",
      "Episode 9882; Testing Loss 0.005813582321720183; Training Loss 0.004693266086002865\n",
      "Episode 9883; Testing Loss 0.0058135052797619; Training Loss 0.004693258550607178\n",
      "Episode 9884; Testing Loss 0.0058136193745723635; Training Loss 0.004693250000070757\n",
      "Episode 9885; Testing Loss 0.005813640444884955; Training Loss 0.004693238986624923\n",
      "Episode 9886; Testing Loss 0.005813543424370636; Training Loss 0.004693233981267659\n",
      "Episode 9887; Testing Loss 0.005813395790168528; Training Loss 0.004693229853769371\n",
      "Episode 9888; Testing Loss 0.005813450529514668; Training Loss 0.004693218628896387\n",
      "Episode 9889; Testing Loss 0.005813619390551381; Training Loss 0.004693205587443744\n",
      "Episode 9890; Testing Loss 0.00581362666680848; Training Loss 0.004693200845065565\n",
      "Episode 9891; Testing Loss 0.005813589761625062; Training Loss 0.004693194664168505\n",
      "Episode 9892; Testing Loss 0.005813659368147013; Training Loss 0.004693181260013092\n",
      "Episode 9893; Testing Loss 0.005813654316981484; Training Loss 0.004693170219528181\n",
      "Episode 9894; Testing Loss 0.005813464876962525; Training Loss 0.004693162640987854\n",
      "Episode 9895; Testing Loss 0.005813366506779173; Training Loss 0.004693154004379939\n",
      "Episode 9896; Testing Loss 0.005813531049349724; Training Loss 0.004693142852389138\n",
      "Episode 9897; Testing Loss 0.005813621347791523; Training Loss 0.004693134909323425\n",
      "Episode 9898; Testing Loss 0.005813511908821843; Training Loss 0.004693126482300037\n",
      "Episode 9899; Testing Loss 0.005813452745495766; Training Loss 0.004693117773676495\n",
      "Episode 9900; Testing Loss 0.005813524274266341; Training Loss 0.0046931090082351565\n",
      "Episode 9901; Testing Loss 0.005813492364127987; Training Loss 0.004693098831734258\n",
      "Episode 9902; Testing Loss 0.005813393181851598; Training Loss 0.004693090685606806\n",
      "Episode 9903; Testing Loss 0.005813464241779775; Training Loss 0.004693081445241024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9904; Testing Loss 0.005813488803869085; Training Loss 0.004693074235001473\n",
      "Episode 9905; Testing Loss 0.005813374961888867; Training Loss 0.004693063884654087\n",
      "Episode 9906; Testing Loss 0.005813438600659257; Training Loss 0.004693055333541465\n",
      "Episode 9907; Testing Loss 0.005813608378457678; Training Loss 0.004693049956094051\n",
      "Episode 9908; Testing Loss 0.005813474273945646; Training Loss 0.004693038369765499\n",
      "Episode 9909; Testing Loss 0.005813340288161619; Training Loss 0.004693029157842852\n",
      "Episode 9910; Testing Loss 0.005813409935300523; Training Loss 0.004693021185651101\n",
      "Episode 9911; Testing Loss 0.005813350936520651; Training Loss 0.004693014409776345\n",
      "Episode 9912; Testing Loss 0.005813228617057015; Training Loss 0.004693007190564411\n",
      "Episode 9913; Testing Loss 0.0058133641706372835; Training Loss 0.004692997336792625\n",
      "Episode 9914; Testing Loss 0.005813517444963544; Training Loss 0.004692987277002422\n",
      "Episode 9915; Testing Loss 0.005813358614410909; Training Loss 0.004692976369940063\n",
      "Episode 9916; Testing Loss 0.005813236256201984; Training Loss 0.004692969663919057\n",
      "Episode 9917; Testing Loss 0.005813431272618633; Training Loss 0.004692962407422643\n",
      "Episode 9918; Testing Loss 0.005813499124980728; Training Loss 0.004692953089965246\n",
      "Episode 9919; Testing Loss 0.005813333679308042; Training Loss 0.004692943305426783\n",
      "Episode 9920; Testing Loss 0.005813379871661183; Training Loss 0.0046929339190395795\n",
      "Episode 9921; Testing Loss 0.005813510572535201; Training Loss 0.004692928212272994\n",
      "Episode 9922; Testing Loss 0.005813325874015682; Training Loss 0.00469291654403529\n",
      "Episode 9923; Testing Loss 0.0058131702352193565; Training Loss 0.004692909159342458\n",
      "Episode 9924; Testing Loss 0.0058133421131407355; Training Loss 0.00469289794363019\n",
      "Episode 9925; Testing Loss 0.00581350642226817; Training Loss 0.004692891799756187\n",
      "Episode 9926; Testing Loss 0.005813355845327634; Training Loss 0.004692881027658362\n",
      "Episode 9927; Testing Loss 0.0058131040945806015; Training Loss 0.0046928758125251695\n",
      "Episode 9928; Testing Loss 0.005813258091204395; Training Loss 0.004692863969864209\n",
      "Episode 9929; Testing Loss 0.005813401752814706; Training Loss 0.004692855630748686\n",
      "Episode 9930; Testing Loss 0.005813297313380474; Training Loss 0.004692846406005334\n",
      "Episode 9931; Testing Loss 0.005813277338525182; Training Loss 0.0046928371406271315\n",
      "Episode 9932; Testing Loss 0.005813325518338897; Training Loss 0.004692828604813073\n",
      "Episode 9933; Testing Loss 0.005813264788950819; Training Loss 0.004692818336409554\n",
      "Episode 9934; Testing Loss 0.005813185719863485; Training Loss 0.004692812210402885\n",
      "Episode 9935; Testing Loss 0.005813230359182743; Training Loss 0.004692801414810049\n",
      "Episode 9936; Testing Loss 0.005813366390221492; Training Loss 0.004692793092165887\n",
      "Episode 9937; Testing Loss 0.005813291943651935; Training Loss 0.004692783944958986\n",
      "Episode 9938; Testing Loss 0.005813206198671867; Training Loss 0.004692778011654042\n",
      "Episode 9939; Testing Loss 0.00581332583907582; Training Loss 0.004692767240720005\n",
      "Episode 9940; Testing Loss 0.005813211438016207; Training Loss 0.004692757382116785\n",
      "Episode 9941; Testing Loss 0.005813098124192217; Training Loss 0.004692750149701647\n",
      "Episode 9942; Testing Loss 0.005813164180058861; Training Loss 0.004692743500201549\n",
      "Episode 9943; Testing Loss 0.005813162247694369; Training Loss 0.0046927316328415816\n",
      "Episode 9944; Testing Loss 0.005813197181480303; Training Loss 0.004692724202471608\n",
      "Episode 9945; Testing Loss 0.005813317184926493; Training Loss 0.004692715873485786\n",
      "Episode 9946; Testing Loss 0.005813279923434722; Training Loss 0.004692707013942442\n",
      "Episode 9947; Testing Loss 0.005813107865143582; Training Loss 0.004692696566761617\n",
      "Episode 9948; Testing Loss 0.005813082645721259; Training Loss 0.004692688224868002\n",
      "Episode 9949; Testing Loss 0.005813223710569455; Training Loss 0.0046926793002666\n",
      "Episode 9950; Testing Loss 0.00581318812717802; Training Loss 0.004692672618736644\n",
      "Episode 9951; Testing Loss 0.005813167517786038; Training Loss 0.004692663097194982\n",
      "Episode 9952; Testing Loss 0.005813188427754151; Training Loss 0.004692654472196282\n",
      "Episode 9953; Testing Loss 0.00581313550733358; Training Loss 0.004692645936106668\n",
      "Episode 9954; Testing Loss 0.005813151279183666; Training Loss 0.0046926358566130415\n",
      "Episode 9955; Testing Loss 0.005813230105637496; Training Loss 0.004692628709696956\n",
      "Episode 9956; Testing Loss 0.005813265129396651; Training Loss 0.004692619005357663\n",
      "Episode 9957; Testing Loss 0.005813099453867583; Training Loss 0.004692609834587395\n",
      "Episode 9958; Testing Loss 0.005812958240371353; Training Loss 0.004692604005762689\n",
      "Episode 9959; Testing Loss 0.005813176928114841; Training Loss 0.00469259180610139\n",
      "Episode 9960; Testing Loss 0.005813201802573328; Training Loss 0.004692584774767589\n",
      "Episode 9961; Testing Loss 0.005813049516870881; Training Loss 0.00469257466315338\n",
      "Episode 9962; Testing Loss 0.005812960658460849; Training Loss 0.004692567525947577\n",
      "Episode 9963; Testing Loss 0.005813028753610956; Training Loss 0.004692558979673354\n",
      "Episode 9964; Testing Loss 0.005813192386559852; Training Loss 0.004692550011843297\n",
      "Episode 9965; Testing Loss 0.00581316034265224; Training Loss 0.0046925409508359744\n",
      "Episode 9966; Testing Loss 0.0058129765227800285; Training Loss 0.004692532480066446\n",
      "Episode 9967; Testing Loss 0.0058128839682420365; Training Loss 0.004692524038833509\n",
      "Episode 9968; Testing Loss 0.005813136715627343; Training Loss 0.0046925172944979054\n",
      "Episode 9969; Testing Loss 0.005813120709160577; Training Loss 0.0046925073722656205\n",
      "Episode 9970; Testing Loss 0.005812894498163363; Training Loss 0.004692501873139365\n",
      "Episode 9971; Testing Loss 0.005812931745169018; Training Loss 0.004692491477839665\n",
      "Episode 9972; Testing Loss 0.005813122239929113; Training Loss 0.004692480813560025\n",
      "Episode 9973; Testing Loss 0.0058130503273244125; Training Loss 0.004692471157193128\n",
      "Episode 9974; Testing Loss 0.005812853219155731; Training Loss 0.00469246604878087\n",
      "Episode 9975; Testing Loss 0.00581299290418929; Training Loss 0.00469245420420891\n",
      "Episode 9976; Testing Loss 0.005813234511576226; Training Loss 0.004692450819952355\n",
      "Episode 9977; Testing Loss 0.005813042623023317; Training Loss 0.004692437850582907\n",
      "Episode 9978; Testing Loss 0.0058127771011602155; Training Loss 0.00469243188336137\n",
      "Episode 9979; Testing Loss 0.00581294815151193; Training Loss 0.00469241959347058\n",
      "Episode 9980; Testing Loss 0.005813111202151884; Training Loss 0.00469241298707423\n",
      "Episode 9981; Testing Loss 0.005812954422425119; Training Loss 0.004692401307920579\n",
      "Episode 9982; Testing Loss 0.0058127462343836155; Training Loss 0.004692394421763664\n",
      "Episode 9983; Testing Loss 0.005812870912989063; Training Loss 0.004692385728048136\n",
      "Episode 9984; Testing Loss 0.0058130203870963345; Training Loss 0.004692375232764637\n",
      "Episode 9985; Testing Loss 0.005813040233470154; Training Loss 0.004692367529979145\n",
      "Episode 9986; Testing Loss 0.005813016268865031; Training Loss 0.004692358720684263\n",
      "Episode 9987; Testing Loss 0.005812905592648127; Training Loss 0.0046923501183039566\n",
      "Episode 9988; Testing Loss 0.005812930639456997; Training Loss 0.004692340506375939\n",
      "Episode 9989; Testing Loss 0.0058129094992479735; Training Loss 0.0046923339748424515\n",
      "Episode 9990; Testing Loss 0.00581285510181001; Training Loss 0.004692323046235337\n",
      "Episode 9991; Testing Loss 0.005812853518744775; Training Loss 0.004692316169125547\n",
      "Episode 9992; Testing Loss 0.00581285179595989; Training Loss 0.004692306597820883\n",
      "Episode 9993; Testing Loss 0.00581288082692792; Training Loss 0.004692300113401802\n",
      "Episode 9994; Testing Loss 0.005813004220684737; Training Loss 0.004692290999208611\n",
      "Episode 9995; Testing Loss 0.005812973474070359; Training Loss 0.004692281086952747\n",
      "Episode 9996; Testing Loss 0.005812713998496617; Training Loss 0.0046922739315793565\n",
      "Episode 9997; Testing Loss 0.005812755463890373; Training Loss 0.00469226357087169\n",
      "Episode 9998; Testing Loss 0.005812981234799649; Training Loss 0.004692255263907892\n",
      "Episode 9999; Testing Loss 0.005812936027393975; Training Loss 0.004692245762902534\n",
      "Episode 10000; Testing Loss 0.00581266254732593; Training Loss 0.004692238475689563\n",
      "Episode 10001; Testing Loss 0.005812733651958251; Training Loss 0.004692227974340611\n",
      "Episode 10002; Testing Loss 0.005812918054379535; Training Loss 0.004692220093816777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10003; Testing Loss 0.005812895694062267; Training Loss 0.004692209987492538\n",
      "Episode 10004; Testing Loss 0.005812810191130558; Training Loss 0.004692202153365172\n",
      "Episode 10005; Testing Loss 0.005812794588443066; Training Loss 0.004692192401433355\n",
      "Episode 10006; Testing Loss 0.005812830542810324; Training Loss 0.004692184029160114\n",
      "Episode 10007; Testing Loss 0.005812907913094176; Training Loss 0.004692175938919959\n",
      "Episode 10008; Testing Loss 0.00581285729380796; Training Loss 0.0046921664704628295\n",
      "Episode 10009; Testing Loss 0.00581269616025965; Training Loss 0.004692159610672335\n",
      "Episode 10010; Testing Loss 0.00581276857421337; Training Loss 0.004692147185385923\n",
      "Episode 10011; Testing Loss 0.005812848275491032; Training Loss 0.004692145204275344\n",
      "Episode 10012; Testing Loss 0.005812669624227382; Training Loss 0.004692132069084779\n",
      "Episode 10013; Testing Loss 0.005812675433484256; Training Loss 0.004692122869336078\n",
      "Episode 10014; Testing Loss 0.005812782931686476; Training Loss 0.004692115929173647\n",
      "Episode 10015; Testing Loss 0.005812850186921169; Training Loss 0.004692112788586513\n",
      "Episode 10016; Testing Loss 0.00581286719075566; Training Loss 0.004692101826303047\n",
      "Episode 10017; Testing Loss 0.005812788957089004; Training Loss 0.004692088769461054\n",
      "Episode 10018; Testing Loss 0.005812729294132823; Training Loss 0.004692082683031717\n",
      "Episode 10019; Testing Loss 0.005812708235829657; Training Loss 0.004692075136018186\n",
      "Episode 10020; Testing Loss 0.005812689211361159; Training Loss 0.004692063965932681\n",
      "Episode 10021; Testing Loss 0.005812693853966377; Training Loss 0.004692055164066712\n",
      "Episode 10022; Testing Loss 0.0058127350999735775; Training Loss 0.004692048648953182\n",
      "Episode 10023; Testing Loss 0.005812616727568497; Training Loss 0.0046920407092623725\n",
      "Episode 10024; Testing Loss 0.005812583508950521; Training Loss 0.004692029374821059\n",
      "Episode 10025; Testing Loss 0.00581266851322226; Training Loss 0.004692019215089334\n",
      "Episode 10026; Testing Loss 0.00581261720590383; Training Loss 0.004692012155574225\n",
      "Episode 10027; Testing Loss 0.005812656397009581; Training Loss 0.004692001584255152\n",
      "Episode 10028; Testing Loss 0.005812709914692452; Training Loss 0.0046919948904769835\n",
      "Episode 10029; Testing Loss 0.005812649436814345; Training Loss 0.00469198543249748\n",
      "Episode 10030; Testing Loss 0.005812638990683628; Training Loss 0.0046919747072964885\n",
      "Episode 10031; Testing Loss 0.005812754845173297; Training Loss 0.004691966321059261\n",
      "Episode 10032; Testing Loss 0.005812757743030504; Training Loss 0.004691957317443204\n",
      "Episode 10033; Testing Loss 0.005812603200197249; Training Loss 0.004691948800369951\n",
      "Episode 10034; Testing Loss 0.0058124624793580645; Training Loss 0.0046919413898691064\n",
      "Episode 10035; Testing Loss 0.005812656486137071; Training Loss 0.004691930021785109\n",
      "Episode 10036; Testing Loss 0.00581274589592643; Training Loss 0.004691922965329929\n",
      "Episode 10037; Testing Loss 0.00581263612291198; Training Loss 0.004691912421887511\n",
      "Episode 10038; Testing Loss 0.005812489634159768; Training Loss 0.004691906455719921\n",
      "Episode 10039; Testing Loss 0.005812643792671677; Training Loss 0.004691896831106948\n",
      "Episode 10040; Testing Loss 0.005812730175914686; Training Loss 0.00469188774050513\n",
      "Episode 10041; Testing Loss 0.005812574161068177; Training Loss 0.004691880557594531\n",
      "Episode 10042; Testing Loss 0.005812575142096007; Training Loss 0.004691871910497648\n",
      "Episode 10043; Testing Loss 0.005812640879416195; Training Loss 0.004691860948688417\n",
      "Episode 10044; Testing Loss 0.005812555449745271; Training Loss 0.004691851133278948\n",
      "Episode 10045; Testing Loss 0.005812452468648491; Training Loss 0.004691845400425448\n",
      "Episode 10046; Testing Loss 0.005812561865048549; Training Loss 0.004691833940384848\n",
      "Episode 10047; Testing Loss 0.00581260436335278; Training Loss 0.004691825810753735\n",
      "Episode 10048; Testing Loss 0.005812530256030509; Training Loss 0.004691817726478363\n",
      "Episode 10049; Testing Loss 0.005812451613054475; Training Loss 0.004691808770334991\n",
      "Episode 10050; Testing Loss 0.005812552796526326; Training Loss 0.004691798630688815\n",
      "Episode 10051; Testing Loss 0.005812603874786101; Training Loss 0.004691792035101894\n",
      "Episode 10052; Testing Loss 0.0058124831867869785; Training Loss 0.004691782626708627\n",
      "Episode 10053; Testing Loss 0.0058124438111877876; Training Loss 0.0046917731524718535\n",
      "Episode 10054; Testing Loss 0.005812441321527239; Training Loss 0.00469176540756627\n",
      "Episode 10055; Testing Loss 0.005812411775717238; Training Loss 0.004691754868895127\n",
      "Episode 10056; Testing Loss 0.005812389132205926; Training Loss 0.004691750222904384\n",
      "Episode 10057; Testing Loss 0.005812510150089002; Training Loss 0.0046917409963821936\n",
      "Episode 10058; Testing Loss 0.005812572518398911; Training Loss 0.004691730136040901\n",
      "Episode 10059; Testing Loss 0.005812419818727509; Training Loss 0.004691720835853874\n",
      "Episode 10060; Testing Loss 0.0058122765775829475; Training Loss 0.004691713639475205\n",
      "Episode 10061; Testing Loss 0.005812480128044426; Training Loss 0.004691703602344379\n",
      "Episode 10062; Testing Loss 0.005812578312294546; Training Loss 0.004691697505321772\n",
      "Episode 10063; Testing Loss 0.005812494749901462; Training Loss 0.004691687014596129\n",
      "Episode 10064; Testing Loss 0.005812364022332203; Training Loss 0.004691677200526144\n",
      "Episode 10065; Testing Loss 0.005812286414939958; Training Loss 0.004691668541988374\n",
      "Episode 10066; Testing Loss 0.0058124024676114204; Training Loss 0.004691659307454989\n",
      "Episode 10067; Testing Loss 0.005812495032463161; Training Loss 0.0046916523780109794\n",
      "Episode 10068; Testing Loss 0.0058122902481831645; Training Loss 0.004691642373004766\n",
      "Episode 10069; Testing Loss 0.005812238007394262; Training Loss 0.00469163500985515\n",
      "Episode 10070; Testing Loss 0.0058124297804524085; Training Loss 0.004691625703991147\n",
      "Episode 10071; Testing Loss 0.005812433420501256; Training Loss 0.004691616636620603\n",
      "Episode 10072; Testing Loss 0.005812403618237712; Training Loss 0.004691605984203797\n",
      "Episode 10073; Testing Loss 0.005812385508606153; Training Loss 0.004691599331161451\n",
      "Episode 10074; Testing Loss 0.005812392732284301; Training Loss 0.004691589843223261\n",
      "Episode 10075; Testing Loss 0.005812439041714811; Training Loss 0.00469158216277331\n",
      "Episode 10076; Testing Loss 0.005812458370186995; Training Loss 0.004691572449925392\n",
      "Episode 10077; Testing Loss 0.0058122348179387405; Training Loss 0.004691564645178824\n",
      "Episode 10078; Testing Loss 0.005812134576260079; Training Loss 0.00469155644543559\n",
      "Episode 10079; Testing Loss 0.00581227351128034; Training Loss 0.004691546209256689\n",
      "Episode 10080; Testing Loss 0.005812460317355743; Training Loss 0.004691541811606021\n",
      "Episode 10081; Testing Loss 0.005812297670600824; Training Loss 0.004691530148340231\n",
      "Episode 10082; Testing Loss 0.0058121861316913345; Training Loss 0.004691525162729068\n",
      "Episode 10083; Testing Loss 0.005812393136712038; Training Loss 0.00469151292683724\n",
      "Episode 10084; Testing Loss 0.005812492551505794; Training Loss 0.004691507229140694\n",
      "Episode 10085; Testing Loss 0.005812247566091318; Training Loss 0.004691496259868269\n",
      "Episode 10086; Testing Loss 0.005812076128734295; Training Loss 0.004691489019338868\n",
      "Episode 10087; Testing Loss 0.005812291757930215; Training Loss 0.004691477220969223\n",
      "Episode 10088; Testing Loss 0.0058123842664676; Training Loss 0.004691473288200099\n",
      "Episode 10089; Testing Loss 0.005812083336634125; Training Loss 0.004691460139898488\n",
      "Episode 10090; Testing Loss 0.005811949448565848; Training Loss 0.0046914581012025775\n",
      "Episode 10091; Testing Loss 0.00581225930043166; Training Loss 0.004691444380987665\n",
      "Episode 10092; Testing Loss 0.0058124726697027064; Training Loss 0.004691440903043559\n",
      "Episode 10093; Testing Loss 0.005812180721916689; Training Loss 0.004691426971977306\n",
      "Episode 10094; Testing Loss 0.005812010100189007; Training Loss 0.004691421532121974\n",
      "Episode 10095; Testing Loss 0.00581230228116291; Training Loss 0.0046914056838694655\n",
      "Episode 10096; Testing Loss 0.005812581482327864; Training Loss 0.004691402614072212\n",
      "Episode 10097; Testing Loss 0.005812408824150843; Training Loss 0.0046913921921907555\n",
      "Episode 10098; Testing Loss 0.005812121413474069; Training Loss 0.004691382896350976\n",
      "Episode 10099; Testing Loss 0.005812160723687867; Training Loss 0.004691375319504438\n",
      "Episode 10100; Testing Loss 0.00581219433060976; Training Loss 0.004691369795403182\n",
      "Episode 10101; Testing Loss 0.005812031561059108; Training Loss 0.004691357066156937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10102; Testing Loss 0.005811940892670613; Training Loss 0.004691347945924451\n",
      "Episode 10103; Testing Loss 0.00581211697950212; Training Loss 0.004691337881917805\n",
      "Episode 10104; Testing Loss 0.005812258387045116; Training Loss 0.004691333409227593\n",
      "Episode 10105; Testing Loss 0.005812028762067923; Training Loss 0.00469132054641413\n",
      "Episode 10106; Testing Loss 0.005811967688481976; Training Loss 0.0046913132070644376\n",
      "Episode 10107; Testing Loss 0.0058121952086424445; Training Loss 0.004691302077435397\n",
      "Episode 10108; Testing Loss 0.0058123035735205344; Training Loss 0.004691293808511096\n",
      "Episode 10109; Testing Loss 0.005812190930731335; Training Loss 0.004691282783729709\n",
      "Episode 10110; Testing Loss 0.0058120390067177416; Training Loss 0.004691276269280773\n",
      "Episode 10111; Testing Loss 0.005812196951598531; Training Loss 0.004691268609988756\n",
      "Episode 10112; Testing Loss 0.005812232151265098; Training Loss 0.004691260630396429\n",
      "Episode 10113; Testing Loss 0.0058120554061819605; Training Loss 0.00469124854973414\n",
      "Episode 10114; Testing Loss 0.00581200271275906; Training Loss 0.004691242945238061\n",
      "Episode 10115; Testing Loss 0.0058121523773990436; Training Loss 0.00469123111247474\n",
      "Episode 10116; Testing Loss 0.0058122449195727646; Training Loss 0.004691226464443629\n",
      "Episode 10117; Testing Loss 0.005811995114984171; Training Loss 0.004691215589746053\n",
      "Episode 10118; Testing Loss 0.005811886695065567; Training Loss 0.0046912069879859975\n",
      "Episode 10119; Testing Loss 0.005812082698218249; Training Loss 0.004691196120432537\n",
      "Episode 10120; Testing Loss 0.005812359281025815; Training Loss 0.004691190042391442\n",
      "Episode 10121; Testing Loss 0.005812277444935621; Training Loss 0.004691181570203799\n",
      "Episode 10122; Testing Loss 0.005811988742417644; Training Loss 0.004691173001991421\n",
      "Episode 10123; Testing Loss 0.005811897230470285; Training Loss 0.004691162220810809\n",
      "Episode 10124; Testing Loss 0.005812071185624919; Training Loss 0.004691152391566516\n",
      "Episode 10125; Testing Loss 0.00581212241449119; Training Loss 0.0046911450235910855\n",
      "Episode 10126; Testing Loss 0.005811978603509677; Training Loss 0.004691138537848102\n",
      "Episode 10127; Testing Loss 0.005811878626892323; Training Loss 0.004691126775822604\n",
      "Episode 10128; Testing Loss 0.0058119771364113886; Training Loss 0.004691119823700242\n",
      "Episode 10129; Testing Loss 0.005812068522881239; Training Loss 0.004691111319001671\n",
      "Episode 10130; Testing Loss 0.00581197831742019; Training Loss 0.004691099296396937\n",
      "Episode 10131; Testing Loss 0.005812002930088077; Training Loss 0.0046910940294704745\n",
      "Episode 10132; Testing Loss 0.005812162252969918; Training Loss 0.0046910827230695435\n",
      "Episode 10133; Testing Loss 0.005812171073561878; Training Loss 0.004691076442252709\n",
      "Episode 10134; Testing Loss 0.005811885972589962; Training Loss 0.004691067309192745\n",
      "Episode 10135; Testing Loss 0.005811787767847313; Training Loss 0.004691058012738147\n",
      "Episode 10136; Testing Loss 0.005812088774472751; Training Loss 0.004691047370863374\n",
      "Episode 10137; Testing Loss 0.005812259234748219; Training Loss 0.004691044043166692\n",
      "Episode 10138; Testing Loss 0.005811957843915123; Training Loss 0.004691029431031365\n",
      "Episode 10139; Testing Loss 0.005811764293610024; Training Loss 0.00469102504592268\n",
      "Episode 10140; Testing Loss 0.0058119980418099935; Training Loss 0.004691012330710533\n",
      "Episode 10141; Testing Loss 0.005812178533991703; Training Loss 0.00469100554270301\n",
      "Episode 10142; Testing Loss 0.005811945569042429; Training Loss 0.00469099833257822\n",
      "Episode 10143; Testing Loss 0.005811834069677195; Training Loss 0.004690991027992194\n",
      "Episode 10144; Testing Loss 0.00581199761359124; Training Loss 0.004690976422979521\n",
      "Episode 10145; Testing Loss 0.005812110856447058; Training Loss 0.004690976641228389\n",
      "Episode 10146; Testing Loss 0.00581186655854148; Training Loss 0.004690966522547477\n",
      "Episode 10147; Testing Loss 0.0058116605301934176; Training Loss 0.004690957609865492\n",
      "Episode 10148; Testing Loss 0.0058117744783687236; Training Loss 0.004690942388776804\n",
      "Episode 10149; Testing Loss 0.005812048500662799; Training Loss 0.00469093656229488\n",
      "Episode 10150; Testing Loss 0.005812041808200362; Training Loss 0.0046909290562959045\n",
      "Episode 10151; Testing Loss 0.005811764410991143; Training Loss 0.004690919447565371\n",
      "Episode 10152; Testing Loss 0.005811810257257252; Training Loss 0.004690911720320742\n",
      "Episode 10153; Testing Loss 0.005812182184305399; Training Loss 0.004690900045449891\n",
      "Episode 10154; Testing Loss 0.005812201570281283; Training Loss 0.0046908942872082564\n",
      "Episode 10155; Testing Loss 0.005811818924147307; Training Loss 0.004690886511814015\n",
      "Episode 10156; Testing Loss 0.005811819259291636; Training Loss 0.004690874754240017\n",
      "Episode 10157; Testing Loss 0.0058121060887265; Training Loss 0.004690867855050917\n",
      "Episode 10158; Testing Loss 0.005811986667306818; Training Loss 0.0046908563132529125\n",
      "Episode 10159; Testing Loss 0.005811661932342784; Training Loss 0.004690848637141747\n",
      "Episode 10160; Testing Loss 0.005811731467856575; Training Loss 0.004690838482824428\n",
      "Episode 10161; Testing Loss 0.0058120109538643576; Training Loss 0.004690829047197566\n",
      "Episode 10162; Testing Loss 0.0058119328567607315; Training Loss 0.004690818285287186\n",
      "Episode 10163; Testing Loss 0.0058115886456749566; Training Loss 0.004690810259611698\n",
      "Episode 10164; Testing Loss 0.005811634988698599; Training Loss 0.004690799694556087\n",
      "Episode 10165; Testing Loss 0.005811995930559842; Training Loss 0.004690795549788557\n",
      "Episode 10166; Testing Loss 0.005811927125106381; Training Loss 0.004690780191013857\n",
      "Episode 10167; Testing Loss 0.005811649796584797; Training Loss 0.004690777192095691\n",
      "Episode 10168; Testing Loss 0.005811794882699296; Training Loss 0.004690764487016616\n",
      "Episode 10169; Testing Loss 0.005812071909502287; Training Loss 0.0046907579632271545\n",
      "Episode 10170; Testing Loss 0.005811860303793209; Training Loss 0.004690743682687146\n",
      "Episode 10171; Testing Loss 0.005811536403034937; Training Loss 0.004690739559402239\n",
      "Episode 10172; Testing Loss 0.005811691893952213; Training Loss 0.00469072484024758\n",
      "Episode 10173; Testing Loss 0.005811948864311469; Training Loss 0.004690720400908929\n",
      "Episode 10174; Testing Loss 0.005811761198813974; Training Loss 0.004690706533791224\n",
      "Episode 10175; Testing Loss 0.005811555025085236; Training Loss 0.0046907038608393955\n",
      "Episode 10176; Testing Loss 0.005811699182605921; Training Loss 0.004690690244887599\n",
      "Episode 10177; Testing Loss 0.005811866856604018; Training Loss 0.004690683216019695\n",
      "Episode 10178; Testing Loss 0.005811669622712994; Training Loss 0.004690669906280974\n",
      "Episode 10179; Testing Loss 0.005811499194146449; Training Loss 0.00469066369977509\n",
      "Episode 10180; Testing Loss 0.005811674977048912; Training Loss 0.00469065296428003\n",
      "Episode 10181; Testing Loss 0.005811720736879662; Training Loss 0.004690642361464029\n",
      "Episode 10182; Testing Loss 0.005811607283936485; Training Loss 0.004690633816924712\n",
      "Episode 10183; Testing Loss 0.005811584507805092; Training Loss 0.004690626861869379\n",
      "Episode 10184; Testing Loss 0.0058117053408236535; Training Loss 0.004690619157004158\n",
      "Episode 10185; Testing Loss 0.005811608465899815; Training Loss 0.004690608484178391\n",
      "Episode 10186; Testing Loss 0.005811428373456907; Training Loss 0.004690598572884936\n",
      "Episode 10187; Testing Loss 0.005811473174907787; Training Loss 0.004690590015246011\n",
      "Episode 10188; Testing Loss 0.005811675940856197; Training Loss 0.004690584894650474\n",
      "Episode 10189; Testing Loss 0.005811544790257664; Training Loss 0.004690572404151444\n",
      "Episode 10190; Testing Loss 0.005811399772009127; Training Loss 0.00469056360864467\n",
      "Episode 10191; Testing Loss 0.0058114277915641615; Training Loss 0.004690554787241308\n",
      "Episode 10192; Testing Loss 0.005811533977626461; Training Loss 0.004690545095485879\n",
      "Episode 10193; Testing Loss 0.005811554284050444; Training Loss 0.004690536456502951\n",
      "Episode 10194; Testing Loss 0.005811317953602799; Training Loss 0.004690530182861552\n",
      "Episode 10195; Testing Loss 0.005811364742671117; Training Loss 0.00469051811225314\n",
      "Episode 10196; Testing Loss 0.005811508903524365; Training Loss 0.004690513160235096\n",
      "Episode 10197; Testing Loss 0.005811435459665459; Training Loss 0.004690502049709442\n",
      "Episode 10198; Testing Loss 0.005811343074718358; Training Loss 0.004690496480298562\n",
      "Episode 10199; Testing Loss 0.0058115139360339725; Training Loss 0.004690485431577105\n",
      "Episode 10200; Testing Loss 0.005811692901988735; Training Loss 0.004690479554116732\n",
      "Episode 10201; Testing Loss 0.005811479340669238; Training Loss 0.004690466349134355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10202; Testing Loss 0.005811225754934819; Training Loss 0.0046904623917642324\n",
      "Episode 10203; Testing Loss 0.0058113963151654145; Training Loss 0.004690448376719015\n",
      "Episode 10204; Testing Loss 0.005811549082774368; Training Loss 0.004690442331581141\n",
      "Episode 10205; Testing Loss 0.005811391674478022; Training Loss 0.004690430439968702\n",
      "Episode 10206; Testing Loss 0.0058111992471725485; Training Loss 0.004690423729960707\n",
      "Episode 10207; Testing Loss 0.005811300025342515; Training Loss 0.004690412366758465\n",
      "Episode 10208; Testing Loss 0.005811473896798825; Training Loss 0.004690407989615865\n",
      "Episode 10209; Testing Loss 0.005811302651569873; Training Loss 0.004690395508851105\n",
      "Episode 10210; Testing Loss 0.005811252079898366; Training Loss 0.004690387312551159\n",
      "Episode 10211; Testing Loss 0.00581133665456571; Training Loss 0.004690379460779924\n",
      "Episode 10212; Testing Loss 0.00581145837367594; Training Loss 0.004690371091195147\n",
      "Episode 10213; Testing Loss 0.0058114042019257964; Training Loss 0.004690361284110212\n",
      "Episode 10214; Testing Loss 0.005811185344807533; Training Loss 0.004690351108173878\n",
      "Episode 10215; Testing Loss 0.005811223111749471; Training Loss 0.004690343505314419\n",
      "Episode 10216; Testing Loss 0.005811270923131775; Training Loss 0.004690336607574893\n",
      "Episode 10217; Testing Loss 0.00581119500579486; Training Loss 0.0046903289120395755\n",
      "Episode 10218; Testing Loss 0.005811269866828323; Training Loss 0.004690319032822557\n",
      "Episode 10219; Testing Loss 0.005811367385806548; Training Loss 0.0046903074280912685\n",
      "Episode 10220; Testing Loss 0.0058113144968336425; Training Loss 0.00469029935664703\n",
      "Episode 10221; Testing Loss 0.005811270485579123; Training Loss 0.0046902922155037\n",
      "Episode 10222; Testing Loss 0.005811409311486339; Training Loss 0.00469028105822206\n",
      "Episode 10223; Testing Loss 0.00581136346850042; Training Loss 0.004690272178777515\n",
      "Episode 10224; Testing Loss 0.005811160747055382; Training Loss 0.004690265139180701\n",
      "Episode 10225; Testing Loss 0.005811216869248676; Training Loss 0.004690253970218878\n",
      "Episode 10226; Testing Loss 0.00581130431151613; Training Loss 0.004690245654996779\n",
      "Episode 10227; Testing Loss 0.005811227285249975; Training Loss 0.00469023653916852\n",
      "Episode 10228; Testing Loss 0.0058112089259428155; Training Loss 0.0046902276733078105\n",
      "Episode 10229; Testing Loss 0.00581118115000465; Training Loss 0.00469021987126233\n",
      "Episode 10230; Testing Loss 0.005811130070120299; Training Loss 0.00469021013805948\n",
      "Episode 10231; Testing Loss 0.005811239604412854; Training Loss 0.004690201300898872\n",
      "Episode 10232; Testing Loss 0.005811284819383954; Training Loss 0.004690194242400391\n",
      "Episode 10233; Testing Loss 0.005811207347865589; Training Loss 0.004690185081494636\n",
      "Episode 10234; Testing Loss 0.005811056797179122; Training Loss 0.004690176446711768\n",
      "Episode 10235; Testing Loss 0.005811121595152202; Training Loss 0.0046901676699700745\n",
      "Episode 10236; Testing Loss 0.005811335602142324; Training Loss 0.004690159863317793\n",
      "Episode 10237; Testing Loss 0.005811233704074731; Training Loss 0.00469015192429921\n",
      "Episode 10238; Testing Loss 0.005810920255434015; Training Loss 0.0046901443298043655\n",
      "Episode 10239; Testing Loss 0.0058109234907946115; Training Loss 0.004690135677180727\n",
      "Episode 10240; Testing Loss 0.005811292489127888; Training Loss 0.004690124464664128\n",
      "Episode 10241; Testing Loss 0.005811325517208752; Training Loss 0.004690117859060967\n",
      "Episode 10242; Testing Loss 0.005811010852047285; Training Loss 0.004690109789067108\n",
      "Episode 10243; Testing Loss 0.005810970747647582; Training Loss 0.0046900990223700595\n",
      "Episode 10244; Testing Loss 0.005811179037542307; Training Loss 0.00469008941260261\n",
      "Episode 10245; Testing Loss 0.005811221815037612; Training Loss 0.0046900802230135175\n",
      "Episode 10246; Testing Loss 0.005810993236535883; Training Loss 0.004690074633589922\n",
      "Episode 10247; Testing Loss 0.005811019447142426; Training Loss 0.004690066887459895\n",
      "Episode 10248; Testing Loss 0.005811209126569755; Training Loss 0.004690052169985343\n",
      "Episode 10249; Testing Loss 0.005811204068466979; Training Loss 0.004690046327560032\n",
      "Episode 10250; Testing Loss 0.005810967003998536; Training Loss 0.004690034314804841\n",
      "Episode 10251; Testing Loss 0.005810854845585032; Training Loss 0.004690030193137434\n",
      "Episode 10252; Testing Loss 0.005811069057864027; Training Loss 0.004690019210945694\n",
      "Episode 10253; Testing Loss 0.005811163350969636; Training Loss 0.004690011801448966\n",
      "Episode 10254; Testing Loss 0.005810864915691546; Training Loss 0.004689999581870193\n",
      "Episode 10255; Testing Loss 0.0058107709157232965; Training Loss 0.004689993082328336\n",
      "Episode 10256; Testing Loss 0.0058110192373933235; Training Loss 0.004689982358550735\n",
      "Episode 10257; Testing Loss 0.005811121856200078; Training Loss 0.0046899736968549635\n",
      "Episode 10258; Testing Loss 0.0058109555512648944; Training Loss 0.004689964367768505\n",
      "Episode 10259; Testing Loss 0.005810889141882817; Training Loss 0.004689955799697826\n",
      "Episode 10260; Testing Loss 0.005810989073327856; Training Loss 0.004689947249794965\n",
      "Episode 10261; Testing Loss 0.005811001562002436; Training Loss 0.004689939049433114\n",
      "Episode 10262; Testing Loss 0.005810860775720599; Training Loss 0.0046899295172912496\n",
      "Episode 10263; Testing Loss 0.005810869946109869; Training Loss 0.004689920686489187\n",
      "Episode 10264; Testing Loss 0.005811001981906932; Training Loss 0.004689910903882802\n",
      "Episode 10265; Testing Loss 0.005810932135285745; Training Loss 0.004689901186174946\n",
      "Episode 10266; Testing Loss 0.005810850384686693; Training Loss 0.004689891146639851\n",
      "Episode 10267; Testing Loss 0.005810880567712142; Training Loss 0.0046898832629588185\n",
      "Episode 10268; Testing Loss 0.005810945659667047; Training Loss 0.004689878046387085\n",
      "Episode 10269; Testing Loss 0.005810939416244189; Training Loss 0.004689869469845295\n",
      "Episode 10270; Testing Loss 0.005810795932143828; Training Loss 0.004689859998081677\n",
      "Episode 10271; Testing Loss 0.005810835216613473; Training Loss 0.004689848245853857\n",
      "Episode 10272; Testing Loss 0.005810872246687663; Training Loss 0.00468984579251887\n",
      "Episode 10273; Testing Loss 0.005810717906991331; Training Loss 0.004689835658681225\n",
      "Episode 10274; Testing Loss 0.005810660931041667; Training Loss 0.0046898227784803685\n",
      "Episode 10275; Testing Loss 0.005810903593086642; Training Loss 0.0046898134609199725\n",
      "Episode 10276; Testing Loss 0.005810901704341265; Training Loss 0.004689808150115146\n",
      "Episode 10277; Testing Loss 0.00581076591903254; Training Loss 0.00468979518945698\n",
      "Episode 10278; Testing Loss 0.0058107073035455524; Training Loss 0.004689788596613862\n",
      "Episode 10279; Testing Loss 0.00581070015690889; Training Loss 0.0046897810825484686\n",
      "Episode 10280; Testing Loss 0.0058107445835276226; Training Loss 0.004689770718161758\n",
      "Episode 10281; Testing Loss 0.005810749147157065; Training Loss 0.004689760905419116\n",
      "Episode 10282; Testing Loss 0.005810867448706654; Training Loss 0.004689753664786548\n",
      "Episode 10283; Testing Loss 0.005810904787182869; Training Loss 0.004689742060538921\n",
      "Episode 10284; Testing Loss 0.005810757735666212; Training Loss 0.004689736545978832\n",
      "Episode 10285; Testing Loss 0.005810563908042071; Training Loss 0.004689733055194582\n",
      "Episode 10286; Testing Loss 0.005810691522578966; Training Loss 0.004689719907745457\n",
      "Episode 10287; Testing Loss 0.005810864831050994; Training Loss 0.004689706586877941\n",
      "Episode 10288; Testing Loss 0.005810833223846245; Training Loss 0.0046897047231097095\n",
      "Episode 10289; Testing Loss 0.005810765235527228; Training Loss 0.004689697936016048\n",
      "Episode 10290; Testing Loss 0.005810753646235523; Training Loss 0.00468968628599051\n",
      "Episode 10291; Testing Loss 0.005810796091310806; Training Loss 0.004689670936101256\n",
      "Episode 10292; Testing Loss 0.005810739771507965; Training Loss 0.004689665115985184\n",
      "Episode 10293; Testing Loss 0.005810555841215547; Training Loss 0.0046896568491603665\n",
      "Episode 10294; Testing Loss 0.005810526423756158; Training Loss 0.004689645365763929\n",
      "Episode 10295; Testing Loss 0.005810772177179188; Training Loss 0.004689635888628661\n",
      "Episode 10296; Testing Loss 0.005810781616789928; Training Loss 0.004689628237736535\n",
      "Episode 10297; Testing Loss 0.0058106007713841145; Training Loss 0.004689617806562573\n",
      "Episode 10298; Testing Loss 0.005810528896421183; Training Loss 0.004689608665120353\n",
      "Episode 10299; Testing Loss 0.005810640535979261; Training Loss 0.00468960183848501\n",
      "Episode 10300; Testing Loss 0.005810640630289082; Training Loss 0.004689591629840364\n",
      "Episode 10301; Testing Loss 0.0058104962248900285; Training Loss 0.004689582735370128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10302; Testing Loss 0.0058105908818264955; Training Loss 0.0046895737216177845\n",
      "Episode 10303; Testing Loss 0.005810664717034271; Training Loss 0.0046895650387574934\n",
      "Episode 10304; Testing Loss 0.005810477538182873; Training Loss 0.004689556808502501\n",
      "Episode 10305; Testing Loss 0.005810497045144894; Training Loss 0.004689547210257657\n",
      "Episode 10306; Testing Loss 0.0058106730673388955; Training Loss 0.004689538818531038\n",
      "Episode 10307; Testing Loss 0.005810620598962892; Training Loss 0.004689527447649597\n",
      "Episode 10308; Testing Loss 0.005810514010241203; Training Loss 0.004689524947010815\n",
      "Episode 10309; Testing Loss 0.005810537489175577; Training Loss 0.004689512245967776\n",
      "Episode 10310; Testing Loss 0.005810596042703273; Training Loss 0.004689501847346416\n",
      "Episode 10311; Testing Loss 0.005810510753544129; Training Loss 0.0046894972968635215\n",
      "Episode 10312; Testing Loss 0.005810462263674736; Training Loss 0.004689488497906308\n",
      "Episode 10313; Testing Loss 0.005810566513191375; Training Loss 0.004689478980515515\n",
      "Episode 10314; Testing Loss 0.0058107041220321854; Training Loss 0.0046894669125860725\n",
      "Episode 10315; Testing Loss 0.005810555127937847; Training Loss 0.004689455845414171\n",
      "Episode 10316; Testing Loss 0.005810465857662756; Training Loss 0.004689449690003071\n",
      "Episode 10317; Testing Loss 0.005810411253235796; Training Loss 0.004689439512521811\n",
      "Episode 10318; Testing Loss 0.005810564018768724; Training Loss 0.004689430474100403\n",
      "Episode 10319; Testing Loss 0.005810543692301332; Training Loss 0.004689422876875824\n",
      "Episode 10320; Testing Loss 0.005810546309464973; Training Loss 0.004689415958937173\n",
      "Episode 10321; Testing Loss 0.0058104725735823445; Training Loss 0.004689403240097224\n",
      "Episode 10322; Testing Loss 0.0058104749053764825; Training Loss 0.004689394081131849\n",
      "Episode 10323; Testing Loss 0.0058106605488759365; Training Loss 0.004689389772386666\n",
      "Episode 10324; Testing Loss 0.005810621211844158; Training Loss 0.004689382974365241\n",
      "Episode 10325; Testing Loss 0.00581036906820968; Training Loss 0.004689369383068047\n",
      "Episode 10326; Testing Loss 0.0058103463895800385; Training Loss 0.004689360341088295\n",
      "Episode 10327; Testing Loss 0.005810699837341641; Training Loss 0.004689350382958206\n",
      "Episode 10328; Testing Loss 0.005810802781070673; Training Loss 0.004689347441137807\n",
      "Episode 10329; Testing Loss 0.005810436836983866; Training Loss 0.004689332238431064\n",
      "Episode 10330; Testing Loss 0.005810308732368228; Training Loss 0.0046893258380232925\n",
      "Episode 10331; Testing Loss 0.00581061030461819; Training Loss 0.0046893130477181945\n",
      "Episode 10332; Testing Loss 0.005810713100371189; Training Loss 0.004689309252323698\n",
      "Episode 10333; Testing Loss 0.005810422775208019; Training Loss 0.004689296109392333\n",
      "Episode 10334; Testing Loss 0.005810334876058563; Training Loss 0.004689287579045222\n",
      "Episode 10335; Testing Loss 0.0058104868490189135; Training Loss 0.004689277571787038\n",
      "Episode 10336; Testing Loss 0.0058105836843974825; Training Loss 0.00468927037927624\n",
      "Episode 10337; Testing Loss 0.005810592503728373; Training Loss 0.004689260745482574\n",
      "Episode 10338; Testing Loss 0.005810526592872701; Training Loss 0.004689248991092148\n",
      "Episode 10339; Testing Loss 0.005810413194381083; Training Loss 0.004689242451325309\n",
      "Episode 10340; Testing Loss 0.005810325852258502; Training Loss 0.004689233393091706\n",
      "Episode 10341; Testing Loss 0.005810407744914163; Training Loss 0.004689222573796228\n",
      "Episode 10342; Testing Loss 0.005810526385914283; Training Loss 0.004689215175930291\n",
      "Episode 10343; Testing Loss 0.005810535968952207; Training Loss 0.004689204486014992\n",
      "Episode 10344; Testing Loss 0.005810412677723532; Training Loss 0.004689196728641567\n",
      "Episode 10345; Testing Loss 0.00581040802996523; Training Loss 0.004689188771292278\n",
      "Episode 10346; Testing Loss 0.005810457532556642; Training Loss 0.004689177478137155\n",
      "Episode 10347; Testing Loss 0.0058105522545269525; Training Loss 0.004689170375864844\n",
      "Episode 10348; Testing Loss 0.005810537638652174; Training Loss 0.004689162611355507\n",
      "Episode 10349; Testing Loss 0.0058104274826180575; Training Loss 0.004689153771229492\n",
      "Episode 10350; Testing Loss 0.0058103685001455; Training Loss 0.004689142652169381\n",
      "Episode 10351; Testing Loss 0.005810409450675932; Training Loss 0.00468913537332436\n",
      "Episode 10352; Testing Loss 0.0058105488977278075; Training Loss 0.004689125574607241\n",
      "Episode 10353; Testing Loss 0.005810566209830869; Training Loss 0.004689115601819838\n",
      "Episode 10354; Testing Loss 0.005810317507751621; Training Loss 0.004689106124743188\n",
      "Episode 10355; Testing Loss 0.005810275331327548; Training Loss 0.004689099005783882\n",
      "Episode 10356; Testing Loss 0.005810385122933944; Training Loss 0.004689089505656831\n",
      "Episode 10357; Testing Loss 0.0058104615682720165; Training Loss 0.004689078946416618\n",
      "Episode 10358; Testing Loss 0.005810448650716149; Training Loss 0.004689072090351087\n",
      "Episode 10359; Testing Loss 0.005810445491418694; Training Loss 0.0046890607187881055\n",
      "Episode 10360; Testing Loss 0.005810392174995741; Training Loss 0.004689052295327831\n",
      "Episode 10361; Testing Loss 0.005810346478254043; Training Loss 0.0046890422027166066\n",
      "Episode 10362; Testing Loss 0.00581033753724714; Training Loss 0.004689034794753529\n",
      "Episode 10363; Testing Loss 0.005810397145152285; Training Loss 0.004689025969489704\n",
      "Episode 10364; Testing Loss 0.005810242226732324; Training Loss 0.004689017454530267\n",
      "Episode 10365; Testing Loss 0.005810262524204885; Training Loss 0.004689007012408682\n",
      "Episode 10366; Testing Loss 0.00581035408189962; Training Loss 0.004688998896888825\n",
      "Episode 10367; Testing Loss 0.005810467702891522; Training Loss 0.004688989703167596\n",
      "Episode 10368; Testing Loss 0.005810364320498616; Training Loss 0.004688979263193023\n",
      "Episode 10369; Testing Loss 0.005810199837435383; Training Loss 0.004688971923877211\n",
      "Episode 10370; Testing Loss 0.005810314587924797; Training Loss 0.0046889675312932816\n",
      "Episode 10371; Testing Loss 0.00581031686745735; Training Loss 0.004688956242829978\n",
      "Episode 10372; Testing Loss 0.005810205568424598; Training Loss 0.004688945269823845\n",
      "Episode 10373; Testing Loss 0.00581030182763903; Training Loss 0.004688937462503014\n",
      "Episode 10374; Testing Loss 0.005810395623491351; Training Loss 0.004688931429401458\n",
      "Episode 10375; Testing Loss 0.005810256753155777; Training Loss 0.004688919234841943\n",
      "Episode 10376; Testing Loss 0.005810236679169891; Training Loss 0.004688908604542735\n",
      "Episode 10377; Testing Loss 0.005810207870868443; Training Loss 0.0046888991267347475\n",
      "Episode 10378; Testing Loss 0.005810225657784486; Training Loss 0.0046888956990971875\n",
      "Episode 10379; Testing Loss 0.005810130101816409; Training Loss 0.004688884277152795\n",
      "Episode 10380; Testing Loss 0.0058101839045290584; Training Loss 0.004688874166143653\n",
      "Episode 10381; Testing Loss 0.005810249102419292; Training Loss 0.004688865199016346\n",
      "Episode 10382; Testing Loss 0.005810277855733158; Training Loss 0.004688859054404321\n",
      "Episode 10383; Testing Loss 0.005810308926777711; Training Loss 0.004688847323073901\n",
      "Episode 10384; Testing Loss 0.005810246837598257; Training Loss 0.004688838599439925\n",
      "Episode 10385; Testing Loss 0.0058101479484467; Training Loss 0.0046888313274393995\n",
      "Episode 10386; Testing Loss 0.005810212741236176; Training Loss 0.004688820307758161\n",
      "Episode 10387; Testing Loss 0.0058103821345408925; Training Loss 0.004688813930600495\n",
      "Episode 10388; Testing Loss 0.005810231290118988; Training Loss 0.004688801973394508\n",
      "Episode 10389; Testing Loss 0.005810044605137868; Training Loss 0.004688794742478769\n",
      "Episode 10390; Testing Loss 0.005810099801989333; Training Loss 0.004688787025444142\n",
      "Episode 10391; Testing Loss 0.005810221072991997; Training Loss 0.004688777594224723\n",
      "Episode 10392; Testing Loss 0.005810212201172784; Training Loss 0.004688768962486827\n",
      "Episode 10393; Testing Loss 0.0058101661295689964; Training Loss 0.004688759710157094\n",
      "Episode 10394; Testing Loss 0.005810163080852597; Training Loss 0.0046887482612182835\n",
      "Episode 10395; Testing Loss 0.005810131531998954; Training Loss 0.004688741982640102\n",
      "Episode 10396; Testing Loss 0.0058100306772356025; Training Loss 0.004688733401848672\n",
      "Episode 10397; Testing Loss 0.005810038103441162; Training Loss 0.0046887223275814465\n",
      "Episode 10398; Testing Loss 0.005810148155421727; Training Loss 0.004688715039176136\n",
      "Episode 10399; Testing Loss 0.005810178975684201; Training Loss 0.004688705910827082\n",
      "Episode 10400; Testing Loss 0.005810094005504272; Training Loss 0.004688694279694718\n",
      "Episode 10401; Testing Loss 0.005809965176525343; Training Loss 0.00468868749462719\n",
      "Episode 10402; Testing Loss 0.005810050190397687; Training Loss 0.004688676618412361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10403; Testing Loss 0.005810177672593309; Training Loss 0.004688667953814955\n",
      "Episode 10404; Testing Loss 0.005810140148591058; Training Loss 0.004688659542853554\n",
      "Episode 10405; Testing Loss 0.005810053274039445; Training Loss 0.004688649950339787\n",
      "Episode 10406; Testing Loss 0.005809958990035976; Training Loss 0.0046886411138320405\n",
      "Episode 10407; Testing Loss 0.005809976268604444; Training Loss 0.004688631839094214\n",
      "Episode 10408; Testing Loss 0.0058100985311124315; Training Loss 0.00468862337388859\n",
      "Episode 10409; Testing Loss 0.005810097697275971; Training Loss 0.0046886141198692445\n",
      "Episode 10410; Testing Loss 0.005809960906553794; Training Loss 0.004688606879667881\n",
      "Episode 10411; Testing Loss 0.005810036512106269; Training Loss 0.004688596803971251\n",
      "Episode 10412; Testing Loss 0.005810087036923217; Training Loss 0.004688587744391145\n",
      "Episode 10413; Testing Loss 0.005810064257121739; Training Loss 0.004688578122514741\n",
      "Episode 10414; Testing Loss 0.00580997905130922; Training Loss 0.004688570921671698\n",
      "Episode 10415; Testing Loss 0.005809865036055672; Training Loss 0.004688562811477549\n",
      "Episode 10416; Testing Loss 0.005809952789252412; Training Loss 0.004688551659134165\n",
      "Episode 10417; Testing Loss 0.005810127796881979; Training Loss 0.004688543987321932\n",
      "Episode 10418; Testing Loss 0.0058100491104067; Training Loss 0.004688533307360325\n",
      "Episode 10419; Testing Loss 0.005809940947108148; Training Loss 0.004688525830172573\n",
      "Episode 10420; Testing Loss 0.0058099270177045465; Training Loss 0.004688516576432738\n",
      "Episode 10421; Testing Loss 0.005810021000607046; Training Loss 0.004688509305742133\n",
      "Episode 10422; Testing Loss 0.005810048817036964; Training Loss 0.004688499885783503\n",
      "Episode 10423; Testing Loss 0.0058099663019125156; Training Loss 0.004688490224145372\n",
      "Episode 10424; Testing Loss 0.005809844872653586; Training Loss 0.004688481386586867\n",
      "Episode 10425; Testing Loss 0.0058098704748499085; Training Loss 0.0046884745231664925\n",
      "Episode 10426; Testing Loss 0.0058100719551600795; Training Loss 0.004688464833876151\n",
      "Episode 10427; Testing Loss 0.005810039049499015; Training Loss 0.0046884569384900154\n",
      "Episode 10428; Testing Loss 0.005809723062125185; Training Loss 0.004688450326979173\n",
      "Episode 10429; Testing Loss 0.005809735113595627; Training Loss 0.004688440511286813\n",
      "Episode 10430; Testing Loss 0.0058100894001981785; Training Loss 0.004688430172071198\n",
      "Episode 10431; Testing Loss 0.0058101497984781505; Training Loss 0.004688422338274111\n",
      "Episode 10432; Testing Loss 0.005809879623978766; Training Loss 0.0046884096132067\n",
      "Episode 10433; Testing Loss 0.00580973424622918; Training Loss 0.00468840689502533\n",
      "Episode 10434; Testing Loss 0.0058098808283334425; Training Loss 0.004688393751005787\n",
      "Episode 10435; Testing Loss 0.005810015033096843; Training Loss 0.004688384968041883\n",
      "Episode 10436; Testing Loss 0.005809969548002172; Training Loss 0.004688376815517507\n",
      "Episode 10437; Testing Loss 0.005809770484485163; Training Loss 0.004688365580691813\n",
      "Episode 10438; Testing Loss 0.005809678333482637; Training Loss 0.00468836024765343\n",
      "Episode 10439; Testing Loss 0.005809783778212653; Training Loss 0.004688351371102679\n",
      "Episode 10440; Testing Loss 0.005809905387048799; Training Loss 0.004688339138768125\n",
      "Episode 10441; Testing Loss 0.0058098258674109966; Training Loss 0.0046883327006833645\n",
      "Episode 10442; Testing Loss 0.005809851226021845; Training Loss 0.0046883251070151425\n",
      "Episode 10443; Testing Loss 0.005809908619113942; Training Loss 0.004688312870659815\n",
      "Episode 10444; Testing Loss 0.005809926591110232; Training Loss 0.004688306290159153\n",
      "Episode 10445; Testing Loss 0.005809730406733225; Training Loss 0.004688299089000302\n",
      "Episode 10446; Testing Loss 0.0058096414278077406; Training Loss 0.004688288358959498\n",
      "Episode 10447; Testing Loss 0.005809741476940717; Training Loss 0.00468827786017386\n",
      "Episode 10448; Testing Loss 0.005809845200103221; Training Loss 0.0046882730976372175\n",
      "Episode 10449; Testing Loss 0.005809798054662044; Training Loss 0.004688264089360631\n",
      "Episode 10450; Testing Loss 0.005809726697324772; Training Loss 0.004688252212942248\n",
      "Episode 10451; Testing Loss 0.005809722487738572; Training Loss 0.004688244361623186\n",
      "Episode 10452; Testing Loss 0.005809730878706014; Training Loss 0.004688235342386416\n",
      "Episode 10453; Testing Loss 0.0058097725283233126; Training Loss 0.004688223559848476\n",
      "Episode 10454; Testing Loss 0.005809784663740798; Training Loss 0.0046882142003098\n",
      "Episode 10455; Testing Loss 0.005809663477368662; Training Loss 0.004688207960311429\n",
      "Episode 10456; Testing Loss 0.0058096625353247; Training Loss 0.004688196276397984\n",
      "Episode 10457; Testing Loss 0.005809729333199237; Training Loss 0.004688186157778541\n",
      "Episode 10458; Testing Loss 0.005809762373260576; Training Loss 0.004688177525755757\n",
      "Episode 10459; Testing Loss 0.005809628383398105; Training Loss 0.0046881690880130125\n",
      "Episode 10460; Testing Loss 0.005809692824012893; Training Loss 0.0046881576165927735\n",
      "Episode 10461; Testing Loss 0.00580981114666976; Training Loss 0.004688148822391522\n",
      "Episode 10462; Testing Loss 0.005809657915249287; Training Loss 0.004688137730376853\n",
      "Episode 10463; Testing Loss 0.005809493794248689; Training Loss 0.004688130697973857\n",
      "Episode 10464; Testing Loss 0.005809595931767847; Training Loss 0.004688119683989575\n",
      "Episode 10465; Testing Loss 0.0058097351864867955; Training Loss 0.00468811129978692\n",
      "Episode 10466; Testing Loss 0.005809583150474575; Training Loss 0.004688100370955668\n",
      "Episode 10467; Testing Loss 0.005809486675066212; Training Loss 0.00468809374445862\n",
      "Episode 10468; Testing Loss 0.005809542794879134; Training Loss 0.0046880845491544905\n",
      "Episode 10469; Testing Loss 0.005809519942697861; Training Loss 0.004688073094123109\n",
      "Episode 10470; Testing Loss 0.005809494385849596; Training Loss 0.004688061673212824\n",
      "Episode 10471; Testing Loss 0.0058095258751349466; Training Loss 0.004688057475426022\n",
      "Episode 10472; Testing Loss 0.005809617132823391; Training Loss 0.0046880490327298845\n",
      "Episode 10473; Testing Loss 0.005809585072466407; Training Loss 0.004688034927146043\n",
      "Episode 10474; Testing Loss 0.005809467112918512; Training Loss 0.004688030808668473\n",
      "Episode 10475; Testing Loss 0.005809439954638551; Training Loss 0.004688022818446621\n",
      "Episode 10476; Testing Loss 0.005809517629912005; Training Loss 0.004688009700107395\n",
      "Episode 10477; Testing Loss 0.005809458312266761; Training Loss 0.004688000267346375\n",
      "Episode 10478; Testing Loss 0.005809433978882002; Training Loss 0.0046879906847576164\n",
      "Episode 10479; Testing Loss 0.005809499456645563; Training Loss 0.004687977767537821\n",
      "Episode 10480; Testing Loss 0.005809501420250669; Training Loss 0.004687974156475224\n",
      "Episode 10481; Testing Loss 0.005809343178036467; Training Loss 0.004687966229318162\n",
      "Episode 10482; Testing Loss 0.005809345486078083; Training Loss 0.004687956163027589\n",
      "Episode 10483; Testing Loss 0.005809472276617281; Training Loss 0.00468794160962482\n",
      "Episode 10484; Testing Loss 0.005809475612470401; Training Loss 0.004687934950608537\n",
      "Episode 10485; Testing Loss 0.005809450753355378; Training Loss 0.004687929141063602\n",
      "Episode 10486; Testing Loss 0.005809361601334915; Training Loss 0.004687916343471402\n",
      "Episode 10487; Testing Loss 0.005809320010780222; Training Loss 0.004687907632199275\n",
      "Episode 10488; Testing Loss 0.00580933487953253; Training Loss 0.004687902488006996\n",
      "Episode 10489; Testing Loss 0.0058093463029380964; Training Loss 0.004687891215859669\n",
      "Episode 10490; Testing Loss 0.005809287932694734; Training Loss 0.004687877512339367\n",
      "Episode 10491; Testing Loss 0.0058092739056181415; Training Loss 0.00468786873829102\n",
      "Episode 10492; Testing Loss 0.005809433789696008; Training Loss 0.004687862934657419\n",
      "Episode 10493; Testing Loss 0.005809446092867997; Training Loss 0.004687853538769833\n",
      "Episode 10494; Testing Loss 0.005809182155772887; Training Loss 0.004687839738284753\n",
      "Episode 10495; Testing Loss 0.005809026906049688; Training Loss 0.004687837533086785\n",
      "Episode 10496; Testing Loss 0.0058092058478062415; Training Loss 0.004687827342568158\n",
      "Episode 10497; Testing Loss 0.0058093070206042124; Training Loss 0.004687817199827722\n",
      "Episode 10498; Testing Loss 0.005809221311165839; Training Loss 0.0046878025955797865\n",
      "Episode 10499; Testing Loss 0.005809156789260804; Training Loss 0.004687793383100315\n",
      "Episode 10500; Testing Loss 0.005809196099067272; Training Loss 0.00468778830154348\n",
      "Episode 10501; Testing Loss 0.005809303861968054; Training Loss 0.004687776842240478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10502; Testing Loss 0.00580923811966358; Training Loss 0.004687766842982273\n",
      "Episode 10503; Testing Loss 0.005808985559781723; Training Loss 0.004687761314710311\n",
      "Episode 10504; Testing Loss 0.005808908769150795; Training Loss 0.004687750518619353\n",
      "Episode 10505; Testing Loss 0.005809098623484567; Training Loss 0.004687738803556237\n",
      "Episode 10506; Testing Loss 0.005809318837105305; Training Loss 0.004687733676369723\n",
      "Episode 10507; Testing Loss 0.0058091915865892355; Training Loss 0.004687719912927917\n",
      "Episode 10508; Testing Loss 0.005808954798598921; Training Loss 0.0046877085087463875\n",
      "Episode 10509; Testing Loss 0.0058089952055357065; Training Loss 0.004687700503975779\n",
      "Episode 10510; Testing Loss 0.005809167132750693; Training Loss 0.004687692491993543\n",
      "Episode 10511; Testing Loss 0.005809122225411629; Training Loss 0.0046876801544526265\n",
      "Episode 10512; Testing Loss 0.005808935027733888; Training Loss 0.004687671477014084\n",
      "Episode 10513; Testing Loss 0.005808989396381929; Training Loss 0.004687661361819174\n",
      "Episode 10514; Testing Loss 0.005809147046622148; Training Loss 0.004687653355243197\n",
      "Episode 10515; Testing Loss 0.005809110867310132; Training Loss 0.004687642004850188\n",
      "Episode 10516; Testing Loss 0.005808858003671002; Training Loss 0.004687633104473448\n",
      "Episode 10517; Testing Loss 0.005808821814119253; Training Loss 0.004687625694371433\n",
      "Episode 10518; Testing Loss 0.005809034190947111; Training Loss 0.004687613872583618\n",
      "Episode 10519; Testing Loss 0.005809041002843375; Training Loss 0.0046876023954183456\n",
      "Episode 10520; Testing Loss 0.0058089083155649715; Training Loss 0.004687592776961786\n",
      "Episode 10521; Testing Loss 0.0058088785854416665; Training Loss 0.004687584109438914\n",
      "Episode 10522; Testing Loss 0.0058088643258968516; Training Loss 0.004687574199940525\n",
      "Episode 10523; Testing Loss 0.005808879189379809; Training Loss 0.004687563473419992\n",
      "Episode 10524; Testing Loss 0.00580890242338112; Training Loss 0.004687557235594295\n",
      "Episode 10525; Testing Loss 0.005808806997529386; Training Loss 0.004687547925621965\n",
      "Episode 10526; Testing Loss 0.00580870574272184; Training Loss 0.0046875336167587055\n",
      "Episode 10527; Testing Loss 0.005808759481588236; Training Loss 0.00468752526318449\n",
      "Episode 10528; Testing Loss 0.0058088758346835166; Training Loss 0.004687515151481769\n",
      "Episode 10529; Testing Loss 0.005808765802652172; Training Loss 0.004687507989802894\n",
      "Episode 10530; Testing Loss 0.0058088033830136065; Training Loss 0.0046874983893233245\n",
      "Episode 10531; Testing Loss 0.005808928122897511; Training Loss 0.00468748982931166\n",
      "Episode 10532; Testing Loss 0.0058087228063797534; Training Loss 0.00468747856035981\n",
      "Episode 10533; Testing Loss 0.005808563799060525; Training Loss 0.004687470581411649\n",
      "Episode 10534; Testing Loss 0.005808782192321234; Training Loss 0.0046874564271632965\n",
      "Episode 10535; Testing Loss 0.0058088866059813424; Training Loss 0.004687454719095462\n",
      "Episode 10536; Testing Loss 0.005808635436239139; Training Loss 0.004687441444177021\n",
      "Episode 10537; Testing Loss 0.005808487678493363; Training Loss 0.004687434319182903\n",
      "Episode 10538; Testing Loss 0.005808678934578037; Training Loss 0.004687422669704982\n",
      "Episode 10539; Testing Loss 0.005808774832851099; Training Loss 0.004687413870410866\n",
      "Episode 10540; Testing Loss 0.0058086958691808055; Training Loss 0.004687399587640092\n",
      "Episode 10541; Testing Loss 0.005808547943928451; Training Loss 0.004687393781284518\n",
      "Episode 10542; Testing Loss 0.005808676774720834; Training Loss 0.004687382962205795\n",
      "Episode 10543; Testing Loss 0.005808691531684574; Training Loss 0.004687372061491532\n",
      "Episode 10544; Testing Loss 0.005808603236242752; Training Loss 0.0046873637724516755\n",
      "Episode 10545; Testing Loss 0.005808653223913651; Training Loss 0.004687354735885044\n",
      "Episode 10546; Testing Loss 0.005808729172373837; Training Loss 0.004687345561957794\n",
      "Episode 10547; Testing Loss 0.0058084921999660465; Training Loss 0.004687335089623618\n",
      "Episode 10548; Testing Loss 0.0058082218737631145; Training Loss 0.004687331579865727\n",
      "Episode 10549; Testing Loss 0.005808389106779101; Training Loss 0.004687315208367271\n",
      "Episode 10550; Testing Loss 0.005808696477583928; Training Loss 0.00468731288465155\n",
      "Episode 10551; Testing Loss 0.005808604670430835; Training Loss 0.0046873017577698265\n",
      "Episode 10552; Testing Loss 0.005808284937714108; Training Loss 0.0046872930898392805\n",
      "Episode 10553; Testing Loss 0.005808340645957314; Training Loss 0.004687278461847027\n",
      "Episode 10554; Testing Loss 0.005808575238256896; Training Loss 0.004687274056944134\n",
      "Episode 10555; Testing Loss 0.005808641413568862; Training Loss 0.004687269284514941\n",
      "Episode 10556; Testing Loss 0.005808444209531148; Training Loss 0.004687255762909656\n",
      "Episode 10557; Testing Loss 0.005808339709317284; Training Loss 0.004687237713438678\n",
      "Episode 10558; Testing Loss 0.005808354311639366; Training Loss 0.004687233272556701\n",
      "Episode 10559; Testing Loss 0.005808350575387258; Training Loss 0.004687223643950959\n",
      "Episode 10560; Testing Loss 0.005808333331214875; Training Loss 0.004687210589388118\n",
      "Episode 10561; Testing Loss 0.005808400135126613; Training Loss 0.004687206326063417\n",
      "Episode 10562; Testing Loss 0.0058084044944241445; Training Loss 0.004687200680260734\n",
      "Episode 10563; Testing Loss 0.005808257839559104; Training Loss 0.004687187221468258\n",
      "Episode 10564; Testing Loss 0.005808186273780422; Training Loss 0.004687173906020722\n",
      "Episode 10565; Testing Loss 0.005808261233703051; Training Loss 0.004687167549525458\n",
      "Episode 10566; Testing Loss 0.005808396527297077; Training Loss 0.004687158722787295\n",
      "Episode 10567; Testing Loss 0.005808298945958086; Training Loss 0.004687144690977759\n",
      "Episode 10568; Testing Loss 0.005808261799262506; Training Loss 0.004687136846992343\n",
      "Episode 10569; Testing Loss 0.00580835165103051; Training Loss 0.004687131069533989\n",
      "Episode 10570; Testing Loss 0.005808280866983875; Training Loss 0.004687115711745199\n",
      "Episode 10571; Testing Loss 0.005808158534903253; Training Loss 0.00468711166837279\n",
      "Episode 10572; Testing Loss 0.005808210946910584; Training Loss 0.004687103840982068\n",
      "Episode 10573; Testing Loss 0.005808311073603729; Training Loss 0.004687092851072859\n",
      "Episode 10574; Testing Loss 0.005808180423090951; Training Loss 0.004687078952300323\n",
      "Episode 10575; Testing Loss 0.005808116921687093; Training Loss 0.004687072047327043\n",
      "Episode 10576; Testing Loss 0.0058082657011565095; Training Loss 0.004687063850213945\n",
      "Episode 10577; Testing Loss 0.005808304784291055; Training Loss 0.004687053108038911\n",
      "Episode 10578; Testing Loss 0.005808272639942024; Training Loss 0.004687041817050392\n",
      "Episode 10579; Testing Loss 0.0058081406736169815; Training Loss 0.004687035144264159\n",
      "Episode 10580; Testing Loss 0.005808113769508862; Training Loss 0.004687026586399446\n",
      "Episode 10581; Testing Loss 0.005808113420582702; Training Loss 0.004687011902026116\n",
      "Episode 10582; Testing Loss 0.005808104817057653; Training Loss 0.004687006828903083\n",
      "Episode 10583; Testing Loss 0.005808182328911807; Training Loss 0.0046869980981973265\n",
      "Episode 10584; Testing Loss 0.005808172402193667; Training Loss 0.0046869873097055625\n",
      "Episode 10585; Testing Loss 0.005808032210321809; Training Loss 0.004686977065951321\n",
      "Episode 10586; Testing Loss 0.005807997462050651; Training Loss 0.0046869727742192314\n",
      "Episode 10587; Testing Loss 0.0058082107111486056; Training Loss 0.004686960324398293\n",
      "Episode 10588; Testing Loss 0.005808237404772422; Training Loss 0.004686949901482318\n",
      "Episode 10589; Testing Loss 0.005807976765319627; Training Loss 0.004686942667670509\n",
      "Episode 10590; Testing Loss 0.00580797784143145; Training Loss 0.00468693507615328\n",
      "Episode 10591; Testing Loss 0.005808271189179958; Training Loss 0.004686921970072589\n",
      "Episode 10592; Testing Loss 0.0058081839680707015; Training Loss 0.004686911488348643\n",
      "Episode 10593; Testing Loss 0.005807924758391498; Training Loss 0.004686904795368706\n",
      "Episode 10594; Testing Loss 0.005807897180037102; Training Loss 0.004686893274804943\n",
      "Episode 10595; Testing Loss 0.00580802376754489; Training Loss 0.0046868834093161645\n",
      "Episode 10596; Testing Loss 0.005807987485528756; Training Loss 0.004686878041263408\n",
      "Episode 10597; Testing Loss 0.0058079116191218525; Training Loss 0.004686872739454094\n",
      "Episode 10598; Testing Loss 0.005807996646705343; Training Loss 0.004686859515980321\n",
      "Episode 10599; Testing Loss 0.0058080781649485665; Training Loss 0.004686847800880419\n",
      "Episode 10600; Testing Loss 0.005807849518848004; Training Loss 0.0046868382861416985\n",
      "Episode 10601; Testing Loss 0.005807743622997781; Training Loss 0.004686829748675786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10602; Testing Loss 0.005807943479246061; Training Loss 0.0046868148900841835\n",
      "Episode 10603; Testing Loss 0.005808047879830181; Training Loss 0.004686807823949648\n",
      "Episode 10604; Testing Loss 0.005807976014303495; Training Loss 0.0046868016853173836\n",
      "Episode 10605; Testing Loss 0.005807953923903881; Training Loss 0.004686789203344518\n",
      "Episode 10606; Testing Loss 0.005807968318501764; Training Loss 0.00468678284781616\n",
      "Episode 10607; Testing Loss 0.005807857261996253; Training Loss 0.004686774916381631\n",
      "Episode 10608; Testing Loss 0.005807792613564769; Training Loss 0.004686766251356248\n",
      "Episode 10609; Testing Loss 0.005807865919318523; Training Loss 0.004686752255281251\n",
      "Episode 10610; Testing Loss 0.005807985007798415; Training Loss 0.004686747544637975\n",
      "Episode 10611; Testing Loss 0.005807934122684141; Training Loss 0.004686737069049007\n",
      "Episode 10612; Testing Loss 0.005807781354155911; Training Loss 0.004686723311053647\n",
      "Episode 10613; Testing Loss 0.005807787757921775; Training Loss 0.004686716542753746\n",
      "Episode 10614; Testing Loss 0.0058078426316932625; Training Loss 0.004686706444527812\n",
      "Episode 10615; Testing Loss 0.005807814173270608; Training Loss 0.004686694111997815\n",
      "Episode 10616; Testing Loss 0.005807812616935408; Training Loss 0.004686689484848642\n",
      "Episode 10617; Testing Loss 0.005807929996987943; Training Loss 0.004686678125951957\n",
      "Episode 10618; Testing Loss 0.0058078841827868205; Training Loss 0.0046866681672380105\n",
      "Episode 10619; Testing Loss 0.005807712977979691; Training Loss 0.004686659676474459\n",
      "Episode 10620; Testing Loss 0.0058076282928920435; Training Loss 0.00468664958219422\n",
      "Episode 10621; Testing Loss 0.005807860625212282; Training Loss 0.004686638959058374\n",
      "Episode 10622; Testing Loss 0.005807952882213508; Training Loss 0.004686631289288135\n",
      "Episode 10623; Testing Loss 0.005807749377778082; Training Loss 0.004686619710685842\n",
      "Episode 10624; Testing Loss 0.005807637894201732; Training Loss 0.004686611781269594\n",
      "Episode 10625; Testing Loss 0.005807810747183243; Training Loss 0.004686603510546477\n",
      "Episode 10626; Testing Loss 0.005807848259170713; Training Loss 0.00468659499977232\n",
      "Episode 10627; Testing Loss 0.0058076573773402865; Training Loss 0.004686583684672131\n",
      "Episode 10628; Testing Loss 0.0058077153990887375; Training Loss 0.004686571500931508\n",
      "Episode 10629; Testing Loss 0.005807769944127757; Training Loss 0.004686567895501485\n",
      "Episode 10630; Testing Loss 0.005807777780398762; Training Loss 0.004686555775228628\n",
      "Episode 10631; Testing Loss 0.0058076488608670125; Training Loss 0.004686546771700775\n",
      "Episode 10632; Testing Loss 0.005807739488172409; Training Loss 0.004686538315878222\n",
      "Episode 10633; Testing Loss 0.005807773824348823; Training Loss 0.004686525863581372\n",
      "Episode 10634; Testing Loss 0.005807683855789383; Training Loss 0.004686523528661344\n",
      "Episode 10635; Testing Loss 0.005807747782725773; Training Loss 0.004686513127067584\n",
      "Episode 10636; Testing Loss 0.005807801334341695; Training Loss 0.004686499731380217\n",
      "Episode 10637; Testing Loss 0.00580762905495785; Training Loss 0.004686491150914926\n",
      "Episode 10638; Testing Loss 0.005807442415131654; Training Loss 0.004686485584174415\n",
      "Episode 10639; Testing Loss 0.005807527222541252; Training Loss 0.004686474951131857\n",
      "Episode 10640; Testing Loss 0.005807750489974023; Training Loss 0.004686465033470428\n",
      "Episode 10641; Testing Loss 0.0058078047764058794; Training Loss 0.004686455782445827\n",
      "Episode 10642; Testing Loss 0.005807646045606257; Training Loss 0.004686447282799685\n",
      "Episode 10643; Testing Loss 0.0058075242972014055; Training Loss 0.004686436360357277\n",
      "Episode 10644; Testing Loss 0.005807697967228384; Training Loss 0.004686429714019958\n",
      "Episode 10645; Testing Loss 0.005807727527136001; Training Loss 0.004686424003043119\n",
      "Episode 10646; Testing Loss 0.005807575676383328; Training Loss 0.004686411346645039\n",
      "Episode 10647; Testing Loss 0.0058074697356416075; Training Loss 0.004686400452635469\n",
      "Episode 10648; Testing Loss 0.005807562052629636; Training Loss 0.00468639375137588\n",
      "Episode 10649; Testing Loss 0.005807726963735732; Training Loss 0.004686386193037464\n",
      "Episode 10650; Testing Loss 0.005807795242045128; Training Loss 0.004686374619359176\n",
      "Episode 10651; Testing Loss 0.005807580813476296; Training Loss 0.004686360258879301\n",
      "Episode 10652; Testing Loss 0.005807390427504841; Training Loss 0.004686356114783229\n",
      "Episode 10653; Testing Loss 0.005807544957166046; Training Loss 0.004686341100528932\n",
      "Episode 10654; Testing Loss 0.00580770640252452; Training Loss 0.004686337650445876\n",
      "Episode 10655; Testing Loss 0.005807524950096385; Training Loss 0.004686324985450335\n",
      "Episode 10656; Testing Loss 0.005807402115645543; Training Loss 0.0046863160808864966\n",
      "Episode 10657; Testing Loss 0.00580757000082557; Training Loss 0.004686311202565965\n",
      "Episode 10658; Testing Loss 0.005807614247515501; Training Loss 0.004686302358375772\n",
      "Episode 10659; Testing Loss 0.005807456255881884; Training Loss 0.004686288754599367\n",
      "Episode 10660; Testing Loss 0.0058074348258909435; Training Loss 0.004686280370793522\n",
      "Episode 10661; Testing Loss 0.005807601277845209; Training Loss 0.004686274646689158\n",
      "Episode 10662; Testing Loss 0.005807506373124807; Training Loss 0.004686260745504706\n",
      "Episode 10663; Testing Loss 0.005807354201579423; Training Loss 0.0046862511861826914\n",
      "Episode 10664; Testing Loss 0.005807417025661316; Training Loss 0.004686243142383773\n",
      "Episode 10665; Testing Loss 0.005807635259634898; Training Loss 0.0046862318746209546\n",
      "Episode 10666; Testing Loss 0.0058075606466177565; Training Loss 0.004686223252885989\n",
      "Episode 10667; Testing Loss 0.005807466510962919; Training Loss 0.004686214675708903\n",
      "Episode 10668; Testing Loss 0.005807536513496146; Training Loss 0.004686203863499699\n",
      "Episode 10669; Testing Loss 0.005807498865052768; Training Loss 0.004686194992157464\n",
      "Episode 10670; Testing Loss 0.005807363511203926; Training Loss 0.004686187542372715\n",
      "Episode 10671; Testing Loss 0.005807451791294785; Training Loss 0.004686176307275176\n",
      "Episode 10672; Testing Loss 0.005807576153529701; Training Loss 0.0046861708952164625\n",
      "Episode 10673; Testing Loss 0.005807389491586938; Training Loss 0.004686159672372493\n",
      "Episode 10674; Testing Loss 0.005807258468614469; Training Loss 0.004686150928255041\n",
      "Episode 10675; Testing Loss 0.005807452789137217; Training Loss 0.004686140516178182\n",
      "Episode 10676; Testing Loss 0.0058075290530696915; Training Loss 0.004686132814433848\n",
      "Episode 10677; Testing Loss 0.005807342460224373; Training Loss 0.004686123355249315\n",
      "Episode 10678; Testing Loss 0.00580731092001234; Training Loss 0.004686112282809974\n",
      "Episode 10679; Testing Loss 0.0058073642774109984; Training Loss 0.0046861041889379625\n",
      "Episode 10680; Testing Loss 0.005807340806297266; Training Loss 0.004686093349408487\n",
      "Episode 10681; Testing Loss 0.00580731493732266; Training Loss 0.004686084412721113\n",
      "Episode 10682; Testing Loss 0.00580746844310349; Training Loss 0.00468607538630279\n",
      "Episode 10683; Testing Loss 0.005807472063396004; Training Loss 0.004686065902220299\n",
      "Episode 10684; Testing Loss 0.005807363730953265; Training Loss 0.004686056223255031\n",
      "Episode 10685; Testing Loss 0.005807280330700204; Training Loss 0.004686047883431003\n",
      "Episode 10686; Testing Loss 0.005807374754334852; Training Loss 0.004686037752795869\n",
      "Episode 10687; Testing Loss 0.005807359021900357; Training Loss 0.004686028773810766\n",
      "Episode 10688; Testing Loss 0.005807294782706217; Training Loss 0.004686020475697885\n",
      "Episode 10689; Testing Loss 0.0058073176705498416; Training Loss 0.004686011496712208\n",
      "Episode 10690; Testing Loss 0.005807219837242209; Training Loss 0.00468600076803483\n",
      "Episode 10691; Testing Loss 0.005807300426489672; Training Loss 0.0046859911686127536\n",
      "Episode 10692; Testing Loss 0.005807400205995896; Training Loss 0.004685982206422567\n",
      "Episode 10693; Testing Loss 0.005807325854126122; Training Loss 0.004685973301472353\n",
      "Episode 10694; Testing Loss 0.005807307360371747; Training Loss 0.004685963700263849\n",
      "Episode 10695; Testing Loss 0.00580722178235585; Training Loss 0.004685956640416247\n",
      "Episode 10696; Testing Loss 0.005807257690441367; Training Loss 0.004685945228206645\n",
      "Episode 10697; Testing Loss 0.00580722639603313; Training Loss 0.004685939316988619\n",
      "Episode 10698; Testing Loss 0.0058073302065413885; Training Loss 0.0046859311386682236\n",
      "Episode 10699; Testing Loss 0.005807268946459925; Training Loss 0.004685918206338251\n",
      "Episode 10700; Testing Loss 0.005807141130511012; Training Loss 0.0046859107315183725\n",
      "Episode 10701; Testing Loss 0.005807258005211642; Training Loss 0.0046859023249022\n",
      "Episode 10702; Testing Loss 0.005807299192009775; Training Loss 0.004685891844119386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10703; Testing Loss 0.005807116341146746; Training Loss 0.004685887621362137\n",
      "Episode 10704; Testing Loss 0.005807064440405877; Training Loss 0.004685879558223254\n",
      "Episode 10705; Testing Loss 0.0058072002423490135; Training Loss 0.0046858645178226915\n",
      "Episode 10706; Testing Loss 0.005807248237812683; Training Loss 0.004685858494079948\n",
      "Episode 10707; Testing Loss 0.005807220721636451; Training Loss 0.004685848016506086\n",
      "Episode 10708; Testing Loss 0.005807233273933409; Training Loss 0.004685837775705816\n",
      "Episode 10709; Testing Loss 0.005807352693888118; Training Loss 0.004685831522689297\n",
      "Episode 10710; Testing Loss 0.005807355471908959; Training Loss 0.004685822664012471\n",
      "Episode 10711; Testing Loss 0.00580717933654808; Training Loss 0.004685811816781081\n",
      "Episode 10712; Testing Loss 0.005807044302582465; Training Loss 0.004685803524084203\n",
      "Episode 10713; Testing Loss 0.005807075691170425; Training Loss 0.00468579125569095\n",
      "Episode 10714; Testing Loss 0.005807175048834007; Training Loss 0.004685789367886056\n",
      "Episode 10715; Testing Loss 0.00580710103411203; Training Loss 0.004685778685705477\n",
      "Episode 10716; Testing Loss 0.005806929357184046; Training Loss 0.0046857650316506\n",
      "Episode 10717; Testing Loss 0.005806935974585363; Training Loss 0.004685759240930393\n",
      "Episode 10718; Testing Loss 0.005807030434005554; Training Loss 0.004685750195401272\n",
      "Episode 10719; Testing Loss 0.005806985659092212; Training Loss 0.004685737926266004\n",
      "Episode 10720; Testing Loss 0.005806943178292901; Training Loss 0.004685726708238741\n",
      "Episode 10721; Testing Loss 0.005807087840260674; Training Loss 0.004685719873450861\n",
      "Episode 10722; Testing Loss 0.005807228907425713; Training Loss 0.004685709077270308\n",
      "Episode 10723; Testing Loss 0.005807174149940542; Training Loss 0.004685699325154179\n",
      "Episode 10724; Testing Loss 0.005806995221272631; Training Loss 0.004685691836741115\n",
      "Episode 10725; Testing Loss 0.005806921412689678; Training Loss 0.004685682330715905\n",
      "Episode 10726; Testing Loss 0.005807054253654137; Training Loss 0.004685673228028668\n",
      "Episode 10727; Testing Loss 0.005807130871762381; Training Loss 0.004685664210532827\n",
      "Episode 10728; Testing Loss 0.005807025636595681; Training Loss 0.004685656066001795\n",
      "Episode 10729; Testing Loss 0.0058069755434563865; Training Loss 0.004685646957959894\n",
      "Episode 10730; Testing Loss 0.005807051372346073; Training Loss 0.004685635370116242\n",
      "Episode 10731; Testing Loss 0.005807031081025015; Training Loss 0.004685626327596234\n",
      "Episode 10732; Testing Loss 0.005806967774035424; Training Loss 0.004685616674408946\n",
      "Episode 10733; Testing Loss 0.005807014349454445; Training Loss 0.004685606039997644\n",
      "Episode 10734; Testing Loss 0.005807023078372423; Training Loss 0.0046855971259523385\n",
      "Episode 10735; Testing Loss 0.005806981036188679; Training Loss 0.004685587710748311\n",
      "Episode 10736; Testing Loss 0.005806938494933841; Training Loss 0.004685578518536298\n",
      "Episode 10737; Testing Loss 0.00580692477376237; Training Loss 0.0046855701755951235\n",
      "Episode 10738; Testing Loss 0.005806978828748029; Training Loss 0.004685562326782682\n",
      "Episode 10739; Testing Loss 0.0058069184302569285; Training Loss 0.004685551176293093\n",
      "Episode 10740; Testing Loss 0.005806933969891542; Training Loss 0.004685544886841833\n",
      "Episode 10741; Testing Loss 0.0058070400199907385; Training Loss 0.004685536487333299\n",
      "Episode 10742; Testing Loss 0.005806965185564719; Training Loss 0.004685524373343981\n",
      "Episode 10743; Testing Loss 0.0058067977579435845; Training Loss 0.0046855156778073\n",
      "Episode 10744; Testing Loss 0.005806805493317049; Training Loss 0.004685509154873251\n",
      "Episode 10745; Testing Loss 0.005806994659106751; Training Loss 0.004685497715920695\n",
      "Episode 10746; Testing Loss 0.005807041854710932; Training Loss 0.00468549197964538\n",
      "Episode 10747; Testing Loss 0.005806834063273615; Training Loss 0.004685483251151362\n",
      "Episode 10748; Testing Loss 0.005806792480220648; Training Loss 0.004685472674015264\n",
      "Episode 10749; Testing Loss 0.005806933330831234; Training Loss 0.0046854645988515485\n",
      "Episode 10750; Testing Loss 0.005807055197451855; Training Loss 0.004685454799037485\n",
      "Episode 10751; Testing Loss 0.005806963419957499; Training Loss 0.004685443957592254\n",
      "Episode 10752; Testing Loss 0.005806777327088158; Training Loss 0.004685436447676376\n",
      "Episode 10753; Testing Loss 0.005806723414243113; Training Loss 0.004685428309431939\n",
      "Episode 10754; Testing Loss 0.005806870860029944; Training Loss 0.004685416464013506\n",
      "Episode 10755; Testing Loss 0.005807001181029497; Training Loss 0.0046854077890742885\n",
      "Episode 10756; Testing Loss 0.005806943831544652; Training Loss 0.004685398802357187\n",
      "Episode 10757; Testing Loss 0.005806795795174136; Training Loss 0.004685388076882724\n",
      "Episode 10758; Testing Loss 0.005806798618481841; Training Loss 0.004685381309205275\n",
      "Episode 10759; Testing Loss 0.005806751604247306; Training Loss 0.004685369881243171\n",
      "Episode 10760; Testing Loss 0.005806703319793248; Training Loss 0.004685365196860719\n",
      "Episode 10761; Testing Loss 0.0058068713059909426; Training Loss 0.004685354938703565\n",
      "Episode 10762; Testing Loss 0.005806964667169584; Training Loss 0.004685344881214287\n",
      "Episode 10763; Testing Loss 0.005806691024892407; Training Loss 0.004685334843769223\n",
      "Episode 10764; Testing Loss 0.005806663454269513; Training Loss 0.00468532595186453\n",
      "Episode 10765; Testing Loss 0.0058068735191920936; Training Loss 0.004685316905984135\n",
      "Episode 10766; Testing Loss 0.005806956332539414; Training Loss 0.004685309061646842\n",
      "Episode 10767; Testing Loss 0.005806709746741775; Training Loss 0.004685300960711541\n",
      "Episode 10768; Testing Loss 0.005806616788727079; Training Loss 0.00468529048527771\n",
      "Episode 10769; Testing Loss 0.005806764511549803; Training Loss 0.004685280129298761\n",
      "Episode 10770; Testing Loss 0.005806833655501352; Training Loss 0.004685272007833438\n",
      "Episode 10771; Testing Loss 0.005806735359507964; Training Loss 0.00468526125677419\n",
      "Episode 10772; Testing Loss 0.005806769408265202; Training Loss 0.004685252077893885\n",
      "Episode 10773; Testing Loss 0.005806722888529663; Training Loss 0.004685241328787165\n",
      "Episode 10774; Testing Loss 0.005806666373755058; Training Loss 0.004685232967434335\n",
      "Episode 10775; Testing Loss 0.005806737595722766; Training Loss 0.004685224907419309\n",
      "Episode 10776; Testing Loss 0.0058066461829715645; Training Loss 0.004685215600257929\n",
      "Episode 10777; Testing Loss 0.005806657608253807; Training Loss 0.00468520754244049\n",
      "Episode 10778; Testing Loss 0.005806782978680647; Training Loss 0.004685197415097406\n",
      "Episode 10779; Testing Loss 0.005806734644744077; Training Loss 0.004685187389304339\n",
      "Episode 10780; Testing Loss 0.005806668174130869; Training Loss 0.004685182429292312\n",
      "Episode 10781; Testing Loss 0.005806724964428706; Training Loss 0.004685170815542981\n",
      "Episode 10782; Testing Loss 0.0058067211571333445; Training Loss 0.004685164505551082\n",
      "Episode 10783; Testing Loss 0.0058065988434700365; Training Loss 0.004685155953160996\n",
      "Episode 10784; Testing Loss 0.005806527827346155; Training Loss 0.004685142622202984\n",
      "Episode 10785; Testing Loss 0.005806599567934189; Training Loss 0.004685132914990705\n",
      "Episode 10786; Testing Loss 0.005806795868219084; Training Loss 0.004685125741889202\n",
      "Episode 10787; Testing Loss 0.0058066996561728515; Training Loss 0.004685116335963449\n",
      "Episode 10788; Testing Loss 0.0058065637396482435; Training Loss 0.004685109703726169\n",
      "Episode 10789; Testing Loss 0.005806656072723597; Training Loss 0.004685097094584808\n",
      "Episode 10790; Testing Loss 0.005806773361023124; Training Loss 0.0046850902847579414\n",
      "Episode 10791; Testing Loss 0.005806650086932531; Training Loss 0.004685080243670164\n",
      "Episode 10792; Testing Loss 0.005806511763513816; Training Loss 0.004685072793964033\n",
      "Episode 10793; Testing Loss 0.005806668651418027; Training Loss 0.004685061433167427\n",
      "Episode 10794; Testing Loss 0.005806789288279844; Training Loss 0.004685053464353425\n",
      "Episode 10795; Testing Loss 0.005806622955110767; Training Loss 0.0046850432977763885\n",
      "Episode 10796; Testing Loss 0.005806498854896481; Training Loss 0.004685036623650407\n",
      "Episode 10797; Testing Loss 0.005806642868455253; Training Loss 0.004685025872988315\n",
      "Episode 10798; Testing Loss 0.005806700364727641; Training Loss 0.004685018232363614\n",
      "Episode 10799; Testing Loss 0.005806518918981527; Training Loss 0.0046850083983951876\n",
      "Episode 10800; Testing Loss 0.005806484610375189; Training Loss 0.004685001422281753\n",
      "Episode 10801; Testing Loss 0.005806591282431657; Training Loss 0.004684993072102533\n",
      "Episode 10802; Testing Loss 0.005806587175558858; Training Loss 0.004684981932289742\n",
      "Episode 10803; Testing Loss 0.005806536144833327; Training Loss 0.004684973524094405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10804; Testing Loss 0.005806626561254907; Training Loss 0.004684966044093164\n",
      "Episode 10805; Testing Loss 0.005806716177783663; Training Loss 0.004684956016049916\n",
      "Episode 10806; Testing Loss 0.005806679664564168; Training Loss 0.004684944906504117\n",
      "Episode 10807; Testing Loss 0.00580645514249762; Training Loss 0.004684937412613179\n",
      "Episode 10808; Testing Loss 0.005806468119397296; Training Loss 0.004684925573094688\n",
      "Episode 10809; Testing Loss 0.005806631507616683; Training Loss 0.004684920032413292\n",
      "Episode 10810; Testing Loss 0.005806514018545065; Training Loss 0.004684913507059034\n",
      "Episode 10811; Testing Loss 0.0058064121580683565; Training Loss 0.0046849027850977085\n",
      "Episode 10812; Testing Loss 0.00580651339758583; Training Loss 0.004684889744375193\n",
      "Episode 10813; Testing Loss 0.00580664314827699; Training Loss 0.004684884586439592\n",
      "Episode 10814; Testing Loss 0.005806492361509299; Training Loss 0.004684875468090757\n",
      "Episode 10815; Testing Loss 0.005806413376093991; Training Loss 0.004684865198155962\n",
      "Episode 10816; Testing Loss 0.0058065514887970515; Training Loss 0.004684853992997837\n",
      "Episode 10817; Testing Loss 0.0058066121891742; Training Loss 0.004684848952216686\n",
      "Episode 10818; Testing Loss 0.0058065375744273525; Training Loss 0.004684840483138959\n",
      "Episode 10819; Testing Loss 0.005806528847551271; Training Loss 0.004684826254413224\n",
      "Episode 10820; Testing Loss 0.005806521893966906; Training Loss 0.004684821874022071\n",
      "Episode 10821; Testing Loss 0.005806338494255165; Training Loss 0.004684815617567713\n",
      "Episode 10822; Testing Loss 0.005806261678069232; Training Loss 0.004684804185623355\n",
      "Episode 10823; Testing Loss 0.005806451184916488; Training Loss 0.004684789687745677\n",
      "Episode 10824; Testing Loss 0.005806590843257928; Training Loss 0.004684784842294362\n",
      "Episode 10825; Testing Loss 0.00580652000398447; Training Loss 0.0046847773320033235\n",
      "Episode 10826; Testing Loss 0.005806417059337285; Training Loss 0.004684766418147523\n",
      "Episode 10827; Testing Loss 0.005806462988866565; Training Loss 0.004684755170852995\n",
      "Episode 10828; Testing Loss 0.005806488862531914; Training Loss 0.004684748520055264\n",
      "Episode 10829; Testing Loss 0.005806421800573202; Training Loss 0.004684741960719718\n",
      "Episode 10830; Testing Loss 0.005806376440215603; Training Loss 0.0046847294667649335\n",
      "Episode 10831; Testing Loss 0.005806357190589858; Training Loss 0.004684720296618559\n",
      "Episode 10832; Testing Loss 0.005806223642849413; Training Loss 0.00468471573678008\n",
      "Episode 10833; Testing Loss 0.005806245406704968; Training Loss 0.004684705038952094\n",
      "Episode 10834; Testing Loss 0.005806360944422707; Training Loss 0.00468469046935691\n",
      "Episode 10835; Testing Loss 0.005806401918275718; Training Loss 0.004684686222093229\n",
      "Episode 10836; Testing Loss 0.005806292136495856; Training Loss 0.004684677221704328\n",
      "Episode 10837; Testing Loss 0.0058062679574526; Training Loss 0.00468466710752186\n",
      "Episode 10838; Testing Loss 0.005806282043299027; Training Loss 0.004684657170218808\n",
      "Episode 10839; Testing Loss 0.005806350798072032; Training Loss 0.004684647554638143\n",
      "Episode 10840; Testing Loss 0.005806494247469726; Training Loss 0.004684639773132554\n",
      "Episode 10841; Testing Loss 0.005806510534489284; Training Loss 0.004684629963938585\n",
      "Episode 10842; Testing Loss 0.005806372265548684; Training Loss 0.004684619554174345\n",
      "Episode 10843; Testing Loss 0.005806171871961369; Training Loss 0.004684611149043556\n",
      "Episode 10844; Testing Loss 0.0058062411778529175; Training Loss 0.004684600110334248\n",
      "Episode 10845; Testing Loss 0.005806354111739413; Training Loss 0.004684591791885351\n",
      "Episode 10846; Testing Loss 0.005806369459591439; Training Loss 0.0046845849258883255\n",
      "Episode 10847; Testing Loss 0.005806306178187674; Training Loss 0.004684573060027298\n",
      "Episode 10848; Testing Loss 0.005806188674672476; Training Loss 0.004684563058575242\n",
      "Episode 10849; Testing Loss 0.005806213185716629; Training Loss 0.004684556872220339\n",
      "Episode 10850; Testing Loss 0.0058062696949492424; Training Loss 0.004684545139553472\n",
      "Episode 10851; Testing Loss 0.0058063323527689495; Training Loss 0.004684538978419393\n",
      "Episode 10852; Testing Loss 0.0058064481099716625; Training Loss 0.004684530707083926\n",
      "Episode 10853; Testing Loss 0.005806416749976579; Training Loss 0.0046845235357564985\n",
      "Episode 10854; Testing Loss 0.00580614585683171; Training Loss 0.004684512738355294\n",
      "Episode 10855; Testing Loss 0.005806036303326618; Training Loss 0.0046845034277390694\n",
      "Episode 10856; Testing Loss 0.005806311829794881; Training Loss 0.004684494499330546\n",
      "Episode 10857; Testing Loss 0.00580650982928616; Training Loss 0.004684489406687344\n",
      "Episode 10858; Testing Loss 0.005806202782704926; Training Loss 0.0046844738363667465\n",
      "Episode 10859; Testing Loss 0.005806108295764472; Training Loss 0.004684467383969922\n",
      "Episode 10860; Testing Loss 0.005806376729707803; Training Loss 0.004684459195407419\n",
      "Episode 10861; Testing Loss 0.0058063663176457355; Training Loss 0.004684450711521853\n",
      "Episode 10862; Testing Loss 0.005806084314679613; Training Loss 0.004684438922247289\n",
      "Episode 10863; Testing Loss 0.0058060551982190385; Training Loss 0.0046844341053690125\n",
      "Episode 10864; Testing Loss 0.0058062954188976956; Training Loss 0.004684421021225766\n",
      "Episode 10865; Testing Loss 0.005806314824958381; Training Loss 0.004684410580476612\n",
      "Episode 10866; Testing Loss 0.005806097021126552; Training Loss 0.004684402148910396\n",
      "Episode 10867; Testing Loss 0.005806032393609252; Training Loss 0.004684394064024611\n",
      "Episode 10868; Testing Loss 0.005806256848196443; Training Loss 0.004684382686519988\n",
      "Episode 10869; Testing Loss 0.005806312867831612; Training Loss 0.00468437426192078\n",
      "Episode 10870; Testing Loss 0.005806096347114026; Training Loss 0.00468436467560864\n",
      "Episode 10871; Testing Loss 0.005806015027379649; Training Loss 0.004684355956228242\n",
      "Episode 10872; Testing Loss 0.0058061783585587395; Training Loss 0.0046843470954801006\n",
      "Episode 10873; Testing Loss 0.005806283976256875; Training Loss 0.0046843382765892096\n",
      "Episode 10874; Testing Loss 0.005806246479091494; Training Loss 0.004684332634313843\n",
      "Episode 10875; Testing Loss 0.0058062295121617405; Training Loss 0.004684322659653631\n",
      "Episode 10876; Testing Loss 0.005806199222709966; Training Loss 0.004684309093376293\n",
      "Episode 10877; Testing Loss 0.005806111003527509; Training Loss 0.004684305354953147\n",
      "Episode 10878; Testing Loss 0.00580596097899102; Training Loss 0.004684300141201451\n",
      "Episode 10879; Testing Loss 0.0058060047361148575; Training Loss 0.004684288289875193\n",
      "Episode 10880; Testing Loss 0.005806095972591896; Training Loss 0.004684274899076383\n",
      "Episode 10881; Testing Loss 0.005806063536864021; Training Loss 0.004684267441155588\n",
      "Episode 10882; Testing Loss 0.005806123563423307; Training Loss 0.004684260415248169\n",
      "Episode 10883; Testing Loss 0.005806221512130184; Training Loss 0.004684250218174624\n",
      "Episode 10884; Testing Loss 0.0058061908520387695; Training Loss 0.00468423934135825\n",
      "Episode 10885; Testing Loss 0.005806057717428655; Training Loss 0.004684231686527846\n",
      "Episode 10886; Testing Loss 0.005806036016545802; Training Loss 0.004684220563918698\n",
      "Episode 10887; Testing Loss 0.005806010475510186; Training Loss 0.004684209990208391\n",
      "Episode 10888; Testing Loss 0.005806108908319964; Training Loss 0.0046842041833396126\n",
      "Episode 10889; Testing Loss 0.005806114940081929; Training Loss 0.004684193844822295\n",
      "Episode 10890; Testing Loss 0.005805959123016414; Training Loss 0.00468418315800087\n",
      "Episode 10891; Testing Loss 0.005805875034463571; Training Loss 0.004684175631497343\n",
      "Episode 10892; Testing Loss 0.005805892441836455; Training Loss 0.004684165002298881\n",
      "Episode 10893; Testing Loss 0.005806042991207891; Training Loss 0.004684157240975912\n",
      "Episode 10894; Testing Loss 0.005806106703929619; Training Loss 0.004684147685135379\n",
      "Episode 10895; Testing Loss 0.0058060273981289864; Training Loss 0.0046841373900493345\n",
      "Episode 10896; Testing Loss 0.00580590825177795; Training Loss 0.004684129204416467\n",
      "Episode 10897; Testing Loss 0.0058059752360534456; Training Loss 0.004684118920984227\n",
      "Episode 10898; Testing Loss 0.005806098342472801; Training Loss 0.004684110604731346\n",
      "Episode 10899; Testing Loss 0.005805961216488626; Training Loss 0.004684102875821911\n",
      "Episode 10900; Testing Loss 0.005805830433674215; Training Loss 0.004684093711114755\n",
      "Episode 10901; Testing Loss 0.005805885902865322; Training Loss 0.004684086369280437\n",
      "Episode 10902; Testing Loss 0.005805952780636579; Training Loss 0.0046840801183010965\n",
      "Episode 10903; Testing Loss 0.005805935838259459; Training Loss 0.004684068026667885\n",
      "Episode 10904; Testing Loss 0.005805821788422847; Training Loss 0.004684055893850741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10905; Testing Loss 0.005805907579151272; Training Loss 0.004684052045814212\n",
      "Episode 10906; Testing Loss 0.005806139225448631; Training Loss 0.004684046691641053\n",
      "Episode 10907; Testing Loss 0.005806129120098098; Training Loss 0.004684031343283771\n",
      "Episode 10908; Testing Loss 0.005805905565407647; Training Loss 0.004684024469600237\n",
      "Episode 10909; Testing Loss 0.0058057720328483576; Training Loss 0.004684020086082409\n",
      "Episode 10910; Testing Loss 0.005805876742222788; Training Loss 0.004684004179507218\n",
      "Episode 10911; Testing Loss 0.00580599954086169; Training Loss 0.004683996006474319\n",
      "Episode 10912; Testing Loss 0.0058059339888585255; Training Loss 0.004683990188241739\n",
      "Episode 10913; Testing Loss 0.0058058492991788545; Training Loss 0.004683976545507919\n",
      "Episode 10914; Testing Loss 0.005805865078549781; Training Loss 0.004683966547225105\n",
      "Episode 10915; Testing Loss 0.005805973147879056; Training Loss 0.004683959266658366\n",
      "Episode 10916; Testing Loss 0.0058059349652042555; Training Loss 0.0046839461393361394\n",
      "Episode 10917; Testing Loss 0.005805817679100921; Training Loss 0.004683943831823227\n",
      "Episode 10918; Testing Loss 0.005805893350105455; Training Loss 0.004683934091434465\n",
      "Episode 10919; Testing Loss 0.005805989043361634; Training Loss 0.004683921303538548\n",
      "Episode 10920; Testing Loss 0.005805906687196824; Training Loss 0.004683914001303632\n",
      "Episode 10921; Testing Loss 0.005805763036421256; Training Loss 0.004683903436389433\n",
      "Episode 10922; Testing Loss 0.0058058037762042936; Training Loss 0.004683893616291295\n",
      "Episode 10923; Testing Loss 0.005805998076965221; Training Loss 0.0046838895150661565\n",
      "Episode 10924; Testing Loss 0.005805846948272327; Training Loss 0.004683877491009625\n",
      "Episode 10925; Testing Loss 0.005805656337654362; Training Loss 0.004683869578995984\n",
      "Episode 10926; Testing Loss 0.005805755206694977; Training Loss 0.0046838608870891614\n",
      "Episode 10927; Testing Loss 0.005805915867012085; Training Loss 0.004683850807328622\n",
      "Episode 10928; Testing Loss 0.0058058361887007885; Training Loss 0.004683838761746226\n",
      "Episode 10929; Testing Loss 0.005805753739437039; Training Loss 0.004683833720484433\n",
      "Episode 10930; Testing Loss 0.005805769826634821; Training Loss 0.004683823529719963\n",
      "Episode 10931; Testing Loss 0.005805911102802695; Training Loss 0.004683812935939678\n",
      "Episode 10932; Testing Loss 0.005805955050652807; Training Loss 0.004683809252535966\n",
      "Episode 10933; Testing Loss 0.005805844325430426; Training Loss 0.0046837963314099096\n",
      "Episode 10934; Testing Loss 0.0058056599479144; Training Loss 0.004683785923728995\n",
      "Episode 10935; Testing Loss 0.005805611245824972; Training Loss 0.0046837781318638996\n",
      "Episode 10936; Testing Loss 0.005805736443460262; Training Loss 0.004683766368801894\n",
      "Episode 10937; Testing Loss 0.005805846269501684; Training Loss 0.004683758657877018\n",
      "Episode 10938; Testing Loss 0.005805826346718759; Training Loss 0.004683752683139981\n",
      "Episode 10939; Testing Loss 0.0058057999394481394; Training Loss 0.004683741141018902\n",
      "Episode 10940; Testing Loss 0.005805783940737855; Training Loss 0.00468373288793249\n",
      "Episode 10941; Testing Loss 0.0058057062249688126; Training Loss 0.004683724671022322\n",
      "Episode 10942; Testing Loss 0.00580567721364113; Training Loss 0.00468371390693588\n",
      "Episode 10943; Testing Loss 0.005805703905585655; Training Loss 0.004683708453521584\n",
      "Episode 10944; Testing Loss 0.0058058084398899855; Training Loss 0.0046836998418661275\n",
      "Episode 10945; Testing Loss 0.005805767995438282; Training Loss 0.00468368889900265\n",
      "Episode 10946; Testing Loss 0.005805628713555071; Training Loss 0.004683679268556676\n",
      "Episode 10947; Testing Loss 0.005805609478390712; Training Loss 0.0046836705344137875\n",
      "Episode 10948; Testing Loss 0.005805765836138976; Training Loss 0.004683659173762998\n",
      "Episode 10949; Testing Loss 0.005805858813459999; Training Loss 0.004683651200500128\n",
      "Episode 10950; Testing Loss 0.005805822422615013; Training Loss 0.004683643980494797\n",
      "Episode 10951; Testing Loss 0.005805829336394979; Training Loss 0.004683633025387956\n",
      "Episode 10952; Testing Loss 0.005805830241313682; Training Loss 0.0046836231651099895\n",
      "Episode 10953; Testing Loss 0.00580564630272722; Training Loss 0.004683614042260515\n",
      "Episode 10954; Testing Loss 0.005805634051539248; Training Loss 0.0046836050241409535\n",
      "Episode 10955; Testing Loss 0.0058057773713306865; Training Loss 0.0046835962777538225\n",
      "Episode 10956; Testing Loss 0.005805777487951762; Training Loss 0.0046835866309799775\n",
      "Episode 10957; Testing Loss 0.005805748093210816; Training Loss 0.004683579121477416\n",
      "Episode 10958; Testing Loss 0.005805861843400433; Training Loss 0.004683570824562935\n",
      "Episode 10959; Testing Loss 0.005805777895119249; Training Loss 0.004683563220279698\n",
      "Episode 10960; Testing Loss 0.005805579488117751; Training Loss 0.004683555065823387\n",
      "Episode 10961; Testing Loss 0.005805650095197382; Training Loss 0.004683544217012712\n",
      "Episode 10962; Testing Loss 0.005805843259626414; Training Loss 0.004683538752101728\n",
      "Episode 10963; Testing Loss 0.005805692905175027; Training Loss 0.004683524923808975\n",
      "Episode 10964; Testing Loss 0.005805463550956977; Training Loss 0.004683520561599798\n",
      "Episode 10965; Testing Loss 0.005805581780084309; Training Loss 0.004683509949338437\n",
      "Episode 10966; Testing Loss 0.005805830749596472; Training Loss 0.004683499932247718\n",
      "Episode 10967; Testing Loss 0.0058057874799201095; Training Loss 0.004683493192631785\n",
      "Episode 10968; Testing Loss 0.005805618955710658; Training Loss 0.00468348811220828\n",
      "Episode 10969; Testing Loss 0.005805685401686366; Training Loss 0.0046834736833697025\n",
      "Episode 10970; Testing Loss 0.005805817204308434; Training Loss 0.004683462935138913\n",
      "Episode 10971; Testing Loss 0.005805683393463111; Training Loss 0.004683456149612448\n",
      "Episode 10972; Testing Loss 0.0058055078607293165; Training Loss 0.004683446773935585\n",
      "Episode 10973; Testing Loss 0.005805601999227315; Training Loss 0.004683436362168018\n",
      "Episode 10974; Testing Loss 0.005805835271485322; Training Loss 0.004683429031407416\n",
      "Episode 10975; Testing Loss 0.005805786335321734; Training Loss 0.004683418537435275\n",
      "Episode 10976; Testing Loss 0.005805606230620572; Training Loss 0.0046834096210840725\n",
      "Episode 10977; Testing Loss 0.005805548196333474; Training Loss 0.004683403779878501\n",
      "Episode 10978; Testing Loss 0.005805581998871883; Training Loss 0.004683394203855583\n",
      "Episode 10979; Testing Loss 0.005805622809086022; Training Loss 0.004683383982705031\n",
      "Episode 10980; Testing Loss 0.00580561428434512; Training Loss 0.004683376326995587\n",
      "Episode 10981; Testing Loss 0.005805603241266353; Training Loss 0.004683367281724943\n",
      "Episode 10982; Testing Loss 0.005805590678420561; Training Loss 0.004683357963538721\n",
      "Episode 10983; Testing Loss 0.005805536538529228; Training Loss 0.004683349721055081\n",
      "Episode 10984; Testing Loss 0.005805474503557366; Training Loss 0.004683338779314425\n",
      "Episode 10985; Testing Loss 0.005805620860429605; Training Loss 0.004683327543702442\n",
      "Episode 10986; Testing Loss 0.005805759582353834; Training Loss 0.004683319167554466\n",
      "Episode 10987; Testing Loss 0.0058057153809722125; Training Loss 0.004683311651761821\n",
      "Episode 10988; Testing Loss 0.005805636978833529; Training Loss 0.0046833004880988864\n",
      "Episode 10989; Testing Loss 0.005805575982714726; Training Loss 0.00468329253764158\n",
      "Episode 10990; Testing Loss 0.005805499021555616; Training Loss 0.0046832841715798885\n",
      "Episode 10991; Testing Loss 0.005805495002667772; Training Loss 0.004683274880402526\n",
      "Episode 10992; Testing Loss 0.005805647127379483; Training Loss 0.004683265416178242\n",
      "Episode 10993; Testing Loss 0.005805715634767822; Training Loss 0.00468325990776412\n",
      "Episode 10994; Testing Loss 0.005805530043288714; Training Loss 0.004683248509400778\n",
      "Episode 10995; Testing Loss 0.005805445492863517; Training Loss 0.004683237457421833\n",
      "Episode 10996; Testing Loss 0.005805595136835265; Training Loss 0.004683233022433443\n",
      "Episode 10997; Testing Loss 0.005805621084569924; Training Loss 0.004683225040489979\n",
      "Episode 10998; Testing Loss 0.00580547579829982; Training Loss 0.004683211817189291\n",
      "Episode 10999; Testing Loss 0.005805451030788787; Training Loss 0.004683205000926314\n",
      "Episode 11000; Testing Loss 0.005805589241653168; Training Loss 0.004683197565831763\n",
      "Episode 11001; Testing Loss 0.005805621203654141; Training Loss 0.00468318588815914\n",
      "Episode 11002; Testing Loss 0.005805649333240595; Training Loss 0.00468317545338078\n",
      "Episode 11003; Testing Loss 0.00580551274337515; Training Loss 0.004683171544194038\n",
      "Episode 11004; Testing Loss 0.005805496827540149; Training Loss 0.004683163433873847\n",
      "Episode 11005; Testing Loss 0.005805562783995396; Training Loss 0.0046831520364376674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11006; Testing Loss 0.005805573872764386; Training Loss 0.004683141043633471\n",
      "Episode 11007; Testing Loss 0.005805672767108573; Training Loss 0.004683133335913145\n",
      "Episode 11008; Testing Loss 0.005805675510902386; Training Loss 0.004683123640632824\n",
      "Episode 11009; Testing Loss 0.00580554716374383; Training Loss 0.004683113418873276\n",
      "Episode 11010; Testing Loss 0.0058054542456685895; Training Loss 0.004683107196578506\n",
      "Episode 11011; Testing Loss 0.005805597845841977; Training Loss 0.004683097722010444\n",
      "Episode 11012; Testing Loss 0.005805647149926741; Training Loss 0.004683086502130198\n",
      "Episode 11013; Testing Loss 0.0058054445537935855; Training Loss 0.0046830784551333635\n",
      "Episode 11014; Testing Loss 0.005805384544967179; Training Loss 0.004683074742754265\n",
      "Episode 11015; Testing Loss 0.0058054498068602686; Training Loss 0.004683065148045631\n",
      "Episode 11016; Testing Loss 0.005805509735837939; Training Loss 0.00468305135121325\n",
      "Episode 11017; Testing Loss 0.005805417240386307; Training Loss 0.004683045470557027\n",
      "Episode 11018; Testing Loss 0.005805428536243202; Training Loss 0.00468303663753636\n",
      "Episode 11019; Testing Loss 0.005805467152733778; Training Loss 0.004683022673248824\n",
      "Episode 11020; Testing Loss 0.005805401756268407; Training Loss 0.00468301714859483\n",
      "Episode 11021; Testing Loss 0.005805294349076808; Training Loss 0.004683007254757288\n",
      "Episode 11022; Testing Loss 0.0058053430799670175; Training Loss 0.004682996939880443\n",
      "Episode 11023; Testing Loss 0.0058055099064515175; Training Loss 0.004682991230600857\n",
      "Episode 11024; Testing Loss 0.005805533196016447; Training Loss 0.0046829786003145584\n",
      "Episode 11025; Testing Loss 0.005805357339049342; Training Loss 0.004682969809758134\n",
      "Episode 11026; Testing Loss 0.005805269445065378; Training Loss 0.004682963137004255\n",
      "Episode 11027; Testing Loss 0.005805523724438462; Training Loss 0.0046829521144980315\n",
      "Episode 11028; Testing Loss 0.0058055382378309674; Training Loss 0.004682945061288698\n",
      "Episode 11029; Testing Loss 0.005805419366505487; Training Loss 0.004682935168573721\n",
      "Episode 11030; Testing Loss 0.005805335001086295; Training Loss 0.004682925702300867\n",
      "Episode 11031; Testing Loss 0.005805334533542311; Training Loss 0.004682917974543197\n",
      "Episode 11032; Testing Loss 0.005805368760478036; Training Loss 0.004682906506671725\n",
      "Episode 11033; Testing Loss 0.005805417038213139; Training Loss 0.00468290058773972\n",
      "Episode 11034; Testing Loss 0.005805522351109449; Training Loss 0.004682889444395176\n",
      "Episode 11035; Testing Loss 0.00580552463793049; Training Loss 0.004682881717081234\n",
      "Episode 11036; Testing Loss 0.0058053347048512925; Training Loss 0.004682875890107971\n",
      "Episode 11037; Testing Loss 0.005805337107118622; Training Loss 0.00468286571306939\n",
      "Episode 11038; Testing Loss 0.005805466997058757; Training Loss 0.0046828530238911\n",
      "Episode 11039; Testing Loss 0.005805477887453744; Training Loss 0.0046828483941890615\n",
      "Episode 11040; Testing Loss 0.005805371012905936; Training Loss 0.0046828412141851836\n",
      "Episode 11041; Testing Loss 0.005805390641810014; Training Loss 0.004682827278384124\n",
      "Episode 11042; Testing Loss 0.005805419945793054; Training Loss 0.004682819146611679\n",
      "Episode 11043; Testing Loss 0.005805378877732361; Training Loss 0.004682814318880828\n",
      "Episode 11044; Testing Loss 0.005805364785371768; Training Loss 0.004682803663164978\n",
      "Episode 11045; Testing Loss 0.005805460539279647; Training Loss 0.004682791130158988\n",
      "Episode 11046; Testing Loss 0.005805462001270474; Training Loss 0.0046827808649700575\n",
      "Episode 11047; Testing Loss 0.00580533165485841; Training Loss 0.004682772839029957\n",
      "Episode 11048; Testing Loss 0.005805304972339124; Training Loss 0.004682763688257103\n",
      "Episode 11049; Testing Loss 0.005805410622315214; Training Loss 0.004682757867604385\n",
      "Episode 11050; Testing Loss 0.005805395117625221; Training Loss 0.004682747698786472\n",
      "Episode 11051; Testing Loss 0.005805282170483505; Training Loss 0.004682736568963753\n",
      "Episode 11052; Testing Loss 0.00580535092143492; Training Loss 0.00468272859684588\n",
      "Episode 11053; Testing Loss 0.005805500962241165; Training Loss 0.004682720097996734\n",
      "Episode 11054; Testing Loss 0.005805332809217761; Training Loss 0.00468270752172539\n",
      "Episode 11055; Testing Loss 0.00580519174500807; Training Loss 0.004682701664378066\n",
      "Episode 11056; Testing Loss 0.005805265447701929; Training Loss 0.004682691679688691\n",
      "Episode 11057; Testing Loss 0.00580536121272414; Training Loss 0.0046826824193469405\n",
      "Episode 11058; Testing Loss 0.005805334379663085; Training Loss 0.004682675509364191\n",
      "Episode 11059; Testing Loss 0.005805367640401573; Training Loss 0.004682663684274004\n",
      "Episode 11060; Testing Loss 0.005805384980521124; Training Loss 0.004682657773643879\n",
      "Episode 11061; Testing Loss 0.005805264426081565; Training Loss 0.004682650253313008\n",
      "Episode 11062; Testing Loss 0.0058052254005734535; Training Loss 0.004682638180141448\n",
      "Episode 11063; Testing Loss 0.005805339808492688; Training Loss 0.004682629279096747\n",
      "Episode 11064; Testing Loss 0.005805416531141592; Training Loss 0.004682619640306015\n",
      "Episode 11065; Testing Loss 0.005805350444950297; Training Loss 0.004682609953016525\n",
      "Episode 11066; Testing Loss 0.005805200157624828; Training Loss 0.004682601746375698\n",
      "Episode 11067; Testing Loss 0.005805233863730174; Training Loss 0.004682592435642247\n",
      "Episode 11068; Testing Loss 0.005805368897960555; Training Loss 0.004682582117904443\n",
      "Episode 11069; Testing Loss 0.00580542115726579; Training Loss 0.004682573525728259\n",
      "Episode 11070; Testing Loss 0.005805315207466794; Training Loss 0.004682564069227981\n",
      "Episode 11071; Testing Loss 0.005805254487932246; Training Loss 0.004682554192136263\n",
      "Episode 11072; Testing Loss 0.005805284628362843; Training Loss 0.004682548988024388\n",
      "Episode 11073; Testing Loss 0.005805183513382379; Training Loss 0.004682538582491394\n",
      "Episode 11074; Testing Loss 0.0058051615197942915; Training Loss 0.004682528860531238\n",
      "Episode 11075; Testing Loss 0.005805316758851397; Training Loss 0.004682519533472791\n",
      "Episode 11076; Testing Loss 0.005805410643434892; Training Loss 0.004682510576247593\n",
      "Episode 11077; Testing Loss 0.005805312874357381; Training Loss 0.004682501457954662\n",
      "Episode 11078; Testing Loss 0.0058053074382206195; Training Loss 0.004682493314765306\n",
      "Episode 11079; Testing Loss 0.005805244908701173; Training Loss 0.004682484603794048\n",
      "Episode 11080; Testing Loss 0.005805175936761937; Training Loss 0.00468247442181696\n",
      "Episode 11081; Testing Loss 0.0058052222339474956; Training Loss 0.004682466476105737\n",
      "Episode 11082; Testing Loss 0.005805320317096365; Training Loss 0.004682458503779699\n",
      "Episode 11083; Testing Loss 0.005805196391185301; Training Loss 0.004682446146728498\n",
      "Episode 11084; Testing Loss 0.0058051209239897905; Training Loss 0.0046824420468037885\n",
      "Episode 11085; Testing Loss 0.005805266333657149; Training Loss 0.004682432360842366\n",
      "Episode 11086; Testing Loss 0.005805329492246814; Training Loss 0.0046824192999402455\n",
      "Episode 11087; Testing Loss 0.0058052759370790545; Training Loss 0.004682415938827119\n",
      "Episode 11088; Testing Loss 0.005805267321392017; Training Loss 0.0046824055001152675\n",
      "Episode 11089; Testing Loss 0.005805261764689599; Training Loss 0.004682394197307237\n",
      "Episode 11090; Testing Loss 0.005805168873918753; Training Loss 0.004682386331336155\n",
      "Episode 11091; Testing Loss 0.005805145605034796; Training Loss 0.004682376085199317\n",
      "Episode 11092; Testing Loss 0.005805285228963351; Training Loss 0.004682367798456574\n",
      "Episode 11093; Testing Loss 0.005805419921649516; Training Loss 0.004682363090721063\n",
      "Episode 11094; Testing Loss 0.00580531151099226; Training Loss 0.004682349449118382\n",
      "Episode 11095; Testing Loss 0.005805106546157922; Training Loss 0.004682340556192183\n",
      "Episode 11096; Testing Loss 0.005805181311187493; Training Loss 0.004682331504398894\n",
      "Episode 11097; Testing Loss 0.005805433063830595; Training Loss 0.004682323428890441\n",
      "Episode 11098; Testing Loss 0.005805359271727707; Training Loss 0.004682313141434458\n",
      "Episode 11099; Testing Loss 0.005805063851489555; Training Loss 0.004682304995669774\n",
      "Episode 11100; Testing Loss 0.005805126914704892; Training Loss 0.004682293141182707\n",
      "Episode 11101; Testing Loss 0.00580528231913093; Training Loss 0.004682288201693004\n",
      "Episode 11102; Testing Loss 0.005805136014404724; Training Loss 0.004682277416878505\n",
      "Episode 11103; Testing Loss 0.005805034342102855; Training Loss 0.004682269742622396\n",
      "Episode 11104; Testing Loss 0.005805208001938629; Training Loss 0.004682257352386887\n",
      "Episode 11105; Testing Loss 0.005805345527928513; Training Loss 0.004682251486963215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11106; Testing Loss 0.005805189308403739; Training Loss 0.004682240740119487\n",
      "Episode 11107; Testing Loss 0.005805126492870567; Training Loss 0.004682231289315087\n",
      "Episode 11108; Testing Loss 0.005805236946353356; Training Loss 0.004682224558837611\n",
      "Episode 11109; Testing Loss 0.005805261132834956; Training Loss 0.004682213429521288\n",
      "Episode 11110; Testing Loss 0.005805183464339802; Training Loss 0.004682205142607917\n",
      "Episode 11111; Testing Loss 0.005805122061881605; Training Loss 0.004682196095442562\n",
      "Episode 11112; Testing Loss 0.005805140824682841; Training Loss 0.004682185268176132\n",
      "Episode 11113; Testing Loss 0.005805096069297801; Training Loss 0.004682176193916648\n",
      "Episode 11114; Testing Loss 0.0058050972259685325; Training Loss 0.0046821670623626195\n",
      "Episode 11115; Testing Loss 0.005805143012894604; Training Loss 0.004682157745526411\n",
      "Episode 11116; Testing Loss 0.005805105314812959; Training Loss 0.004682149060961465\n",
      "Episode 11117; Testing Loss 0.005805161520891954; Training Loss 0.004682139491719239\n",
      "Episode 11118; Testing Loss 0.005805228262218727; Training Loss 0.004682132508406289\n",
      "Episode 11119; Testing Loss 0.005805170258925646; Training Loss 0.004682122517424943\n",
      "Episode 11120; Testing Loss 0.0058051610752151974; Training Loss 0.004682112550877719\n",
      "Episode 11121; Testing Loss 0.00580523735376523; Training Loss 0.004682104915542909\n",
      "Episode 11122; Testing Loss 0.00580513108450761; Training Loss 0.004682095798527296\n",
      "Episode 11123; Testing Loss 0.005805086370373822; Training Loss 0.004682088138616728\n",
      "Episode 11124; Testing Loss 0.005805066762944682; Training Loss 0.004682079515320939\n",
      "Episode 11125; Testing Loss 0.005805065128569158; Training Loss 0.004682067922232867\n",
      "Episode 11126; Testing Loss 0.005805120726640215; Training Loss 0.004682065148826249\n",
      "Episode 11127; Testing Loss 0.005805189523127076; Training Loss 0.004682054557589479\n",
      "Episode 11128; Testing Loss 0.005805183831454569; Training Loss 0.0046820419376449995\n",
      "Episode 11129; Testing Loss 0.0058051234970821096; Training Loss 0.004682034062376503\n",
      "Episode 11130; Testing Loss 0.005805081531915796; Training Loss 0.004682023112904596\n",
      "Episode 11131; Testing Loss 0.00580514918992511; Training Loss 0.004682015828240924\n",
      "Episode 11132; Testing Loss 0.005805102329201107; Training Loss 0.004682005917931016\n",
      "Episode 11133; Testing Loss 0.005805005530315615; Training Loss 0.004681998141330934\n",
      "Episode 11134; Testing Loss 0.005805156777417821; Training Loss 0.004681990777883357\n",
      "Episode 11135; Testing Loss 0.00580536384409732; Training Loss 0.004681983032652763\n",
      "Episode 11136; Testing Loss 0.005805188690315003; Training Loss 0.004681972275192466\n",
      "Episode 11137; Testing Loss 0.005804998322323407; Training Loss 0.004681966884691021\n",
      "Episode 11138; Testing Loss 0.005805143859795102; Training Loss 0.004681952059628897\n",
      "Episode 11139; Testing Loss 0.005805280651278669; Training Loss 0.0046819470676370024\n",
      "Episode 11140; Testing Loss 0.005805104124889919; Training Loss 0.004681937684582077\n",
      "Episode 11141; Testing Loss 0.005804945670418695; Training Loss 0.004681926300206166\n",
      "Episode 11142; Testing Loss 0.005804946086783967; Training Loss 0.004681917546428318\n",
      "Episode 11143; Testing Loss 0.005805019791956878; Training Loss 0.004681909196509112\n",
      "Episode 11144; Testing Loss 0.00580499020748691; Training Loss 0.0046818984408926664\n",
      "Episode 11145; Testing Loss 0.005804974956106163; Training Loss 0.004681893600720809\n",
      "Episode 11146; Testing Loss 0.005805104358578323; Training Loss 0.004681881582722659\n",
      "Episode 11147; Testing Loss 0.005805159521581915; Training Loss 0.0046818739103692926\n",
      "Episode 11148; Testing Loss 0.005805001340978894; Training Loss 0.004681866565938073\n",
      "Episode 11149; Testing Loss 0.005804941698406492; Training Loss 0.004681856907182701\n",
      "Episode 11150; Testing Loss 0.00580511142058924; Training Loss 0.0046818462257336\n",
      "Episode 11151; Testing Loss 0.005805184403462346; Training Loss 0.004681837647811315\n",
      "Episode 11152; Testing Loss 0.005805006265666174; Training Loss 0.00468182850396187\n",
      "Episode 11153; Testing Loss 0.005804918928397209; Training Loss 0.004681821507694541\n",
      "Episode 11154; Testing Loss 0.005805011244547456; Training Loss 0.004681812855788746\n",
      "Episode 11155; Testing Loss 0.0058050087395502825; Training Loss 0.004681801192266016\n",
      "Episode 11156; Testing Loss 0.005804890273276092; Training Loss 0.00468179498408282\n",
      "Episode 11157; Testing Loss 0.005804902775208672; Training Loss 0.004681790236314513\n",
      "Episode 11158; Testing Loss 0.00580506688285981; Training Loss 0.004681776108463512\n",
      "Episode 11159; Testing Loss 0.005805101022400012; Training Loss 0.004681765268824752\n",
      "Episode 11160; Testing Loss 0.005804946884027986; Training Loss 0.004681760907458951\n",
      "Episode 11161; Testing Loss 0.00580491350120133; Training Loss 0.004681751133928916\n",
      "Episode 11162; Testing Loss 0.005805013193894083; Training Loss 0.004681737401318814\n",
      "Episode 11163; Testing Loss 0.005805135859799393; Training Loss 0.004681733515862845\n",
      "Episode 11164; Testing Loss 0.005805102542984957; Training Loss 0.004681724608596139\n",
      "Episode 11165; Testing Loss 0.005804971311090883; Training Loss 0.004681714525513802\n",
      "Episode 11166; Testing Loss 0.0058049138851914; Training Loss 0.004681703387155271\n",
      "Episode 11167; Testing Loss 0.005805039278235217; Training Loss 0.004681692473950061\n",
      "Episode 11168; Testing Loss 0.005805129579215606; Training Loss 0.0046816867029284654\n",
      "Episode 11169; Testing Loss 0.005805071713283515; Training Loss 0.004681676459383618\n",
      "Episode 11170; Testing Loss 0.005804905493108787; Training Loss 0.0046816661473215795\n",
      "Episode 11171; Testing Loss 0.005804866872447316; Training Loss 0.004681658444586076\n",
      "Episode 11172; Testing Loss 0.0058049487299549184; Training Loss 0.004681649586617545\n",
      "Episode 11173; Testing Loss 0.005804965691406232; Training Loss 0.004681637078608029\n",
      "Episode 11174; Testing Loss 0.005804973086307596; Training Loss 0.0046816282994940304\n",
      "Episode 11175; Testing Loss 0.005804983857805078; Training Loss 0.004681620202008904\n",
      "Episode 11176; Testing Loss 0.005805059050123838; Training Loss 0.004681610842038659\n",
      "Episode 11177; Testing Loss 0.0058050222078253385; Training Loss 0.004681601234478989\n",
      "Episode 11178; Testing Loss 0.005804874531795085; Training Loss 0.004681591986845735\n",
      "Episode 11179; Testing Loss 0.005804839419078925; Training Loss 0.004681583517298822\n",
      "Episode 11180; Testing Loss 0.005804861898365038; Training Loss 0.004681573326039487\n",
      "Episode 11181; Testing Loss 0.005804970204499796; Training Loss 0.004681567470705773\n",
      "Episode 11182; Testing Loss 0.005804870777794959; Training Loss 0.004681556789334301\n",
      "Episode 11183; Testing Loss 0.005804795685465129; Training Loss 0.004681550408761759\n",
      "Episode 11184; Testing Loss 0.00580497339392605; Training Loss 0.004681538413163775\n",
      "Episode 11185; Testing Loss 0.005805095805045342; Training Loss 0.004681532018765985\n",
      "Episode 11186; Testing Loss 0.0058048950904428945; Training Loss 0.00468152154513609\n",
      "Episode 11187; Testing Loss 0.0058048055985583545; Training Loss 0.004681512335717498\n",
      "Episode 11188; Testing Loss 0.005805034059364376; Training Loss 0.004681501851593597\n",
      "Episode 11189; Testing Loss 0.005805125069534768; Training Loss 0.0046814960115746035\n",
      "Episode 11190; Testing Loss 0.005804835793643873; Training Loss 0.004681484105269755\n",
      "Episode 11191; Testing Loss 0.005804706042899725; Training Loss 0.004681479456299551\n",
      "Episode 11192; Testing Loss 0.005805002544618178; Training Loss 0.004681464520306124\n",
      "Episode 11193; Testing Loss 0.0058051469398056185; Training Loss 0.0046814603162017975\n",
      "Episode 11194; Testing Loss 0.005804891474064833; Training Loss 0.004681446898300031\n",
      "Episode 11195; Testing Loss 0.005804775622448087; Training Loss 0.0046814408501326035\n",
      "Episode 11196; Testing Loss 0.005804963103177352; Training Loss 0.004681430097651424\n",
      "Episode 11197; Testing Loss 0.005804986452571693; Training Loss 0.004681421339228675\n",
      "Episode 11198; Testing Loss 0.005804777933442631; Training Loss 0.004681411167204925\n",
      "Episode 11199; Testing Loss 0.005804769290403833; Training Loss 0.004681402101879867\n",
      "Episode 11200; Testing Loss 0.005804937183935456; Training Loss 0.004681393871731695\n",
      "Episode 11201; Testing Loss 0.005804945161648785; Training Loss 0.00468138302541639\n",
      "Episode 11202; Testing Loss 0.005804836774956596; Training Loss 0.0046813741656463554\n",
      "Episode 11203; Testing Loss 0.0058048650864680066; Training Loss 0.004681365933295334\n",
      "Episode 11204; Testing Loss 0.0058048841629510465; Training Loss 0.004681355601490077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11205; Testing Loss 0.005804891639500332; Training Loss 0.004681347371617954\n",
      "Episode 11206; Testing Loss 0.005804816164276701; Training Loss 0.004681338101598707\n",
      "Episode 11207; Testing Loss 0.005804871282957058; Training Loss 0.0046813285833302316\n",
      "Episode 11208; Testing Loss 0.005804958686677097; Training Loss 0.004681322590344313\n",
      "Episode 11209; Testing Loss 0.005804836944951503; Training Loss 0.0046813104397982796\n",
      "Episode 11210; Testing Loss 0.00580475628224424; Training Loss 0.0046813079757413644\n",
      "Episode 11211; Testing Loss 0.005804962973892099; Training Loss 0.004681296967865779\n",
      "Episode 11212; Testing Loss 0.0058050377175410065; Training Loss 0.004681289308710734\n",
      "Episode 11213; Testing Loss 0.005804709221592214; Training Loss 0.004681278035175136\n",
      "Episode 11214; Testing Loss 0.005804551497933933; Training Loss 0.004681270801240136\n",
      "Episode 11215; Testing Loss 0.0058048172033538115; Training Loss 0.004681257073745168\n",
      "Episode 11216; Testing Loss 0.005805167496109196; Training Loss 0.0046812551658298755\n",
      "Episode 11217; Testing Loss 0.005804988275387919; Training Loss 0.004681240296389428\n",
      "Episode 11218; Testing Loss 0.0058046051604888835; Training Loss 0.004681232115857949\n",
      "Episode 11219; Testing Loss 0.005804623877388721; Training Loss 0.004681225459281404\n",
      "Episode 11220; Testing Loss 0.005804837816033406; Training Loss 0.004681214425364082\n",
      "Episode 11221; Testing Loss 0.005804899035159594; Training Loss 0.004681203628302325\n",
      "Episode 11222; Testing Loss 0.005804773457068521; Training Loss 0.004681196702369403\n",
      "Episode 11223; Testing Loss 0.005804805763932057; Training Loss 0.004681185487305769\n",
      "Episode 11224; Testing Loss 0.005804858299774256; Training Loss 0.004681176429164371\n",
      "Episode 11225; Testing Loss 0.005804800446487606; Training Loss 0.004681166623827173\n",
      "Episode 11226; Testing Loss 0.005804823959019421; Training Loss 0.00468115692530799\n",
      "Episode 11227; Testing Loss 0.005804907484784593; Training Loss 0.004681148636544065\n",
      "Episode 11228; Testing Loss 0.005804853246934089; Training Loss 0.004681139461205831\n",
      "Episode 11229; Testing Loss 0.005804797119913758; Training Loss 0.004681130257536042\n",
      "Episode 11230; Testing Loss 0.005804722312834682; Training Loss 0.004681120924230724\n",
      "Episode 11231; Testing Loss 0.005804741574217169; Training Loss 0.004681114263943475\n",
      "Episode 11232; Testing Loss 0.005804865351479191; Training Loss 0.0046811056574225475\n",
      "Episode 11233; Testing Loss 0.005804912800434315; Training Loss 0.004681094405152012\n",
      "Episode 11234; Testing Loss 0.00580480217095001; Training Loss 0.004681089612126059\n",
      "Episode 11235; Testing Loss 0.005804717987130482; Training Loss 0.004681079430830006\n",
      "Episode 11236; Testing Loss 0.005804738519963633; Training Loss 0.004681066664534768\n",
      "Episode 11237; Testing Loss 0.005804829167853892; Training Loss 0.004681059509756828\n",
      "Episode 11238; Testing Loss 0.005804863506976028; Training Loss 0.004681048825562753\n",
      "Episode 11239; Testing Loss 0.00580476992280199; Training Loss 0.004681040726104581\n",
      "Episode 11240; Testing Loss 0.005804615735509611; Training Loss 0.004681033288844749\n",
      "Episode 11241; Testing Loss 0.005804719014794851; Training Loss 0.004681021636779264\n",
      "Episode 11242; Testing Loss 0.0058048732431698205; Training Loss 0.004681015723448236\n",
      "Episode 11243; Testing Loss 0.005804784022698228; Training Loss 0.004681007058113342\n",
      "Episode 11244; Testing Loss 0.005804692926598592; Training Loss 0.00468099447373384\n",
      "Episode 11245; Testing Loss 0.0058047335991053195; Training Loss 0.004680988921319548\n",
      "Episode 11246; Testing Loss 0.005804766399576703; Training Loss 0.004680979593510875\n",
      "Episode 11247; Testing Loss 0.005804729814084658; Training Loss 0.004680970580812138\n",
      "Episode 11248; Testing Loss 0.005804742004218166; Training Loss 0.0046809610207280555\n",
      "Episode 11249; Testing Loss 0.005804811140866981; Training Loss 0.004680952479015556\n",
      "Episode 11250; Testing Loss 0.005804676245905215; Training Loss 0.004680943632846721\n",
      "Episode 11251; Testing Loss 0.005804550948203852; Training Loss 0.004680933037087279\n",
      "Episode 11252; Testing Loss 0.005804721508270309; Training Loss 0.004680923168545055\n",
      "Episode 11253; Testing Loss 0.005804840308829375; Training Loss 0.004680915946258584\n",
      "Episode 11254; Testing Loss 0.005804686711829928; Training Loss 0.004680905701174827\n",
      "Episode 11255; Testing Loss 0.005804544999262719; Training Loss 0.004680898241514515\n",
      "Episode 11256; Testing Loss 0.005804666491497534; Training Loss 0.004680888143296522\n",
      "Episode 11257; Testing Loss 0.005804895472631944; Training Loss 0.004680882572523413\n",
      "Episode 11258; Testing Loss 0.00580478433794753; Training Loss 0.00468086945981503\n",
      "Episode 11259; Testing Loss 0.00580456813944765; Training Loss 0.0046808668876242646\n",
      "Episode 11260; Testing Loss 0.005804726408835632; Training Loss 0.004680853513669085\n",
      "Episode 11261; Testing Loss 0.005804930302139503; Training Loss 0.004680847308343852\n",
      "Episode 11262; Testing Loss 0.005804694163987549; Training Loss 0.0046808342176356136\n",
      "Episode 11263; Testing Loss 0.005804412006400117; Training Loss 0.004680830599693213\n",
      "Episode 11264; Testing Loss 0.005804616289244165; Training Loss 0.004680815858570242\n",
      "Episode 11265; Testing Loss 0.005804874804057512; Training Loss 0.004680811705017929\n",
      "Episode 11266; Testing Loss 0.005804731247395374; Training Loss 0.004680799193088978\n",
      "Episode 11267; Testing Loss 0.005804563209057123; Training Loss 0.004680791521435255\n",
      "Episode 11268; Testing Loss 0.005804692242141495; Training Loss 0.004680781610578338\n",
      "Episode 11269; Testing Loss 0.005804864156431014; Training Loss 0.004680774281407615\n",
      "Episode 11270; Testing Loss 0.005804694524418948; Training Loss 0.0046807618280248625\n",
      "Episode 11271; Testing Loss 0.005804459701284495; Training Loss 0.004680756774855902\n",
      "Episode 11272; Testing Loss 0.0058046287345262584; Training Loss 0.00468074411494169\n",
      "Episode 11273; Testing Loss 0.005804853994593946; Training Loss 0.004680737744425369\n",
      "Episode 11274; Testing Loss 0.005804694805211834; Training Loss 0.004680725855523923\n",
      "Episode 11275; Testing Loss 0.0058045898420398204; Training Loss 0.004680721934696109\n",
      "Episode 11276; Testing Loss 0.005804780111202708; Training Loss 0.004680709120073021\n",
      "Episode 11277; Testing Loss 0.005804779587118995; Training Loss 0.004680704300624356\n",
      "Episode 11278; Testing Loss 0.00580450082964969; Training Loss 0.004680692755721915\n",
      "Episode 11279; Testing Loss 0.005804461796587308; Training Loss 0.004680684123249611\n",
      "Episode 11280; Testing Loss 0.005804692159802792; Training Loss 0.004680674424672666\n",
      "Episode 11281; Testing Loss 0.0058047202631581595; Training Loss 0.004680665275645487\n",
      "Episode 11282; Testing Loss 0.005804551090702837; Training Loss 0.004680656618043412\n",
      "Episode 11283; Testing Loss 0.005804531117550786; Training Loss 0.004680647293837264\n",
      "Episode 11284; Testing Loss 0.005804625089487264; Training Loss 0.004680637532575552\n",
      "Episode 11285; Testing Loss 0.005804693107938607; Training Loss 0.004680627729903803\n",
      "Episode 11286; Testing Loss 0.005804703148889209; Training Loss 0.0046806187677103294\n",
      "Episode 11287; Testing Loss 0.00580459831166603; Training Loss 0.004680610864315556\n",
      "Episode 11288; Testing Loss 0.005804632248645745; Training Loss 0.004680602296010534\n",
      "Episode 11289; Testing Loss 0.005804661038376276; Training Loss 0.004680592398821919\n",
      "Episode 11290; Testing Loss 0.005804652334251936; Training Loss 0.0046805830835407285\n",
      "Episode 11291; Testing Loss 0.0058046336387345; Training Loss 0.004680574305461093\n",
      "Episode 11292; Testing Loss 0.00580463303466527; Training Loss 0.004680565331930813\n",
      "Episode 11293; Testing Loss 0.005804607794834226; Training Loss 0.004680557689022819\n",
      "Episode 11294; Testing Loss 0.0058046571093297835; Training Loss 0.004680548572998538\n",
      "Episode 11295; Testing Loss 0.00580463365026578; Training Loss 0.004680538323583865\n",
      "Episode 11296; Testing Loss 0.0058046180415378896; Training Loss 0.004680532140424148\n",
      "Episode 11297; Testing Loss 0.005804745518888792; Training Loss 0.004680522064008001\n",
      "Episode 11298; Testing Loss 0.005804729463202081; Training Loss 0.004680513715525311\n",
      "Episode 11299; Testing Loss 0.00580455247122731; Training Loss 0.004680506532886184\n",
      "Episode 11300; Testing Loss 0.0058044898260612016; Training Loss 0.004680494481547824\n",
      "Episode 11301; Testing Loss 0.005804608751386804; Training Loss 0.0046804871421938414\n",
      "Episode 11302; Testing Loss 0.0058046072189914975; Training Loss 0.004680477075323956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11303; Testing Loss 0.005804486093485506; Training Loss 0.004680469114711478\n",
      "Episode 11304; Testing Loss 0.005804485370325347; Training Loss 0.00468046087409867\n",
      "Episode 11305; Testing Loss 0.00580456273857774; Training Loss 0.0046804491584978136\n",
      "Episode 11306; Testing Loss 0.005804675261424669; Training Loss 0.004680445632637949\n",
      "Episode 11307; Testing Loss 0.005804546885818297; Training Loss 0.004680434695174042\n",
      "Episode 11308; Testing Loss 0.005804462110568609; Training Loss 0.004680423818440231\n",
      "Episode 11309; Testing Loss 0.005804642432380161; Training Loss 0.00468041579395243\n",
      "Episode 11310; Testing Loss 0.005804676191951893; Training Loss 0.0046804070576542175\n",
      "Episode 11311; Testing Loss 0.00580455009736151; Training Loss 0.004680394913689852\n",
      "Episode 11312; Testing Loss 0.00580450872708618; Training Loss 0.004680389561479153\n",
      "Episode 11313; Testing Loss 0.005804588371086271; Training Loss 0.004680378834184329\n",
      "Episode 11314; Testing Loss 0.005804569901694625; Training Loss 0.004680369394349034\n",
      "Episode 11315; Testing Loss 0.0058045708162306475; Training Loss 0.004680360129425123\n",
      "Episode 11316; Testing Loss 0.005804676563818612; Training Loss 0.004680351470573157\n",
      "Episode 11317; Testing Loss 0.005804631605012676; Training Loss 0.004680342756675095\n",
      "Episode 11318; Testing Loss 0.005804477147972717; Training Loss 0.004680334185834536\n",
      "Episode 11319; Testing Loss 0.00580447621694811; Training Loss 0.0046803274959752094\n",
      "Episode 11320; Testing Loss 0.005804681016720518; Training Loss 0.004680318243417061\n",
      "Episode 11321; Testing Loss 0.005804565549609095; Training Loss 0.004680305922488328\n",
      "Episode 11322; Testing Loss 0.005804339245978198; Training Loss 0.004680302524746329\n",
      "Episode 11323; Testing Loss 0.005804472094368727; Training Loss 0.004680290812531213\n",
      "Episode 11324; Testing Loss 0.005804719942869909; Training Loss 0.004680281254146908\n",
      "Episode 11325; Testing Loss 0.0058046439347928525; Training Loss 0.004680272941135216\n",
      "Episode 11326; Testing Loss 0.00580453609977071; Training Loss 0.004680263548199566\n",
      "Episode 11327; Testing Loss 0.005804549211724987; Training Loss 0.004680252712007165\n",
      "Episode 11328; Testing Loss 0.005804613672896388; Training Loss 0.004680246264874966\n",
      "Episode 11329; Testing Loss 0.0058044637410479254; Training Loss 0.004680236227908557\n",
      "Episode 11330; Testing Loss 0.005804278415236287; Training Loss 0.004680228449213297\n",
      "Episode 11331; Testing Loss 0.0058043941389276646; Training Loss 0.004680220528193356\n",
      "Episode 11332; Testing Loss 0.005804591469890885; Training Loss 0.004680211474767895\n",
      "Episode 11333; Testing Loss 0.005804467755208164; Training Loss 0.00468019934276264\n",
      "Episode 11334; Testing Loss 0.005804270329666379; Training Loss 0.0046801922263719625\n",
      "Episode 11335; Testing Loss 0.005804333717764892; Training Loss 0.004680181781300109\n",
      "Episode 11336; Testing Loss 0.005804512573733628; Training Loss 0.004680172637243404\n",
      "Episode 11337; Testing Loss 0.0058046942522818916; Training Loss 0.0046801700235756276\n",
      "Episode 11338; Testing Loss 0.005804506258339807; Training Loss 0.004680156864262221\n",
      "Episode 11339; Testing Loss 0.005804368521382047; Training Loss 0.00468014945495932\n",
      "Episode 11340; Testing Loss 0.005804503770667814; Training Loss 0.0046801413009284155\n",
      "Episode 11341; Testing Loss 0.005804542218718928; Training Loss 0.004680136748928646\n",
      "Episode 11342; Testing Loss 0.005804312487411905; Training Loss 0.004680123184148227\n",
      "Episode 11343; Testing Loss 0.005804221908524888; Training Loss 0.00468011572386342\n",
      "Episode 11344; Testing Loss 0.005804489595286345; Training Loss 0.004680107790567104\n",
      "Episode 11345; Testing Loss 0.005804741326676215; Training Loss 0.0046800992620081505\n",
      "Episode 11346; Testing Loss 0.005804590229290786; Training Loss 0.004680085638465602\n",
      "Episode 11347; Testing Loss 0.005804239057174793; Training Loss 0.00468008063580786\n",
      "Episode 11348; Testing Loss 0.005804312908399801; Training Loss 0.004680070453382607\n",
      "Episode 11349; Testing Loss 0.005804755462245256; Training Loss 0.004680059173181568\n",
      "Episode 11350; Testing Loss 0.005804760320494006; Training Loss 0.004680052685903792\n",
      "Episode 11351; Testing Loss 0.0058043996009158666; Training Loss 0.004680042726413961\n",
      "Episode 11352; Testing Loss 0.00580431749613477; Training Loss 0.004680031101959878\n",
      "Episode 11353; Testing Loss 0.005804502679582132; Training Loss 0.004680025972245735\n",
      "Episode 11354; Testing Loss 0.005804471849667264; Training Loss 0.004680016814949884\n",
      "Episode 11355; Testing Loss 0.005804314147961325; Training Loss 0.004680005769029866\n",
      "Episode 11356; Testing Loss 0.00580433375490994; Training Loss 0.004679996190931483\n",
      "Episode 11357; Testing Loss 0.005804540547125213; Training Loss 0.004679991047470937\n",
      "Episode 11358; Testing Loss 0.005804547429377173; Training Loss 0.0046799799109341\n",
      "Episode 11359; Testing Loss 0.0058043960407684675; Training Loss 0.004679968718404281\n",
      "Episode 11360; Testing Loss 0.005804305818509855; Training Loss 0.004679964034895014\n",
      "Episode 11361; Testing Loss 0.0058043143257898175; Training Loss 0.0046799567016842115\n",
      "Episode 11362; Testing Loss 0.005804398980220692; Training Loss 0.004679945526275061\n",
      "Episode 11363; Testing Loss 0.005804381231051902; Training Loss 0.004679931871337368\n",
      "Episode 11364; Testing Loss 0.00580443716623235; Training Loss 0.004679924436001167\n",
      "Episode 11365; Testing Loss 0.00580452249203541; Training Loss 0.004679915988647507\n",
      "Episode 11366; Testing Loss 0.005804490739740732; Training Loss 0.00467990647563489\n",
      "Episode 11367; Testing Loss 0.005804370955190561; Training Loss 0.004679898862376251\n",
      "Episode 11368; Testing Loss 0.005804347996659111; Training Loss 0.004679888275327412\n",
      "Episode 11369; Testing Loss 0.005804383930064985; Training Loss 0.004679880553621802\n",
      "Episode 11370; Testing Loss 0.005804369624063321; Training Loss 0.0046798707608582315\n",
      "Episode 11371; Testing Loss 0.005804334536865644; Training Loss 0.004679861953359428\n",
      "Episode 11372; Testing Loss 0.005804373820330106; Training Loss 0.004679853789670986\n",
      "Episode 11373; Testing Loss 0.0058044845825180125; Training Loss 0.0046798459043538064\n",
      "Episode 11374; Testing Loss 0.005804447205061602; Training Loss 0.004679836004783453\n",
      "Episode 11375; Testing Loss 0.00580433715602023; Training Loss 0.00467982643916645\n",
      "Episode 11376; Testing Loss 0.0058043305682975695; Training Loss 0.0046798172757297925\n",
      "Episode 11377; Testing Loss 0.005804402741854575; Training Loss 0.004679809183837615\n",
      "Episode 11378; Testing Loss 0.005804394510309715; Training Loss 0.004679798809618102\n",
      "Episode 11379; Testing Loss 0.005804269665817698; Training Loss 0.004679789913622151\n",
      "Episode 11380; Testing Loss 0.005804391984268077; Training Loss 0.004679783468799263\n",
      "Episode 11381; Testing Loss 0.005804506462063014; Training Loss 0.004679773621673222\n",
      "Episode 11382; Testing Loss 0.005804400266858794; Training Loss 0.004679761367319814\n",
      "Episode 11383; Testing Loss 0.0058042210747666235; Training Loss 0.004679755709167438\n",
      "Episode 11384; Testing Loss 0.005804268844676981; Training Loss 0.004679748678378952\n",
      "Episode 11385; Testing Loss 0.0058043838145195835; Training Loss 0.004679734579004404\n",
      "Episode 11386; Testing Loss 0.0058044089085145445; Training Loss 0.004679733275712113\n",
      "Episode 11387; Testing Loss 0.005804480304301219; Training Loss 0.00467972626036464\n",
      "Episode 11388; Testing Loss 0.0058044545427929075; Training Loss 0.004679709767506566\n",
      "Episode 11389; Testing Loss 0.005804346467138718; Training Loss 0.0046797024465656265\n",
      "Episode 11390; Testing Loss 0.005804224428543259; Training Loss 0.0046797000083323576\n",
      "Episode 11391; Testing Loss 0.005804209374126277; Training Loss 0.00467969031247078\n",
      "Episode 11392; Testing Loss 0.0058042673003682; Training Loss 0.004679677259516624\n",
      "Episode 11393; Testing Loss 0.005804407371956111; Training Loss 0.004679665540532813\n",
      "Episode 11394; Testing Loss 0.00580442647572982; Training Loss 0.004679661374590483\n",
      "Episode 11395; Testing Loss 0.005804262565787068; Training Loss 0.0046796524931205\n",
      "Episode 11396; Testing Loss 0.005804232767128286; Training Loss 0.004679638659415463\n",
      "Episode 11397; Testing Loss 0.005804418726836592; Training Loss 0.004679632139676724\n",
      "Episode 11398; Testing Loss 0.005804572580809895; Training Loss 0.004679625421333179\n",
      "Episode 11399; Testing Loss 0.005804415729768656; Training Loss 0.004679608713463545\n",
      "Episode 11400; Testing Loss 0.005804245820396798; Training Loss 0.0046796055956930946\n",
      "Episode 11401; Testing Loss 0.005804323808939199; Training Loss 0.004679595879845478\n",
      "Episode 11402; Testing Loss 0.00580446659724109; Training Loss 0.004679584858834197\n",
      "Episode 11403; Testing Loss 0.005804396506553163; Training Loss 0.004679577628814636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11404; Testing Loss 0.00580429961724935; Training Loss 0.004679568907747088\n",
      "Episode 11405; Testing Loss 0.00580429183766428; Training Loss 0.004679558386730771\n",
      "Episode 11406; Testing Loss 0.005804291645560884; Training Loss 0.004679550366675916\n",
      "Episode 11407; Testing Loss 0.0058042549124260835; Training Loss 0.004679541291998489\n",
      "Episode 11408; Testing Loss 0.005804245418624802; Training Loss 0.004679530923325864\n",
      "Episode 11409; Testing Loss 0.005804230231213209; Training Loss 0.004679523224828663\n",
      "Episode 11410; Testing Loss 0.005804342983568825; Training Loss 0.004679515957101127\n",
      "Episode 11411; Testing Loss 0.0058044704499648665; Training Loss 0.004679506523724767\n",
      "Episode 11412; Testing Loss 0.005804338661072114; Training Loss 0.0046794958582271\n",
      "Episode 11413; Testing Loss 0.005804214467311637; Training Loss 0.004679485952909125\n",
      "Episode 11414; Testing Loss 0.00580422912715184; Training Loss 0.004679478077862142\n",
      "Episode 11415; Testing Loss 0.005804268085855373; Training Loss 0.004679468102365627\n",
      "Episode 11416; Testing Loss 0.005804246658696641; Training Loss 0.004679458394613014\n",
      "Episode 11417; Testing Loss 0.005804292418543465; Training Loss 0.004679448945202335\n",
      "Episode 11418; Testing Loss 0.005804351961869585; Training Loss 0.004679440617251816\n",
      "Episode 11419; Testing Loss 0.005804378754404802; Training Loss 0.00467943184460447\n",
      "Episode 11420; Testing Loss 0.005804320754786516; Training Loss 0.004679421404884391\n",
      "Episode 11421; Testing Loss 0.005804304687246841; Training Loss 0.004679413536502348\n",
      "Episode 11422; Testing Loss 0.005804253823583265; Training Loss 0.004679403691687535\n",
      "Episode 11423; Testing Loss 0.005804315312106269; Training Loss 0.004679393730560748\n",
      "Episode 11424; Testing Loss 0.005804386620350753; Training Loss 0.0046793853607125305\n",
      "Episode 11425; Testing Loss 0.005804369895309558; Training Loss 0.004679374875546038\n",
      "Episode 11426; Testing Loss 0.005804286122883238; Training Loss 0.004679367280782383\n",
      "Episode 11427; Testing Loss 0.00580438420615156; Training Loss 0.004679357244159802\n",
      "Episode 11428; Testing Loss 0.005804486429073549; Training Loss 0.004679349013825516\n",
      "Episode 11429; Testing Loss 0.005804365184941141; Training Loss 0.00467934034794914\n",
      "Episode 11430; Testing Loss 0.005804317095855573; Training Loss 0.004679329749837516\n",
      "Episode 11431; Testing Loss 0.005804377649403301; Training Loss 0.0046793200232696525\n",
      "Episode 11432; Testing Loss 0.005804559745399816; Training Loss 0.004679314455494312\n",
      "Episode 11433; Testing Loss 0.005804545533668394; Training Loss 0.004679304511408989\n",
      "Episode 11434; Testing Loss 0.005804428996256424; Training Loss 0.00467929702418405\n",
      "Episode 11435; Testing Loss 0.005804469360804718; Training Loss 0.004679286475877054\n",
      "Episode 11436; Testing Loss 0.005804587316057175; Training Loss 0.004679280089511876\n",
      "Episode 11437; Testing Loss 0.0058044391972150375; Training Loss 0.00467927002769928\n",
      "Episode 11438; Testing Loss 0.0058042790857947765; Training Loss 0.004679259679481536\n",
      "Episode 11439; Testing Loss 0.005804387547821724; Training Loss 0.00467925402065856\n",
      "Episode 11440; Testing Loss 0.005804623329159688; Training Loss 0.004679244190481035\n",
      "Episode 11441; Testing Loss 0.0058045630874126895; Training Loss 0.004679231710269535\n",
      "Episode 11442; Testing Loss 0.0058043382325676275; Training Loss 0.004679228752305618\n",
      "Episode 11443; Testing Loss 0.005804411117094597; Training Loss 0.004679218190520287\n",
      "Episode 11444; Testing Loss 0.0058045477646193454; Training Loss 0.004679206675956051\n",
      "Episode 11445; Testing Loss 0.005804454664264621; Training Loss 0.004679199212930162\n",
      "Episode 11446; Testing Loss 0.005804268868580269; Training Loss 0.004679191699114895\n",
      "Episode 11447; Testing Loss 0.005804319834599598; Training Loss 0.004679178836574629\n",
      "Episode 11448; Testing Loss 0.0058043810544618; Training Loss 0.0046791722020282775\n",
      "Episode 11449; Testing Loss 0.005804426202970506; Training Loss 0.0046791651400267175\n",
      "Episode 11450; Testing Loss 0.005804458941676845; Training Loss 0.004679153765976682\n",
      "Episode 11451; Testing Loss 0.005804430903172355; Training Loss 0.004679143177364543\n",
      "Episode 11452; Testing Loss 0.005804263662633006; Training Loss 0.004679135474767243\n",
      "Episode 11453; Testing Loss 0.005804204096663892; Training Loss 0.004679125889296616\n",
      "Episode 11454; Testing Loss 0.005804310346681758; Training Loss 0.004679117492811541\n",
      "Episode 11455; Testing Loss 0.005804449841494801; Training Loss 0.004679109268670701\n",
      "Episode 11456; Testing Loss 0.005804473941968589; Training Loss 0.004679100300875051\n",
      "Episode 11457; Testing Loss 0.005804401480020843; Training Loss 0.004679088576066543\n",
      "Episode 11458; Testing Loss 0.005804330555419322; Training Loss 0.004679082768286804\n",
      "Episode 11459; Testing Loss 0.005804321783659889; Training Loss 0.0046790735462544095\n",
      "Episode 11460; Testing Loss 0.005804316379564882; Training Loss 0.004679061192421986\n",
      "Episode 11461; Testing Loss 0.005804283729317445; Training Loss 0.004679057147814883\n",
      "Episode 11462; Testing Loss 0.005804393650970998; Training Loss 0.004679047612883897\n",
      "Episode 11463; Testing Loss 0.005804378775113967; Training Loss 0.004679035578572229\n",
      "Episode 11464; Testing Loss 0.0058041675485072265; Training Loss 0.0046790270322952816\n",
      "Episode 11465; Testing Loss 0.005804113910809814; Training Loss 0.004679017603390634\n",
      "Episode 11466; Testing Loss 0.005804382219335959; Training Loss 0.0046790068900904366\n",
      "Episode 11467; Testing Loss 0.005804523786645349; Training Loss 0.0046790000706381785\n",
      "Episode 11468; Testing Loss 0.005804411259341737; Training Loss 0.0046789898533517085\n",
      "Episode 11469; Testing Loss 0.005804256829369132; Training Loss 0.004678983404261697\n",
      "Episode 11470; Testing Loss 0.005804402132902961; Training Loss 0.004678971634468552\n",
      "Episode 11471; Testing Loss 0.005804450212073443; Training Loss 0.004678961334210322\n",
      "Episode 11472; Testing Loss 0.005804344259979031; Training Loss 0.004678957794767091\n",
      "Episode 11473; Testing Loss 0.0058042846214385385; Training Loss 0.004678946241708989\n",
      "Episode 11474; Testing Loss 0.005804326704420669; Training Loss 0.004678935630606054\n",
      "Episode 11475; Testing Loss 0.005804384484087182; Training Loss 0.004678927220253619\n",
      "Episode 11476; Testing Loss 0.005804339562282007; Training Loss 0.004678919769048234\n",
      "Episode 11477; Testing Loss 0.005804402462981176; Training Loss 0.004678908930952269\n",
      "Episode 11478; Testing Loss 0.005804505739798931; Training Loss 0.004678903499507122\n",
      "Episode 11479; Testing Loss 0.005804359581296354; Training Loss 0.004678893873713072\n",
      "Episode 11480; Testing Loss 0.0058042324477456; Training Loss 0.004678884291744631\n",
      "Episode 11481; Testing Loss 0.0058043296151329095; Training Loss 0.004678875090233032\n",
      "Episode 11482; Testing Loss 0.005804459920427635; Training Loss 0.004678865308209431\n",
      "Episode 11483; Testing Loss 0.0058043294011325235; Training Loss 0.004678854894271535\n",
      "Episode 11484; Testing Loss 0.005804193615492898; Training Loss 0.004678849914527787\n",
      "Episode 11485; Testing Loss 0.005804281283887078; Training Loss 0.004678838159244036\n",
      "Episode 11486; Testing Loss 0.005804349758485597; Training Loss 0.004678827578021532\n",
      "Episode 11487; Testing Loss 0.005804337823615846; Training Loss 0.004678820087975488\n",
      "Episode 11488; Testing Loss 0.00580428173406738; Training Loss 0.004678811354919632\n",
      "Episode 11489; Testing Loss 0.005804164968355992; Training Loss 0.004678800869810227\n",
      "Episode 11490; Testing Loss 0.005804217911344144; Training Loss 0.004678793004378457\n",
      "Episode 11491; Testing Loss 0.005804432489055284; Training Loss 0.004678785203537636\n",
      "Episode 11492; Testing Loss 0.005804324731895379; Training Loss 0.004678774548667559\n",
      "Episode 11493; Testing Loss 0.005804078261820331; Training Loss 0.004678767030269853\n",
      "Episode 11494; Testing Loss 0.0058042266549490065; Training Loss 0.004678755015727754\n",
      "Episode 11495; Testing Loss 0.005804430296595161; Training Loss 0.004678747802394656\n",
      "Episode 11496; Testing Loss 0.005804331228454818; Training Loss 0.004678737109326247\n",
      "Episode 11497; Testing Loss 0.005804209662866557; Training Loss 0.004678728247718398\n",
      "Episode 11498; Testing Loss 0.0058042707056472015; Training Loss 0.00467872011090992\n",
      "Episode 11499; Testing Loss 0.005804287125757228; Training Loss 0.0046787104938936525\n",
      "Episode 11500; Testing Loss 0.00580424943902009; Training Loss 0.00467870280765185\n",
      "Episode 11501; Testing Loss 0.005804194440363832; Training Loss 0.004678692909252196\n",
      "Episode 11502; Testing Loss 0.005804169975708462; Training Loss 0.004678683570904852\n",
      "Episode 11503; Testing Loss 0.005804258910384193; Training Loss 0.004678674166379595\n",
      "Episode 11504; Testing Loss 0.005804303433034528; Training Loss 0.004678665079972349\n",
      "Episode 11505; Testing Loss 0.0058042409092198275; Training Loss 0.0046786574989230445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11506; Testing Loss 0.00580428176316975; Training Loss 0.004678646538357693\n",
      "Episode 11507; Testing Loss 0.00580428649649016; Training Loss 0.0046786382277971125\n",
      "Episode 11508; Testing Loss 0.005804264171819032; Training Loss 0.0046786302260176885\n",
      "Episode 11509; Testing Loss 0.00580432506766318; Training Loss 0.004678622032220432\n",
      "Episode 11510; Testing Loss 0.005804214967587781; Training Loss 0.004678611811510668\n",
      "Episode 11511; Testing Loss 0.005804150312117306; Training Loss 0.004678604263377299\n",
      "Episode 11512; Testing Loss 0.0058043228761334685; Training Loss 0.004678594137918774\n",
      "Episode 11513; Testing Loss 0.005804464542002384; Training Loss 0.004678586579026972\n",
      "Episode 11514; Testing Loss 0.005804302984335215; Training Loss 0.004678576084099399\n",
      "Episode 11515; Testing Loss 0.005804098480011736; Training Loss 0.004678568912199932\n",
      "Episode 11516; Testing Loss 0.005804238373907627; Training Loss 0.004678557161132702\n",
      "Episode 11517; Testing Loss 0.005804383519016774; Training Loss 0.004678552699439286\n",
      "Episode 11518; Testing Loss 0.00580420394352934; Training Loss 0.004678539371017761\n",
      "Episode 11519; Testing Loss 0.0058040961385015305; Training Loss 0.004678534642865858\n",
      "Episode 11520; Testing Loss 0.005804315540141813; Training Loss 0.004678526835662985\n",
      "Episode 11521; Testing Loss 0.005804370402466534; Training Loss 0.004678517798986258\n",
      "Episode 11522; Testing Loss 0.005804212107091249; Training Loss 0.00467850464431732\n",
      "Episode 11523; Testing Loss 0.005804155834342638; Training Loss 0.004678495784526048\n",
      "Episode 11524; Testing Loss 0.005804248701401801; Training Loss 0.004678486694164223\n",
      "Episode 11525; Testing Loss 0.005804298248567534; Training Loss 0.00467847837974044\n",
      "Episode 11526; Testing Loss 0.005804157029898309; Training Loss 0.004678469210991273\n",
      "Episode 11527; Testing Loss 0.00580417643519862; Training Loss 0.004678460847692922\n",
      "Episode 11528; Testing Loss 0.005804230206010241; Training Loss 0.004678450764218001\n",
      "Episode 11529; Testing Loss 0.005804213722510342; Training Loss 0.004678442607208416\n",
      "Episode 11530; Testing Loss 0.005804202469330187; Training Loss 0.00467843209691158\n",
      "Episode 11531; Testing Loss 0.005804248040081766; Training Loss 0.004678425970083378\n",
      "Episode 11532; Testing Loss 0.005804181915132212; Training Loss 0.0046784177694477085\n",
      "Episode 11533; Testing Loss 0.005804120769666535; Training Loss 0.004678406149038298\n",
      "Episode 11534; Testing Loss 0.0058041502264290475; Training Loss 0.004678399948250397\n",
      "Episode 11535; Testing Loss 0.005804318558488523; Training Loss 0.004678392408935154\n",
      "Episode 11536; Testing Loss 0.005804297302460738; Training Loss 0.004678380157585544\n",
      "Episode 11537; Testing Loss 0.005804112186395674; Training Loss 0.004678373300592909\n",
      "Episode 11538; Testing Loss 0.005804024838212709; Training Loss 0.004678365652293818\n",
      "Episode 11539; Testing Loss 0.005804168795325644; Training Loss 0.004678353335943939\n",
      "Episode 11540; Testing Loss 0.005804297052600755; Training Loss 0.004678345524126079\n",
      "Episode 11541; Testing Loss 0.00580429252603431; Training Loss 0.004678338515547522\n",
      "Episode 11542; Testing Loss 0.0058042705764228516; Training Loss 0.004678327830588844\n",
      "Episode 11543; Testing Loss 0.005804198434303487; Training Loss 0.004678319898694801\n",
      "Episode 11544; Testing Loss 0.005804191764209304; Training Loss 0.004678309789754951\n",
      "Episode 11545; Testing Loss 0.00580422644579507; Training Loss 0.004678298681126918\n",
      "Episode 11546; Testing Loss 0.005804231910817558; Training Loss 0.004678289025196029\n",
      "Episode 11547; Testing Loss 0.005804182936437225; Training Loss 0.004678281464601744\n",
      "Episode 11548; Testing Loss 0.005804211515438274; Training Loss 0.004678273337814189\n",
      "Episode 11549; Testing Loss 0.005804258586162848; Training Loss 0.004678265269616571\n",
      "Episode 11550; Testing Loss 0.0058041937267969195; Training Loss 0.004678254561730324\n",
      "Episode 11551; Testing Loss 0.005804104769075067; Training Loss 0.004678244653059301\n",
      "Episode 11552; Testing Loss 0.005804136722786839; Training Loss 0.004678237894760644\n",
      "Episode 11553; Testing Loss 0.0058041807720522216; Training Loss 0.004678227792946292\n",
      "Episode 11554; Testing Loss 0.005804235779357989; Training Loss 0.004678216913168723\n",
      "Episode 11555; Testing Loss 0.005804212497594083; Training Loss 0.00467820917901819\n",
      "Episode 11556; Testing Loss 0.005804141873515405; Training Loss 0.004678201538733977\n",
      "Episode 11557; Testing Loss 0.005804063182503572; Training Loss 0.004678191616498142\n",
      "Episode 11558; Testing Loss 0.005804139469515087; Training Loss 0.0046781814646150765\n",
      "Episode 11559; Testing Loss 0.005804300707824803; Training Loss 0.004678172544931837\n",
      "Episode 11560; Testing Loss 0.0058042150054524325; Training Loss 0.004678162485534495\n",
      "Episode 11561; Testing Loss 0.005804178068343074; Training Loss 0.00467815482263744\n",
      "Episode 11562; Testing Loss 0.005804214963355104; Training Loss 0.004678145514379302\n",
      "Episode 11563; Testing Loss 0.005804277416863684; Training Loss 0.004678138261665643\n",
      "Episode 11564; Testing Loss 0.005804261205407039; Training Loss 0.004678127576838272\n",
      "Episode 11565; Testing Loss 0.005804162718496641; Training Loss 0.0046781179766239266\n",
      "Episode 11566; Testing Loss 0.0058041882130491105; Training Loss 0.0046781080517149035\n",
      "Episode 11567; Testing Loss 0.005804276310632167; Training Loss 0.0046780997424468096\n",
      "Episode 11568; Testing Loss 0.005804234263348432; Training Loss 0.004678092084131907\n",
      "Episode 11569; Testing Loss 0.005804204471850452; Training Loss 0.004678082918505687\n",
      "Episode 11570; Testing Loss 0.005804165219683585; Training Loss 0.004678075647359834\n",
      "Episode 11571; Testing Loss 0.005804243092415398; Training Loss 0.004678067442655789\n",
      "Episode 11572; Testing Loss 0.005804183646345589; Training Loss 0.004678057057165014\n",
      "Episode 11573; Testing Loss 0.005804098714496192; Training Loss 0.004678047246278011\n",
      "Episode 11574; Testing Loss 0.005804182060955685; Training Loss 0.004678035508360498\n",
      "Episode 11575; Testing Loss 0.00580427958528024; Training Loss 0.004678029095525409\n",
      "Episode 11576; Testing Loss 0.005804336458931963; Training Loss 0.004678020862792419\n",
      "Episode 11577; Testing Loss 0.005804169104545366; Training Loss 0.00467801211248038\n",
      "Episode 11578; Testing Loss 0.005804117811031563; Training Loss 0.004678002984213397\n",
      "Episode 11579; Testing Loss 0.005804113049455173; Training Loss 0.004677992304454781\n",
      "Episode 11580; Testing Loss 0.0058041379993529285; Training Loss 0.004677984198597533\n",
      "Episode 11581; Testing Loss 0.005804236067345253; Training Loss 0.004677977092119623\n",
      "Episode 11582; Testing Loss 0.005804226797624128; Training Loss 0.00467796688334612\n",
      "Episode 11583; Testing Loss 0.005804110491860411; Training Loss 0.004677956717529299\n",
      "Episode 11584; Testing Loss 0.005804114932736619; Training Loss 0.004677948400431529\n",
      "Episode 11585; Testing Loss 0.005804244950002989; Training Loss 0.004677940803287778\n",
      "Episode 11586; Testing Loss 0.005804211316491218; Training Loss 0.004677931687567807\n",
      "Episode 11587; Testing Loss 0.005804076818401631; Training Loss 0.004677921837906866\n",
      "Episode 11588; Testing Loss 0.005804109345158247; Training Loss 0.004677911684840058\n",
      "Episode 11589; Testing Loss 0.005804157825782714; Training Loss 0.004677902482332907\n",
      "Episode 11590; Testing Loss 0.005804121299737401; Training Loss 0.0046778936720523094\n",
      "Episode 11591; Testing Loss 0.005804037378765222; Training Loss 0.0046778834996729045\n",
      "Episode 11592; Testing Loss 0.0058041215322014806; Training Loss 0.004677874295767838\n",
      "Episode 11593; Testing Loss 0.005804170247627577; Training Loss 0.0046778668883838675\n",
      "Episode 11594; Testing Loss 0.005804130917237151; Training Loss 0.004677856179413932\n",
      "Episode 11595; Testing Loss 0.005804051442501869; Training Loss 0.004677848960912381\n",
      "Episode 11596; Testing Loss 0.005804005431536117; Training Loss 0.004677839442307723\n",
      "Episode 11597; Testing Loss 0.0058040914041203135; Training Loss 0.00467782889600029\n",
      "Episode 11598; Testing Loss 0.005804229611068976; Training Loss 0.004677820737573681\n",
      "Episode 11599; Testing Loss 0.0058042498531229265; Training Loss 0.004677811246836386\n",
      "Episode 11600; Testing Loss 0.005804079673935997; Training Loss 0.004677800939636353\n",
      "Episode 11601; Testing Loss 0.005804000618950103; Training Loss 0.0046777954554443274\n",
      "Episode 11602; Testing Loss 0.0058040362471046105; Training Loss 0.004677784492315956\n",
      "Episode 11603; Testing Loss 0.005804185178576208; Training Loss 0.004677776261049312\n",
      "Episode 11604; Testing Loss 0.005804316162966177; Training Loss 0.004677766681739435\n",
      "Episode 11605; Testing Loss 0.0058041792007313174; Training Loss 0.004677757149855352\n",
      "Episode 11606; Testing Loss 0.005804031555666248; Training Loss 0.0046777505679507485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11607; Testing Loss 0.005804120157499322; Training Loss 0.004677738966923205\n",
      "Episode 11608; Testing Loss 0.005804255283740805; Training Loss 0.004677731233544672\n",
      "Episode 11609; Testing Loss 0.005804230796236928; Training Loss 0.004677722252455697\n",
      "Episode 11610; Testing Loss 0.005804009999660682; Training Loss 0.004677712328250126\n",
      "Episode 11611; Testing Loss 0.005804011817942539; Training Loss 0.0046777024264225585\n",
      "Episode 11612; Testing Loss 0.005804215944732858; Training Loss 0.004677695338070418\n",
      "Episode 11613; Testing Loss 0.005804224507925464; Training Loss 0.0046776854736346974\n",
      "Episode 11614; Testing Loss 0.005804112785938687; Training Loss 0.004677675057690665\n",
      "Episode 11615; Testing Loss 0.005804085899384218; Training Loss 0.0046776699038371325\n",
      "Episode 11616; Testing Loss 0.005804022644223686; Training Loss 0.004677661171100514\n",
      "Episode 11617; Testing Loss 0.00580401559285255; Training Loss 0.00467765168245262\n",
      "Episode 11618; Testing Loss 0.005804056435986979; Training Loss 0.004677641023704474\n",
      "Episode 11619; Testing Loss 0.005804275538461927; Training Loss 0.004677631014161768\n",
      "Episode 11620; Testing Loss 0.00580430802821754; Training Loss 0.0046776250148763505\n",
      "Episode 11621; Testing Loss 0.005804094499096925; Training Loss 0.004677615705331032\n",
      "Episode 11622; Testing Loss 0.005804001440772381; Training Loss 0.004677604460304764\n",
      "Episode 11623; Testing Loss 0.005804197415873864; Training Loss 0.004677595556423103\n",
      "Episode 11624; Testing Loss 0.005804264317595436; Training Loss 0.0046775862524953\n",
      "Episode 11625; Testing Loss 0.005804134352000408; Training Loss 0.00467757699191917\n",
      "Episode 11626; Testing Loss 0.0058040916416803034; Training Loss 0.004677569054868834\n",
      "Episode 11627; Testing Loss 0.005804091639916027; Training Loss 0.004677559765894375\n",
      "Episode 11628; Testing Loss 0.005804102760117698; Training Loss 0.004677549411644372\n",
      "Episode 11629; Testing Loss 0.005804107439684727; Training Loss 0.0046775416268027056\n",
      "Episode 11630; Testing Loss 0.005804164873811624; Training Loss 0.004677531552863352\n",
      "Episode 11631; Testing Loss 0.0058041053569666224; Training Loss 0.004677522213777924\n",
      "Episode 11632; Testing Loss 0.0058040144391850726; Training Loss 0.004677514627428591\n",
      "Episode 11633; Testing Loss 0.005803977759727043; Training Loss 0.004677504666051999\n",
      "Episode 11634; Testing Loss 0.005804099000609712; Training Loss 0.00467749775632461\n",
      "Episode 11635; Testing Loss 0.005804214431401022; Training Loss 0.004677488388941154\n",
      "Episode 11636; Testing Loss 0.00580420715770626; Training Loss 0.004677478570297066\n",
      "Episode 11637; Testing Loss 0.0058041019077312425; Training Loss 0.004677473419179548\n",
      "Episode 11638; Testing Loss 0.005804110375214202; Training Loss 0.0046774652679873125\n",
      "Episode 11639; Testing Loss 0.005804111011817614; Training Loss 0.004677451122842212\n",
      "Episode 11640; Testing Loss 0.005804091854673404; Training Loss 0.004677441647301054\n",
      "Episode 11641; Testing Loss 0.005804029572090428; Training Loss 0.004677433196963357\n",
      "Episode 11642; Testing Loss 0.0058040672154960355; Training Loss 0.004677425081183985\n",
      "Episode 11643; Testing Loss 0.005804001961088804; Training Loss 0.004677417545314332\n",
      "Episode 11644; Testing Loss 0.005803912145339241; Training Loss 0.004677407300170966\n",
      "Episode 11645; Testing Loss 0.005804009046770583; Training Loss 0.004677400101804608\n",
      "Episode 11646; Testing Loss 0.0058042318874623544; Training Loss 0.004677393239762473\n",
      "Episode 11647; Testing Loss 0.0058041680293453735; Training Loss 0.004677379678243202\n",
      "Episode 11648; Testing Loss 0.005803881492015638; Training Loss 0.004677373132475636\n",
      "Episode 11649; Testing Loss 0.005803959065121399; Training Loss 0.004677364440737066\n",
      "Episode 11650; Testing Loss 0.005804251250833033; Training Loss 0.004677355872829451\n",
      "Episode 11651; Testing Loss 0.005804241196642133; Training Loss 0.004677344089233484\n",
      "Episode 11652; Testing Loss 0.005803978666604864; Training Loss 0.004677338410919375\n",
      "Episode 11653; Testing Loss 0.005803916529272582; Training Loss 0.0046773292642975\n",
      "Episode 11654; Testing Loss 0.005804092555346679; Training Loss 0.004677317221168872\n",
      "Episode 11655; Testing Loss 0.005804121589144497; Training Loss 0.004677307981775284\n",
      "Episode 11656; Testing Loss 0.005803963052629225; Training Loss 0.004677301055685919\n",
      "Episode 11657; Testing Loss 0.005804044271328523; Training Loss 0.004677289478173642\n",
      "Episode 11658; Testing Loss 0.005804210082079958; Training Loss 0.004677284061323931\n",
      "Episode 11659; Testing Loss 0.0058040738450103255; Training Loss 0.0046772742583271005\n",
      "Episode 11660; Testing Loss 0.005803924081170111; Training Loss 0.004677264202709719\n",
      "Episode 11661; Testing Loss 0.005803968074627324; Training Loss 0.0046772530348099\n",
      "Episode 11662; Testing Loss 0.005804182113547394; Training Loss 0.004677247669466494\n",
      "Episode 11663; Testing Loss 0.005804200720236423; Training Loss 0.00467723771832474\n",
      "Episode 11664; Testing Loss 0.005804000113584929; Training Loss 0.004677224968133064\n",
      "Episode 11665; Testing Loss 0.005803864566695617; Training Loss 0.004677221484079357\n",
      "Episode 11666; Testing Loss 0.0058038977394229115; Training Loss 0.004677212114799191\n",
      "Episode 11667; Testing Loss 0.005803985783495209; Training Loss 0.004677201469575235\n",
      "Episode 11668; Testing Loss 0.005803955576342489; Training Loss 0.004677191129760601\n",
      "Episode 11669; Testing Loss 0.005803965730078886; Training Loss 0.004677185326587568\n",
      "Episode 11670; Testing Loss 0.0058040828419224375; Training Loss 0.004677173814125075\n",
      "Episode 11671; Testing Loss 0.005804092980275732; Training Loss 0.004677164361717216\n",
      "Episode 11672; Testing Loss 0.005803961978373767; Training Loss 0.004677158128757967\n",
      "Episode 11673; Testing Loss 0.005803924507503849; Training Loss 0.004677150199713272\n",
      "Episode 11674; Testing Loss 0.005804016377080336; Training Loss 0.0046771386693540504\n",
      "Episode 11675; Testing Loss 0.005804066712946561; Training Loss 0.0046771272090618075\n",
      "Episode 11676; Testing Loss 0.0058040452729149; Training Loss 0.0046771184358290864\n",
      "Episode 11677; Testing Loss 0.005803965136481714; Training Loss 0.00467711312202017\n",
      "Episode 11678; Testing Loss 0.005803979497325309; Training Loss 0.004677101668863012\n",
      "Episode 11679; Testing Loss 0.005804001932818475; Training Loss 0.004677092442814705\n",
      "Episode 11680; Testing Loss 0.005803939985501669; Training Loss 0.004677085527177805\n",
      "Episode 11681; Testing Loss 0.005803811429120504; Training Loss 0.004677076824219073\n",
      "Episode 11682; Testing Loss 0.0058038131591052495; Training Loss 0.004677065085224901\n",
      "Episode 11683; Testing Loss 0.005803875706566259; Training Loss 0.0046770569259749805\n",
      "Episode 11684; Testing Loss 0.005804000665678115; Training Loss 0.004677050483674505\n",
      "Episode 11685; Testing Loss 0.005804024298739005; Training Loss 0.004677043129751925\n",
      "Episode 11686; Testing Loss 0.005803963588960946; Training Loss 0.004677028903874143\n",
      "Episode 11687; Testing Loss 0.0058039116206205854; Training Loss 0.004677019946996841\n",
      "Episode 11688; Testing Loss 0.005803924169376601; Training Loss 0.004677010995636433\n",
      "Episode 11689; Testing Loss 0.005803970133275966; Training Loss 0.0046770062855954544\n",
      "Episode 11690; Testing Loss 0.005803928249795315; Training Loss 0.004676996432148821\n",
      "Episode 11691; Testing Loss 0.005803828234470268; Training Loss 0.0046769846546833175\n",
      "Episode 11692; Testing Loss 0.005803849231086306; Training Loss 0.00467697484447719\n",
      "Episode 11693; Testing Loss 0.005803946305027729; Training Loss 0.004676968423073918\n",
      "Episode 11694; Testing Loss 0.005803956535771789; Training Loss 0.0046769567893513485\n",
      "Episode 11695; Testing Loss 0.005803888826372267; Training Loss 0.004676948205962158\n",
      "Episode 11696; Testing Loss 0.005803929494654907; Training Loss 0.004676938307592523\n",
      "Episode 11697; Testing Loss 0.005804008277001225; Training Loss 0.004676931366862141\n",
      "Episode 11698; Testing Loss 0.005803893109445685; Training Loss 0.004676920697419143\n",
      "Episode 11699; Testing Loss 0.0058037694280438125; Training Loss 0.004676911874541053\n",
      "Episode 11700; Testing Loss 0.005803873931322201; Training Loss 0.004676903351502191\n",
      "Episode 11701; Testing Loss 0.005803959535867299; Training Loss 0.004676894626116447\n",
      "Episode 11702; Testing Loss 0.0058039441775172415; Training Loss 0.004676883320863323\n",
      "Episode 11703; Testing Loss 0.005803795451343749; Training Loss 0.0046768743874764235\n",
      "Episode 11704; Testing Loss 0.005803750105546724; Training Loss 0.004676866210050988\n",
      "Episode 11705; Testing Loss 0.005803836562693936; Training Loss 0.004676856576678127\n",
      "Episode 11706; Testing Loss 0.005803974656862259; Training Loss 0.004676849997581096\n",
      "Episode 11707; Testing Loss 0.005803938500118937; Training Loss 0.004676840617368356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11708; Testing Loss 0.005803769724338567; Training Loss 0.004676829615386717\n",
      "Episode 11709; Testing Loss 0.0058037311048960115; Training Loss 0.0046768205314076695\n",
      "Episode 11710; Testing Loss 0.005803807393632249; Training Loss 0.00467681112208001\n",
      "Episode 11711; Testing Loss 0.005803939234365386; Training Loss 0.00467680278397674\n",
      "Episode 11712; Testing Loss 0.005803914929606735; Training Loss 0.004676796051612634\n",
      "Episode 11713; Testing Loss 0.005803787555722966; Training Loss 0.004676786793903123\n",
      "Episode 11714; Testing Loss 0.005803782426258918; Training Loss 0.004676774758512404\n",
      "Episode 11715; Testing Loss 0.005803830748902422; Training Loss 0.0046767665877373065\n",
      "Episode 11716; Testing Loss 0.005803802457517742; Training Loss 0.004676758838795883\n",
      "Episode 11717; Testing Loss 0.00580376289077844; Training Loss 0.004676748517883029\n",
      "Episode 11718; Testing Loss 0.0058037805016983735; Training Loss 0.004676742399344924\n",
      "Episode 11719; Testing Loss 0.00580377493652234; Training Loss 0.004676732655058312\n",
      "Episode 11720; Testing Loss 0.005803747429579016; Training Loss 0.004676722972890346\n",
      "Episode 11721; Testing Loss 0.005803817453384758; Training Loss 0.004676712623841013\n",
      "Episode 11722; Testing Loss 0.005803732557727496; Training Loss 0.004676704512408695\n",
      "Episode 11723; Testing Loss 0.005803802211034775; Training Loss 0.004676695270768167\n",
      "Episode 11724; Testing Loss 0.005803763163978279; Training Loss 0.004676685502056472\n",
      "Episode 11725; Testing Loss 0.005803727657004471; Training Loss 0.0046766780558413585\n",
      "Episode 11726; Testing Loss 0.005803872622325361; Training Loss 0.004676669457602204\n",
      "Episode 11727; Testing Loss 0.005803879564303468; Training Loss 0.004676658316850678\n",
      "Episode 11728; Testing Loss 0.005803750056706323; Training Loss 0.004676649814210311\n",
      "Episode 11729; Testing Loss 0.005803680032279154; Training Loss 0.004676642003467291\n",
      "Episode 11730; Testing Loss 0.0058037993729909525; Training Loss 0.004676632444918144\n",
      "Episode 11731; Testing Loss 0.005803949543891505; Training Loss 0.004676624863115885\n",
      "Episode 11732; Testing Loss 0.0058037883989044995; Training Loss 0.004676613512163436\n",
      "Episode 11733; Testing Loss 0.005803615566588662; Training Loss 0.004676608053341266\n",
      "Episode 11734; Testing Loss 0.005803776364283186; Training Loss 0.004676595426053794\n",
      "Episode 11735; Testing Loss 0.00580387610739187; Training Loss 0.004676588507988052\n",
      "Episode 11736; Testing Loss 0.005803699285716065; Training Loss 0.004676578555135334\n",
      "Episode 11737; Testing Loss 0.0058035161611686625; Training Loss 0.00467657101202032\n",
      "Episode 11738; Testing Loss 0.005803674261396354; Training Loss 0.004676560179422355\n",
      "Episode 11739; Testing Loss 0.0058038474520835165; Training Loss 0.004676551542552528\n",
      "Episode 11740; Testing Loss 0.005803789817994641; Training Loss 0.0046765422214823694\n",
      "Episode 11741; Testing Loss 0.005803582285082495; Training Loss 0.004676534172756847\n",
      "Episode 11742; Testing Loss 0.0058036762337493235; Training Loss 0.004676524264209877\n",
      "Episode 11743; Testing Loss 0.0058038245241133915; Training Loss 0.004676517284012026\n",
      "Episode 11744; Testing Loss 0.005803698324316378; Training Loss 0.0046765054619583015\n",
      "Episode 11745; Testing Loss 0.005803562406621157; Training Loss 0.0046764987192056675\n",
      "Episode 11746; Testing Loss 0.0058035869786006436; Training Loss 0.004676488251193058\n",
      "Episode 11747; Testing Loss 0.005803746823212291; Training Loss 0.004676479046405759\n",
      "Episode 11748; Testing Loss 0.005803755918653778; Training Loss 0.004676470166260708\n",
      "Episode 11749; Testing Loss 0.0058036199081438426; Training Loss 0.0046764633588797285\n",
      "Episode 11750; Testing Loss 0.005803727131559403; Training Loss 0.0046764522482305336\n",
      "Episode 11751; Testing Loss 0.005803772483478987; Training Loss 0.004676444153335894\n",
      "Episode 11752; Testing Loss 0.005803734462713317; Training Loss 0.004676432863363711\n",
      "Episode 11753; Testing Loss 0.005803668270496771; Training Loss 0.0046764245647245625\n",
      "Episode 11754; Testing Loss 0.00580366058105326; Training Loss 0.004676415639182389\n",
      "Episode 11755; Testing Loss 0.005803689535817329; Training Loss 0.004676407506968158\n",
      "Episode 11756; Testing Loss 0.005803671216123353; Training Loss 0.0046763988509172716\n",
      "Episode 11757; Testing Loss 0.005803723738128772; Training Loss 0.004676388952639548\n",
      "Episode 11758; Testing Loss 0.005803666568397768; Training Loss 0.0046763796376869905\n",
      "Episode 11759; Testing Loss 0.005803635393924224; Training Loss 0.004676372339068436\n",
      "Episode 11760; Testing Loss 0.005803688293494074; Training Loss 0.0046763653465177165\n",
      "Episode 11761; Testing Loss 0.00580363412350238; Training Loss 0.004676355961752682\n",
      "Episode 11762; Testing Loss 0.005803580812232836; Training Loss 0.0046763452620231195\n",
      "Episode 11763; Testing Loss 0.005803729370660484; Training Loss 0.004676336906874473\n",
      "Episode 11764; Testing Loss 0.005803687477005044; Training Loss 0.004676326226173286\n",
      "Episode 11765; Testing Loss 0.005803622335652303; Training Loss 0.004676317791184681\n",
      "Episode 11766; Testing Loss 0.005803671042668528; Training Loss 0.004676309601866154\n",
      "Episode 11767; Testing Loss 0.005803658204359329; Training Loss 0.004676299798051043\n",
      "Episode 11768; Testing Loss 0.005803638165964251; Training Loss 0.004676292538068615\n",
      "Episode 11769; Testing Loss 0.005803707367440638; Training Loss 0.004676281169317458\n",
      "Episode 11770; Testing Loss 0.005803721147280966; Training Loss 0.00467627278382161\n",
      "Episode 11771; Testing Loss 0.005803642211237654; Training Loss 0.00467626512981916\n",
      "Episode 11772; Testing Loss 0.005803701681706199; Training Loss 0.004676255914954113\n",
      "Episode 11773; Testing Loss 0.005803809046597053; Training Loss 0.004676247494669175\n",
      "Episode 11774; Testing Loss 0.00580367895778905; Training Loss 0.004676236767308391\n",
      "Episode 11775; Testing Loss 0.005803534109495767; Training Loss 0.004676228631636465\n",
      "Episode 11776; Testing Loss 0.005803566353434624; Training Loss 0.004676219381588865\n",
      "Episode 11777; Testing Loss 0.005803655946089747; Training Loss 0.004676210434716062\n",
      "Episode 11778; Testing Loss 0.005803666545424847; Training Loss 0.004676201948039038\n",
      "Episode 11779; Testing Loss 0.005803623435049138; Training Loss 0.00467619200510194\n",
      "Episode 11780; Testing Loss 0.005803528688472222; Training Loss 0.004676183024171561\n",
      "Episode 11781; Testing Loss 0.005803529138537436; Training Loss 0.0046761774505192245\n",
      "Episode 11782; Testing Loss 0.005803661952871531; Training Loss 0.004676165678612182\n",
      "Episode 11783; Testing Loss 0.0058037536812212825; Training Loss 0.004676159561106043\n",
      "Episode 11784; Testing Loss 0.005803579653613975; Training Loss 0.004676149938042375\n",
      "Episode 11785; Testing Loss 0.0058034750928301826; Training Loss 0.004676139758244667\n",
      "Episode 11786; Testing Loss 0.005803614477788295; Training Loss 0.004676131852414232\n",
      "Episode 11787; Testing Loss 0.005803812521628676; Training Loss 0.00467612522848245\n",
      "Episode 11788; Testing Loss 0.005803677275247976; Training Loss 0.00467611270287003\n",
      "Episode 11789; Testing Loss 0.005803400157496675; Training Loss 0.004676106915388661\n",
      "Episode 11790; Testing Loss 0.0058034576343823745; Training Loss 0.004676096202403297\n",
      "Episode 11791; Testing Loss 0.005803746058671539; Training Loss 0.00467608806754077\n",
      "Episode 11792; Testing Loss 0.005803700443793007; Training Loss 0.0046760782735765175\n",
      "Episode 11793; Testing Loss 0.005803441723103572; Training Loss 0.004676070148524427\n",
      "Episode 11794; Testing Loss 0.005803434162137511; Training Loss 0.00467606050466504\n",
      "Episode 11795; Testing Loss 0.005803718989695131; Training Loss 0.004676050193664695\n",
      "Episode 11796; Testing Loss 0.005803809945131342; Training Loss 0.004676042960789949\n",
      "Episode 11797; Testing Loss 0.005803585946442175; Training Loss 0.004676033198373789\n",
      "Episode 11798; Testing Loss 0.005803423899633419; Training Loss 0.0046760251923159205\n",
      "Episode 11799; Testing Loss 0.005803589219834421; Training Loss 0.004676015145451263\n",
      "Episode 11800; Testing Loss 0.0058036531550958404; Training Loss 0.004676005644229189\n",
      "Episode 11801; Testing Loss 0.005803450909889516; Training Loss 0.004675998522912318\n",
      "Episode 11802; Testing Loss 0.005803434311654739; Training Loss 0.00467599065030005\n",
      "Episode 11803; Testing Loss 0.005803553321943266; Training Loss 0.004675979131446629\n",
      "Episode 11804; Testing Loss 0.005803543800558133; Training Loss 0.0046759688537968145\n",
      "Episode 11805; Testing Loss 0.005803537648344062; Training Loss 0.0046759620531121775\n",
      "Episode 11806; Testing Loss 0.005803461795915002; Training Loss 0.004675952194097001\n",
      "Episode 11807; Testing Loss 0.005803492117661095; Training Loss 0.0046759418311065025\n",
      "Episode 11808; Testing Loss 0.005803647394369852; Training Loss 0.004675936640123352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11809; Testing Loss 0.005803616356580066; Training Loss 0.00467592623620147\n",
      "Episode 11810; Testing Loss 0.005803472323640555; Training Loss 0.004675915434526246\n",
      "Episode 11811; Testing Loss 0.005803554452383953; Training Loss 0.004675909546416537\n",
      "Episode 11812; Testing Loss 0.005803545733759308; Training Loss 0.004675899148683404\n",
      "Episode 11813; Testing Loss 0.005803516715821127; Training Loss 0.004675888449241389\n",
      "Episode 11814; Testing Loss 0.005803560796336478; Training Loss 0.0046758821157756\n",
      "Episode 11815; Testing Loss 0.0058035045856091455; Training Loss 0.004675874234969872\n",
      "Episode 11816; Testing Loss 0.005803455390831798; Training Loss 0.004675862915342875\n",
      "Episode 11817; Testing Loss 0.005803573058856199; Training Loss 0.004675854013113864\n",
      "Episode 11818; Testing Loss 0.005803651745014956; Training Loss 0.004675845833841374\n",
      "Episode 11819; Testing Loss 0.0058035355883196025; Training Loss 0.004675837021423423\n",
      "Episode 11820; Testing Loss 0.005803393756970227; Training Loss 0.004675828056893277\n",
      "Episode 11821; Testing Loss 0.005803472647801916; Training Loss 0.004675819315740744\n",
      "Episode 11822; Testing Loss 0.005803664275366335; Training Loss 0.004675813038173038\n",
      "Episode 11823; Testing Loss 0.005803644707373625; Training Loss 0.004675801446461872\n",
      "Episode 11824; Testing Loss 0.005803423826929946; Training Loss 0.004675793247663552\n",
      "Episode 11825; Testing Loss 0.00580335047812888; Training Loss 0.004675785055610344\n",
      "Episode 11826; Testing Loss 0.005803481577229248; Training Loss 0.004675776149727279\n",
      "Episode 11827; Testing Loss 0.0058034843898265845; Training Loss 0.004675765564173553\n",
      "Episode 11828; Testing Loss 0.005803440186967644; Training Loss 0.004675760385961163\n",
      "Episode 11829; Testing Loss 0.005803482731531261; Training Loss 0.004675749366286252\n",
      "Episode 11830; Testing Loss 0.005803544304898874; Training Loss 0.004675742654139864\n",
      "Episode 11831; Testing Loss 0.0058034163279896; Training Loss 0.004675733650554301\n",
      "Episode 11832; Testing Loss 0.005803329264727669; Training Loss 0.004675723467496319\n",
      "Episode 11833; Testing Loss 0.005803412242095211; Training Loss 0.004675712134950512\n",
      "Episode 11834; Testing Loss 0.0058035562120967886; Training Loss 0.004675708299556015\n",
      "Episode 11835; Testing Loss 0.005803485639076228; Training Loss 0.004675699554590005\n",
      "Episode 11836; Testing Loss 0.00580336352701798; Training Loss 0.00467568649952205\n",
      "Episode 11837; Testing Loss 0.005803406004443475; Training Loss 0.004675677297036669\n",
      "Episode 11838; Testing Loss 0.005803481914822935; Training Loss 0.004675673089039959\n",
      "Episode 11839; Testing Loss 0.005803395174795837; Training Loss 0.004675663910028098\n",
      "Episode 11840; Testing Loss 0.005803258371113711; Training Loss 0.004675652922785266\n",
      "Episode 11841; Testing Loss 0.0058033468387859895; Training Loss 0.004675640818376632\n",
      "Episode 11842; Testing Loss 0.005803532142980672; Training Loss 0.004675635778876483\n",
      "Episode 11843; Testing Loss 0.005803515008358576; Training Loss 0.004675624873281233\n",
      "Episode 11844; Testing Loss 0.005803336455797285; Training Loss 0.004675616345794431\n",
      "Episode 11845; Testing Loss 0.005803424845999526; Training Loss 0.004675607272110676\n",
      "Episode 11846; Testing Loss 0.005803617439120409; Training Loss 0.00467559875754725\n",
      "Episode 11847; Testing Loss 0.005803639191754877; Training Loss 0.004675589462517875\n",
      "Episode 11848; Testing Loss 0.005803463226214303; Training Loss 0.0046755805020555095\n",
      "Episode 11849; Testing Loss 0.005803406618402137; Training Loss 0.004675570712685133\n",
      "Episode 11850; Testing Loss 0.005803504812233432; Training Loss 0.004675563286441638\n",
      "Episode 11851; Testing Loss 0.00580340321504632; Training Loss 0.004675551633184706\n",
      "Episode 11852; Testing Loss 0.005803262442438943; Training Loss 0.0046755437190571895\n",
      "Episode 11853; Testing Loss 0.005803285874731497; Training Loss 0.004675535517100626\n",
      "Episode 11854; Testing Loss 0.00580336923520526; Training Loss 0.004675527033750147\n",
      "Episode 11855; Testing Loss 0.005803360204437102; Training Loss 0.004675516465094406\n",
      "Episode 11856; Testing Loss 0.005803315335304726; Training Loss 0.004675506703635482\n",
      "Episode 11857; Testing Loss 0.005803267581602413; Training Loss 0.0046754969344795505\n",
      "Episode 11858; Testing Loss 0.005803356480442986; Training Loss 0.0046754911290974675\n",
      "Episode 11859; Testing Loss 0.005803448328241265; Training Loss 0.004675483103767588\n",
      "Episode 11860; Testing Loss 0.005803340735399609; Training Loss 0.004675471097408651\n",
      "Episode 11861; Testing Loss 0.005803291701287046; Training Loss 0.004675464596651866\n",
      "Episode 11862; Testing Loss 0.005803419052283461; Training Loss 0.004675453771202261\n",
      "Episode 11863; Testing Loss 0.005803446101474643; Training Loss 0.004675448824841886\n",
      "Episode 11864; Testing Loss 0.005803221854091867; Training Loss 0.004675439894552932\n",
      "Episode 11865; Testing Loss 0.005803141469096749; Training Loss 0.004675430914879051\n",
      "Episode 11866; Testing Loss 0.005803317845748056; Training Loss 0.004675417406376934\n",
      "Episode 11867; Testing Loss 0.0058035504567751795; Training Loss 0.004675411202485678\n",
      "Episode 11868; Testing Loss 0.005803366334567102; Training Loss 0.00467539898345017\n",
      "Episode 11869; Testing Loss 0.0058031195351927; Training Loss 0.004675395846983849\n",
      "Episode 11870; Testing Loss 0.005803347966988869; Training Loss 0.00467538120178219\n",
      "Episode 11871; Testing Loss 0.005803559043520506; Training Loss 0.004675375542508761\n",
      "Episode 11872; Testing Loss 0.005803338253503114; Training Loss 0.004675364760968312\n",
      "Episode 11873; Testing Loss 0.005803158057408878; Training Loss 0.004675356633723184\n",
      "Episode 11874; Testing Loss 0.005803270550129264; Training Loss 0.004675345976171158\n",
      "Episode 11875; Testing Loss 0.0058033044458320115; Training Loss 0.004675338378308663\n",
      "Episode 11876; Testing Loss 0.005803223665642042; Training Loss 0.004675327859523472\n",
      "Episode 11877; Testing Loss 0.005803229794374464; Training Loss 0.004675319639753283\n",
      "Episode 11878; Testing Loss 0.005803279099701785; Training Loss 0.004675311037517225\n",
      "Episode 11879; Testing Loss 0.0058032743009914815; Training Loss 0.004675302043451974\n",
      "Episode 11880; Testing Loss 0.005803234869423934; Training Loss 0.004675292395014559\n",
      "Episode 11881; Testing Loss 0.005803240389523262; Training Loss 0.00467528362431688\n",
      "Episode 11882; Testing Loss 0.0058032583306452355; Training Loss 0.004675274886358357\n",
      "Episode 11883; Testing Loss 0.005803247648572011; Training Loss 0.004675265659059414\n",
      "Episode 11884; Testing Loss 0.005803258967623261; Training Loss 0.004675256247757119\n",
      "Episode 11885; Testing Loss 0.005803237565879158; Training Loss 0.004675247077654307\n",
      "Episode 11886; Testing Loss 0.005803223915272264; Training Loss 0.004675238649196551\n",
      "Episode 11887; Testing Loss 0.005803210663917853; Training Loss 0.0046752283194462915\n",
      "Episode 11888; Testing Loss 0.005803293318508423; Training Loss 0.0046752198209489065\n",
      "Episode 11889; Testing Loss 0.005803270518801943; Training Loss 0.004675209263956781\n",
      "Episode 11890; Testing Loss 0.005803213170341197; Training Loss 0.004675199601889416\n",
      "Episode 11891; Testing Loss 0.005803235028416923; Training Loss 0.00467518955557707\n",
      "Episode 11892; Testing Loss 0.005803273325366669; Training Loss 0.00467518042827885\n",
      "Episode 11893; Testing Loss 0.005803273715182774; Training Loss 0.004675171099749011\n",
      "Episode 11894; Testing Loss 0.005803241005047576; Training Loss 0.004675160851306772\n",
      "Episode 11895; Testing Loss 0.005803234528068312; Training Loss 0.004675150360451311\n",
      "Episode 11896; Testing Loss 0.005803233688309998; Training Loss 0.004675141086619769\n",
      "Episode 11897; Testing Loss 0.005803268714042955; Training Loss 0.00467513182625722\n",
      "Episode 11898; Testing Loss 0.00580329530061268; Training Loss 0.0046751218696087175\n",
      "Episode 11899; Testing Loss 0.005803189981799146; Training Loss 0.004675111100023145\n",
      "Episode 11900; Testing Loss 0.005803135935533945; Training Loss 0.004675103081516694\n",
      "Episode 11901; Testing Loss 0.005803279459801369; Training Loss 0.004675091973433279\n",
      "Episode 11902; Testing Loss 0.005803324956335331; Training Loss 0.0046750825401662135\n",
      "Episode 11903; Testing Loss 0.0058032132444096685; Training Loss 0.004675072907026441\n",
      "Episode 11904; Testing Loss 0.0058031375382299145; Training Loss 0.004675063455760514\n",
      "Episode 11905; Testing Loss 0.005803262448560293; Training Loss 0.004675052408348891\n",
      "Episode 11906; Testing Loss 0.0058033503685472046; Training Loss 0.00467504372509648\n",
      "Episode 11907; Testing Loss 0.005803220848040825; Training Loss 0.004675033305662747\n",
      "Episode 11908; Testing Loss 0.005803161298209785; Training Loss 0.004675026898714462\n",
      "Episode 11909; Testing Loss 0.005803306063494874; Training Loss 0.004675013908707039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11910; Testing Loss 0.005803360768237711; Training Loss 0.004675006990225177\n",
      "Episode 11911; Testing Loss 0.005803225786485539; Training Loss 0.004674997024235096\n",
      "Episode 11912; Testing Loss 0.005803177521838329; Training Loss 0.004674989443588401\n",
      "Episode 11913; Testing Loss 0.005803310824998752; Training Loss 0.004674977573918556\n",
      "Episode 11914; Testing Loss 0.005803401667010512; Training Loss 0.004674967862725967\n",
      "Episode 11915; Testing Loss 0.00580327802597848; Training Loss 0.004674957863108103\n",
      "Episode 11916; Testing Loss 0.005803230059220308; Training Loss 0.004674947996445089\n",
      "Episode 11917; Testing Loss 0.005803316336644333; Training Loss 0.004674940748755905\n",
      "Episode 11918; Testing Loss 0.005803328121097276; Training Loss 0.004674931190479722\n",
      "Episode 11919; Testing Loss 0.005803251078010674; Training Loss 0.004674918736359753\n",
      "Episode 11920; Testing Loss 0.005803271277179071; Training Loss 0.004674910205188944\n",
      "Episode 11921; Testing Loss 0.005803338180994176; Training Loss 0.004674902488092081\n",
      "Episode 11922; Testing Loss 0.005803434932074585; Training Loss 0.00467489119018598\n",
      "Episode 11923; Testing Loss 0.005803397902902553; Training Loss 0.004674881883410628\n",
      "Episode 11924; Testing Loss 0.005803274736981904; Training Loss 0.004674873091460614\n",
      "Episode 11925; Testing Loss 0.0058033544622998505; Training Loss 0.004674862732835974\n",
      "Episode 11926; Testing Loss 0.005803433985924302; Training Loss 0.00467485277794908\n",
      "Episode 11927; Testing Loss 0.005803381299166149; Training Loss 0.004674843860891438\n",
      "Episode 11928; Testing Loss 0.005803257586612914; Training Loss 0.004674834172616613\n",
      "Episode 11929; Testing Loss 0.00580327496461197; Training Loss 0.004674826500748454\n",
      "Episode 11930; Testing Loss 0.005803404281107504; Training Loss 0.004674817062069781\n",
      "Episode 11931; Testing Loss 0.0058035018284506475; Training Loss 0.004674807864940953\n",
      "Episode 11932; Testing Loss 0.005803454951925197; Training Loss 0.004674798454528879\n",
      "Episode 11933; Testing Loss 0.005803406599548574; Training Loss 0.004674788828021598\n",
      "Episode 11934; Testing Loss 0.005803431031448838; Training Loss 0.004674778518778555\n",
      "Episode 11935; Testing Loss 0.005803475663121535; Training Loss 0.004674769249007061\n",
      "Episode 11936; Testing Loss 0.0058034176097197165; Training Loss 0.004674761306438095\n",
      "Episode 11937; Testing Loss 0.0058033723226443585; Training Loss 0.004674752788124985\n",
      "Episode 11938; Testing Loss 0.005803398168760559; Training Loss 0.004674743141636099\n",
      "Episode 11939; Testing Loss 0.005803441266992614; Training Loss 0.0046747336508314585\n",
      "Episode 11940; Testing Loss 0.005803407748674015; Training Loss 0.004674722677883741\n",
      "Episode 11941; Testing Loss 0.005803419546911741; Training Loss 0.004674714630556076\n",
      "Episode 11942; Testing Loss 0.005803461312574248; Training Loss 0.004674704139734585\n",
      "Episode 11943; Testing Loss 0.005803500480352463; Training Loss 0.0046746941219682125\n",
      "Episode 11944; Testing Loss 0.005803619907502924; Training Loss 0.004674685865383331\n",
      "Episode 11945; Testing Loss 0.005803626342587386; Training Loss 0.004674676711129332\n",
      "Episode 11946; Testing Loss 0.005803510085836785; Training Loss 0.004674666980244646\n",
      "Episode 11947; Testing Loss 0.00580346972426082; Training Loss 0.004674656819244363\n",
      "Episode 11948; Testing Loss 0.00580355593182114; Training Loss 0.004674648597176263\n",
      "Episode 11949; Testing Loss 0.005803507375179303; Training Loss 0.004674639468529321\n",
      "Episode 11950; Testing Loss 0.005803478798022276; Training Loss 0.004674629556514351\n",
      "Episode 11951; Testing Loss 0.005803508629809517; Training Loss 0.004674620485633689\n",
      "Episode 11952; Testing Loss 0.005803562230896229; Training Loss 0.0046746117336361765\n",
      "Episode 11953; Testing Loss 0.005803539823262782; Training Loss 0.004674602027317935\n",
      "Episode 11954; Testing Loss 0.005803542299039151; Training Loss 0.004674592100144786\n",
      "Episode 11955; Testing Loss 0.005803586938274653; Training Loss 0.004674583961569825\n",
      "Episode 11956; Testing Loss 0.005803498324182898; Training Loss 0.00467457392835508\n",
      "Episode 11957; Testing Loss 0.00580343242710333; Training Loss 0.0046745659005616585\n",
      "Episode 11958; Testing Loss 0.005803486457573683; Training Loss 0.004674556944676177\n",
      "Episode 11959; Testing Loss 0.005803612353685857; Training Loss 0.00467454847251155\n",
      "Episode 11960; Testing Loss 0.005803600183156239; Training Loss 0.00467453765266845\n",
      "Episode 11961; Testing Loss 0.005803532328382468; Training Loss 0.00467452760244759\n",
      "Episode 11962; Testing Loss 0.005803555015099947; Training Loss 0.00467452052168268\n",
      "Episode 11963; Testing Loss 0.00580367069947733; Training Loss 0.004674512372778484\n",
      "Episode 11964; Testing Loss 0.005803529514348906; Training Loss 0.00467450110247095\n",
      "Episode 11965; Testing Loss 0.005803407610900353; Training Loss 0.004674495472545603\n",
      "Episode 11966; Testing Loss 0.00580356310704177; Training Loss 0.004674482985009541\n",
      "Episode 11967; Testing Loss 0.0058037103608241675; Training Loss 0.004674477751016792\n",
      "Episode 11968; Testing Loss 0.005803558982338922; Training Loss 0.004674467475874669\n",
      "Episode 11969; Testing Loss 0.005803449882092531; Training Loss 0.004674459141667638\n",
      "Episode 11970; Testing Loss 0.005803555733953478; Training Loss 0.004674446182385748\n",
      "Episode 11971; Testing Loss 0.005803704231181344; Training Loss 0.004674438009466636\n",
      "Episode 11972; Testing Loss 0.00580365685935365; Training Loss 0.0046744301508179825\n",
      "Episode 11973; Testing Loss 0.005803514221947115; Training Loss 0.004674420149842171\n",
      "Episode 11974; Testing Loss 0.005803454849967494; Training Loss 0.004674411747247634\n",
      "Episode 11975; Testing Loss 0.005803516472493814; Training Loss 0.004674401174525091\n",
      "Episode 11976; Testing Loss 0.005803530894120366; Training Loss 0.0046743928411380874\n",
      "Episode 11977; Testing Loss 0.005803507967102153; Training Loss 0.004674383128338586\n",
      "Episode 11978; Testing Loss 0.005803492847519251; Training Loss 0.004674373600768203\n",
      "Episode 11979; Testing Loss 0.005803511957677178; Training Loss 0.004674365637997313\n",
      "Episode 11980; Testing Loss 0.005803546774305731; Training Loss 0.004674356838853715\n",
      "Episode 11981; Testing Loss 0.005803599093655194; Training Loss 0.0046743449984806154\n",
      "Episode 11982; Testing Loss 0.005803591439276631; Training Loss 0.00467433856080913\n",
      "Episode 11983; Testing Loss 0.0058035048839121025; Training Loss 0.004674329929819263\n",
      "Episode 11984; Testing Loss 0.005803520971739331; Training Loss 0.004674317799537276\n",
      "Episode 11985; Testing Loss 0.005803675993145885; Training Loss 0.004674309607433647\n",
      "Episode 11986; Testing Loss 0.00580368833110518; Training Loss 0.004674300780872928\n",
      "Episode 11987; Testing Loss 0.005803546440562354; Training Loss 0.0046742912438595815\n",
      "Episode 11988; Testing Loss 0.0058034739766697424; Training Loss 0.00467428138774664\n",
      "Episode 11989; Testing Loss 0.005803508991512204; Training Loss 0.004674273502310026\n",
      "Episode 11990; Testing Loss 0.005803531926026655; Training Loss 0.00467426627404317\n",
      "Episode 11991; Testing Loss 0.005803511924590963; Training Loss 0.0046742563903586385\n",
      "Episode 11992; Testing Loss 0.005803486760468183; Training Loss 0.004674246172968273\n",
      "Episode 11993; Testing Loss 0.005803530279878572; Training Loss 0.004674237726978131\n",
      "Episode 11994; Testing Loss 0.005803614902726636; Training Loss 0.004674229109025177\n",
      "Episode 11995; Testing Loss 0.005803561756563413; Training Loss 0.004674218230967606\n",
      "Episode 11996; Testing Loss 0.0058034839827516; Training Loss 0.004674208857149587\n",
      "Episode 11997; Testing Loss 0.0058034617054949856; Training Loss 0.0046742006142172655\n",
      "Episode 11998; Testing Loss 0.0058034913992473834; Training Loss 0.004674191151036458\n",
      "Episode 11999; Testing Loss 0.005803548967834969; Training Loss 0.004674182255014066\n",
      "Episode 12000; Testing Loss 0.00580354940085393; Training Loss 0.0046741721599665165\n",
      "Episode 12001; Testing Loss 0.005803457517141021; Training Loss 0.004674163078760907\n",
      "Episode 12002; Testing Loss 0.005803487448418992; Training Loss 0.004674153396306251\n",
      "Episode 12003; Testing Loss 0.005803571667016548; Training Loss 0.004674143900411932\n",
      "Episode 12004; Testing Loss 0.005803501774724152; Training Loss 0.004674134314987911\n",
      "Episode 12005; Testing Loss 0.005803417993813321; Training Loss 0.004674126122106618\n",
      "Episode 12006; Testing Loss 0.005803450118953056; Training Loss 0.004674116502534967\n",
      "Episode 12007; Testing Loss 0.005803532930435643; Training Loss 0.004674106805828504\n",
      "Episode 12008; Testing Loss 0.005803510497743016; Training Loss 0.00467409875670216\n",
      "Episode 12009; Testing Loss 0.005803454130187319; Training Loss 0.004674089697663384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12010; Testing Loss 0.005803481216851125; Training Loss 0.004674080496958595\n",
      "Episode 12011; Testing Loss 0.005803559728299364; Training Loss 0.00467407255123197\n",
      "Episode 12012; Testing Loss 0.005803518281753263; Training Loss 0.004674063458616933\n",
      "Episode 12013; Testing Loss 0.005803410253300949; Training Loss 0.0046740540147035955\n",
      "Episode 12014; Testing Loss 0.00580339400813909; Training Loss 0.004674043309979253\n",
      "Episode 12015; Testing Loss 0.005803530799982977; Training Loss 0.004674034074660672\n",
      "Episode 12016; Testing Loss 0.005803512928651518; Training Loss 0.0046740250919327314\n",
      "Episode 12017; Testing Loss 0.005803530669218116; Training Loss 0.004674016507521145\n",
      "Episode 12018; Testing Loss 0.005803499629685396; Training Loss 0.004674008068545941\n",
      "Episode 12019; Testing Loss 0.005803432072381318; Training Loss 0.00467399784903373\n",
      "Episode 12020; Testing Loss 0.005803398619360696; Training Loss 0.004673991299900528\n",
      "Episode 12021; Testing Loss 0.005803497532513967; Training Loss 0.00467398085929028\n",
      "Episode 12022; Testing Loss 0.005803517522836207; Training Loss 0.004673969995852971\n",
      "Episode 12023; Testing Loss 0.005803446943476445; Training Loss 0.0046739629133803525\n",
      "Episode 12024; Testing Loss 0.005803352136675521; Training Loss 0.004673955061091979\n",
      "Episode 12025; Testing Loss 0.0058034347898982255; Training Loss 0.0046739436471276855\n",
      "Episode 12026; Testing Loss 0.005803622794731494; Training Loss 0.004673936518352351\n",
      "Episode 12027; Testing Loss 0.005803596388473615; Training Loss 0.004673926956227023\n",
      "Episode 12028; Testing Loss 0.00580334666368613; Training Loss 0.00467391685900012\n",
      "Episode 12029; Testing Loss 0.00580324977492751; Training Loss 0.004673908684505549\n",
      "Episode 12030; Testing Loss 0.005803492807674947; Training Loss 0.004673897390842714\n",
      "Episode 12031; Testing Loss 0.0058035635180496625; Training Loss 0.004673888328743186\n",
      "Episode 12032; Testing Loss 0.005803423117774668; Training Loss 0.0046738805201185\n",
      "Episode 12033; Testing Loss 0.0058033782498361965; Training Loss 0.004673872788767305\n",
      "Episode 12034; Testing Loss 0.005803500287806681; Training Loss 0.004673862477847252\n",
      "Episode 12035; Testing Loss 0.005803535168771923; Training Loss 0.004673852151618548\n",
      "Episode 12036; Testing Loss 0.005803378927104008; Training Loss 0.004673842172390052\n",
      "Episode 12037; Testing Loss 0.005803236957784676; Training Loss 0.004673834407487042\n",
      "Episode 12038; Testing Loss 0.005803364346607137; Training Loss 0.004673823445797784\n",
      "Episode 12039; Testing Loss 0.005803540787609965; Training Loss 0.004673816748568594\n",
      "Episode 12040; Testing Loss 0.005803400905153511; Training Loss 0.004673806902948891\n",
      "Episode 12041; Testing Loss 0.0058033079898925895; Training Loss 0.004673799571449137\n",
      "Episode 12042; Testing Loss 0.005803433419782987; Training Loss 0.004673786370866375\n",
      "Episode 12043; Testing Loss 0.005803455224775083; Training Loss 0.004673783655962514\n",
      "Episode 12044; Testing Loss 0.005803208789146118; Training Loss 0.004673772697201875\n",
      "Episode 12045; Testing Loss 0.0058030982433455326; Training Loss 0.004673763341042835\n",
      "Episode 12046; Testing Loss 0.005803337029109775; Training Loss 0.004673753346728003\n",
      "Episode 12047; Testing Loss 0.005803493934923401; Training Loss 0.004673746978868968\n",
      "Episode 12048; Testing Loss 0.0058033243734374055; Training Loss 0.004673734565682426\n",
      "Episode 12049; Testing Loss 0.005803200461461894; Training Loss 0.004673725789730215\n",
      "Episode 12050; Testing Loss 0.0058032525715125995; Training Loss 0.00467371828260869\n",
      "Episode 12051; Testing Loss 0.005803369285691955; Training Loss 0.00467370741952068\n",
      "Episode 12052; Testing Loss 0.005803396651853603; Training Loss 0.004673697292175696\n",
      "Episode 12053; Testing Loss 0.005803447212675053; Training Loss 0.004673689034073204\n",
      "Episode 12054; Testing Loss 0.005803410623062298; Training Loss 0.004673679383152701\n",
      "Episode 12055; Testing Loss 0.005803338928280208; Training Loss 0.004673669693987611\n",
      "Episode 12056; Testing Loss 0.005803302104053784; Training Loss 0.004673660609642821\n",
      "Episode 12057; Testing Loss 0.00580333982880634; Training Loss 0.004673650656211544\n",
      "Episode 12058; Testing Loss 0.00580336514151652; Training Loss 0.004673643288674729\n",
      "Episode 12059; Testing Loss 0.005803303408516433; Training Loss 0.004673633436984099\n",
      "Episode 12060; Testing Loss 0.005803288068561947; Training Loss 0.004673624581357221\n",
      "Episode 12061; Testing Loss 0.0058032935811136305; Training Loss 0.0046736151062738394\n",
      "Episode 12062; Testing Loss 0.005803305684629606; Training Loss 0.004673606907508999\n",
      "Episode 12063; Testing Loss 0.005803405999752247; Training Loss 0.004673597216584333\n",
      "Episode 12064; Testing Loss 0.005803424373419818; Training Loss 0.0046735873041581005\n",
      "Episode 12065; Testing Loss 0.005803333745023626; Training Loss 0.004673578259897546\n",
      "Episode 12066; Testing Loss 0.0058032206616443404; Training Loss 0.004673568736473556\n",
      "Episode 12067; Testing Loss 0.005803264832100136; Training Loss 0.004673560767788368\n",
      "Episode 12068; Testing Loss 0.005803391486652509; Training Loss 0.0046735529380326195\n",
      "Episode 12069; Testing Loss 0.005803314791962762; Training Loss 0.0046735419541090685\n",
      "Episode 12070; Testing Loss 0.005803218652934172; Training Loss 0.00467353400795525\n",
      "Episode 12071; Testing Loss 0.005803206207677048; Training Loss 0.004673524296962678\n",
      "Episode 12072; Testing Loss 0.005803302183816525; Training Loss 0.004673515189482064\n",
      "Episode 12073; Testing Loss 0.0058032752782291265; Training Loss 0.004673505995095274\n",
      "Episode 12074; Testing Loss 0.005803257983160923; Training Loss 0.004673498022777608\n",
      "Episode 12075; Testing Loss 0.0058033223955168516; Training Loss 0.00467348690983182\n",
      "Episode 12076; Testing Loss 0.005803312869645178; Training Loss 0.004673481367717642\n",
      "Episode 12077; Testing Loss 0.005803137280362721; Training Loss 0.004673471570761233\n",
      "Episode 12078; Testing Loss 0.005803126154344142; Training Loss 0.004673460975900943\n",
      "Episode 12079; Testing Loss 0.005803328028885526; Training Loss 0.004673451061796228\n",
      "Episode 12080; Testing Loss 0.005803378320114854; Training Loss 0.004673442640079502\n",
      "Episode 12081; Testing Loss 0.005803280728081969; Training Loss 0.004673435284277957\n",
      "Episode 12082; Testing Loss 0.005803272483288155; Training Loss 0.004673425457651727\n",
      "Episode 12083; Testing Loss 0.0058032861604366435; Training Loss 0.0046734165057760865\n",
      "Episode 12084; Testing Loss 0.005803239744464582; Training Loss 0.004673405760314476\n",
      "Episode 12085; Testing Loss 0.005803158906620939; Training Loss 0.004673398589315728\n",
      "Episode 12086; Testing Loss 0.005803251270828828; Training Loss 0.004673387920031369\n",
      "Episode 12087; Testing Loss 0.005803328033682981; Training Loss 0.004673379701674334\n",
      "Episode 12088; Testing Loss 0.005803157328398611; Training Loss 0.004673370396496548\n",
      "Episode 12089; Testing Loss 0.005803099250247365; Training Loss 0.004673361189958714\n",
      "Episode 12090; Testing Loss 0.005803257581478728; Training Loss 0.004673350985110458\n",
      "Episode 12091; Testing Loss 0.005803337989932463; Training Loss 0.004673342588566393\n",
      "Episode 12092; Testing Loss 0.005803151581061398; Training Loss 0.0046733336132020195\n",
      "Episode 12093; Testing Loss 0.005803112684481816; Training Loss 0.004673325086623126\n",
      "Episode 12094; Testing Loss 0.005803250802450739; Training Loss 0.004673316297211213\n",
      "Episode 12095; Testing Loss 0.005803246200718421; Training Loss 0.004673307108636795\n",
      "Episode 12096; Testing Loss 0.005803077274169451; Training Loss 0.004673297760692375\n",
      "Episode 12097; Testing Loss 0.005803039158141944; Training Loss 0.004673288718517368\n",
      "Episode 12098; Testing Loss 0.00580321484777273; Training Loss 0.004673279620286717\n",
      "Episode 12099; Testing Loss 0.005803282219749974; Training Loss 0.004673272310831122\n",
      "Episode 12100; Testing Loss 0.005803102279015274; Training Loss 0.004673261083424902\n",
      "Episode 12101; Testing Loss 0.005803088102714765; Training Loss 0.004673252749858142\n",
      "Episode 12102; Testing Loss 0.005803242229571609; Training Loss 0.004673244365767425\n",
      "Episode 12103; Testing Loss 0.00580326384348841; Training Loss 0.004673234080737005\n",
      "Episode 12104; Testing Loss 0.005803093397730411; Training Loss 0.004673223344944615\n",
      "Episode 12105; Testing Loss 0.00580299961931824; Training Loss 0.004673215723069134\n",
      "Episode 12106; Testing Loss 0.005803097096842639; Training Loss 0.004673207661330199\n",
      "Episode 12107; Testing Loss 0.005803126197034274; Training Loss 0.004673196410691176\n",
      "Episode 12108; Testing Loss 0.005803048348008474; Training Loss 0.0046731874640488285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12109; Testing Loss 0.005803076654148798; Training Loss 0.004673177073002734\n",
      "Episode 12110; Testing Loss 0.005803118509690517; Training Loss 0.0046731679012212\n",
      "Episode 12111; Testing Loss 0.00580305195866468; Training Loss 0.004673159007090902\n",
      "Episode 12112; Testing Loss 0.005802989378511162; Training Loss 0.00467314925933258\n",
      "Episode 12113; Testing Loss 0.00580300140978245; Training Loss 0.00467314073993161\n",
      "Episode 12114; Testing Loss 0.005803071506080079; Training Loss 0.0046731332206516076\n",
      "Episode 12115; Testing Loss 0.005803073906868734; Training Loss 0.004673122061093558\n",
      "Episode 12116; Testing Loss 0.005802958284304664; Training Loss 0.004673115566795502\n",
      "Episode 12117; Testing Loss 0.005802877870751121; Training Loss 0.004673106058351479\n",
      "Episode 12118; Testing Loss 0.005802948800075509; Training Loss 0.0046730931964984145\n",
      "Episode 12119; Testing Loss 0.005803139715985438; Training Loss 0.004673085972602908\n",
      "Episode 12120; Testing Loss 0.0058031925492295555; Training Loss 0.004673077356030279\n",
      "Episode 12121; Testing Loss 0.005802905578519052; Training Loss 0.004673065670536585\n",
      "Episode 12122; Testing Loss 0.0058027513280919105; Training Loss 0.004673058763428317\n",
      "Episode 12123; Testing Loss 0.005802954653054724; Training Loss 0.004673048581488644\n",
      "Episode 12124; Testing Loss 0.005803082734049443; Training Loss 0.00467303997685707\n",
      "Episode 12125; Testing Loss 0.005802957482473166; Training Loss 0.004673032869948794\n",
      "Episode 12126; Testing Loss 0.00580291069501566; Training Loss 0.004673023037766297\n",
      "Episode 12127; Testing Loss 0.005802990279101783; Training Loss 0.0046730107421511955\n",
      "Episode 12128; Testing Loss 0.005802905109671284; Training Loss 0.004673001295074192\n",
      "Episode 12129; Testing Loss 0.005802767945923021; Training Loss 0.0046729934392264255\n",
      "Episode 12130; Testing Loss 0.005802831994365487; Training Loss 0.004672985281249599\n",
      "Episode 12131; Testing Loss 0.005802966548397011; Training Loss 0.004672974652581708\n",
      "Episode 12132; Testing Loss 0.005802929118533374; Training Loss 0.0046729663961236545\n",
      "Episode 12133; Testing Loss 0.005802724341661331; Training Loss 0.004672958166545363\n",
      "Episode 12134; Testing Loss 0.005802770936997181; Training Loss 0.004672947306706394\n",
      "Episode 12135; Testing Loss 0.005802993581329866; Training Loss 0.004672940222616936\n",
      "Episode 12136; Testing Loss 0.0058030477866310055; Training Loss 0.004672933257316034\n",
      "Episode 12137; Testing Loss 0.005802824351737585; Training Loss 0.0046729175340733\n",
      "Episode 12138; Testing Loss 0.005802636903503371; Training Loss 0.004672911919379713\n",
      "Episode 12139; Testing Loss 0.00580278545216837; Training Loss 0.00467290138158999\n",
      "Episode 12140; Testing Loss 0.0058029899617625235; Training Loss 0.004672891162922929\n",
      "Episode 12141; Testing Loss 0.005802928941604947; Training Loss 0.004672882036443851\n",
      "Episode 12142; Testing Loss 0.005802785417442763; Training Loss 0.004672875372371924\n",
      "Episode 12143; Testing Loss 0.0058028655694474; Training Loss 0.004672863008492729\n",
      "Episode 12144; Testing Loss 0.0058029912517177295; Training Loss 0.00467285644681229\n",
      "Episode 12145; Testing Loss 0.005802864692288284; Training Loss 0.0046728470802593425\n",
      "Episode 12146; Testing Loss 0.0058026713983557535; Training Loss 0.004672835938605966\n",
      "Episode 12147; Testing Loss 0.005802693036841543; Training Loss 0.004672828315670093\n",
      "Episode 12148; Testing Loss 0.005802840212406419; Training Loss 0.004672820528788229\n",
      "Episode 12149; Testing Loss 0.005802864642553237; Training Loss 0.004672809747089684\n",
      "Episode 12150; Testing Loss 0.0058028042064717554; Training Loss 0.004672797710923868\n",
      "Episode 12151; Testing Loss 0.005802721294222929; Training Loss 0.004672792281519096\n",
      "Episode 12152; Testing Loss 0.00580260673930881; Training Loss 0.004672785373550979\n",
      "Episode 12153; Testing Loss 0.005802628767519446; Training Loss 0.004672772680901141\n",
      "Episode 12154; Testing Loss 0.0058027503130585354; Training Loss 0.004672762343357886\n",
      "Episode 12155; Testing Loss 0.0058028446479421875; Training Loss 0.004672759701354972\n",
      "Episode 12156; Testing Loss 0.005802832605108577; Training Loss 0.004672749015691285\n",
      "Episode 12157; Testing Loss 0.005802719836439462; Training Loss 0.004672738101663761\n",
      "Episode 12158; Testing Loss 0.0058026930863197615; Training Loss 0.004672729539382039\n",
      "Episode 12159; Testing Loss 0.0058026482434265295; Training Loss 0.004672721094685889\n",
      "Episode 12160; Testing Loss 0.005802580224506448; Training Loss 0.004672713823116818\n",
      "Episode 12161; Testing Loss 0.0058025955702305666; Training Loss 0.004672702817270149\n",
      "Episode 12162; Testing Loss 0.005802679881305527; Training Loss 0.004672689243524146\n",
      "Episode 12163; Testing Loss 0.005802744378882882; Training Loss 0.0046726834831917405\n",
      "Episode 12164; Testing Loss 0.0058027225515642124; Training Loss 0.00467267476900297\n",
      "Episode 12165; Testing Loss 0.005802638226748114; Training Loss 0.0046726638382159095\n",
      "Episode 12166; Testing Loss 0.005802649190411421; Training Loss 0.004672652312732735\n",
      "Episode 12167; Testing Loss 0.0058027495618971974; Training Loss 0.004672647362979593\n",
      "Episode 12168; Testing Loss 0.005802658812091297; Training Loss 0.004672638668352525\n",
      "Episode 12169; Testing Loss 0.005802509558073958; Training Loss 0.0046726263415678496\n",
      "Episode 12170; Testing Loss 0.005802546612265758; Training Loss 0.004672615745680348\n",
      "Episode 12171; Testing Loss 0.005802642726052559; Training Loss 0.004672607877409551\n",
      "Episode 12172; Testing Loss 0.005802633674562899; Training Loss 0.0046725969415834815\n",
      "Episode 12173; Testing Loss 0.0058025915926985284; Training Loss 0.004672588541703933\n",
      "Episode 12174; Testing Loss 0.005802600899291192; Training Loss 0.004672577876006185\n",
      "Episode 12175; Testing Loss 0.005802617904601933; Training Loss 0.004672570675808806\n",
      "Episode 12176; Testing Loss 0.005802493969890889; Training Loss 0.00467255975909154\n",
      "Episode 12177; Testing Loss 0.005802411725563394; Training Loss 0.004672551241287703\n",
      "Episode 12178; Testing Loss 0.005802551499669086; Training Loss 0.004672543089012715\n",
      "Episode 12179; Testing Loss 0.005802605646006528; Training Loss 0.0046725332483674815\n",
      "Episode 12180; Testing Loss 0.005802542819071788; Training Loss 0.0046725227849764035\n",
      "Episode 12181; Testing Loss 0.005802548270139994; Training Loss 0.004672513490817656\n",
      "Episode 12182; Testing Loss 0.0058025632353364606; Training Loss 0.004672508417343589\n",
      "Episode 12183; Testing Loss 0.005802528572340692; Training Loss 0.004672500305457259\n",
      "Episode 12184; Testing Loss 0.005802450368209398; Training Loss 0.004672488786392078\n",
      "Episode 12185; Testing Loss 0.005802419664648508; Training Loss 0.004672477409433371\n",
      "Episode 12186; Testing Loss 0.0058024852745190165; Training Loss 0.004672469472393954\n",
      "Episode 12187; Testing Loss 0.005802568417999408; Training Loss 0.004672460431426218\n",
      "Episode 12188; Testing Loss 0.005802561690884413; Training Loss 0.004672451026323203\n",
      "Episode 12189; Testing Loss 0.0058024694391443634; Training Loss 0.004672442673435016\n",
      "Episode 12190; Testing Loss 0.005802453088092693; Training Loss 0.0046724326006942095\n",
      "Episode 12191; Testing Loss 0.005802531011860342; Training Loss 0.004672421882746076\n",
      "Episode 12192; Testing Loss 0.005802500550735455; Training Loss 0.0046724138750166315\n",
      "Episode 12193; Testing Loss 0.005802415116163406; Training Loss 0.004672404761948209\n",
      "Episode 12194; Testing Loss 0.0058024112374628276; Training Loss 0.004672395992276364\n",
      "Episode 12195; Testing Loss 0.005802550021043608; Training Loss 0.004672387033816893\n",
      "Episode 12196; Testing Loss 0.005802542879416399; Training Loss 0.004672377434495369\n",
      "Episode 12197; Testing Loss 0.005802437931330997; Training Loss 0.004672368585110134\n",
      "Episode 12198; Testing Loss 0.005802494631200614; Training Loss 0.0046723595543436665\n",
      "Episode 12199; Testing Loss 0.005802551217228694; Training Loss 0.004672348840923589\n",
      "Episode 12200; Testing Loss 0.005802495111608126; Training Loss 0.004672339237117716\n",
      "Episode 12201; Testing Loss 0.00580235993169292; Training Loss 0.004672331282015111\n",
      "Episode 12202; Testing Loss 0.005802394784838038; Training Loss 0.004672321776370301\n",
      "Episode 12203; Testing Loss 0.005802490102040833; Training Loss 0.004672314327439559\n",
      "Episode 12204; Testing Loss 0.005802450130607241; Training Loss 0.004672303741689463\n",
      "Episode 12205; Testing Loss 0.005802367461566344; Training Loss 0.004672295141542074\n",
      "Episode 12206; Testing Loss 0.0058023705651735705; Training Loss 0.004672287295159832\n",
      "Episode 12207; Testing Loss 0.005802436665300229; Training Loss 0.00467227631741425\n",
      "Episode 12208; Testing Loss 0.005802437030420979; Training Loss 0.004672268802062032\n",
      "Episode 12209; Testing Loss 0.005802342249896462; Training Loss 0.004672257773029675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12210; Testing Loss 0.005802323037069682; Training Loss 0.004672249474537136\n",
      "Episode 12211; Testing Loss 0.005802439614107312; Training Loss 0.00467223973477798\n",
      "Episode 12212; Testing Loss 0.005802478048699255; Training Loss 0.004672231068292359\n",
      "Episode 12213; Testing Loss 0.005802385156757487; Training Loss 0.004672221473567973\n",
      "Episode 12214; Testing Loss 0.005802352520601719; Training Loss 0.004672214680422923\n",
      "Episode 12215; Testing Loss 0.005802522420636514; Training Loss 0.004672203320427072\n",
      "Episode 12216; Testing Loss 0.005802450339730575; Training Loss 0.004672193309167142\n",
      "Episode 12217; Testing Loss 0.005802263228202919; Training Loss 0.004672185912885507\n",
      "Episode 12218; Testing Loss 0.005802242445789053; Training Loss 0.004672176586549644\n",
      "Episode 12219; Testing Loss 0.005802387045077723; Training Loss 0.004672167387691555\n",
      "Episode 12220; Testing Loss 0.005802485648159162; Training Loss 0.004672159264868867\n",
      "Episode 12221; Testing Loss 0.005802428866506478; Training Loss 0.004672149178594498\n",
      "Episode 12222; Testing Loss 0.005802249767074377; Training Loss 0.0046721390810960045\n",
      "Episode 12223; Testing Loss 0.005802216693795579; Training Loss 0.004672134228899711\n",
      "Episode 12224; Testing Loss 0.005802311479738901; Training Loss 0.004672127357022576\n",
      "Episode 12225; Testing Loss 0.005802359179709195; Training Loss 0.004672113326556156\n",
      "Episode 12226; Testing Loss 0.005802281519814159; Training Loss 0.004672104615494573\n",
      "Episode 12227; Testing Loss 0.005802312944307671; Training Loss 0.004672095360517085\n",
      "Episode 12228; Testing Loss 0.005802378461241617; Training Loss 0.004672084937718036\n",
      "Episode 12229; Testing Loss 0.005802372882443089; Training Loss 0.00467207782521131\n",
      "Episode 12230; Testing Loss 0.005802246756671654; Training Loss 0.004672069673503786\n",
      "Episode 12231; Testing Loss 0.005802189962591462; Training Loss 0.004672059369747847\n",
      "Episode 12232; Testing Loss 0.005802258766642811; Training Loss 0.004672048423937156\n",
      "Episode 12233; Testing Loss 0.005802370032606888; Training Loss 0.0046720432137706755\n",
      "Episode 12234; Testing Loss 0.005802310039776166; Training Loss 0.00467203283756563\n",
      "Episode 12235; Testing Loss 0.005802212397505453; Training Loss 0.004672024835161074\n",
      "Episode 12236; Testing Loss 0.005802342840965912; Training Loss 0.004672015291487348\n",
      "Episode 12237; Testing Loss 0.005802449643772225; Training Loss 0.0046720045627480535\n",
      "Episode 12238; Testing Loss 0.00580232583173243; Training Loss 0.004671996542325392\n",
      "Episode 12239; Testing Loss 0.00580214177081836; Training Loss 0.004671989868192842\n",
      "Episode 12240; Testing Loss 0.00580211495691581; Training Loss 0.0046719796503421115\n",
      "Episode 12241; Testing Loss 0.005802203523973327; Training Loss 0.00467196913304667\n",
      "Episode 12242; Testing Loss 0.005802288250085304; Training Loss 0.004671962395826097\n",
      "Episode 12243; Testing Loss 0.005802214775983553; Training Loss 0.004671953994573624\n",
      "Episode 12244; Testing Loss 0.0058021344431535265; Training Loss 0.004671942364152194\n",
      "Episode 12245; Testing Loss 0.005802158607764693; Training Loss 0.004671933586122527\n",
      "Episode 12246; Testing Loss 0.005802220778026522; Training Loss 0.0046719231030177225\n",
      "Episode 12247; Testing Loss 0.0058022039556295466; Training Loss 0.0046719140086959555\n",
      "Episode 12248; Testing Loss 0.005802230966003539; Training Loss 0.004671907025151593\n",
      "Episode 12249; Testing Loss 0.005802376198173967; Training Loss 0.004671896057596549\n",
      "Episode 12250; Testing Loss 0.005802400650576655; Training Loss 0.0046718867320883544\n",
      "Episode 12251; Testing Loss 0.005802296962442387; Training Loss 0.004671880076883746\n",
      "Episode 12252; Testing Loss 0.005802222700986179; Training Loss 0.004671869024215514\n",
      "Episode 12253; Testing Loss 0.005802250526838725; Training Loss 0.004671858542922181\n",
      "Episode 12254; Testing Loss 0.005802266649455856; Training Loss 0.004671851589233168\n",
      "Episode 12255; Testing Loss 0.005802158931400308; Training Loss 0.004671840106506923\n",
      "Episode 12256; Testing Loss 0.005802090946894497; Training Loss 0.004671830677084722\n",
      "Episode 12257; Testing Loss 0.005802115532483418; Training Loss 0.004671822394068299\n",
      "Episode 12258; Testing Loss 0.00580214411192004; Training Loss 0.004671812911349026\n",
      "Episode 12259; Testing Loss 0.005802223376573507; Training Loss 0.004671803677843481\n",
      "Episode 12260; Testing Loss 0.0058022682826451896; Training Loss 0.004671794415937932\n",
      "Episode 12261; Testing Loss 0.00580223565835651; Training Loss 0.0046717843257221125\n",
      "Episode 12262; Testing Loss 0.005802230134581073; Training Loss 0.004671775486870752\n",
      "Episode 12263; Testing Loss 0.005802202329092679; Training Loss 0.004671765271101749\n",
      "Episode 12264; Testing Loss 0.005802243743240101; Training Loss 0.0046717564690661705\n",
      "Episode 12265; Testing Loss 0.005802251637915024; Training Loss 0.004671749035961822\n",
      "Episode 12266; Testing Loss 0.005802171695292206; Training Loss 0.004671738277340216\n",
      "Episode 12267; Testing Loss 0.005802157794154015; Training Loss 0.004671730154104905\n",
      "Episode 12268; Testing Loss 0.005802136571095493; Training Loss 0.004671719901803416\n",
      "Episode 12269; Testing Loss 0.005802166956403337; Training Loss 0.004671714570246914\n",
      "Episode 12270; Testing Loss 0.005802170553694549; Training Loss 0.004671703155051085\n",
      "Episode 12271; Testing Loss 0.005802152796335294; Training Loss 0.004671695883223884\n",
      "Episode 12272; Testing Loss 0.00580212377871225; Training Loss 0.004671691276778126\n",
      "Episode 12273; Testing Loss 0.0058021654083393204; Training Loss 0.0046716790117440786\n",
      "Episode 12274; Testing Loss 0.0058021878572549835; Training Loss 0.004671665559691585\n",
      "Episode 12275; Testing Loss 0.005802180035121213; Training Loss 0.004671661950682154\n",
      "Episode 12276; Testing Loss 0.005802054145185621; Training Loss 0.004671655319658756\n",
      "Episode 12277; Testing Loss 0.005801986298587103; Training Loss 0.004671644917842234\n",
      "Episode 12278; Testing Loss 0.005802142892921085; Training Loss 0.004671630043298584\n",
      "Episode 12279; Testing Loss 0.005802270262068504; Training Loss 0.004671625280896523\n",
      "Episode 12280; Testing Loss 0.0058022097234702195; Training Loss 0.004671618222918191\n",
      "Episode 12281; Testing Loss 0.005802097146659812; Training Loss 0.004671607563141314\n",
      "Episode 12282; Testing Loss 0.00580221091359231; Training Loss 0.004671594699508454\n",
      "Episode 12283; Testing Loss 0.005802238458892939; Training Loss 0.00467159004763921\n",
      "Episode 12284; Testing Loss 0.005802082576691715; Training Loss 0.0046715812348552205\n",
      "Episode 12285; Testing Loss 0.00580206581392314; Training Loss 0.004671572051952608\n",
      "Episode 12286; Testing Loss 0.0058022116032548044; Training Loss 0.004671558821912433\n",
      "Episode 12287; Testing Loss 0.005802248537681194; Training Loss 0.004671554120141397\n",
      "Episode 12288; Testing Loss 0.005802087704205842; Training Loss 0.00467154400596992\n",
      "Episode 12289; Testing Loss 0.005802002810963042; Training Loss 0.004671534943426918\n",
      "Episode 12290; Testing Loss 0.00580221861457997; Training Loss 0.004671523037354323\n",
      "Episode 12291; Testing Loss 0.005802300548608419; Training Loss 0.004671516127639008\n",
      "Episode 12292; Testing Loss 0.005802067471638093; Training Loss 0.004671504331063912\n",
      "Episode 12293; Testing Loss 0.0058018804549733165; Training Loss 0.004671496529026225\n",
      "Episode 12294; Testing Loss 0.005802035580252131; Training Loss 0.0046714878192942524\n",
      "Episode 12295; Testing Loss 0.00580216914353345; Training Loss 0.0046714791083329425\n",
      "Episode 12296; Testing Loss 0.005802070679685828; Training Loss 0.004671467287595406\n",
      "Episode 12297; Testing Loss 0.0058019846297703735; Training Loss 0.004671461247088957\n",
      "Episode 12298; Testing Loss 0.00580205563859529; Training Loss 0.004671449702080488\n",
      "Episode 12299; Testing Loss 0.005802151285880252; Training Loss 0.00467144232205441\n",
      "Episode 12300; Testing Loss 0.005802031206111408; Training Loss 0.004671431970822487\n",
      "Episode 12301; Testing Loss 0.0058018881620590275; Training Loss 0.004671424825577627\n",
      "Episode 12302; Testing Loss 0.005801993025844414; Training Loss 0.004671415952590409\n",
      "Episode 12303; Testing Loss 0.00580215629234883; Training Loss 0.00467140560862115\n",
      "Episode 12304; Testing Loss 0.005802079052988038; Training Loss 0.004671397432938733\n",
      "Episode 12305; Testing Loss 0.005801846903831033; Training Loss 0.004671389112462539\n",
      "Episode 12306; Testing Loss 0.005801899742091728; Training Loss 0.004671378236159893\n",
      "Episode 12307; Testing Loss 0.005802162075862432; Training Loss 0.004671370556173029\n",
      "Episode 12308; Testing Loss 0.005802156086485314; Training Loss 0.004671360322648621\n",
      "Episode 12309; Testing Loss 0.005801892794984073; Training Loss 0.004671353012665359\n",
      "Episode 12310; Testing Loss 0.00580181320302732; Training Loss 0.004671345038820637\n",
      "Episode 12311; Testing Loss 0.005802022926254962; Training Loss 0.004671332389282994\n",
      "Episode 12312; Testing Loss 0.0058021999422557; Training Loss 0.004671327542111492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12313; Testing Loss 0.005802059669867752; Training Loss 0.0046713145258160055\n",
      "Episode 12314; Testing Loss 0.0058019109270141325; Training Loss 0.004671308936963998\n",
      "Episode 12315; Testing Loss 0.005802053638083638; Training Loss 0.004671298619142102\n",
      "Episode 12316; Testing Loss 0.00580226783294639; Training Loss 0.004671290113618194\n",
      "Episode 12317; Testing Loss 0.005802140058714753; Training Loss 0.004671279747007178\n",
      "Episode 12318; Testing Loss 0.005801859119255892; Training Loss 0.0046712718790433705\n",
      "Episode 12319; Testing Loss 0.005801836996299263; Training Loss 0.004671262992072342\n",
      "Episode 12320; Testing Loss 0.005802121325510339; Training Loss 0.00467125181384403\n",
      "Episode 12321; Testing Loss 0.0058022155093625525; Training Loss 0.004671244806880798\n",
      "Episode 12322; Testing Loss 0.005801934762307257; Training Loss 0.004671235566941882\n",
      "Episode 12323; Testing Loss 0.005801750243347066; Training Loss 0.0046712290345949075\n",
      "Episode 12324; Testing Loss 0.005801918027841053; Training Loss 0.004671214312899925\n",
      "Episode 12325; Testing Loss 0.005802143934566026; Training Loss 0.004671213126160527\n",
      "Episode 12326; Testing Loss 0.005802008739058112; Training Loss 0.0046711991393221935\n",
      "Episode 12327; Testing Loss 0.0058017175155460465; Training Loss 0.004671189872934826\n",
      "Episode 12328; Testing Loss 0.005801764234533708; Training Loss 0.004671183557595246\n",
      "Episode 12329; Testing Loss 0.005801998656876508; Training Loss 0.0046711753310773475\n",
      "Episode 12330; Testing Loss 0.00580201546124301; Training Loss 0.004671163484239414\n",
      "Episode 12331; Testing Loss 0.005801806831341822; Training Loss 0.004671153693721943\n",
      "Episode 12332; Testing Loss 0.005801809298157341; Training Loss 0.004671146720908399\n",
      "Episode 12333; Testing Loss 0.0058019713680698215; Training Loss 0.004671137084717064\n",
      "Episode 12334; Testing Loss 0.005801929687535094; Training Loss 0.004671125498630743\n",
      "Episode 12335; Testing Loss 0.005801815961727611; Training Loss 0.004671117595626083\n",
      "Episode 12336; Testing Loss 0.005801875325245628; Training Loss 0.0046711083526696125\n",
      "Episode 12337; Testing Loss 0.005802001204343945; Training Loss 0.004671098246449699\n",
      "Episode 12338; Testing Loss 0.005801959061949174; Training Loss 0.004671088469218465\n",
      "Episode 12339; Testing Loss 0.005801794139599291; Training Loss 0.004671080707598956\n",
      "Episode 12340; Testing Loss 0.005801726419285403; Training Loss 0.004671072558042348\n",
      "Episode 12341; Testing Loss 0.005801810497936411; Training Loss 0.004671063685152648\n",
      "Episode 12342; Testing Loss 0.005801912853667572; Training Loss 0.004671055304024381\n",
      "Episode 12343; Testing Loss 0.005801932909316198; Training Loss 0.004671045838736918\n",
      "Episode 12344; Testing Loss 0.005801857233809534; Training Loss 0.0046710358122858155\n",
      "Episode 12345; Testing Loss 0.005801857388572166; Training Loss 0.0046710284354305465\n",
      "Episode 12346; Testing Loss 0.005801848001156165; Training Loss 0.0046710205407434644\n",
      "Episode 12347; Testing Loss 0.005801835427158459; Training Loss 0.004671008940816474\n",
      "Episode 12348; Testing Loss 0.0058018232150909115; Training Loss 0.004670998806423579\n",
      "Episode 12349; Testing Loss 0.005801889160107759; Training Loss 0.004670994377177784\n",
      "Episode 12350; Testing Loss 0.005801949872155566; Training Loss 0.004670984845090021\n",
      "Episode 12351; Testing Loss 0.005801878080342394; Training Loss 0.00467097143580993\n",
      "Episode 12352; Testing Loss 0.005801753848050127; Training Loss 0.004670963520955749\n",
      "Episode 12353; Testing Loss 0.005801737918920386; Training Loss 0.0046709553609192486\n",
      "Episode 12354; Testing Loss 0.005801853919992636; Training Loss 0.004670944382019426\n",
      "Episode 12355; Testing Loss 0.0058018439135413685; Training Loss 0.004670935889138841\n",
      "Episode 12356; Testing Loss 0.005801803336826338; Training Loss 0.00467092893695743\n",
      "Episode 12357; Testing Loss 0.005801750102135062; Training Loss 0.004670918787368172\n",
      "Episode 12358; Testing Loss 0.005801836557650348; Training Loss 0.004670908377744519\n",
      "Episode 12359; Testing Loss 0.005801892535580041; Training Loss 0.004670898146222515\n",
      "Episode 12360; Testing Loss 0.0058017934167494045; Training Loss 0.004670889934938472\n",
      "Episode 12361; Testing Loss 0.00580169263354331; Training Loss 0.0046708820249436985\n",
      "Episode 12362; Testing Loss 0.005801802266545873; Training Loss 0.004670872967547025\n",
      "Episode 12363; Testing Loss 0.005801811669930895; Training Loss 0.004670863653486014\n",
      "Episode 12364; Testing Loss 0.005801674062772574; Training Loss 0.0046708556970966155\n",
      "Episode 12365; Testing Loss 0.005801720997914958; Training Loss 0.004670844700367925\n",
      "Episode 12366; Testing Loss 0.0058018001290248; Training Loss 0.004670840100874093\n",
      "Episode 12367; Testing Loss 0.005801667590262295; Training Loss 0.004670832437211114\n",
      "Episode 12368; Testing Loss 0.005801599913971739; Training Loss 0.004670823952429127\n",
      "Episode 12369; Testing Loss 0.005801723898590179; Training Loss 0.004670810964565136\n",
      "Episode 12370; Testing Loss 0.005801847324894119; Training Loss 0.004670802929405439\n",
      "Episode 12371; Testing Loss 0.005801826074041788; Training Loss 0.004670793934739031\n",
      "Episode 12372; Testing Loss 0.005801697786096999; Training Loss 0.0046707804534729565\n",
      "Episode 12373; Testing Loss 0.005801638719243018; Training Loss 0.004670771386966037\n",
      "Episode 12374; Testing Loss 0.005801688263329052; Training Loss 0.004670763032236425\n",
      "Episode 12375; Testing Loss 0.005801716315613579; Training Loss 0.004670753542685311\n",
      "Episode 12376; Testing Loss 0.005801775692370297; Training Loss 0.004670744693850252\n",
      "Episode 12377; Testing Loss 0.005801785942742915; Training Loss 0.004670735317540949\n",
      "Episode 12378; Testing Loss 0.005801739138043738; Training Loss 0.004670726461430828\n",
      "Episode 12379; Testing Loss 0.005801707167704231; Training Loss 0.004670718906379576\n",
      "Episode 12380; Testing Loss 0.005801683892229029; Training Loss 0.0046707088048850535\n",
      "Episode 12381; Testing Loss 0.005801638850624545; Training Loss 0.00467069871088349\n",
      "Episode 12382; Testing Loss 0.005801726869576087; Training Loss 0.00467069054680185\n",
      "Episode 12383; Testing Loss 0.005801764589970871; Training Loss 0.004670681612775314\n",
      "Episode 12384; Testing Loss 0.005801716711585664; Training Loss 0.0046706721386197275\n",
      "Episode 12385; Testing Loss 0.005801639351499582; Training Loss 0.004670661872319163\n",
      "Episode 12386; Testing Loss 0.005801720654292663; Training Loss 0.004670654230554079\n",
      "Episode 12387; Testing Loss 0.005801769719179162; Training Loss 0.004670646341853067\n",
      "Episode 12388; Testing Loss 0.005801629047823872; Training Loss 0.004670635369392594\n",
      "Episode 12389; Testing Loss 0.005801658647369947; Training Loss 0.004670627319998132\n",
      "Episode 12390; Testing Loss 0.005801794516524919; Training Loss 0.004670618276279226\n",
      "Episode 12391; Testing Loss 0.005801785234865198; Training Loss 0.004670607782560623\n",
      "Episode 12392; Testing Loss 0.005801672944392886; Training Loss 0.004670598420911274\n",
      "Episode 12393; Testing Loss 0.005801650850128646; Training Loss 0.00467058898669998\n",
      "Episode 12394; Testing Loss 0.005801732557128039; Training Loss 0.004670578945621377\n",
      "Episode 12395; Testing Loss 0.005801791439753038; Training Loss 0.00467057164197988\n",
      "Episode 12396; Testing Loss 0.005801659972520543; Training Loss 0.0046705630491877675\n",
      "Episode 12397; Testing Loss 0.005801596247148387; Training Loss 0.004670554396470761\n",
      "Episode 12398; Testing Loss 0.005801720658149158; Training Loss 0.004670542232043159\n",
      "Episode 12399; Testing Loss 0.005801761352885894; Training Loss 0.004670534849470991\n",
      "Episode 12400; Testing Loss 0.005801669655242491; Training Loss 0.0046705253710633415\n",
      "Episode 12401; Testing Loss 0.0058015845481017665; Training Loss 0.00467051610316227\n",
      "Episode 12402; Testing Loss 0.0058016821610215865; Training Loss 0.004670506864422978\n",
      "Episode 12403; Testing Loss 0.005801782627605675; Training Loss 0.004670498131453462\n",
      "Episode 12404; Testing Loss 0.005801703873440679; Training Loss 0.004670488929691845\n",
      "Episode 12405; Testing Loss 0.005801642128395125; Training Loss 0.004670480118813851\n",
      "Episode 12406; Testing Loss 0.005801654344122049; Training Loss 0.0046704705488569325\n",
      "Episode 12407; Testing Loss 0.00580166377704332; Training Loss 0.004670461550537493\n",
      "Episode 12408; Testing Loss 0.005801657435538788; Training Loss 0.004670453325479395\n",
      "Episode 12409; Testing Loss 0.005801714144825918; Training Loss 0.004670443095607696\n",
      "Episode 12410; Testing Loss 0.00580174484928271; Training Loss 0.004670433716693064\n",
      "Episode 12411; Testing Loss 0.005801661350905307; Training Loss 0.004670424742113736\n",
      "Episode 12412; Testing Loss 0.005801674042368518; Training Loss 0.004670416570679641\n",
      "Episode 12413; Testing Loss 0.005801720488398983; Training Loss 0.004670406737519942\n",
      "Episode 12414; Testing Loss 0.005801677499666723; Training Loss 0.004670398742314799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12415; Testing Loss 0.005801605539292113; Training Loss 0.004670391591697881\n",
      "Episode 12416; Testing Loss 0.0058017035313728494; Training Loss 0.0046703810887107336\n",
      "Episode 12417; Testing Loss 0.005801770836135162; Training Loss 0.004670370806338602\n",
      "Episode 12418; Testing Loss 0.005801677332045964; Training Loss 0.004670362289539191\n",
      "Episode 12419; Testing Loss 0.005801578760835685; Training Loss 0.00467035345726286\n",
      "Episode 12420; Testing Loss 0.005801727045663805; Training Loss 0.004670343479686755\n",
      "Episode 12421; Testing Loss 0.005801785904854797; Training Loss 0.004670335987815206\n",
      "Episode 12422; Testing Loss 0.005801718456286804; Training Loss 0.004670324877079757\n",
      "Episode 12423; Testing Loss 0.005801606219119172; Training Loss 0.004670315620623517\n",
      "Episode 12424; Testing Loss 0.005801627548214235; Training Loss 0.004670305915332398\n",
      "Episode 12425; Testing Loss 0.005801745515861745; Training Loss 0.004670297675936553\n",
      "Episode 12426; Testing Loss 0.005801755621198299; Training Loss 0.004670289673246964\n",
      "Episode 12427; Testing Loss 0.0058016826210921515; Training Loss 0.004670278855349007\n",
      "Episode 12428; Testing Loss 0.005801627354767213; Training Loss 0.004670271442356441\n",
      "Episode 12429; Testing Loss 0.005801551537760062; Training Loss 0.004670262522596175\n",
      "Episode 12430; Testing Loss 0.00580154590711628; Training Loss 0.0046702523741853855\n",
      "Episode 12431; Testing Loss 0.005801636472812579; Training Loss 0.0046702446273167685\n",
      "Episode 12432; Testing Loss 0.005801647059471879; Training Loss 0.004670234946675883\n",
      "Episode 12433; Testing Loss 0.005801663192001271; Training Loss 0.0046702250805097135\n",
      "Episode 12434; Testing Loss 0.00580167658872003; Training Loss 0.00467021783183891\n",
      "Episode 12435; Testing Loss 0.005801585433884218; Training Loss 0.004670210015552214\n",
      "Episode 12436; Testing Loss 0.005801482891785459; Training Loss 0.004670200076075218\n",
      "Episode 12437; Testing Loss 0.00580156256782638; Training Loss 0.004670190315301989\n",
      "Episode 12438; Testing Loss 0.0058017064006896015; Training Loss 0.004670184629361306\n",
      "Episode 12439; Testing Loss 0.005801696226643189; Training Loss 0.004670172164591443\n",
      "Episode 12440; Testing Loss 0.005801577589185023; Training Loss 0.0046701619908536204\n",
      "Episode 12441; Testing Loss 0.005801569703324312; Training Loss 0.004670154166670467\n",
      "Episode 12442; Testing Loss 0.005801644132295256; Training Loss 0.0046701481785169216\n",
      "Episode 12443; Testing Loss 0.005801609669644733; Training Loss 0.004670136550203398\n",
      "Episode 12444; Testing Loss 0.005801536642010365; Training Loss 0.004670127846165002\n",
      "Episode 12445; Testing Loss 0.005801582417278159; Training Loss 0.004670120879588475\n",
      "Episode 12446; Testing Loss 0.005801685225172436; Training Loss 0.004670109315347331\n",
      "Episode 12447; Testing Loss 0.005801600652400297; Training Loss 0.004670098919051688\n",
      "Episode 12448; Testing Loss 0.005801458734851414; Training Loss 0.004670089977648998\n",
      "Episode 12449; Testing Loss 0.005801517388435115; Training Loss 0.004670081552346017\n",
      "Episode 12450; Testing Loss 0.005801728503213987; Training Loss 0.004670074103272464\n",
      "Episode 12451; Testing Loss 0.00580165297180371; Training Loss 0.004670064823359499\n",
      "Episode 12452; Testing Loss 0.005801490933758349; Training Loss 0.004670056317603816\n",
      "Episode 12453; Testing Loss 0.005801567896266144; Training Loss 0.004670045789759462\n",
      "Episode 12454; Testing Loss 0.005801591438346941; Training Loss 0.00467003743187474\n",
      "Episode 12455; Testing Loss 0.005801513528368438; Training Loss 0.0046700261471655825\n",
      "Episode 12456; Testing Loss 0.005801561330914326; Training Loss 0.00467001802521356\n",
      "Episode 12457; Testing Loss 0.005801634371221537; Training Loss 0.004670009141619759\n",
      "Episode 12458; Testing Loss 0.005801582377861124; Training Loss 0.0046699986913343264\n",
      "Episode 12459; Testing Loss 0.005801447799479721; Training Loss 0.004669990834728645\n",
      "Episode 12460; Testing Loss 0.00580146109171331; Training Loss 0.004669981036953999\n",
      "Episode 12461; Testing Loss 0.005801548855619206; Training Loss 0.004669972798505777\n",
      "Episode 12462; Testing Loss 0.0058015969669792085; Training Loss 0.004669964110020227\n",
      "Episode 12463; Testing Loss 0.005801590968706691; Training Loss 0.0046699534954306125\n",
      "Episode 12464; Testing Loss 0.005801516400806231; Training Loss 0.004669946038371232\n",
      "Episode 12465; Testing Loss 0.0058013955180693565; Training Loss 0.004669935829147224\n",
      "Episode 12466; Testing Loss 0.005801397872696928; Training Loss 0.00466992776716955\n",
      "Episode 12467; Testing Loss 0.005801534967734335; Training Loss 0.004669920554430469\n",
      "Episode 12468; Testing Loss 0.005801551305755639; Training Loss 0.004669909144536033\n",
      "Episode 12469; Testing Loss 0.005801470685397304; Training Loss 0.004669899600311341\n",
      "Episode 12470; Testing Loss 0.005801487658598442; Training Loss 0.004669892187166182\n",
      "Episode 12471; Testing Loss 0.005801541764269865; Training Loss 0.004669884507524604\n",
      "Episode 12472; Testing Loss 0.005801606647753916; Training Loss 0.004669874263599566\n",
      "Episode 12473; Testing Loss 0.0058015690806211264; Training Loss 0.004669865774002367\n",
      "Episode 12474; Testing Loss 0.00580154693728016; Training Loss 0.004669855742902812\n",
      "Episode 12475; Testing Loss 0.005801543034004327; Training Loss 0.004669847079310408\n",
      "Episode 12476; Testing Loss 0.005801397262213804; Training Loss 0.004669837834180335\n",
      "Episode 12477; Testing Loss 0.005801304703094165; Training Loss 0.004669828597920738\n",
      "Episode 12478; Testing Loss 0.005801520006016837; Training Loss 0.004669819518680395\n",
      "Episode 12479; Testing Loss 0.00580157692372068; Training Loss 0.00466981186374324\n",
      "Episode 12480; Testing Loss 0.005801361826444692; Training Loss 0.004669801241770569\n",
      "Episode 12481; Testing Loss 0.005801327232957674; Training Loss 0.004669792676283522\n",
      "Episode 12482; Testing Loss 0.005801561381589806; Training Loss 0.004669785772195764\n",
      "Episode 12483; Testing Loss 0.005801601574049475; Training Loss 0.004669777076621096\n",
      "Episode 12484; Testing Loss 0.005801397913660807; Training Loss 0.004669763638120864\n",
      "Episode 12485; Testing Loss 0.005801330189074006; Training Loss 0.004669756894350996\n",
      "Episode 12486; Testing Loss 0.005801418230164344; Training Loss 0.004669747193576976\n",
      "Episode 12487; Testing Loss 0.0058014289383928885; Training Loss 0.004669736670889754\n",
      "Episode 12488; Testing Loss 0.005801393740368859; Training Loss 0.0046697277965766035\n",
      "Episode 12489; Testing Loss 0.005801375175097984; Training Loss 0.004669718708219655\n",
      "Episode 12490; Testing Loss 0.005801519299925396; Training Loss 0.004669709795640704\n",
      "Episode 12491; Testing Loss 0.005801521418889486; Training Loss 0.004669700694654641\n",
      "Episode 12492; Testing Loss 0.0058014237109471; Training Loss 0.004669691877868095\n",
      "Episode 12493; Testing Loss 0.005801403478471298; Training Loss 0.004669682418661503\n",
      "Episode 12494; Testing Loss 0.005801493969754977; Training Loss 0.004669672716560851\n",
      "Episode 12495; Testing Loss 0.005801460654010398; Training Loss 0.004669666590966097\n",
      "Episode 12496; Testing Loss 0.005801272943935934; Training Loss 0.004669656946975926\n",
      "Episode 12497; Testing Loss 0.005801299501891135; Training Loss 0.004669645519772558\n",
      "Episode 12498; Testing Loss 0.005801471015102341; Training Loss 0.004669639242400765\n",
      "Episode 12499; Testing Loss 0.005801390963633706; Training Loss 0.004669629380889486\n",
      "Episode 12500; Testing Loss 0.005801266058019435; Training Loss 0.004669619806501402\n",
      "Episode 12501; Testing Loss 0.005801376474113926; Training Loss 0.004669610120471126\n",
      "Episode 12502; Testing Loss 0.005801497849988143; Training Loss 0.004669601998122172\n",
      "Episode 12503; Testing Loss 0.005801423254532858; Training Loss 0.004669591115817744\n",
      "Episode 12504; Testing Loss 0.005801288941445724; Training Loss 0.004669585432948947\n",
      "Episode 12505; Testing Loss 0.00580143304114849; Training Loss 0.004669572868219743\n",
      "Episode 12506; Testing Loss 0.005801473955153882; Training Loss 0.004669566642417459\n",
      "Episode 12507; Testing Loss 0.005801313559236712; Training Loss 0.0046695580128527725\n",
      "Episode 12508; Testing Loss 0.005801306020434314; Training Loss 0.0046695455877055255\n",
      "Episode 12509; Testing Loss 0.005801471657397782; Training Loss 0.004669538557424093\n",
      "Episode 12510; Testing Loss 0.005801436555659497; Training Loss 0.00466952969552576\n",
      "Episode 12511; Testing Loss 0.005801323594992355; Training Loss 0.004669519206204884\n",
      "Episode 12512; Testing Loss 0.005801318284091254; Training Loss 0.004669511461938865\n",
      "Episode 12513; Testing Loss 0.005801417722623347; Training Loss 0.004669500715954319\n",
      "Episode 12514; Testing Loss 0.005801362287810398; Training Loss 0.004669492596644521\n",
      "Episode 12515; Testing Loss 0.005801281742169682; Training Loss 0.004669484533244097\n",
      "Episode 12516; Testing Loss 0.005801346376552397; Training Loss 0.004669474163659602\n",
      "Episode 12517; Testing Loss 0.005801425585321818; Training Loss 0.004669467700002715\n",
      "Episode 12518; Testing Loss 0.005801346547470659; Training Loss 0.004669457364046865\n",
      "Episode 12519; Testing Loss 0.00580131093947708; Training Loss 0.004669446721251273\n",
      "Episode 12520; Testing Loss 0.0058013496735067425; Training Loss 0.004669437963024793\n",
      "Episode 12521; Testing Loss 0.005801340774909983; Training Loss 0.004669429084407249\n",
      "Episode 12522; Testing Loss 0.005801325077495111; Training Loss 0.004669418909979731\n",
      "Episode 12523; Testing Loss 0.005801278544024534; Training Loss 0.004669410455597416\n",
      "Episode 12524; Testing Loss 0.005801323803138873; Training Loss 0.004669401149753604\n",
      "Episode 12525; Testing Loss 0.005801357999353643; Training Loss 0.004669393505081946\n",
      "Episode 12526; Testing Loss 0.005801310037261182; Training Loss 0.004669385208569534\n",
      "Episode 12527; Testing Loss 0.005801218069254298; Training Loss 0.004669373948950787\n",
      "Episode 12528; Testing Loss 0.0058012667509171004; Training Loss 0.004669367086902713\n",
      "Episode 12529; Testing Loss 0.005801243034612149; Training Loss 0.004669357758788888\n",
      "Episode 12530; Testing Loss 0.005801240812969015; Training Loss 0.004669346453488549\n",
      "Episode 12531; Testing Loss 0.005801365543869299; Training Loss 0.004669338877194216\n",
      "Episode 12532; Testing Loss 0.005801402514760163; Training Loss 0.004669329010240551\n",
      "Episode 12533; Testing Loss 0.005801254397164328; Training Loss 0.004669319994047514\n",
      "Episode 12534; Testing Loss 0.005801246402886686; Training Loss 0.00466931189684364\n",
      "Episode 12535; Testing Loss 0.005801298951007277; Training Loss 0.004669301750781086\n",
      "Episode 12536; Testing Loss 0.0058013138340232175; Training Loss 0.004669293101704901\n",
      "Episode 12537; Testing Loss 0.005801258290362332; Training Loss 0.004669285464412699\n",
      "Episode 12538; Testing Loss 0.005801210131406087; Training Loss 0.004669276505494269\n",
      "Episode 12539; Testing Loss 0.005801269087079074; Training Loss 0.004669266679600039\n",
      "Episode 12540; Testing Loss 0.005801288028903044; Training Loss 0.004669256064616601\n",
      "Episode 12541; Testing Loss 0.005801251396786933; Training Loss 0.004669249512646169\n",
      "Episode 12542; Testing Loss 0.0058013186420984835; Training Loss 0.004669239613339176\n",
      "Episode 12543; Testing Loss 0.005801406327597142; Training Loss 0.004669235415849272\n",
      "Episode 12544; Testing Loss 0.005801273198222303; Training Loss 0.004669225031808911\n",
      "Episode 12545; Testing Loss 0.0058010823962870075; Training Loss 0.004669215951778441\n",
      "Episode 12546; Testing Loss 0.0058010922842228144; Training Loss 0.004669203996937507\n",
      "Episode 12547; Testing Loss 0.005801218732086423; Training Loss 0.004669196120913444\n",
      "Episode 12548; Testing Loss 0.00580136259385422; Training Loss 0.00466918917623984\n",
      "Episode 12549; Testing Loss 0.005801308422939737; Training Loss 0.004669176209113263\n",
      "Episode 12550; Testing Loss 0.0058011568334917; Training Loss 0.004669167408631382\n",
      "Episode 12551; Testing Loss 0.005801096141177682; Training Loss 0.004669161076058757\n",
      "Episode 12552; Testing Loss 0.00580118974349403; Training Loss 0.004669149134557682\n",
      "Episode 12553; Testing Loss 0.005801280325367114; Training Loss 0.00466913897171732\n",
      "Episode 12554; Testing Loss 0.005801342008320777; Training Loss 0.004669132378012096\n",
      "Episode 12555; Testing Loss 0.005801279559043447; Training Loss 0.004669121891836375\n",
      "Episode 12556; Testing Loss 0.005801220175971844; Training Loss 0.004669111507518638\n",
      "Episode 12557; Testing Loss 0.005801200533961539; Training Loss 0.004669102102473763\n",
      "Episode 12558; Testing Loss 0.0058012179137203555; Training Loss 0.004669093583862775\n",
      "Episode 12559; Testing Loss 0.005801205719903038; Training Loss 0.004669085626586434\n",
      "Episode 12560; Testing Loss 0.0058011516336787935; Training Loss 0.00466907684020313\n",
      "Episode 12561; Testing Loss 0.005801172041387245; Training Loss 0.004669067174176596\n",
      "Episode 12562; Testing Loss 0.0058012553927942285; Training Loss 0.0046690599112076065\n",
      "Episode 12563; Testing Loss 0.005801249506710035; Training Loss 0.0046690490160287805\n",
      "Episode 12564; Testing Loss 0.005801157366570619; Training Loss 0.004669039779574555\n",
      "Episode 12565; Testing Loss 0.005801177883726832; Training Loss 0.004669031284764803\n",
      "Episode 12566; Testing Loss 0.00580122714061225; Training Loss 0.004669022131104301\n",
      "Episode 12567; Testing Loss 0.005801123337450885; Training Loss 0.004669010946363449\n",
      "Episode 12568; Testing Loss 0.005801008267277737; Training Loss 0.004669004450107025\n",
      "Episode 12569; Testing Loss 0.005801149637007073; Training Loss 0.004668995123921671\n",
      "Episode 12570; Testing Loss 0.005801381845587166; Training Loss 0.004668986771270668\n",
      "Episode 12571; Testing Loss 0.005801273949443841; Training Loss 0.004668978425851223\n",
      "Episode 12572; Testing Loss 0.00580104087823897; Training Loss 0.004668970237835771\n",
      "Episode 12573; Testing Loss 0.005801071111653215; Training Loss 0.004668956898611715\n",
      "Episode 12574; Testing Loss 0.005801267046733491; Training Loss 0.004668949415378406\n",
      "Episode 12575; Testing Loss 0.005801226141404126; Training Loss 0.004668941509935926\n",
      "Episode 12576; Testing Loss 0.0058010038994150545; Training Loss 0.0046689326797157565\n",
      "Episode 12577; Testing Loss 0.0058009424285144235; Training Loss 0.00466892433312911\n",
      "Episode 12578; Testing Loss 0.005801078040717457; Training Loss 0.004668912524362761\n",
      "Episode 12579; Testing Loss 0.00580117874669944; Training Loss 0.004668905883978515\n",
      "Episode 12580; Testing Loss 0.005801019408292594; Training Loss 0.004668896206199606\n",
      "Episode 12581; Testing Loss 0.005801012213060738; Training Loss 0.004668884752323566\n",
      "Episode 12582; Testing Loss 0.0058011461460409875; Training Loss 0.004668877274648423\n",
      "Episode 12583; Testing Loss 0.005801177109940933; Training Loss 0.004668869797059385\n",
      "Episode 12584; Testing Loss 0.005801149824237003; Training Loss 0.004668859035591556\n",
      "Episode 12585; Testing Loss 0.0058011890286708955; Training Loss 0.0046688487015490035\n",
      "Episode 12586; Testing Loss 0.005801183411416598; Training Loss 0.0046688399123759675\n",
      "Episode 12587; Testing Loss 0.0058010988420161055; Training Loss 0.004668830914367553\n",
      "Episode 12588; Testing Loss 0.005801060341663609; Training Loss 0.004668821591675389\n",
      "Episode 12589; Testing Loss 0.005801072580074086; Training Loss 0.004668813803509612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12590; Testing Loss 0.005801092469736189; Training Loss 0.00466880435873298\n",
      "Episode 12591; Testing Loss 0.005801061484440122; Training Loss 0.004668794390254293\n",
      "Episode 12592; Testing Loss 0.005801105497614293; Training Loss 0.004668785146838053\n",
      "Episode 12593; Testing Loss 0.0058011160095320525; Training Loss 0.004668776548526232\n",
      "Episode 12594; Testing Loss 0.005801103112609784; Training Loss 0.004668768438589564\n",
      "Episode 12595; Testing Loss 0.005801113959048703; Training Loss 0.004668761327126158\n",
      "Episode 12596; Testing Loss 0.0058011724394855515; Training Loss 0.004668750939175424\n",
      "Episode 12597; Testing Loss 0.005801154080454491; Training Loss 0.004668739652409645\n",
      "Episode 12598; Testing Loss 0.0058010006294445145; Training Loss 0.004668731259195384\n",
      "Episode 12599; Testing Loss 0.005801040720551751; Training Loss 0.0046687227345683785\n",
      "Episode 12600; Testing Loss 0.005801216947947641; Training Loss 0.004668713241600408\n",
      "Episode 12601; Testing Loss 0.005801215710402149; Training Loss 0.004668705059297241\n",
      "Episode 12602; Testing Loss 0.005801013427333972; Training Loss 0.004668694563420363\n",
      "Episode 12603; Testing Loss 0.005800956368169272; Training Loss 0.004668687598205449\n",
      "Episode 12604; Testing Loss 0.005801057159208931; Training Loss 0.00466867658589586\n",
      "Episode 12605; Testing Loss 0.005801122929725773; Training Loss 0.004668668418178082\n",
      "Episode 12606; Testing Loss 0.0058010774970560395; Training Loss 0.004668659644625606\n",
      "Episode 12607; Testing Loss 0.005801063192121225; Training Loss 0.004668650743866989\n",
      "Episode 12608; Testing Loss 0.005801082705074435; Training Loss 0.004668640127021822\n",
      "Episode 12609; Testing Loss 0.0058010527152773834; Training Loss 0.004668633574967963\n",
      "Episode 12610; Testing Loss 0.005801068512705914; Training Loss 0.004668623457454187\n",
      "Episode 12611; Testing Loss 0.005801081204692072; Training Loss 0.004668618186044057\n",
      "Episode 12612; Testing Loss 0.0058009210502860615; Training Loss 0.0046686102432256325\n",
      "Episode 12613; Testing Loss 0.005800868193957862; Training Loss 0.004668599677065542\n",
      "Episode 12614; Testing Loss 0.005800979212937303; Training Loss 0.0046685886500807796\n",
      "Episode 12615; Testing Loss 0.005801010623954898; Training Loss 0.004668581706170383\n",
      "Episode 12616; Testing Loss 0.0058010078884978035; Training Loss 0.004668572263516736\n",
      "Episode 12617; Testing Loss 0.005801097358967773; Training Loss 0.004668561472596005\n",
      "Episode 12618; Testing Loss 0.005801082340775428; Training Loss 0.004668554506668306\n",
      "Episode 12619; Testing Loss 0.005800922327340768; Training Loss 0.004668545476402487\n",
      "Episode 12620; Testing Loss 0.005800854765752455; Training Loss 0.0046685347460045604\n",
      "Episode 12621; Testing Loss 0.005800942811113821; Training Loss 0.0046685240650773995\n",
      "Episode 12622; Testing Loss 0.005801079116985777; Training Loss 0.004668517270470769\n",
      "Episode 12623; Testing Loss 0.005801022161085594; Training Loss 0.00466850966245205\n",
      "Episode 12624; Testing Loss 0.005800952315341524; Training Loss 0.004668500071450288\n",
      "Episode 12625; Testing Loss 0.005801005020667665; Training Loss 0.004668488642997684\n",
      "Episode 12626; Testing Loss 0.005801033911218519; Training Loss 0.004668479422749622\n",
      "Episode 12627; Testing Loss 0.005800948685762318; Training Loss 0.004668469728673618\n",
      "Episode 12628; Testing Loss 0.0058009476712741585; Training Loss 0.004668462208336436\n",
      "Episode 12629; Testing Loss 0.005800945530547079; Training Loss 0.004668454188020639\n",
      "Episode 12630; Testing Loss 0.00580096888716307; Training Loss 0.004668444844758167\n",
      "Episode 12631; Testing Loss 0.005800936788632219; Training Loss 0.004668434233699477\n",
      "Episode 12632; Testing Loss 0.005800917707065679; Training Loss 0.0046684282435148815\n",
      "Episode 12633; Testing Loss 0.005800909438956405; Training Loss 0.00466841920903554\n",
      "Episode 12634; Testing Loss 0.005800960711300547; Training Loss 0.004668407584529479\n",
      "Episode 12635; Testing Loss 0.005800909299733913; Training Loss 0.004668399985830649\n",
      "Episode 12636; Testing Loss 0.005800841995520179; Training Loss 0.004668394899998535\n",
      "Episode 12637; Testing Loss 0.0058008812178016456; Training Loss 0.004668384848977366\n",
      "Episode 12638; Testing Loss 0.005800979006805038; Training Loss 0.004668372686409946\n",
      "Episode 12639; Testing Loss 0.005800863630839928; Training Loss 0.004668364336468943\n",
      "Episode 12640; Testing Loss 0.005800700825883841; Training Loss 0.004668360059432321\n",
      "Episode 12641; Testing Loss 0.005800903499364452; Training Loss 0.0046683467765777364\n",
      "Episode 12642; Testing Loss 0.005801051212250679; Training Loss 0.004668337186450077\n",
      "Episode 12643; Testing Loss 0.005800896694240936; Training Loss 0.004668327679166228\n",
      "Episode 12644; Testing Loss 0.005800786299335938; Training Loss 0.00466831954213409\n",
      "Episode 12645; Testing Loss 0.005800914797929993; Training Loss 0.00466830887558778\n",
      "Episode 12646; Testing Loss 0.0058010387077471305; Training Loss 0.0046683000786219885\n",
      "Episode 12647; Testing Loss 0.0058009543038975; Training Loss 0.004668291020290219\n",
      "Episode 12648; Testing Loss 0.005800815110298028; Training Loss 0.004668281774739245\n",
      "Episode 12649; Testing Loss 0.005800788114651249; Training Loss 0.004668273108784988\n",
      "Episode 12650; Testing Loss 0.005800886850680395; Training Loss 0.004668264303125986\n",
      "Episode 12651; Testing Loss 0.005800893225494793; Training Loss 0.004668255550322485\n",
      "Episode 12652; Testing Loss 0.005800754328492132; Training Loss 0.004668246448334386\n",
      "Episode 12653; Testing Loss 0.005800745104420937; Training Loss 0.004668238321832036\n",
      "Episode 12654; Testing Loss 0.00580084604696106; Training Loss 0.0046682284164617565\n",
      "Episode 12655; Testing Loss 0.005800850805171406; Training Loss 0.004668218173155755\n",
      "Episode 12656; Testing Loss 0.005800791409649748; Training Loss 0.0046682101720636315\n",
      "Episode 12657; Testing Loss 0.005800857219667796; Training Loss 0.004668200312824937\n",
      "Episode 12658; Testing Loss 0.005800921320331534; Training Loss 0.0046681906444768495\n",
      "Episode 12659; Testing Loss 0.005800787075938231; Training Loss 0.0046681823793264686\n",
      "Episode 12660; Testing Loss 0.0058007060305917905; Training Loss 0.004668171783178635\n",
      "Episode 12661; Testing Loss 0.005800786212556643; Training Loss 0.00466816490002817\n",
      "Episode 12662; Testing Loss 0.005800818344658102; Training Loss 0.004668157519037323\n",
      "Episode 12663; Testing Loss 0.005800741597838291; Training Loss 0.004668146953193235\n",
      "Episode 12664; Testing Loss 0.005800702234573475; Training Loss 0.0046681395856658205\n",
      "Episode 12665; Testing Loss 0.005800879394515548; Training Loss 0.004668127316843046\n",
      "Episode 12666; Testing Loss 0.00580090881026058; Training Loss 0.0046681213982992294\n",
      "Episode 12667; Testing Loss 0.005800752358636146; Training Loss 0.004668112215174589\n",
      "Episode 12668; Testing Loss 0.005800736176068323; Training Loss 0.004668101547735592\n",
      "Episode 12669; Testing Loss 0.005800862404460277; Training Loss 0.0046680941965509615\n",
      "Episode 12670; Testing Loss 0.005800835973116465; Training Loss 0.004668085579493558\n",
      "Episode 12671; Testing Loss 0.005800729314433779; Training Loss 0.004668076385193233\n",
      "Episode 12672; Testing Loss 0.005800774126352342; Training Loss 0.0046680659948710596\n",
      "Episode 12673; Testing Loss 0.005800795090244742; Training Loss 0.0046680596273720515\n",
      "Episode 12674; Testing Loss 0.0058007283217135835; Training Loss 0.00466805083090198\n",
      "Episode 12675; Testing Loss 0.005800767418890589; Training Loss 0.004668038501233842\n",
      "Episode 12676; Testing Loss 0.0058008131286325796; Training Loss 0.004668031386210642\n",
      "Episode 12677; Testing Loss 0.005800765805611153; Training Loss 0.004668025809048359\n",
      "Episode 12678; Testing Loss 0.005800690340188715; Training Loss 0.004668016758803211\n",
      "Episode 12679; Testing Loss 0.00580068792583661; Training Loss 0.004668003683980811\n",
      "Episode 12680; Testing Loss 0.005800731287932822; Training Loss 0.004667994766106432\n",
      "Episode 12681; Testing Loss 0.005800808218975493; Training Loss 0.004667988278091348\n",
      "Episode 12682; Testing Loss 0.005800762840386481; Training Loss 0.00466797856240779\n",
      "Episode 12683; Testing Loss 0.0058006751791841275; Training Loss 0.004667967462124888\n",
      "Episode 12684; Testing Loss 0.005800688226261154; Training Loss 0.00466795902720215\n",
      "Episode 12685; Testing Loss 0.005800791673449342; Training Loss 0.004667950955080466\n",
      "Episode 12686; Testing Loss 0.005800875697427964; Training Loss 0.004667941975364258\n",
      "Episode 12687; Testing Loss 0.005800677234197896; Training Loss 0.004667930575679816\n",
      "Episode 12688; Testing Loss 0.005800624507195286; Training Loss 0.004667923328210511\n",
      "Episode 12689; Testing Loss 0.005800699103510297; Training Loss 0.0046679133541643195\n",
      "Episode 12690; Testing Loss 0.005800775128759247; Training Loss 0.004667904021472379\n",
      "Episode 12691; Testing Loss 0.005800766614733006; Training Loss 0.004667895564407163\n",
      "Episode 12692; Testing Loss 0.00580069265640943; Training Loss 0.004667885629058791\n",
      "Episode 12693; Testing Loss 0.005800626845258465; Training Loss 0.004667876572501909\n",
      "Episode 12694; Testing Loss 0.0058007411788379756; Training Loss 0.004667867350456094\n",
      "Episode 12695; Testing Loss 0.005800732230407848; Training Loss 0.004667858543370948\n",
      "Episode 12696; Testing Loss 0.005800578768723046; Training Loss 0.004667850986451327\n",
      "Episode 12697; Testing Loss 0.005800637105641089; Training Loss 0.004667842329857522\n",
      "Episode 12698; Testing Loss 0.005800808755239329; Training Loss 0.00466783344226572\n",
      "Episode 12699; Testing Loss 0.0058007581376285046; Training Loss 0.004667823368929933\n",
      "Episode 12700; Testing Loss 0.005800648718440324; Training Loss 0.004667814016782851\n",
      "Episode 12701; Testing Loss 0.005800664770804723; Training Loss 0.004667807920846587\n",
      "Episode 12702; Testing Loss 0.005800587618396678; Training Loss 0.004667798773807251\n",
      "Episode 12703; Testing Loss 0.005800538114000801; Training Loss 0.004667787339136509\n",
      "Episode 12704; Testing Loss 0.00580068265459099; Training Loss 0.004667778544926406\n",
      "Episode 12705; Testing Loss 0.005800786507416715; Training Loss 0.0046677707312539505\n",
      "Episode 12706; Testing Loss 0.005800683605797138; Training Loss 0.004667759775480387\n",
      "Episode 12707; Testing Loss 0.005800574749598878; Training Loss 0.004667754602924311\n",
      "Episode 12708; Testing Loss 0.005800599817833072; Training Loss 0.004667743542053525\n",
      "Episode 12709; Testing Loss 0.005800703437173275; Training Loss 0.004667735094451525\n",
      "Episode 12710; Testing Loss 0.005800766138990113; Training Loss 0.004667726994461459\n",
      "Episode 12711; Testing Loss 0.005800603661099901; Training Loss 0.00466771687988936\n",
      "Episode 12712; Testing Loss 0.00580044704248255; Training Loss 0.004667708886825969\n",
      "Episode 12713; Testing Loss 0.00580057421455783; Training Loss 0.004667696115383286\n",
      "Episode 12714; Testing Loss 0.005800770472578152; Training Loss 0.004667694948470309\n",
      "Episode 12715; Testing Loss 0.005800641904663573; Training Loss 0.0046676826178011\n",
      "Episode 12716; Testing Loss 0.00580046692455; Training Loss 0.004667675603849673\n",
      "Episode 12717; Testing Loss 0.0058006506673925; Training Loss 0.004667663163793443\n",
      "Episode 12718; Testing Loss 0.005800783778485971; Training Loss 0.004667659000604863\n",
      "Episode 12719; Testing Loss 0.00580054943754894; Training Loss 0.004667647780606361\n",
      "Episode 12720; Testing Loss 0.005800368537251712; Training Loss 0.004667640056440689\n",
      "Episode 12721; Testing Loss 0.00580050789151399; Training Loss 0.004667626540786857\n",
      "Episode 12722; Testing Loss 0.005800762178450062; Training Loss 0.004667621347372008\n",
      "Episode 12723; Testing Loss 0.00580081923745761; Training Loss 0.004667613417697926\n",
      "Episode 12724; Testing Loss 0.005800632020472378; Training Loss 0.004667601193334223\n",
      "Episode 12725; Testing Loss 0.00580051016974058; Training Loss 0.004667595697934642\n",
      "Episode 12726; Testing Loss 0.005800584716241216; Training Loss 0.004667586176343249\n",
      "Episode 12727; Testing Loss 0.005800645106939359; Training Loss 0.004667577305631118\n",
      "Episode 12728; Testing Loss 0.00580050595891331; Training Loss 0.004667565735987169\n",
      "Episode 12729; Testing Loss 0.005800444017318167; Training Loss 0.004667558137994254\n",
      "Episode 12730; Testing Loss 0.00580059673368893; Training Loss 0.00466754930922147\n",
      "Episode 12731; Testing Loss 0.00580067327505807; Training Loss 0.004667539877129319\n",
      "Episode 12732; Testing Loss 0.005800514914589834; Training Loss 0.00466752901233976\n",
      "Episode 12733; Testing Loss 0.005800379714149345; Training Loss 0.004667521308460734\n",
      "Episode 12734; Testing Loss 0.005800557093091928; Training Loss 0.00466751342042324\n",
      "Episode 12735; Testing Loss 0.005800657000643854; Training Loss 0.0046675062653198\n",
      "Episode 12736; Testing Loss 0.005800448340939084; Training Loss 0.0046674935186831286\n",
      "Episode 12737; Testing Loss 0.005800358366428996; Training Loss 0.004667488062970582\n",
      "Episode 12738; Testing Loss 0.005800586217000001; Training Loss 0.004667479290859756\n",
      "Episode 12739; Testing Loss 0.005800787524829926; Training Loss 0.004667470835883435\n",
      "Episode 12740; Testing Loss 0.005800579831134121; Training Loss 0.004667457237091664\n",
      "Episode 12741; Testing Loss 0.005800346801056309; Training Loss 0.004667454578271596\n",
      "Episode 12742; Testing Loss 0.005800379399198276; Training Loss 0.004667445417417082\n",
      "Episode 12743; Testing Loss 0.005800504935939044; Training Loss 0.00466743397230126\n",
      "Episode 12744; Testing Loss 0.005800495113215129; Training Loss 0.004667421815983783\n",
      "Episode 12745; Testing Loss 0.0058003974657357986; Training Loss 0.004667415589871992\n",
      "Episode 12746; Testing Loss 0.005800451974827229; Training Loss 0.0046674095685759805\n",
      "Episode 12747; Testing Loss 0.0058006628722095535; Training Loss 0.0046673972824086175\n",
      "Episode 12748; Testing Loss 0.005800653456176635; Training Loss 0.004667387235392502\n",
      "Episode 12749; Testing Loss 0.005800465893533666; Training Loss 0.004667379251655148\n",
      "Episode 12750; Testing Loss 0.0058004042036785995; Training Loss 0.004667370713468326\n",
      "Episode 12751; Testing Loss 0.00580053589252664; Training Loss 0.004667359107274283\n",
      "Episode 12752; Testing Loss 0.0058005982990731635; Training Loss 0.004667352430572482\n",
      "Episode 12753; Testing Loss 0.005800424196489816; Training Loss 0.004667342262439509\n",
      "Episode 12754; Testing Loss 0.005800350251039942; Training Loss 0.004667333416239123\n",
      "Episode 12755; Testing Loss 0.005800453689655826; Training Loss 0.004667324050133567\n",
      "Episode 12756; Testing Loss 0.005800495427170489; Training Loss 0.004667315199119148\n",
      "Episode 12757; Testing Loss 0.005800399375004659; Training Loss 0.004667305328288289\n",
      "Episode 12758; Testing Loss 0.005800355478480042; Training Loss 0.004667297559855901\n",
      "Episode 12759; Testing Loss 0.005800483410265534; Training Loss 0.004667288520615778\n",
      "Episode 12760; Testing Loss 0.005800654769015467; Training Loss 0.0046672807739272446\n",
      "Episode 12761; Testing Loss 0.005800589183805041; Training Loss 0.00466727006919659\n",
      "Episode 12762; Testing Loss 0.005800385984333861; Training Loss 0.004667261300935499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12763; Testing Loss 0.005800361348320084; Training Loss 0.004667251388103081\n",
      "Episode 12764; Testing Loss 0.005800520218649984; Training Loss 0.00466724375571669\n",
      "Episode 12765; Testing Loss 0.005800512230592638; Training Loss 0.004667235585266009\n",
      "Episode 12766; Testing Loss 0.005800342430923964; Training Loss 0.004667225330542563\n",
      "Episode 12767; Testing Loss 0.005800365476682094; Training Loss 0.004667215482476379\n",
      "Episode 12768; Testing Loss 0.005800461369767033; Training Loss 0.004667205566554977\n",
      "Episode 12769; Testing Loss 0.0058004610740708075; Training Loss 0.004667197001262095\n",
      "Episode 12770; Testing Loss 0.005800354661177414; Training Loss 0.004667187902314793\n",
      "Episode 12771; Testing Loss 0.005800398023172233; Training Loss 0.004667179768023257\n",
      "Episode 12772; Testing Loss 0.005800445498113632; Training Loss 0.004667169964889318\n",
      "Episode 12773; Testing Loss 0.005800454012124478; Training Loss 0.004667161379070399\n",
      "Episode 12774; Testing Loss 0.005800385712558577; Training Loss 0.004667152239551511\n",
      "Episode 12775; Testing Loss 0.0058004520083965025; Training Loss 0.004667145554364351\n",
      "Episode 12776; Testing Loss 0.005800460257876292; Training Loss 0.004667136070270055\n",
      "Episode 12777; Testing Loss 0.005800379390192463; Training Loss 0.0046671254616085615\n",
      "Episode 12778; Testing Loss 0.005800381177683898; Training Loss 0.004667122001699545\n",
      "Episode 12779; Testing Loss 0.005800455960401509; Training Loss 0.004667113258241106\n",
      "Episode 12780; Testing Loss 0.0058004784145470134; Training Loss 0.004667101324822594\n",
      "Episode 12781; Testing Loss 0.005800297680978696; Training Loss 0.0046670919334848675\n",
      "Episode 12782; Testing Loss 0.005800251059777846; Training Loss 0.004667082310672162\n",
      "Episode 12783; Testing Loss 0.005800403908429899; Training Loss 0.004667072941565013\n",
      "Episode 12784; Testing Loss 0.00580056796257569; Training Loss 0.004667066040968857\n",
      "Episode 12785; Testing Loss 0.005800482835385685; Training Loss 0.00466705541697838\n",
      "Episode 12786; Testing Loss 0.005800282806618924; Training Loss 0.004667047863693336\n",
      "Episode 12787; Testing Loss 0.0058002285192265; Training Loss 0.004667039323819243\n",
      "Episode 12788; Testing Loss 0.005800352074707953; Training Loss 0.0046670274226618385\n",
      "Episode 12789; Testing Loss 0.005800420635809724; Training Loss 0.004667019124805538\n",
      "Episode 12790; Testing Loss 0.005800385297727462; Training Loss 0.00466701517363893\n",
      "Episode 12791; Testing Loss 0.005800470827234509; Training Loss 0.004667003572350037\n",
      "Episode 12792; Testing Loss 0.005800439582298926; Training Loss 0.004666993970163096\n",
      "Episode 12793; Testing Loss 0.005800267164694806; Training Loss 0.004666987632432678\n",
      "Episode 12794; Testing Loss 0.005800252061543229; Training Loss 0.004666976244118175\n",
      "Episode 12795; Testing Loss 0.005800347479712875; Training Loss 0.0046669669838109535\n",
      "Episode 12796; Testing Loss 0.005800312447423508; Training Loss 0.004666959601713651\n",
      "Episode 12797; Testing Loss 0.005800234341945496; Training Loss 0.004666953015257139\n",
      "Episode 12798; Testing Loss 0.005800356964101878; Training Loss 0.004666941145336223\n",
      "Episode 12799; Testing Loss 0.005800388764799841; Training Loss 0.00466693167919786\n",
      "Episode 12800; Testing Loss 0.005800265016393061; Training Loss 0.004666921453330946\n",
      "Episode 12801; Testing Loss 0.005800239152047685; Training Loss 0.004666916051215241\n",
      "Episode 12802; Testing Loss 0.005800259287473251; Training Loss 0.004666905950860126\n",
      "Episode 12803; Testing Loss 0.005800321790665583; Training Loss 0.00466689430514122\n",
      "Episode 12804; Testing Loss 0.005800380285064106; Training Loss 0.004666887123618701\n",
      "Episode 12805; Testing Loss 0.005800374009564224; Training Loss 0.004666879322004515\n",
      "Episode 12806; Testing Loss 0.005800330878227735; Training Loss 0.004666868918001407\n",
      "Episode 12807; Testing Loss 0.005800327066139088; Training Loss 0.004666859946301005\n",
      "Episode 12808; Testing Loss 0.005800345513363086; Training Loss 0.004666850838510401\n",
      "Episode 12809; Testing Loss 0.005800343746493614; Training Loss 0.004666841581234133\n",
      "Episode 12810; Testing Loss 0.00580033038633525; Training Loss 0.004666833863034522\n",
      "Episode 12811; Testing Loss 0.0058002726137815356; Training Loss 0.004666826786988235\n",
      "Episode 12812; Testing Loss 0.005800177744773391; Training Loss 0.004666816183185019\n",
      "Episode 12813; Testing Loss 0.005800188546526613; Training Loss 0.004666804619207215\n",
      "Episode 12814; Testing Loss 0.005800341406005491; Training Loss 0.004666797525460626\n",
      "Episode 12815; Testing Loss 0.005800416565665414; Training Loss 0.0046667889974572165\n",
      "Episode 12816; Testing Loss 0.005800337587744142; Training Loss 0.004666779020244512\n",
      "Episode 12817; Testing Loss 0.005800332355849985; Training Loss 0.004666769608570079\n",
      "Episode 12818; Testing Loss 0.005800408077924733; Training Loss 0.004666762565026572\n",
      "Episode 12819; Testing Loss 0.005800335008062071; Training Loss 0.00466675255526766\n",
      "Episode 12820; Testing Loss 0.005800236130348308; Training Loss 0.004666741508749025\n",
      "Episode 12821; Testing Loss 0.005800291346716308; Training Loss 0.004666732561664978\n",
      "Episode 12822; Testing Loss 0.005800433723945685; Training Loss 0.004666724438774477\n",
      "Episode 12823; Testing Loss 0.005800369270842918; Training Loss 0.004666714952045413\n",
      "Episode 12824; Testing Loss 0.0058002869470216; Training Loss 0.0046667059271295434\n",
      "Episode 12825; Testing Loss 0.005800321454838594; Training Loss 0.004666697590063826\n",
      "Episode 12826; Testing Loss 0.005800324876120401; Training Loss 0.00466668734129274\n",
      "Episode 12827; Testing Loss 0.005800292192052781; Training Loss 0.00466668092424265\n",
      "Episode 12828; Testing Loss 0.005800327621568185; Training Loss 0.004666670126867864\n",
      "Episode 12829; Testing Loss 0.005800280717227572; Training Loss 0.0046666603707876805\n",
      "Episode 12830; Testing Loss 0.005800331551672677; Training Loss 0.004666652992052659\n",
      "Episode 12831; Testing Loss 0.005800361183930921; Training Loss 0.004666645120397355\n",
      "Episode 12832; Testing Loss 0.0058003887409341605; Training Loss 0.0046666353785587925\n",
      "Episode 12833; Testing Loss 0.005800286964877211; Training Loss 0.004666623912933112\n",
      "Episode 12834; Testing Loss 0.005800180302352626; Training Loss 0.004666619570175146\n",
      "Episode 12835; Testing Loss 0.005800230106087292; Training Loss 0.00466660937497435\n",
      "Episode 12836; Testing Loss 0.005800239696278839; Training Loss 0.004666599843043562\n",
      "Episode 12837; Testing Loss 0.005800144722988379; Training Loss 0.004666590647016054\n",
      "Episode 12838; Testing Loss 0.005800120773432715; Training Loss 0.004666582829148699\n",
      "Episode 12839; Testing Loss 0.005800328708109141; Training Loss 0.004666572494737379\n",
      "Episode 12840; Testing Loss 0.005800369338680044; Training Loss 0.004666563779694734\n",
      "Episode 12841; Testing Loss 0.005800248497721535; Training Loss 0.004666553817742025\n",
      "Episode 12842; Testing Loss 0.005800201341770203; Training Loss 0.004666544856890281\n",
      "Episode 12843; Testing Loss 0.005800256400668566; Training Loss 0.004666535935340357\n",
      "Episode 12844; Testing Loss 0.005800311121712431; Training Loss 0.004666527282265718\n",
      "Episode 12845; Testing Loss 0.005800164074194447; Training Loss 0.004666518659595802\n",
      "Episode 12846; Testing Loss 0.005800133833418871; Training Loss 0.004666509839650072\n",
      "Episode 12847; Testing Loss 0.005800160189050417; Training Loss 0.004666500448812177\n",
      "Episode 12848; Testing Loss 0.005800174898631264; Training Loss 0.004666493513825674\n",
      "Episode 12849; Testing Loss 0.005800196944739044; Training Loss 0.0046664836675823305\n",
      "Episode 12850; Testing Loss 0.005800245427422563; Training Loss 0.004666475446586747\n",
      "Episode 12851; Testing Loss 0.005800249603740994; Training Loss 0.004666465918988955\n",
      "Episode 12852; Testing Loss 0.005800216936496497; Training Loss 0.004666455946230039\n",
      "Episode 12853; Testing Loss 0.005800165034083287; Training Loss 0.00466644599186507\n",
      "Episode 12854; Testing Loss 0.005800132330339627; Training Loss 0.00466643763535718\n",
      "Episode 12855; Testing Loss 0.005800147876095428; Training Loss 0.004666429701775206\n",
      "Episode 12856; Testing Loss 0.005800194842990956; Training Loss 0.004666419227048648\n",
      "Episode 12857; Testing Loss 0.005800215069971274; Training Loss 0.00466641140223523\n",
      "Episode 12858; Testing Loss 0.005800274896944335; Training Loss 0.0046664026042761795\n",
      "Episode 12859; Testing Loss 0.005800180360624191; Training Loss 0.004666392794763585\n",
      "Episode 12860; Testing Loss 0.00580008705427519; Training Loss 0.004666384439703025\n",
      "Episode 12861; Testing Loss 0.00580023804974776; Training Loss 0.00466637510813607\n",
      "Episode 12862; Testing Loss 0.005800355724453577; Training Loss 0.004666367851363356\n",
      "Episode 12863; Testing Loss 0.0058002231317848835; Training Loss 0.0046663571763542865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12864; Testing Loss 0.005800103769431374; Training Loss 0.004666350378911416\n",
      "Episode 12865; Testing Loss 0.005800245534380188; Training Loss 0.004666340477838174\n",
      "Episode 12866; Testing Loss 0.005800260766943682; Training Loss 0.004666331500626683\n",
      "Episode 12867; Testing Loss 0.005800106581911392; Training Loss 0.00466632292528106\n",
      "Episode 12868; Testing Loss 0.0058001814846287995; Training Loss 0.004666312508712058\n",
      "Episode 12869; Testing Loss 0.005800255406351368; Training Loss 0.00466630534902825\n",
      "Episode 12870; Testing Loss 0.005800238225377345; Training Loss 0.004666296477919444\n",
      "Episode 12871; Testing Loss 0.005800199015590977; Training Loss 0.0046662862627435565\n",
      "Episode 12872; Testing Loss 0.005800285627256174; Training Loss 0.004666276287859225\n",
      "Episode 12873; Testing Loss 0.005800300407819182; Training Loss 0.004666270167696663\n",
      "Episode 12874; Testing Loss 0.00580013421755029; Training Loss 0.004666259738706932\n",
      "Episode 12875; Testing Loss 0.005800056257337226; Training Loss 0.004666251256726724\n",
      "Episode 12876; Testing Loss 0.005800186905451078; Training Loss 0.004666242129711028\n",
      "Episode 12877; Testing Loss 0.005800279617831188; Training Loss 0.004666234005135834\n",
      "Episode 12878; Testing Loss 0.005800297131906787; Training Loss 0.004666223408767593\n",
      "Episode 12879; Testing Loss 0.005800185627403515; Training Loss 0.0046662143385477525\n",
      "Episode 12880; Testing Loss 0.005800134517447526; Training Loss 0.00466620777415902\n",
      "Episode 12881; Testing Loss 0.00580011979389965; Training Loss 0.004666199029855402\n",
      "Episode 12882; Testing Loss 0.00580017618225118; Training Loss 0.004666187615462179\n",
      "Episode 12883; Testing Loss 0.0058002578966384185; Training Loss 0.004666180346701546\n",
      "Episode 12884; Testing Loss 0.005800169255915277; Training Loss 0.004666174021928635\n",
      "Episode 12885; Testing Loss 0.00580007564297276; Training Loss 0.004666164171907448\n",
      "Episode 12886; Testing Loss 0.005800212202555671; Training Loss 0.004666151461745635\n",
      "Episode 12887; Testing Loss 0.00580029249322161; Training Loss 0.00466614564777956\n",
      "Episode 12888; Testing Loss 0.005800142870977773; Training Loss 0.004666137918998972\n",
      "Episode 12889; Testing Loss 0.005800073104461179; Training Loss 0.004666126643875562\n",
      "Episode 12890; Testing Loss 0.005800176232210257; Training Loss 0.004666116939101748\n",
      "Episode 12891; Testing Loss 0.005800206236469663; Training Loss 0.004666109767561695\n",
      "Episode 12892; Testing Loss 0.005800119952470633; Training Loss 0.004666099816027923\n",
      "Episode 12893; Testing Loss 0.00580012051826782; Training Loss 0.004666090602065238\n",
      "Episode 12894; Testing Loss 0.005800192369842211; Training Loss 0.004666079813475973\n",
      "Episode 12895; Testing Loss 0.005800214447963577; Training Loss 0.0046660702072827635\n",
      "Episode 12896; Testing Loss 0.005800097819454481; Training Loss 0.004666061700889338\n",
      "Episode 12897; Testing Loss 0.005800160256128208; Training Loss 0.004666052633527657\n",
      "Episode 12898; Testing Loss 0.005800202217751877; Training Loss 0.004666044352357253\n",
      "Episode 12899; Testing Loss 0.0058001606487950485; Training Loss 0.0046660351580891\n",
      "Episode 12900; Testing Loss 0.00580012500243374; Training Loss 0.004666026498059716\n",
      "Episode 12901; Testing Loss 0.005800143072951102; Training Loss 0.004666017050216513\n",
      "Episode 12902; Testing Loss 0.005800043309924329; Training Loss 0.004666007771176498\n",
      "Episode 12903; Testing Loss 0.005800025503891204; Training Loss 0.004665999775409898\n",
      "Episode 12904; Testing Loss 0.005800208752530664; Training Loss 0.0046659912842737\n",
      "Episode 12905; Testing Loss 0.005800226233223826; Training Loss 0.004665982213068825\n",
      "Episode 12906; Testing Loss 0.005800069902791443; Training Loss 0.00466597253651587\n",
      "Episode 12907; Testing Loss 0.0058000534272607904; Training Loss 0.004665963455105346\n",
      "Episode 12908; Testing Loss 0.005800137846474644; Training Loss 0.004665954611380352\n",
      "Episode 12909; Testing Loss 0.005800100039824326; Training Loss 0.004665944839806145\n",
      "Episode 12910; Testing Loss 0.005800047249980192; Training Loss 0.004665938187616445\n",
      "Episode 12911; Testing Loss 0.0058001637570696705; Training Loss 0.0046659279521826524\n",
      "Episode 12912; Testing Loss 0.005800136484323502; Training Loss 0.004665918241367709\n",
      "Episode 12913; Testing Loss 0.005800051612968461; Training Loss 0.00466590866088628\n",
      "Episode 12914; Testing Loss 0.00580014851036489; Training Loss 0.004665900726475053\n",
      "Episode 12915; Testing Loss 0.005800130154471713; Training Loss 0.00466589175002351\n",
      "Episode 12916; Testing Loss 0.005800108036958243; Training Loss 0.004665881904290043\n",
      "Episode 12917; Testing Loss 0.0058001404779698465; Training Loss 0.004665873132836538\n",
      "Episode 12918; Testing Loss 0.005800098072229611; Training Loss 0.0046658648134342146\n",
      "Episode 12919; Testing Loss 0.005800091449971618; Training Loss 0.004665855537064597\n",
      "Episode 12920; Testing Loss 0.005800122074487757; Training Loss 0.004665845079236965\n",
      "Episode 12921; Testing Loss 0.005800120162468753; Training Loss 0.004665838523233155\n",
      "Episode 12922; Testing Loss 0.005800047216313427; Training Loss 0.004665828167301544\n",
      "Episode 12923; Testing Loss 0.00580000755881017; Training Loss 0.004665820278152708\n",
      "Episode 12924; Testing Loss 0.005800165054439024; Training Loss 0.004665810106081827\n",
      "Episode 12925; Testing Loss 0.0058001720441902705; Training Loss 0.004665802647072569\n",
      "Episode 12926; Testing Loss 0.005800035610008018; Training Loss 0.00466579277303055\n",
      "Episode 12927; Testing Loss 0.00580001502979774; Training Loss 0.0046657826564430425\n",
      "Episode 12928; Testing Loss 0.005800138185995721; Training Loss 0.004665777183476673\n",
      "Episode 12929; Testing Loss 0.0058001507816032765; Training Loss 0.004665767492550381\n",
      "Episode 12930; Testing Loss 0.005800042511010473; Training Loss 0.004665755955761348\n",
      "Episode 12931; Testing Loss 0.005800040342528731; Training Loss 0.004665747062424209\n",
      "Episode 12932; Testing Loss 0.005800127408953058; Training Loss 0.004665739254850341\n",
      "Episode 12933; Testing Loss 0.005800093243877205; Training Loss 0.004665729710852616\n",
      "Episode 12934; Testing Loss 0.005800033030537657; Training Loss 0.004665720238538961\n",
      "Episode 12935; Testing Loss 0.005800062395624413; Training Loss 0.0046657124273371256\n",
      "Episode 12936; Testing Loss 0.005800090913720473; Training Loss 0.004665702208511722\n",
      "Episode 12937; Testing Loss 0.00580000378308279; Training Loss 0.004665694231941212\n",
      "Episode 12938; Testing Loss 0.005800039015553061; Training Loss 0.004665685590870003\n",
      "Episode 12939; Testing Loss 0.005800085508686589; Training Loss 0.004665675340183738\n",
      "Episode 12940; Testing Loss 0.00580011220077457; Training Loss 0.004665665986120605\n",
      "Episode 12941; Testing Loss 0.005800116809639498; Training Loss 0.004665658130024788\n",
      "Episode 12942; Testing Loss 0.005800063812294123; Training Loss 0.004665648231234214\n",
      "Episode 12943; Testing Loss 0.00579996262650494; Training Loss 0.004665639586314758\n",
      "Episode 12944; Testing Loss 0.005800038032413818; Training Loss 0.004665630875724747\n",
      "Episode 12945; Testing Loss 0.005800159035239375; Training Loss 0.004665623193136081\n",
      "Episode 12946; Testing Loss 0.005800056084674235; Training Loss 0.0046656117418686135\n",
      "Episode 12947; Testing Loss 0.005800008137292524; Training Loss 0.00466560431769272\n",
      "Episode 12948; Testing Loss 0.005800123221105862; Training Loss 0.0046655957399525305\n",
      "Episode 12949; Testing Loss 0.0058001185117385595; Training Loss 0.004665588273843912\n",
      "Episode 12950; Testing Loss 0.005799986702402182; Training Loss 0.004665578541420513\n",
      "Episode 12951; Testing Loss 0.005799969711036663; Training Loss 0.0046655711710905795\n",
      "Episode 12952; Testing Loss 0.005800187257115626; Training Loss 0.004665559617853897\n",
      "Episode 12953; Testing Loss 0.0058001401441072605; Training Loss 0.0046655504427223285\n",
      "Episode 12954; Testing Loss 0.005799895063099694; Training Loss 0.004665541287175759\n",
      "Episode 12955; Testing Loss 0.005799910267252716; Training Loss 0.004665533819935063\n",
      "Episode 12956; Testing Loss 0.005800175834645238; Training Loss 0.004665524315748235\n",
      "Episode 12957; Testing Loss 0.005800159952544087; Training Loss 0.00466551742514042\n",
      "Episode 12958; Testing Loss 0.005799879266055673; Training Loss 0.0046655066007809595\n",
      "Episode 12959; Testing Loss 0.0057998358694863385; Training Loss 0.0046654994631198194\n",
      "Episode 12960; Testing Loss 0.0058001988515555694; Training Loss 0.004665489216513613\n",
      "Episode 12961; Testing Loss 0.005800177488018889; Training Loss 0.004665478293933814\n",
      "Episode 12962; Testing Loss 0.005799917321345646; Training Loss 0.004665471255193617\n",
      "Episode 12963; Testing Loss 0.00579995015127591; Training Loss 0.004665462639080938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12964; Testing Loss 0.005800107674724865; Training Loss 0.004665453055099101\n",
      "Episode 12965; Testing Loss 0.005800069043283206; Training Loss 0.004665444255877962\n",
      "Episode 12966; Testing Loss 0.005799975066446624; Training Loss 0.004665433603048125\n",
      "Episode 12967; Testing Loss 0.005799952812311065; Training Loss 0.004665425624893337\n",
      "Episode 12968; Testing Loss 0.005800035807545846; Training Loss 0.0046654162653510705\n",
      "Episode 12969; Testing Loss 0.005800065894162661; Training Loss 0.004665406012559189\n",
      "Episode 12970; Testing Loss 0.005800087771379356; Training Loss 0.004665398524811644\n",
      "Episode 12971; Testing Loss 0.005800150831440728; Training Loss 0.004665390547311991\n",
      "Episode 12972; Testing Loss 0.005800060077910882; Training Loss 0.004665380941053428\n",
      "Episode 12973; Testing Loss 0.005799965218524461; Training Loss 0.00466537074854944\n",
      "Episode 12974; Testing Loss 0.005800000492714499; Training Loss 0.004665364264070091\n",
      "Episode 12975; Testing Loss 0.00579998646327505; Training Loss 0.004665354890648748\n",
      "Episode 12976; Testing Loss 0.005800009887755642; Training Loss 0.004665343034244728\n",
      "Episode 12977; Testing Loss 0.005800091078055171; Training Loss 0.004665336284094059\n",
      "Episode 12978; Testing Loss 0.0058000903375159145; Training Loss 0.004665326625959252\n",
      "Episode 12979; Testing Loss 0.005800049083037212; Training Loss 0.004665316013202309\n",
      "Episode 12980; Testing Loss 0.005800018623369679; Training Loss 0.0046653072085014865\n",
      "Episode 12981; Testing Loss 0.005799953313865145; Training Loss 0.004665299492804755\n",
      "Episode 12982; Testing Loss 0.005799927736099786; Training Loss 0.0046652901028097155\n",
      "Episode 12983; Testing Loss 0.005799947971049631; Training Loss 0.004665283812478968\n",
      "Episode 12984; Testing Loss 0.0058001025744877505; Training Loss 0.00466527327024959\n",
      "Episode 12985; Testing Loss 0.00580005682520151; Training Loss 0.004665264144480991\n",
      "Episode 12986; Testing Loss 0.00579988103590837; Training Loss 0.004665254493642503\n",
      "Episode 12987; Testing Loss 0.005799926703808887; Training Loss 0.00466524844539433\n",
      "Episode 12988; Testing Loss 0.00580015598245961; Training Loss 0.004665238039856417\n",
      "Episode 12989; Testing Loss 0.005800129762920243; Training Loss 0.004665226688813363\n",
      "Episode 12990; Testing Loss 0.005799957686215504; Training Loss 0.004665218821129865\n",
      "Episode 12991; Testing Loss 0.005799926999280362; Training Loss 0.004665210299931238\n",
      "Episode 12992; Testing Loss 0.00579998561674924; Training Loss 0.004665201416627037\n",
      "Episode 12993; Testing Loss 0.005800049401283916; Training Loss 0.0046651913184855565\n",
      "Episode 12994; Testing Loss 0.005800021462713827; Training Loss 0.0046651821738872175\n",
      "Episode 12995; Testing Loss 0.0057999993244479365; Training Loss 0.004665173116420339\n",
      "Episode 12996; Testing Loss 0.005799985545248525; Training Loss 0.00466516477652104\n",
      "Episode 12997; Testing Loss 0.005800031539702413; Training Loss 0.0046651564203243154\n",
      "Episode 12998; Testing Loss 0.005800064014728465; Training Loss 0.004665148474401453\n",
      "Episode 12999; Testing Loss 0.00579994532771837; Training Loss 0.004665137732261044\n",
      "Episode 13000; Testing Loss 0.005799913924840027; Training Loss 0.004665127621480332\n",
      "Episode 13001; Testing Loss 0.005800008005902426; Training Loss 0.004665117904905522\n",
      "Episode 13002; Testing Loss 0.005800022973894457; Training Loss 0.004665111143174588\n",
      "Episode 13003; Testing Loss 0.005800030151866017; Training Loss 0.004665101935426521\n",
      "Episode 13004; Testing Loss 0.00579994829497466; Training Loss 0.004665092714484139\n",
      "Episode 13005; Testing Loss 0.005799896296302098; Training Loss 0.004665082753894435\n",
      "Episode 13006; Testing Loss 0.005799939550629285; Training Loss 0.004665074157733773\n",
      "Episode 13007; Testing Loss 0.005799951613556651; Training Loss 0.0046650652000549365\n",
      "Episode 13008; Testing Loss 0.005799941058479007; Training Loss 0.00466505611774021\n",
      "Episode 13009; Testing Loss 0.005799886193000114; Training Loss 0.0046650474746272414\n",
      "Episode 13010; Testing Loss 0.005799983895086026; Training Loss 0.004665039130273397\n",
      "Episode 13011; Testing Loss 0.005799937620211129; Training Loss 0.004665028997321474\n",
      "Episode 13012; Testing Loss 0.005799876834356777; Training Loss 0.004665019784790593\n",
      "Episode 13013; Testing Loss 0.005799953627941184; Training Loss 0.004665012692117505\n",
      "Episode 13014; Testing Loss 0.005799992440823116; Training Loss 0.004665003106174449\n",
      "Episode 13015; Testing Loss 0.0057998511779318775; Training Loss 0.004664995705894013\n",
      "Episode 13016; Testing Loss 0.005799871994353243; Training Loss 0.004664987376339183\n",
      "Episode 13017; Testing Loss 0.005800124978535474; Training Loss 0.004664977797229686\n",
      "Episode 13018; Testing Loss 0.005800180846923475; Training Loss 0.004664968666026858\n",
      "Episode 13019; Testing Loss 0.005799951047395304; Training Loss 0.00466495660215461\n",
      "Episode 13020; Testing Loss 0.005799824686168158; Training Loss 0.004664949682128417\n",
      "Episode 13021; Testing Loss 0.0057998552155087615; Training Loss 0.004664940375190525\n",
      "Episode 13022; Testing Loss 0.00579990983950987; Training Loss 0.004664931168212923\n",
      "Episode 13023; Testing Loss 0.005799941966949077; Training Loss 0.004664922876985809\n",
      "Episode 13024; Testing Loss 0.005799913108465817; Training Loss 0.004664912320894083\n",
      "Episode 13025; Testing Loss 0.0057998454384833075; Training Loss 0.004664903455219345\n",
      "Episode 13026; Testing Loss 0.005799816322651248; Training Loss 0.004664895253665694\n",
      "Episode 13027; Testing Loss 0.005799950657887107; Training Loss 0.00466488392043102\n",
      "Episode 13028; Testing Loss 0.005800051249409002; Training Loss 0.004664876544087084\n",
      "Episode 13029; Testing Loss 0.005799977456078362; Training Loss 0.00466486769978335\n",
      "Episode 13030; Testing Loss 0.005799899659143285; Training Loss 0.004664857716522052\n",
      "Episode 13031; Testing Loss 0.005799909245960618; Training Loss 0.004664847545231424\n",
      "Episode 13032; Testing Loss 0.00579998386340751; Training Loss 0.004664842212736214\n",
      "Episode 13033; Testing Loss 0.005799911813649959; Training Loss 0.004664832075318234\n",
      "Episode 13034; Testing Loss 0.005799866489382905; Training Loss 0.004664822006535665\n",
      "Episode 13035; Testing Loss 0.005799959201439651; Training Loss 0.00466481385879471\n",
      "Episode 13036; Testing Loss 0.005800129685374704; Training Loss 0.004664806536352734\n",
      "Episode 13037; Testing Loss 0.005800083417547071; Training Loss 0.004664795897284409\n",
      "Episode 13038; Testing Loss 0.005799973542720976; Training Loss 0.004664784129637872\n",
      "Episode 13039; Testing Loss 0.005799973783733204; Training Loss 0.004664779814782057\n",
      "Episode 13040; Testing Loss 0.005799949517952921; Training Loss 0.004664772345213127\n",
      "Episode 13041; Testing Loss 0.005799915885077242; Training Loss 0.004664758539556952\n",
      "Episode 13042; Testing Loss 0.005799896885265568; Training Loss 0.004664750359699119\n",
      "Episode 13043; Testing Loss 0.005800004856759784; Training Loss 0.004664743657144653\n",
      "Episode 13044; Testing Loss 0.005800089936939589; Training Loss 0.004664735491490533\n",
      "Episode 13045; Testing Loss 0.005800014685756922; Training Loss 0.0046647249046633\n",
      "Episode 13046; Testing Loss 0.005799934926847705; Training Loss 0.0046647134262440175\n",
      "Episode 13047; Testing Loss 0.005799937367550593; Training Loss 0.004664702339646437\n",
      "Episode 13048; Testing Loss 0.0057999119415632185; Training Loss 0.004664697643866598\n",
      "Episode 13049; Testing Loss 0.00579989310544936; Training Loss 0.004664689843791315\n",
      "Episode 13050; Testing Loss 0.005799929443730397; Training Loss 0.004664678125821842\n",
      "Episode 13051; Testing Loss 0.005799960950705699; Training Loss 0.004664668427255021\n",
      "Episode 13052; Testing Loss 0.0058000120597731936; Training Loss 0.004664661041210987\n",
      "Episode 13053; Testing Loss 0.005800006067671546; Training Loss 0.004664651187781083\n",
      "Episode 13054; Testing Loss 0.0057999746532667335; Training Loss 0.0046646423530426265\n",
      "Episode 13055; Testing Loss 0.005799973212517993; Training Loss 0.004664632330476611\n",
      "Episode 13056; Testing Loss 0.005799992941295448; Training Loss 0.004664620712533029\n",
      "Episode 13057; Testing Loss 0.005799955837484188; Training Loss 0.004664612554798411\n",
      "Episode 13058; Testing Loss 0.005799901084054765; Training Loss 0.004664604075938534\n",
      "Episode 13059; Testing Loss 0.005799923862972097; Training Loss 0.0046645950465417215\n",
      "Episode 13060; Testing Loss 0.005799952258295954; Training Loss 0.004664586532567973\n",
      "Episode 13061; Testing Loss 0.005799884088933999; Training Loss 0.004664575811077512\n",
      "Episode 13062; Testing Loss 0.0057998191325741195; Training Loss 0.004664567861245831\n",
      "Episode 13063; Testing Loss 0.0057999838801063965; Training Loss 0.004664558190183628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13064; Testing Loss 0.0058000886665911635; Training Loss 0.004664549270623667\n",
      "Episode 13065; Testing Loss 0.005800034206372837; Training Loss 0.004664540629168138\n",
      "Episode 13066; Testing Loss 0.00579996250744491; Training Loss 0.004664531812995313\n",
      "Episode 13067; Testing Loss 0.00579997332910076; Training Loss 0.004664522430255444\n",
      "Episode 13068; Testing Loss 0.005800000332522359; Training Loss 0.004664512497881242\n",
      "Episode 13069; Testing Loss 0.00580006855007257; Training Loss 0.004664503281704746\n",
      "Episode 13070; Testing Loss 0.00579996043545415; Training Loss 0.00466449582697173\n",
      "Episode 13071; Testing Loss 0.005799886053817862; Training Loss 0.004664488050582454\n",
      "Episode 13072; Testing Loss 0.0057999736582127395; Training Loss 0.004664476968584857\n",
      "Episode 13073; Testing Loss 0.0058001331687029655; Training Loss 0.004664468202577234\n",
      "Episode 13074; Testing Loss 0.0058000495862143165; Training Loss 0.004664458564872433\n",
      "Episode 13075; Testing Loss 0.005799909974901462; Training Loss 0.00466445002253968\n",
      "Episode 13076; Testing Loss 0.005799949838997235; Training Loss 0.00466444141789345\n",
      "Episode 13077; Testing Loss 0.005799963197967479; Training Loss 0.004664431868394856\n",
      "Episode 13078; Testing Loss 0.005799921055000341; Training Loss 0.004664422768452108\n",
      "Episode 13079; Testing Loss 0.00579992749388966; Training Loss 0.004664414775892556\n",
      "Episode 13080; Testing Loss 0.005799889719349911; Training Loss 0.004664405392095407\n",
      "Episode 13081; Testing Loss 0.005799933457637834; Training Loss 0.004664396545311004\n",
      "Episode 13082; Testing Loss 0.005799910261704696; Training Loss 0.004664389047850151\n",
      "Episode 13083; Testing Loss 0.005799840643253045; Training Loss 0.004664377676674753\n",
      "Episode 13084; Testing Loss 0.005799935725901157; Training Loss 0.004664368878060663\n",
      "Episode 13085; Testing Loss 0.005800070391590221; Training Loss 0.0046643623773450354\n",
      "Episode 13086; Testing Loss 0.005800134017175624; Training Loss 0.004664352828678951\n",
      "Episode 13087; Testing Loss 0.005799983751725143; Training Loss 0.004664344137432457\n",
      "Episode 13088; Testing Loss 0.005799833266297252; Training Loss 0.0046643351866151135\n",
      "Episode 13089; Testing Loss 0.005799874307488344; Training Loss 0.004664326443876675\n",
      "Episode 13090; Testing Loss 0.005799939037133866; Training Loss 0.004664318444946864\n",
      "Episode 13091; Testing Loss 0.005799924599479469; Training Loss 0.0046643108529830085\n",
      "Episode 13092; Testing Loss 0.005799957832010402; Training Loss 0.004664299237009653\n",
      "Episode 13093; Testing Loss 0.005800000858015931; Training Loss 0.004664289241595749\n",
      "Episode 13094; Testing Loss 0.005799921890396416; Training Loss 0.004664280950867013\n",
      "Episode 13095; Testing Loss 0.005799874811357987; Training Loss 0.004664273785704439\n",
      "Episode 13096; Testing Loss 0.005799896170433769; Training Loss 0.004664261988936254\n",
      "Episode 13097; Testing Loss 0.00579994380600744; Training Loss 0.0046642535980328465\n",
      "Episode 13098; Testing Loss 0.005799998547663174; Training Loss 0.004664246650312104\n",
      "Episode 13099; Testing Loss 0.005799986674133619; Training Loss 0.0046642369993380025\n",
      "Episode 13100; Testing Loss 0.005799953461106927; Training Loss 0.004664226358943709\n",
      "Episode 13101; Testing Loss 0.005799972827518565; Training Loss 0.004664218847242233\n",
      "Episode 13102; Testing Loss 0.0058000032028140145; Training Loss 0.004664208695623519\n",
      "Episode 13103; Testing Loss 0.0057999972909122495; Training Loss 0.004664199017801719\n",
      "Episode 13104; Testing Loss 0.005799869385481171; Training Loss 0.004664191221673359\n",
      "Episode 13105; Testing Loss 0.005799857914465842; Training Loss 0.004664181049488444\n",
      "Episode 13106; Testing Loss 0.005799984724684203; Training Loss 0.004664171258090734\n",
      "Episode 13107; Testing Loss 0.005800000484455118; Training Loss 0.004664164099479888\n",
      "Episode 13108; Testing Loss 0.005799885895121419; Training Loss 0.004664153755343374\n",
      "Episode 13109; Testing Loss 0.0057998499375639; Training Loss 0.004664144392014588\n",
      "Episode 13110; Testing Loss 0.005799913197439112; Training Loss 0.004664135754715221\n",
      "Episode 13111; Testing Loss 0.005799937006875428; Training Loss 0.004664127842127182\n",
      "Episode 13112; Testing Loss 0.005799916483251434; Training Loss 0.004664118031093455\n",
      "Episode 13113; Testing Loss 0.005799885741701507; Training Loss 0.004664110287883641\n",
      "Episode 13114; Testing Loss 0.005799890084672013; Training Loss 0.00466410021463136\n",
      "Episode 13115; Testing Loss 0.005799953086344916; Training Loss 0.004664089466793558\n",
      "Episode 13116; Testing Loss 0.005799922691424646; Training Loss 0.004664081402288377\n",
      "Episode 13117; Testing Loss 0.005799945270243182; Training Loss 0.004664072649509861\n",
      "Episode 13118; Testing Loss 0.005799888209611367; Training Loss 0.004664063120673621\n",
      "Episode 13119; Testing Loss 0.005799899719351175; Training Loss 0.004664054481703766\n",
      "Episode 13120; Testing Loss 0.005799896021012741; Training Loss 0.004664044972724011\n",
      "Episode 13121; Testing Loss 0.005799885817652358; Training Loss 0.004664036096266175\n",
      "Episode 13122; Testing Loss 0.005799901092890641; Training Loss 0.004664028240631762\n",
      "Episode 13123; Testing Loss 0.005799869708683371; Training Loss 0.0046640186264975255\n",
      "Episode 13124; Testing Loss 0.00579985770269982; Training Loss 0.004664009534968844\n",
      "Episode 13125; Testing Loss 0.005799874196337242; Training Loss 0.0046640013668420586\n",
      "Episode 13126; Testing Loss 0.005799925585349582; Training Loss 0.004663991229906548\n",
      "Episode 13127; Testing Loss 0.0057998247339256905; Training Loss 0.004663983456819004\n",
      "Episode 13128; Testing Loss 0.005799838019577692; Training Loss 0.004663974139258103\n",
      "Episode 13129; Testing Loss 0.005799981393057813; Training Loss 0.004663965853162743\n",
      "Episode 13130; Testing Loss 0.005799944182190024; Training Loss 0.004663955964919045\n",
      "Episode 13131; Testing Loss 0.005799752936200686; Training Loss 0.004663947086965399\n",
      "Episode 13132; Testing Loss 0.005799651328400517; Training Loss 0.004663938747957124\n",
      "Episode 13133; Testing Loss 0.005799828229231732; Training Loss 0.004663928514179417\n",
      "Episode 13134; Testing Loss 0.0057999716131767395; Training Loss 0.004663921015119553\n",
      "Episode 13135; Testing Loss 0.0057999472900412945; Training Loss 0.004663912046855211\n",
      "Episode 13136; Testing Loss 0.005799870050496054; Training Loss 0.004663902453405871\n",
      "Episode 13137; Testing Loss 0.005799846038505785; Training Loss 0.004663893185316664\n",
      "Episode 13138; Testing Loss 0.005799828944703688; Training Loss 0.004663885584749122\n",
      "Episode 13139; Testing Loss 0.005799768763752443; Training Loss 0.0046638771347395445\n",
      "Episode 13140; Testing Loss 0.005799767738688198; Training Loss 0.004663868461154617\n",
      "Episode 13141; Testing Loss 0.005799856251295334; Training Loss 0.004663860395309663\n",
      "Episode 13142; Testing Loss 0.005799930241349709; Training Loss 0.004663849318297886\n",
      "Episode 13143; Testing Loss 0.005799811266568791; Training Loss 0.0046638394896002435\n",
      "Episode 13144; Testing Loss 0.005799716311853716; Training Loss 0.004663832294883734\n",
      "Episode 13145; Testing Loss 0.005799913377257805; Training Loss 0.0046638216870485884\n",
      "Episode 13146; Testing Loss 0.005800010204206126; Training Loss 0.004663814709203051\n",
      "Episode 13147; Testing Loss 0.005799818147563588; Training Loss 0.004663803586832156\n",
      "Episode 13148; Testing Loss 0.005799703438140466; Training Loss 0.004663795551405994\n",
      "Episode 13149; Testing Loss 0.005799830932049613; Training Loss 0.004663785338633757\n",
      "Episode 13150; Testing Loss 0.005799923551455528; Training Loss 0.004663776048694438\n",
      "Episode 13151; Testing Loss 0.005799856404008843; Training Loss 0.004663767927295358\n",
      "Episode 13152; Testing Loss 0.005799740171046874; Training Loss 0.00466376032037718\n",
      "Episode 13153; Testing Loss 0.005799768261068914; Training Loss 0.004663751605826994\n",
      "Episode 13154; Testing Loss 0.005799913191440531; Training Loss 0.004663741966887577\n",
      "Episode 13155; Testing Loss 0.00579992401866114; Training Loss 0.00466373203006903\n",
      "Episode 13156; Testing Loss 0.005799752768512428; Training Loss 0.004663722438549674\n",
      "Episode 13157; Testing Loss 0.005799660602949601; Training Loss 0.004663714494363926\n",
      "Episode 13158; Testing Loss 0.005799704403513942; Training Loss 0.004663705768761044\n",
      "Episode 13159; Testing Loss 0.005799826269401365; Training Loss 0.00466369689811604\n",
      "Episode 13160; Testing Loss 0.005799790443047444; Training Loss 0.004663687055550438\n",
      "Episode 13161; Testing Loss 0.00579971411561022; Training Loss 0.004663680893710769\n",
      "Episode 13162; Testing Loss 0.005799761425743254; Training Loss 0.004663671017874414\n",
      "Episode 13163; Testing Loss 0.005799822045141185; Training Loss 0.004663662609253034\n",
      "Episode 13164; Testing Loss 0.005799805815736871; Training Loss 0.004663652679680517\n",
      "Episode 13165; Testing Loss 0.005799734524924562; Training Loss 0.004663647179271283\n",
      "Episode 13166; Testing Loss 0.0057997827035692106; Training Loss 0.004663635917144943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13167; Testing Loss 0.005799868531889214; Training Loss 0.004663626060135648\n",
      "Episode 13168; Testing Loss 0.005799763091596601; Training Loss 0.0046636214616652145\n",
      "Episode 13169; Testing Loss 0.005799660420301771; Training Loss 0.004663612928587229\n",
      "Episode 13170; Testing Loss 0.005799718024578439; Training Loss 0.004663599280763223\n",
      "Episode 13171; Testing Loss 0.005799826576779782; Training Loss 0.004663591488052817\n",
      "Episode 13172; Testing Loss 0.005799839229243976; Training Loss 0.004663585055249198\n",
      "Episode 13173; Testing Loss 0.0057997581551292; Training Loss 0.004663573673194772\n",
      "Episode 13174; Testing Loss 0.005799692223679136; Training Loss 0.004663565459882527\n",
      "Episode 13175; Testing Loss 0.005799735216785598; Training Loss 0.004663555941421493\n",
      "Episode 13176; Testing Loss 0.005799786868778824; Training Loss 0.004663546749271238\n",
      "Episode 13177; Testing Loss 0.005799797790824298; Training Loss 0.004663537328805612\n",
      "Episode 13178; Testing Loss 0.005799822461794225; Training Loss 0.004663528844436493\n",
      "Episode 13179; Testing Loss 0.005799770082596074; Training Loss 0.004663518871688216\n",
      "Episode 13180; Testing Loss 0.005799735729206695; Training Loss 0.004663510194484009\n",
      "Episode 13181; Testing Loss 0.005799740997346544; Training Loss 0.004663500995435499\n",
      "Episode 13182; Testing Loss 0.005799758481670995; Training Loss 0.004663492407325701\n",
      "Episode 13183; Testing Loss 0.005799848652020972; Training Loss 0.004663484947411045\n",
      "Episode 13184; Testing Loss 0.005799849725550015; Training Loss 0.00466347621970473\n",
      "Episode 13185; Testing Loss 0.005799760142093231; Training Loss 0.0046634659923723105\n",
      "Episode 13186; Testing Loss 0.005799695735493749; Training Loss 0.004663456646496651\n",
      "Episode 13187; Testing Loss 0.005799656004273402; Training Loss 0.004663447494397537\n",
      "Episode 13188; Testing Loss 0.005799675423959388; Training Loss 0.004663441203163482\n",
      "Episode 13189; Testing Loss 0.005799698363282736; Training Loss 0.0046634326616794425\n",
      "Episode 13190; Testing Loss 0.005799713712982502; Training Loss 0.004663421481044213\n",
      "Episode 13191; Testing Loss 0.0057997058740698205; Training Loss 0.00466341304101682\n",
      "Episode 13192; Testing Loss 0.005799728238537991; Training Loss 0.00466340723276289\n",
      "Episode 13193; Testing Loss 0.005799815393077083; Training Loss 0.0046633956622638955\n",
      "Episode 13194; Testing Loss 0.005799753426487232; Training Loss 0.0046633860831249985\n",
      "Episode 13195; Testing Loss 0.005799712753568832; Training Loss 0.0046633772698784276\n",
      "Episode 13196; Testing Loss 0.005799722727161537; Training Loss 0.004663368745661628\n",
      "Episode 13197; Testing Loss 0.005799736114153602; Training Loss 0.004663358870568021\n",
      "Episode 13198; Testing Loss 0.005799739629515017; Training Loss 0.004663350099695083\n",
      "Episode 13199; Testing Loss 0.00579967880679089; Training Loss 0.004663340146238021\n",
      "Episode 13200; Testing Loss 0.0057996686516643085; Training Loss 0.004663331239337242\n",
      "Episode 13201; Testing Loss 0.005799732173893485; Training Loss 0.004663324631172593\n",
      "Episode 13202; Testing Loss 0.005799660928441092; Training Loss 0.004663315183151979\n",
      "Episode 13203; Testing Loss 0.005799536459498685; Training Loss 0.004663306007741797\n",
      "Episode 13204; Testing Loss 0.005799624463126529; Training Loss 0.004663296654730177\n",
      "Episode 13205; Testing Loss 0.005799752518811898; Training Loss 0.004663287361256637\n",
      "Episode 13206; Testing Loss 0.0057997605066329875; Training Loss 0.004663278381573166\n",
      "Episode 13207; Testing Loss 0.0057996588825487425; Training Loss 0.004663269409220616\n",
      "Episode 13208; Testing Loss 0.005799640779597421; Training Loss 0.0046632613272659775\n",
      "Episode 13209; Testing Loss 0.0057997727316983294; Training Loss 0.004663251425565522\n",
      "Episode 13210; Testing Loss 0.005799779102515437; Training Loss 0.004663242551297499\n",
      "Episode 13211; Testing Loss 0.00579966736321512; Training Loss 0.0046632332897585714\n",
      "Episode 13212; Testing Loss 0.0057996441561830505; Training Loss 0.004663224719385811\n",
      "Episode 13213; Testing Loss 0.0057996487653152375; Training Loss 0.004663216393193478\n",
      "Episode 13214; Testing Loss 0.005799690127389416; Training Loss 0.004663208170700649\n",
      "Episode 13215; Testing Loss 0.005799659980621774; Training Loss 0.00466319852212194\n",
      "Episode 13216; Testing Loss 0.005799608370207812; Training Loss 0.004663189247115762\n",
      "Episode 13217; Testing Loss 0.0057996765019261215; Training Loss 0.0046631834737271075\n",
      "Episode 13218; Testing Loss 0.0057997314812831685; Training Loss 0.0046631731045817405\n",
      "Episode 13219; Testing Loss 0.005799704967375563; Training Loss 0.004663165328569807\n",
      "Episode 13220; Testing Loss 0.0057996656838142325; Training Loss 0.004663158418443586\n",
      "Episode 13221; Testing Loss 0.0057996794299767845; Training Loss 0.004663146966624559\n",
      "Episode 13222; Testing Loss 0.005799694612623372; Training Loss 0.004663136721316301\n",
      "Episode 13223; Testing Loss 0.0057996639740515036; Training Loss 0.0046631288588175075\n",
      "Episode 13224; Testing Loss 0.005799653235087472; Training Loss 0.004663121103440074\n",
      "Episode 13225; Testing Loss 0.005799577298520319; Training Loss 0.0046631113290671825\n",
      "Episode 13226; Testing Loss 0.005799517668538197; Training Loss 0.0046631007554853695\n",
      "Episode 13227; Testing Loss 0.005799600010941069; Training Loss 0.004663092382735286\n",
      "Episode 13228; Testing Loss 0.0057997676989156016; Training Loss 0.004663083690216397\n",
      "Episode 13229; Testing Loss 0.005799777476963994; Training Loss 0.004663074256343657\n",
      "Episode 13230; Testing Loss 0.005799628641732678; Training Loss 0.004663066365257263\n",
      "Episode 13231; Testing Loss 0.005799665135740082; Training Loss 0.004663057441713564\n",
      "Episode 13232; Testing Loss 0.0057997486188275426; Training Loss 0.004663047992981724\n",
      "Episode 13233; Testing Loss 0.005799630560567412; Training Loss 0.004663039259276635\n",
      "Episode 13234; Testing Loss 0.005799583513831451; Training Loss 0.004663030791578609\n",
      "Episode 13235; Testing Loss 0.005799640334579287; Training Loss 0.004663021966777137\n",
      "Episode 13236; Testing Loss 0.005799615753459625; Training Loss 0.004663011645577077\n",
      "Episode 13237; Testing Loss 0.005799601792136076; Training Loss 0.004663003961392229\n",
      "Episode 13238; Testing Loss 0.005799590145928585; Training Loss 0.0046629951398307\n",
      "Episode 13239; Testing Loss 0.005799579793031931; Training Loss 0.004662985240872939\n",
      "Episode 13240; Testing Loss 0.0057996652137310475; Training Loss 0.004662977157299105\n",
      "Episode 13241; Testing Loss 0.005799744941446388; Training Loss 0.004662969004394609\n",
      "Episode 13242; Testing Loss 0.005799669342535309; Training Loss 0.0046629580944517095\n",
      "Episode 13243; Testing Loss 0.005799524417700734; Training Loss 0.004662952240113597\n",
      "Episode 13244; Testing Loss 0.005799525600322876; Training Loss 0.004662944045085183\n",
      "Episode 13245; Testing Loss 0.005799647972218222; Training Loss 0.004662934857708685\n",
      "Episode 13246; Testing Loss 0.00579964253825197; Training Loss 0.004662922772777731\n",
      "Episode 13247; Testing Loss 0.0057996164050202305; Training Loss 0.004662914974189186\n",
      "Episode 13248; Testing Loss 0.0057996064760365805; Training Loss 0.004662907600588915\n",
      "Episode 13249; Testing Loss 0.005799649131217677; Training Loss 0.004662898137006101\n",
      "Episode 13250; Testing Loss 0.005799628176125491; Training Loss 0.004662888556284806\n",
      "Episode 13251; Testing Loss 0.005799584730035639; Training Loss 0.004662879468087847\n",
      "Episode 13252; Testing Loss 0.005799528075016666; Training Loss 0.004662872406881786\n",
      "Episode 13253; Testing Loss 0.005799470330335881; Training Loss 0.004662863437605337\n",
      "Episode 13254; Testing Loss 0.0057995151903388106; Training Loss 0.004662851680028231\n",
      "Episode 13255; Testing Loss 0.0057995905579412855; Training Loss 0.004662844978991621\n",
      "Episode 13256; Testing Loss 0.005799648921738388; Training Loss 0.004662838513000823\n",
      "Episode 13257; Testing Loss 0.005799549154921384; Training Loss 0.004662828839303681\n",
      "Episode 13258; Testing Loss 0.005799462884879093; Training Loss 0.004662819135860886\n",
      "Episode 13259; Testing Loss 0.005799560648293247; Training Loss 0.0046628072776803125\n",
      "Episode 13260; Testing Loss 0.005799690532097579; Training Loss 0.004662803060176977\n",
      "Episode 13261; Testing Loss 0.005799597019463646; Training Loss 0.004662792585261066\n",
      "Episode 13262; Testing Loss 0.005799382732678395; Training Loss 0.004662781716392254\n",
      "Episode 13263; Testing Loss 0.005799342466819913; Training Loss 0.004662773543744724\n",
      "Episode 13264; Testing Loss 0.005799551134350572; Training Loss 0.004662764248330845\n",
      "Episode 13265; Testing Loss 0.005799683423720697; Training Loss 0.004662757200783584\n",
      "Episode 13266; Testing Loss 0.005799527168977116; Training Loss 0.00466274551349244\n",
      "Episode 13267; Testing Loss 0.005799358240849541; Training Loss 0.0046627362482804845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13268; Testing Loss 0.005799443101964312; Training Loss 0.004662723777226592\n",
      "Episode 13269; Testing Loss 0.005799571515259183; Training Loss 0.0046627226406798635\n",
      "Episode 13270; Testing Loss 0.005799462920682055; Training Loss 0.004662712440329715\n",
      "Episode 13271; Testing Loss 0.005799293447972159; Training Loss 0.004662698987913373\n",
      "Episode 13272; Testing Loss 0.005799368917872125; Training Loss 0.004662690663119338\n",
      "Episode 13273; Testing Loss 0.005799534445429644; Training Loss 0.004662684779440711\n",
      "Episode 13274; Testing Loss 0.005799502382315536; Training Loss 0.004662676142843433\n",
      "Episode 13275; Testing Loss 0.0057993500769446165; Training Loss 0.004662665548791972\n",
      "Episode 13276; Testing Loss 0.00579938925539102; Training Loss 0.004662653812862057\n",
      "Episode 13277; Testing Loss 0.005799523419172788; Training Loss 0.004662647352015187\n",
      "Episode 13278; Testing Loss 0.005799516774995447; Training Loss 0.004662640945249573\n",
      "Episode 13279; Testing Loss 0.005799356159656765; Training Loss 0.004662628169811718\n",
      "Episode 13280; Testing Loss 0.005799385625861071; Training Loss 0.004662616552194656\n",
      "Episode 13281; Testing Loss 0.005799563169895377; Training Loss 0.004662609806451028\n",
      "Episode 13282; Testing Loss 0.005799613889083031; Training Loss 0.004662603129740674\n",
      "Episode 13283; Testing Loss 0.0057994158302367505; Training Loss 0.004662592055353297\n",
      "Episode 13284; Testing Loss 0.005799285953865466; Training Loss 0.004662582743315835\n",
      "Episode 13285; Testing Loss 0.005799396066864202; Training Loss 0.004662572165386895\n",
      "Episode 13286; Testing Loss 0.005799453154754076; Training Loss 0.0046625646661736814\n",
      "Episode 13287; Testing Loss 0.00579936116085834; Training Loss 0.004662555001069663\n",
      "Episode 13288; Testing Loss 0.005799293227371236; Training Loss 0.004662545907667367\n",
      "Episode 13289; Testing Loss 0.005799399331944439; Training Loss 0.004662535833968169\n",
      "Episode 13290; Testing Loss 0.005799446227325888; Training Loss 0.004662528795255787\n",
      "Episode 13291; Testing Loss 0.005799326944606665; Training Loss 0.004662519128995628\n",
      "Episode 13292; Testing Loss 0.005799248995361106; Training Loss 0.004662507611592759\n",
      "Episode 13293; Testing Loss 0.005799368849708582; Training Loss 0.004662501370568401\n",
      "Episode 13294; Testing Loss 0.00579942351598384; Training Loss 0.004662492970598118\n",
      "Episode 13295; Testing Loss 0.005799347468409517; Training Loss 0.004662480521013096\n",
      "Episode 13296; Testing Loss 0.005799269645729002; Training Loss 0.00466247296260796\n",
      "Episode 13297; Testing Loss 0.005799410609289039; Training Loss 0.004662462978816797\n",
      "Episode 13298; Testing Loss 0.005799435973365144; Training Loss 0.0046624541415939275\n",
      "Episode 13299; Testing Loss 0.005799339951426891; Training Loss 0.004662447501786567\n",
      "Episode 13300; Testing Loss 0.005799308787597454; Training Loss 0.004662437071701577\n",
      "Episode 13301; Testing Loss 0.005799382498253287; Training Loss 0.004662427723343973\n",
      "Episode 13302; Testing Loss 0.005799401193756643; Training Loss 0.00466242063713891\n",
      "Episode 13303; Testing Loss 0.005799279988318956; Training Loss 0.004662411094202685\n",
      "Episode 13304; Testing Loss 0.0057992607221206425; Training Loss 0.004662401563670626\n",
      "Episode 13305; Testing Loss 0.005799320275284164; Training Loss 0.0046623921074643426\n",
      "Episode 13306; Testing Loss 0.0057993794407350235; Training Loss 0.004662383035504011\n",
      "Episode 13307; Testing Loss 0.0057993446798746; Training Loss 0.004662373970464592\n",
      "Episode 13308; Testing Loss 0.0057993162939106715; Training Loss 0.00466236491073193\n",
      "Episode 13309; Testing Loss 0.00579933287217251; Training Loss 0.004662358425491936\n",
      "Episode 13310; Testing Loss 0.005799247970348421; Training Loss 0.004662349049822571\n",
      "Episode 13311; Testing Loss 0.005799149144918735; Training Loss 0.004662338892168457\n",
      "Episode 13312; Testing Loss 0.005799211697559217; Training Loss 0.004662329825084429\n",
      "Episode 13313; Testing Loss 0.005799293311681337; Training Loss 0.004662319671138265\n",
      "Episode 13314; Testing Loss 0.005799266066960727; Training Loss 0.004662311932786653\n",
      "Episode 13315; Testing Loss 0.005799166122388486; Training Loss 0.0046623045402004\n",
      "Episode 13316; Testing Loss 0.0057991705895790065; Training Loss 0.004662293052886835\n",
      "Episode 13317; Testing Loss 0.005799266503702488; Training Loss 0.004662285190252179\n",
      "Episode 13318; Testing Loss 0.0057993273661977075; Training Loss 0.004662278276831942\n",
      "Episode 13319; Testing Loss 0.0057992088789392444; Training Loss 0.004662267470655498\n",
      "Episode 13320; Testing Loss 0.005799118281103988; Training Loss 0.004662258242420052\n",
      "Episode 13321; Testing Loss 0.005799211546633584; Training Loss 0.004662250630100092\n",
      "Episode 13322; Testing Loss 0.005799294545424684; Training Loss 0.004662243358535002\n",
      "Episode 13323; Testing Loss 0.005799201357627556; Training Loss 0.0046622311390341\n",
      "Episode 13324; Testing Loss 0.005799146861311672; Training Loss 0.004662221575527224\n",
      "Episode 13325; Testing Loss 0.005799257858647154; Training Loss 0.004662214498428238\n",
      "Episode 13326; Testing Loss 0.005799254488468655; Training Loss 0.004662206023162944\n",
      "Episode 13327; Testing Loss 0.005799216474323694; Training Loss 0.004662195310849758\n",
      "Episode 13328; Testing Loss 0.00579917250080223; Training Loss 0.0046621861128877395\n",
      "Episode 13329; Testing Loss 0.005799141185028838; Training Loss 0.0046621796446202225\n",
      "Episode 13330; Testing Loss 0.005799161226748812; Training Loss 0.0046621685787111705\n",
      "Episode 13331; Testing Loss 0.005799209159174086; Training Loss 0.004662159842513503\n",
      "Episode 13332; Testing Loss 0.005799300392398125; Training Loss 0.004662153296142392\n",
      "Episode 13333; Testing Loss 0.00579929054114056; Training Loss 0.004662144661617087\n",
      "Episode 13334; Testing Loss 0.00579920314459391; Training Loss 0.004662133903713541\n",
      "Episode 13335; Testing Loss 0.0057991300773193415; Training Loss 0.004662124388012867\n",
      "Episode 13336; Testing Loss 0.0057991656021079835; Training Loss 0.0046621162600824\n",
      "Episode 13337; Testing Loss 0.0057992024231064325; Training Loss 0.004662108509638446\n",
      "Episode 13338; Testing Loss 0.005799134202806262; Training Loss 0.004662097924669284\n",
      "Episode 13339; Testing Loss 0.005799092932711385; Training Loss 0.004662088174249759\n",
      "Episode 13340; Testing Loss 0.005799173321428005; Training Loss 0.004662079002612347\n",
      "Episode 13341; Testing Loss 0.005799215962779547; Training Loss 0.004662069303510954\n",
      "Episode 13342; Testing Loss 0.00579915793265575; Training Loss 0.004662061710560337\n",
      "Episode 13343; Testing Loss 0.005799084328383044; Training Loss 0.004662051567205656\n",
      "Episode 13344; Testing Loss 0.0057991176928552475; Training Loss 0.0046620433377837075\n",
      "Episode 13345; Testing Loss 0.005799149457588486; Training Loss 0.004662034675206834\n",
      "Episode 13346; Testing Loss 0.005799138728245903; Training Loss 0.004662023801856858\n",
      "Episode 13347; Testing Loss 0.005799121700992074; Training Loss 0.004662015248741892\n",
      "Episode 13348; Testing Loss 0.005799115653027249; Training Loss 0.004662008052518953\n",
      "Episode 13349; Testing Loss 0.00579908956017126; Training Loss 0.004661998409300126\n",
      "Episode 13350; Testing Loss 0.005799029113021948; Training Loss 0.004661988682519525\n",
      "Episode 13351; Testing Loss 0.005799071651700566; Training Loss 0.004661981830032228\n",
      "Episode 13352; Testing Loss 0.005799105701965724; Training Loss 0.004661972301646599\n",
      "Episode 13353; Testing Loss 0.0057990542381825775; Training Loss 0.004661961672179978\n",
      "Episode 13354; Testing Loss 0.0057991172282335825; Training Loss 0.004661953143681667\n",
      "Episode 13355; Testing Loss 0.00579912097046569; Training Loss 0.004661944045244091\n",
      "Episode 13356; Testing Loss 0.005799086845147292; Training Loss 0.004661933768655369\n",
      "Episode 13357; Testing Loss 0.005799144366174327; Training Loss 0.004661925684462612\n",
      "Episode 13358; Testing Loss 0.00579914228288375; Training Loss 0.004661915921677391\n",
      "Episode 13359; Testing Loss 0.005799086638189106; Training Loss 0.0046619061363349\n",
      "Episode 13360; Testing Loss 0.005799039666247594; Training Loss 0.004661898157599833\n",
      "Episode 13361; Testing Loss 0.005799026321303061; Training Loss 0.004661889658227384\n",
      "Episode 13362; Testing Loss 0.00579910488319152; Training Loss 0.004661879717070574\n",
      "Episode 13363; Testing Loss 0.005799169066300645; Training Loss 0.004661870867425266\n",
      "Episode 13364; Testing Loss 0.00579913820941144; Training Loss 0.00466186305708524\n",
      "Episode 13365; Testing Loss 0.005799065372904828; Training Loss 0.004661853630385425\n",
      "Episode 13366; Testing Loss 0.00579909043226456; Training Loss 0.004661843357141679\n",
      "Episode 13367; Testing Loss 0.00579906755705032; Training Loss 0.004661834808710575\n",
      "Episode 13368; Testing Loss 0.005798994979990736; Training Loss 0.004661825986975662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13369; Testing Loss 0.0057990378015753226; Training Loss 0.004661818400159611\n",
      "Episode 13370; Testing Loss 0.005799096377450564; Training Loss 0.004661807861973365\n",
      "Episode 13371; Testing Loss 0.005799099392687398; Training Loss 0.004661799973230823\n",
      "Episode 13372; Testing Loss 0.0057991260825895575; Training Loss 0.004661791950689988\n",
      "Episode 13373; Testing Loss 0.00579917631230178; Training Loss 0.004661783064656631\n",
      "Episode 13374; Testing Loss 0.005799101646411592; Training Loss 0.004661773829116552\n",
      "Episode 13375; Testing Loss 0.0057990622140145385; Training Loss 0.004661763899479369\n",
      "Episode 13376; Testing Loss 0.005799049207836105; Training Loss 0.004661755757051228\n",
      "Episode 13377; Testing Loss 0.005799013059935622; Training Loss 0.004661746385555207\n",
      "Episode 13378; Testing Loss 0.005799016088546982; Training Loss 0.004661735792214091\n",
      "Episode 13379; Testing Loss 0.005799055264922089; Training Loss 0.00466172883459528\n",
      "Episode 13380; Testing Loss 0.005799056585741674; Training Loss 0.004661720704627123\n",
      "Episode 13381; Testing Loss 0.005799052088695417; Training Loss 0.004661709137599254\n",
      "Episode 13382; Testing Loss 0.005799068249194333; Training Loss 0.004661700287805795\n",
      "Episode 13383; Testing Loss 0.005799040508911611; Training Loss 0.004661690968303605\n",
      "Episode 13384; Testing Loss 0.005799075477597212; Training Loss 0.004661682391120832\n",
      "Episode 13385; Testing Loss 0.005799048545483171; Training Loss 0.004661673994944733\n",
      "Episode 13386; Testing Loss 0.005799000223851252; Training Loss 0.004661665223988698\n",
      "Episode 13387; Testing Loss 0.005799058709906804; Training Loss 0.004661655950769391\n",
      "Episode 13388; Testing Loss 0.005799160848825648; Training Loss 0.004661647070721152\n",
      "Episode 13389; Testing Loss 0.005799214185573565; Training Loss 0.004661639897020576\n",
      "Episode 13390; Testing Loss 0.005799051197028577; Training Loss 0.00466163018861663\n",
      "Episode 13391; Testing Loss 0.005798872620428385; Training Loss 0.00466162330089403\n",
      "Episode 13392; Testing Loss 0.005798973143601558; Training Loss 0.004661612496661744\n",
      "Episode 13393; Testing Loss 0.0057991799229659535; Training Loss 0.004661605031480704\n",
      "Episode 13394; Testing Loss 0.005799172248307572; Training Loss 0.004661596829110285\n",
      "Episode 13395; Testing Loss 0.0057990270963399005; Training Loss 0.004661584791340838\n",
      "Episode 13396; Testing Loss 0.005798989765717304; Training Loss 0.00466157879577757\n",
      "Episode 13397; Testing Loss 0.00579907596107887; Training Loss 0.0046615694011167014\n",
      "Episode 13398; Testing Loss 0.005799042692670261; Training Loss 0.004661556805045065\n",
      "Episode 13399; Testing Loss 0.005798949932582966; Training Loss 0.0046615511985832496\n",
      "Episode 13400; Testing Loss 0.005799089814096183; Training Loss 0.004661543346529317\n",
      "Episode 13401; Testing Loss 0.0057991941744539085; Training Loss 0.004661534286205739\n",
      "Episode 13402; Testing Loss 0.005799083852073777; Training Loss 0.004661523466226425\n",
      "Episode 13403; Testing Loss 0.005798948180349574; Training Loss 0.004661514470105041\n",
      "Episode 13404; Testing Loss 0.005799069009578673; Training Loss 0.004661506581853276\n",
      "Episode 13405; Testing Loss 0.005799134238201576; Training Loss 0.004661499459783111\n",
      "Episode 13406; Testing Loss 0.005798978581773223; Training Loss 0.004661487524037567\n",
      "Episode 13407; Testing Loss 0.005798858096975022; Training Loss 0.004661480084640284\n",
      "Episode 13408; Testing Loss 0.005799019156365699; Training Loss 0.0046614706853341705\n",
      "Episode 13409; Testing Loss 0.005799123216057616; Training Loss 0.004661464094270259\n",
      "Episode 13410; Testing Loss 0.005798956945970019; Training Loss 0.0046614534550745435\n",
      "Episode 13411; Testing Loss 0.0057988692079847404; Training Loss 0.0046614449533505745\n",
      "Episode 13412; Testing Loss 0.005799019721592561; Training Loss 0.00466143339426338\n",
      "Episode 13413; Testing Loss 0.005799123420516706; Training Loss 0.004661426652867591\n",
      "Episode 13414; Testing Loss 0.005798925794364036; Training Loss 0.0046614161140483435\n",
      "Episode 13415; Testing Loss 0.005798800725811928; Training Loss 0.004661408253577337\n",
      "Episode 13416; Testing Loss 0.005798971356690083; Training Loss 0.004661399075959851\n",
      "Episode 13417; Testing Loss 0.005799105603570624; Training Loss 0.004661391751384719\n",
      "Episode 13418; Testing Loss 0.0057989618815182005; Training Loss 0.004661380449966293\n",
      "Episode 13419; Testing Loss 0.0057987606568084184; Training Loss 0.004661373400266987\n",
      "Episode 13420; Testing Loss 0.005798898579308494; Training Loss 0.004661361316847139\n",
      "Episode 13421; Testing Loss 0.005799104374540743; Training Loss 0.004661354555104021\n",
      "Episode 13422; Testing Loss 0.005799002502396727; Training Loss 0.004661343234025594\n",
      "Episode 13423; Testing Loss 0.00579878715361396; Training Loss 0.004661335374583667\n",
      "Episode 13424; Testing Loss 0.005798808657192204; Training Loss 0.004661326312018946\n",
      "Episode 13425; Testing Loss 0.005798984777675404; Training Loss 0.004661319009062008\n",
      "Episode 13426; Testing Loss 0.005798998330543462; Training Loss 0.004661308835137362\n",
      "Episode 13427; Testing Loss 0.005798861408582888; Training Loss 0.004661299581515387\n",
      "Episode 13428; Testing Loss 0.005798881307620841; Training Loss 0.00466129296421921\n",
      "Episode 13429; Testing Loss 0.005798984602841442; Training Loss 0.004661283176409731\n",
      "Episode 13430; Testing Loss 0.005799041176301562; Training Loss 0.004661272433891614\n",
      "Episode 13431; Testing Loss 0.005799010912139751; Training Loss 0.004661262905655277\n",
      "Episode 13432; Testing Loss 0.0057989372017660315; Training Loss 0.004661255988007437\n",
      "Episode 13433; Testing Loss 0.005798856700009122; Training Loss 0.0046612472481131284\n",
      "Episode 13434; Testing Loss 0.005798912330984566; Training Loss 0.004661235933092785\n",
      "Episode 13435; Testing Loss 0.005798983098214163; Training Loss 0.004661228589490827\n",
      "Episode 13436; Testing Loss 0.005798917219977795; Training Loss 0.004661218905531414\n",
      "Episode 13437; Testing Loss 0.005798874286329107; Training Loss 0.004661209883247079\n",
      "Episode 13438; Testing Loss 0.005798980020360852; Training Loss 0.004661201802308099\n",
      "Episode 13439; Testing Loss 0.0057989770579082315; Training Loss 0.004661192473286562\n",
      "Episode 13440; Testing Loss 0.005798883149097862; Training Loss 0.004661183048070284\n",
      "Episode 13441; Testing Loss 0.005798894891334042; Training Loss 0.004661175041878361\n",
      "Episode 13442; Testing Loss 0.005798937418686366; Training Loss 0.0046611663282086655\n",
      "Episode 13443; Testing Loss 0.005798871061796113; Training Loss 0.004661156094069829\n",
      "Episode 13444; Testing Loss 0.005798864633193366; Training Loss 0.0046611458713200285\n",
      "Episode 13445; Testing Loss 0.005798939496775794; Training Loss 0.004661139793215003\n",
      "Episode 13446; Testing Loss 0.005798899312754537; Training Loss 0.004661129714228523\n",
      "Episode 13447; Testing Loss 0.005798883574228878; Training Loss 0.004661119596221933\n",
      "Episode 13448; Testing Loss 0.005798925323396997; Training Loss 0.00466111103864278\n",
      "Episode 13449; Testing Loss 0.005798976860799137; Training Loss 0.00466110291017549\n",
      "Episode 13450; Testing Loss 0.0057989612458814324; Training Loss 0.004661093155438835\n",
      "Episode 13451; Testing Loss 0.005798954066043944; Training Loss 0.004661083287742942\n",
      "Episode 13452; Testing Loss 0.005798870674298121; Training Loss 0.004661074570224655\n",
      "Episode 13453; Testing Loss 0.005798940086218835; Training Loss 0.0046610658661991045\n",
      "Episode 13454; Testing Loss 0.005798888791864295; Training Loss 0.004661057183650928\n",
      "Episode 13455; Testing Loss 0.005798886756077015; Training Loss 0.004661048397136041\n",
      "Episode 13456; Testing Loss 0.005798967508414786; Training Loss 0.004661039213206809\n",
      "Episode 13457; Testing Loss 0.00579889331229513; Training Loss 0.004661030920270309\n",
      "Episode 13458; Testing Loss 0.005798840384390574; Training Loss 0.004661021065754928\n",
      "Episode 13459; Testing Loss 0.005798912496108729; Training Loss 0.004661012333879679\n",
      "Episode 13460; Testing Loss 0.00579894207850553; Training Loss 0.004661003677062378\n",
      "Episode 13461; Testing Loss 0.005798830659086542; Training Loss 0.004660995860498954\n",
      "Episode 13462; Testing Loss 0.005798777918977104; Training Loss 0.004660986869969495\n",
      "Episode 13463; Testing Loss 0.005798907854007084; Training Loss 0.004660977457952624\n",
      "Episode 13464; Testing Loss 0.005798948939633055; Training Loss 0.004660969177832806\n",
      "Episode 13465; Testing Loss 0.005798895920866084; Training Loss 0.004660958661517365\n",
      "Episode 13466; Testing Loss 0.005798882424548204; Training Loss 0.004660951370851036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13467; Testing Loss 0.005798900487871883; Training Loss 0.004660941963563859\n",
      "Episode 13468; Testing Loss 0.005798865879306659; Training Loss 0.0046609321037398505\n",
      "Episode 13469; Testing Loss 0.005798922255696544; Training Loss 0.004660924967830838\n",
      "Episode 13470; Testing Loss 0.005798968371678129; Training Loss 0.004660914671459782\n",
      "Episode 13471; Testing Loss 0.005798844460745324; Training Loss 0.004660907920232106\n",
      "Episode 13472; Testing Loss 0.005798692299136958; Training Loss 0.004660899710593056\n",
      "Episode 13473; Testing Loss 0.0057988114995544005; Training Loss 0.004660886941185057\n",
      "Episode 13474; Testing Loss 0.0057989752061480245; Training Loss 0.0046608814141737166\n",
      "Episode 13475; Testing Loss 0.0057989773982753704; Training Loss 0.004660871379770639\n",
      "Episode 13476; Testing Loss 0.005798804255966612; Training Loss 0.004660861482016243\n",
      "Episode 13477; Testing Loss 0.005798734027183431; Training Loss 0.004660852645892876\n",
      "Episode 13478; Testing Loss 0.0057988536197452825; Training Loss 0.004660843445827208\n",
      "Episode 13479; Testing Loss 0.005798842666610649; Training Loss 0.00466083399442356\n",
      "Episode 13480; Testing Loss 0.005798814124429254; Training Loss 0.004660824765922387\n",
      "Episode 13481; Testing Loss 0.005798797073297177; Training Loss 0.004660817652688435\n",
      "Episode 13482; Testing Loss 0.005798762922326508; Training Loss 0.004660806786255961\n",
      "Episode 13483; Testing Loss 0.005798845769324557; Training Loss 0.004660798293902953\n",
      "Episode 13484; Testing Loss 0.005798893885314542; Training Loss 0.004660790357091773\n",
      "Episode 13485; Testing Loss 0.005798841968846711; Training Loss 0.004660782280055026\n",
      "Episode 13486; Testing Loss 0.00579872949527713; Training Loss 0.004660773756971488\n",
      "Episode 13487; Testing Loss 0.00579881020208409; Training Loss 0.00466076337015771\n",
      "Episode 13488; Testing Loss 0.005798870500232992; Training Loss 0.004660755652837055\n",
      "Episode 13489; Testing Loss 0.005798824987340628; Training Loss 0.004660745737036631\n",
      "Episode 13490; Testing Loss 0.005798781261431834; Training Loss 0.004660737660951369\n",
      "Episode 13491; Testing Loss 0.00579880111608815; Training Loss 0.004660729349425315\n",
      "Episode 13492; Testing Loss 0.00579888066352987; Training Loss 0.004660720400755035\n",
      "Episode 13493; Testing Loss 0.0057989160797526085; Training Loss 0.0046607120806152645\n",
      "Episode 13494; Testing Loss 0.0057988015544001075; Training Loss 0.00466070209881157\n",
      "Episode 13495; Testing Loss 0.005798704280314105; Training Loss 0.004660695765956178\n",
      "Episode 13496; Testing Loss 0.005798755545726198; Training Loss 0.004660684510706426\n",
      "Episode 13497; Testing Loss 0.00579880221806877; Training Loss 0.00466067674960162\n",
      "Episode 13498; Testing Loss 0.005798750026848697; Training Loss 0.004660669222094936\n",
      "Episode 13499; Testing Loss 0.005798707751033143; Training Loss 0.004660659656530474\n",
      "Episode 13500; Testing Loss 0.0057987896758741825; Training Loss 0.004660648295083498\n",
      "Episode 13501; Testing Loss 0.00579885817412953; Training Loss 0.004660639216942772\n",
      "Episode 13502; Testing Loss 0.005798811168540851; Training Loss 0.004660632984202371\n",
      "Episode 13503; Testing Loss 0.005798713976023444; Training Loss 0.0046606241153707515\n",
      "Episode 13504; Testing Loss 0.005798728368931771; Training Loss 0.004660613371575872\n",
      "Episode 13505; Testing Loss 0.005798813860954009; Training Loss 0.004660604665532768\n",
      "Episode 13506; Testing Loss 0.005798822779376204; Training Loss 0.004660595085896843\n",
      "Episode 13507; Testing Loss 0.005798717908333394; Training Loss 0.004660587274259473\n",
      "Episode 13508; Testing Loss 0.0057986522487936225; Training Loss 0.004660577131214266\n",
      "Episode 13509; Testing Loss 0.0057986805062915005; Training Loss 0.004660570187628827\n",
      "Episode 13510; Testing Loss 0.005798705892444072; Training Loss 0.004660561920970014\n",
      "Episode 13511; Testing Loss 0.0057986586455851035; Training Loss 0.004660550580659555\n",
      "Episode 13512; Testing Loss 0.005798667333855562; Training Loss 0.00466054238920159\n",
      "Episode 13513; Testing Loss 0.005798744857441749; Training Loss 0.004660533167845329\n",
      "Episode 13514; Testing Loss 0.005798721388428198; Training Loss 0.004660523416514219\n",
      "Episode 13515; Testing Loss 0.005798696826419619; Training Loss 0.00466051523520748\n",
      "Episode 13516; Testing Loss 0.00579867958787894; Training Loss 0.004660506708239086\n",
      "Episode 13517; Testing Loss 0.00579875797126184; Training Loss 0.004660496529815653\n",
      "Episode 13518; Testing Loss 0.005798831247887783; Training Loss 0.004660489205284689\n",
      "Episode 13519; Testing Loss 0.005798728252210723; Training Loss 0.00466047884178946\n",
      "Episode 13520; Testing Loss 0.005798655002721623; Training Loss 0.00466047003978727\n",
      "Episode 13521; Testing Loss 0.005798729435896225; Training Loss 0.004660463299817142\n",
      "Episode 13522; Testing Loss 0.005798688735124758; Training Loss 0.004660455117749482\n",
      "Episode 13523; Testing Loss 0.005798646174064124; Training Loss 0.004660443792393461\n",
      "Episode 13524; Testing Loss 0.005798681930759849; Training Loss 0.004660434833097322\n",
      "Episode 13525; Testing Loss 0.00579879802019296; Training Loss 0.004660431686758315\n",
      "Episode 13526; Testing Loss 0.005798751115372692; Training Loss 0.004660419355310654\n",
      "Episode 13527; Testing Loss 0.005798617262349552; Training Loss 0.0046604086703714185\n",
      "Episode 13528; Testing Loss 0.005798605601230732; Training Loss 0.004660404677964975\n",
      "Episode 13529; Testing Loss 0.005798671320703873; Training Loss 0.0046603979429492215\n",
      "Episode 13530; Testing Loss 0.00579869675120616; Training Loss 0.0046603873071757495\n",
      "Episode 13531; Testing Loss 0.005798644409968113; Training Loss 0.004660374350345239\n",
      "Episode 13532; Testing Loss 0.005798640477154947; Training Loss 0.004660364429352994\n",
      "Episode 13533; Testing Loss 0.005798663654250356; Training Loss 0.004660357245591722\n",
      "Episode 13534; Testing Loss 0.005798686867475101; Training Loss 0.004660348075274381\n",
      "Episode 13535; Testing Loss 0.005798675327991546; Training Loss 0.004660339396797932\n",
      "Episode 13536; Testing Loss 0.005798697738355097; Training Loss 0.004660329886706942\n",
      "Episode 13537; Testing Loss 0.005798678804011745; Training Loss 0.004660320365899957\n",
      "Episode 13538; Testing Loss 0.005798641016752121; Training Loss 0.004660310833122362\n",
      "Episode 13539; Testing Loss 0.005798691290830927; Training Loss 0.004660303203460409\n",
      "Episode 13540; Testing Loss 0.005798624950196814; Training Loss 0.004660293315279018\n",
      "Episode 13541; Testing Loss 0.005798577697299457; Training Loss 0.004660285538873769\n",
      "Episode 13542; Testing Loss 0.0057986382806400035; Training Loss 0.004660277139509327\n",
      "Episode 13543; Testing Loss 0.005798683988471474; Training Loss 0.004660267815107052\n",
      "Episode 13544; Testing Loss 0.005798626891928926; Training Loss 0.004660257899404791\n",
      "Episode 13545; Testing Loss 0.005798605468637672; Training Loss 0.004660249545007048\n",
      "Episode 13546; Testing Loss 0.00579863614862972; Training Loss 0.004660242893623585\n",
      "Episode 13547; Testing Loss 0.005798661914389094; Training Loss 0.004660232664071149\n",
      "Episode 13548; Testing Loss 0.00579853241388752; Training Loss 0.004660222586739729\n",
      "Episode 13549; Testing Loss 0.005798545067064512; Training Loss 0.004660214103407487\n",
      "Episode 13550; Testing Loss 0.0057986565820411385; Training Loss 0.004660205244303688\n",
      "Episode 13551; Testing Loss 0.005798609205698275; Training Loss 0.00466019439311581\n",
      "Episode 13552; Testing Loss 0.005798505821323666; Training Loss 0.00466018355669048\n",
      "Episode 13553; Testing Loss 0.005798498808355006; Training Loss 0.004660177284000079\n",
      "Episode 13554; Testing Loss 0.005798536418026792; Training Loss 0.004660169972697347\n",
      "Episode 13555; Testing Loss 0.005798448025381488; Training Loss 0.004660156104272056\n",
      "Episode 13556; Testing Loss 0.005798372207280449; Training Loss 0.00466014702280895\n",
      "Episode 13557; Testing Loss 0.005798457397254219; Training Loss 0.004660137423029047\n",
      "Episode 13558; Testing Loss 0.005798543436030464; Training Loss 0.004660127683299204\n",
      "Episode 13559; Testing Loss 0.005798486997876911; Training Loss 0.004660119651108426\n",
      "Episode 13560; Testing Loss 0.00579839852109361; Training Loss 0.004660109617808808\n",
      "Episode 13561; Testing Loss 0.005798438361636208; Training Loss 0.004660098333540878\n",
      "Episode 13562; Testing Loss 0.005798518515615925; Training Loss 0.00466008934746462\n",
      "Episode 13563; Testing Loss 0.005798450502052757; Training Loss 0.0046600791054864565\n",
      "Episode 13564; Testing Loss 0.005798291254892759; Training Loss 0.004660070455802007\n",
      "Episode 13565; Testing Loss 0.00579833256355721; Training Loss 0.004660058946953499\n",
      "Episode 13566; Testing Loss 0.005798451750019808; Training Loss 0.004660050652692605\n",
      "Episode 13567; Testing Loss 0.005798314774585281; Training Loss 0.004660038667207739\n",
      "Episode 13568; Testing Loss 0.005798231617552726; Training Loss 0.004660030062185343\n",
      "Episode 13569; Testing Loss 0.0057983183756266845; Training Loss 0.0046600206545001285\n",
      "Episode 13570; Testing Loss 0.0057983734250357435; Training Loss 0.004660009906968954\n",
      "Episode 13571; Testing Loss 0.005798355377691838; Training Loss 0.004660000051643867\n",
      "Episode 13572; Testing Loss 0.005798280261387619; Training Loss 0.0046599903188609135\n",
      "Episode 13573; Testing Loss 0.005798335582419382; Training Loss 0.004659981411473351\n",
      "Episode 13574; Testing Loss 0.005798407419565626; Training Loss 0.004659970987528752\n",
      "Episode 13575; Testing Loss 0.005798376528010537; Training Loss 0.004659962949537428\n",
      "Episode 13576; Testing Loss 0.0057982880601969615; Training Loss 0.004659953314720556\n",
      "Episode 13577; Testing Loss 0.005798264515040745; Training Loss 0.004659944880701137\n",
      "Episode 13578; Testing Loss 0.005798340947550696; Training Loss 0.004659933409352892\n",
      "Episode 13579; Testing Loss 0.005798349423135476; Training Loss 0.004659923056388911\n",
      "Episode 13580; Testing Loss 0.005798203311692737; Training Loss 0.004659915911387044\n",
      "Episode 13581; Testing Loss 0.0057982058067211975; Training Loss 0.004659907314888953\n",
      "Episode 13582; Testing Loss 0.005798340289454985; Training Loss 0.004659894021901713\n",
      "Episode 13583; Testing Loss 0.005798403263236066; Training Loss 0.004659886099808802\n",
      "Episode 13584; Testing Loss 0.005798256868992053; Training Loss 0.004659878542024572\n",
      "Episode 13585; Testing Loss 0.00579822248028915; Training Loss 0.004659868753663223\n",
      "Episode 13586; Testing Loss 0.005798253746013385; Training Loss 0.004659857277574811\n",
      "Episode 13587; Testing Loss 0.005798245292014592; Training Loss 0.004659846817285079\n",
      "Episode 13588; Testing Loss 0.005798254094930622; Training Loss 0.00465983773819579\n",
      "Episode 13589; Testing Loss 0.005798155541829049; Training Loss 0.004659827382531186\n",
      "Episode 13590; Testing Loss 0.0057981881333574155; Training Loss 0.004659818150706317\n",
      "Episode 13591; Testing Loss 0.005798285430034469; Training Loss 0.004659809633393127\n",
      "Episode 13592; Testing Loss 0.005798215921779689; Training Loss 0.004659799728285103\n",
      "Episode 13593; Testing Loss 0.005798137469840158; Training Loss 0.0046597907325635155\n",
      "Episode 13594; Testing Loss 0.0057981309714856466; Training Loss 0.004659780567734557\n",
      "Episode 13595; Testing Loss 0.005798143614304549; Training Loss 0.004659771083278082\n",
      "Episode 13596; Testing Loss 0.00579808220222; Training Loss 0.004659761893885945\n",
      "Episode 13597; Testing Loss 0.005797971927281628; Training Loss 0.004659753488521514\n",
      "Episode 13598; Testing Loss 0.00579804390417356; Training Loss 0.004659743538588221\n",
      "Episode 13599; Testing Loss 0.005798155301048934; Training Loss 0.004659734833734977\n",
      "Episode 13600; Testing Loss 0.005798041391248116; Training Loss 0.0046597237861276\n",
      "Episode 13601; Testing Loss 0.005797939526329382; Training Loss 0.004659716264982312\n",
      "Episode 13602; Testing Loss 0.005798062312405465; Training Loss 0.004659705165753725\n",
      "Episode 13603; Testing Loss 0.005798186805115711; Training Loss 0.004659696191444404\n",
      "Episode 13604; Testing Loss 0.005798061454391755; Training Loss 0.004659687374576279\n",
      "Episode 13605; Testing Loss 0.005797909177052095; Training Loss 0.004659677612526514\n",
      "Episode 13606; Testing Loss 0.005797961467238789; Training Loss 0.004659667540570925\n",
      "Episode 13607; Testing Loss 0.005798055174786886; Training Loss 0.004659658519611786\n",
      "Episode 13608; Testing Loss 0.005797926404018491; Training Loss 0.004659649532012077\n",
      "Episode 13609; Testing Loss 0.005797831234001329; Training Loss 0.004659640313830899\n",
      "Episode 13610; Testing Loss 0.005797945112022629; Training Loss 0.004659629514267095\n",
      "Episode 13611; Testing Loss 0.0057980347879840405; Training Loss 0.00465962122662328\n",
      "Episode 13612; Testing Loss 0.005797899128116221; Training Loss 0.004659611582826749\n",
      "Episode 13613; Testing Loss 0.005797889576103639; Training Loss 0.004659602159758299\n",
      "Episode 13614; Testing Loss 0.005797995027902638; Training Loss 0.004659592424320839\n",
      "Episode 13615; Testing Loss 0.005797969656631921; Training Loss 0.004659582865638742\n",
      "Episode 13616; Testing Loss 0.00579784080667019; Training Loss 0.00465957457797367\n",
      "Episode 13617; Testing Loss 0.005797797837535171; Training Loss 0.004659564519182447\n",
      "Episode 13618; Testing Loss 0.005797880248997803; Training Loss 0.00465955613320283\n",
      "Episode 13619; Testing Loss 0.005797960096535935; Training Loss 0.004659548279137031\n",
      "Episode 13620; Testing Loss 0.005797862189765108; Training Loss 0.004659538189818463\n",
      "Episode 13621; Testing Loss 0.005797829755361187; Training Loss 0.004659529185709882\n",
      "Episode 13622; Testing Loss 0.0057979126866762164; Training Loss 0.0046595183122416384\n",
      "Episode 13623; Testing Loss 0.005797941537686862; Training Loss 0.004659509843351328\n",
      "Episode 13624; Testing Loss 0.0057977615902075445; Training Loss 0.004659501987266938\n",
      "Episode 13625; Testing Loss 0.005797694112361974; Training Loss 0.0046594928139539835\n",
      "Episode 13626; Testing Loss 0.005797793063105413; Training Loss 0.0046594806746872485\n",
      "Episode 13627; Testing Loss 0.005797850272167969; Training Loss 0.004659472783028563\n",
      "Episode 13628; Testing Loss 0.005797828236043513; Training Loss 0.004659463829220502\n",
      "Episode 13629; Testing Loss 0.005797820204128958; Training Loss 0.004659453111188809\n",
      "Episode 13630; Testing Loss 0.0057977440566405235; Training Loss 0.004659443373920677\n",
      "Episode 13631; Testing Loss 0.005797767256611735; Training Loss 0.004659436479320619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13632; Testing Loss 0.00579789649832593; Training Loss 0.004659426177604743\n",
      "Episode 13633; Testing Loss 0.005797876573677483; Training Loss 0.004659416622137739\n",
      "Episode 13634; Testing Loss 0.005797774561427112; Training Loss 0.004659408373028164\n",
      "Episode 13635; Testing Loss 0.005797747545783512; Training Loss 0.004659398838603012\n",
      "Episode 13636; Testing Loss 0.005797757348330825; Training Loss 0.004659389970160213\n",
      "Episode 13637; Testing Loss 0.0057976769037944844; Training Loss 0.004659379240250461\n",
      "Episode 13638; Testing Loss 0.0057976589214953425; Training Loss 0.004659369980123939\n",
      "Episode 13639; Testing Loss 0.005797741516841841; Training Loss 0.004659364176241581\n",
      "Episode 13640; Testing Loss 0.005797729661794; Training Loss 0.004659354966880597\n",
      "Episode 13641; Testing Loss 0.005797685986305499; Training Loss 0.004659342318904765\n",
      "Episode 13642; Testing Loss 0.00579761736557235; Training Loss 0.0046593360683766984\n",
      "Episode 13643; Testing Loss 0.0057975891658657375; Training Loss 0.004659327617028718\n",
      "Episode 13644; Testing Loss 0.005797659792665504; Training Loss 0.004659315605963801\n",
      "Episode 13645; Testing Loss 0.005797707165797618; Training Loss 0.004659306609519891\n",
      "Episode 13646; Testing Loss 0.005797751465093078; Training Loss 0.004659299670313661\n",
      "Episode 13647; Testing Loss 0.0057977101535502396; Training Loss 0.004659287828910231\n",
      "Episode 13648; Testing Loss 0.005797592647915826; Training Loss 0.004659279772794811\n",
      "Episode 13649; Testing Loss 0.005797616514409726; Training Loss 0.0046592702403428065\n",
      "Episode 13650; Testing Loss 0.005797717735030511; Training Loss 0.004659263977901232\n",
      "Episode 13651; Testing Loss 0.005797686600556372; Training Loss 0.004659252964524563\n",
      "Episode 13652; Testing Loss 0.0057975898240153045; Training Loss 0.004659240350693774\n",
      "Episode 13653; Testing Loss 0.005797640465249115; Training Loss 0.004659235606536181\n",
      "Episode 13654; Testing Loss 0.005797714059165987; Training Loss 0.004659226803053945\n",
      "Episode 13655; Testing Loss 0.0057976983818995706; Training Loss 0.0046592140953984566\n",
      "Episode 13656; Testing Loss 0.00579764458293711; Training Loss 0.004659203893694597\n",
      "Episode 13657; Testing Loss 0.005797686543357172; Training Loss 0.004659196810901244\n",
      "Episode 13658; Testing Loss 0.005797679856003022; Training Loss 0.004659188026179706\n",
      "Episode 13659; Testing Loss 0.005797619088330601; Training Loss 0.00465917721871306\n",
      "Episode 13660; Testing Loss 0.005797635230030732; Training Loss 0.004659166655146116\n",
      "Episode 13661; Testing Loss 0.005797709752246398; Training Loss 0.004659157254871261\n",
      "Episode 13662; Testing Loss 0.0057977037999333465; Training Loss 0.004659148342928459\n",
      "Episode 13663; Testing Loss 0.005797619730066897; Training Loss 0.004659137442278538\n",
      "Episode 13664; Testing Loss 0.005797647157832093; Training Loss 0.004659127502633585\n",
      "Episode 13665; Testing Loss 0.005797703408735643; Training Loss 0.004659117951874736\n",
      "Episode 13666; Testing Loss 0.005797646056799135; Training Loss 0.0046591096828064275\n",
      "Episode 13667; Testing Loss 0.005797546478630106; Training Loss 0.004659100696076957\n",
      "Episode 13668; Testing Loss 0.005797498003857403; Training Loss 0.0046590896783841864\n",
      "Episode 13669; Testing Loss 0.0057975669337260935; Training Loss 0.004659081224054582\n",
      "Episode 13670; Testing Loss 0.005797672930920345; Training Loss 0.004659073777544884\n",
      "Episode 13671; Testing Loss 0.005797641836121743; Training Loss 0.004659063674585747\n",
      "Episode 13672; Testing Loss 0.005797534435214054; Training Loss 0.004659053355440207\n",
      "Episode 13673; Testing Loss 0.005797522189533937; Training Loss 0.00465904390764581\n",
      "Episode 13674; Testing Loss 0.005797604213562443; Training Loss 0.0046590368948990135\n",
      "Episode 13675; Testing Loss 0.005797537713319909; Training Loss 0.004659025539301184\n",
      "Episode 13676; Testing Loss 0.005797390448516872; Training Loss 0.0046590170366070565\n",
      "Episode 13677; Testing Loss 0.005797488221510727; Training Loss 0.004659008502672844\n",
      "Episode 13678; Testing Loss 0.005797644070570869; Training Loss 0.0046589999742889\n",
      "Episode 13679; Testing Loss 0.005797589905257339; Training Loss 0.0046589890166689645\n",
      "Episode 13680; Testing Loss 0.005797491372174715; Training Loss 0.004658978367448644\n",
      "Episode 13681; Testing Loss 0.0057975296404931724; Training Loss 0.004658969834243789\n",
      "Episode 13682; Testing Loss 0.005797640389065514; Training Loss 0.004658962553353687\n",
      "Episode 13683; Testing Loss 0.005797599703909123; Training Loss 0.004658951621198845\n",
      "Episode 13684; Testing Loss 0.005797533494115743; Training Loss 0.004658943021211624\n",
      "Episode 13685; Testing Loss 0.0057976534119597006; Training Loss 0.0046589337095822745\n",
      "Episode 13686; Testing Loss 0.005797716633010277; Training Loss 0.004658924896214165\n",
      "Episode 13687; Testing Loss 0.00579758426817482; Training Loss 0.004658914209745117\n",
      "Episode 13688; Testing Loss 0.005797531630118601; Training Loss 0.0046589069233791925\n",
      "Episode 13689; Testing Loss 0.005797614048324414; Training Loss 0.004658896665799164\n",
      "Episode 13690; Testing Loss 0.0057976513659420146; Training Loss 0.004658886483757796\n",
      "Episode 13691; Testing Loss 0.0057976854689250675; Training Loss 0.004658877561426227\n",
      "Episode 13692; Testing Loss 0.005797660182631736; Training Loss 0.00465886890268988\n",
      "Episode 13693; Testing Loss 0.005797535124588117; Training Loss 0.004658859675521171\n",
      "Episode 13694; Testing Loss 0.005797546229216563; Training Loss 0.00465885007695813\n",
      "Episode 13695; Testing Loss 0.005797639469355291; Training Loss 0.004658840783378718\n",
      "Episode 13696; Testing Loss 0.005797661637681759; Training Loss 0.004658831287096983\n",
      "Episode 13697; Testing Loss 0.0057975192371727755; Training Loss 0.004658823973943672\n",
      "Episode 13698; Testing Loss 0.005797577607724332; Training Loss 0.004658814541719756\n",
      "Episode 13699; Testing Loss 0.005797743818666817; Training Loss 0.00465880604265072\n",
      "Episode 13700; Testing Loss 0.005797753798703168; Training Loss 0.004658797150133378\n",
      "Episode 13701; Testing Loss 0.005797534081580123; Training Loss 0.004658786872306112\n",
      "Episode 13702; Testing Loss 0.005797475972273114; Training Loss 0.004658777773479644\n",
      "Episode 13703; Testing Loss 0.0057976306184133845; Training Loss 0.00465876869538424\n",
      "Episode 13704; Testing Loss 0.005797596433518654; Training Loss 0.004658761027871604\n",
      "Episode 13705; Testing Loss 0.005797482105456057; Training Loss 0.0046587523972977255\n",
      "Episode 13706; Testing Loss 0.005797470759022015; Training Loss 0.004658742800215009\n",
      "Episode 13707; Testing Loss 0.00579756709468039; Training Loss 0.004658732020227488\n",
      "Episode 13708; Testing Loss 0.0057976203252647004; Training Loss 0.004658725953916485\n",
      "Episode 13709; Testing Loss 0.005797514574234271; Training Loss 0.00465871501701695\n",
      "Episode 13710; Testing Loss 0.005797480881744377; Training Loss 0.004658706190707164\n",
      "Episode 13711; Testing Loss 0.005797697220251841; Training Loss 0.0046586970727281055\n",
      "Episode 13712; Testing Loss 0.005797808238844144; Training Loss 0.004658689598484528\n",
      "Episode 13713; Testing Loss 0.005797649787781776; Training Loss 0.0046586782838848005\n",
      "Episode 13714; Testing Loss 0.005797475904113885; Training Loss 0.004658669502863149\n",
      "Episode 13715; Testing Loss 0.005797570836345512; Training Loss 0.004658661168872049\n",
      "Episode 13716; Testing Loss 0.005797667927774061; Training Loss 0.0046586533909481346\n",
      "Episode 13717; Testing Loss 0.005797567492832664; Training Loss 0.0046586416550880114\n",
      "Episode 13718; Testing Loss 0.005797507320919747; Training Loss 0.004658633705037411\n",
      "Episode 13719; Testing Loss 0.005797599390699389; Training Loss 0.004658624561761092\n",
      "Episode 13720; Testing Loss 0.0057976829735289495; Training Loss 0.004658614490443805\n",
      "Episode 13721; Testing Loss 0.005797615105105284; Training Loss 0.004658606191378536\n",
      "Episode 13722; Testing Loss 0.005797532118611153; Training Loss 0.004658596242272504\n",
      "Episode 13723; Testing Loss 0.005797581214720459; Training Loss 0.00465858933982788\n",
      "Episode 13724; Testing Loss 0.005797593801792869; Training Loss 0.004658581469748068\n",
      "Episode 13725; Testing Loss 0.005797521456283652; Training Loss 0.00465857269951417\n",
      "Episode 13726; Testing Loss 0.005797546780228312; Training Loss 0.004658561836254666\n",
      "Episode 13727; Testing Loss 0.005797563994578853; Training Loss 0.004658554074298489\n",
      "Episode 13728; Testing Loss 0.005797524010957385; Training Loss 0.004658545674939169\n",
      "Episode 13729; Testing Loss 0.005797521661112226; Training Loss 0.004658534078384551\n",
      "Episode 13730; Testing Loss 0.005797549838751853; Training Loss 0.0046585247363689205\n",
      "Episode 13731; Testing Loss 0.005797603821782022; Training Loss 0.0046585198401988045\n",
      "Episode 13732; Testing Loss 0.005797565928703762; Training Loss 0.004658509930103576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13733; Testing Loss 0.005797480718503938; Training Loss 0.004658498048060573\n",
      "Episode 13734; Testing Loss 0.005797489899962013; Training Loss 0.004658488795618452\n",
      "Episode 13735; Testing Loss 0.005797586229859153; Training Loss 0.004658480575077927\n",
      "Episode 13736; Testing Loss 0.005797631026248346; Training Loss 0.004658472236950156\n",
      "Episode 13737; Testing Loss 0.005797605446598016; Training Loss 0.0046584638584892726\n",
      "Episode 13738; Testing Loss 0.005797540243806118; Training Loss 0.004658454625763493\n",
      "Episode 13739; Testing Loss 0.005797548202699256; Training Loss 0.004658444916341417\n",
      "Episode 13740; Testing Loss 0.005797550429343644; Training Loss 0.004658435024737811\n",
      "Episode 13741; Testing Loss 0.0057974467513525274; Training Loss 0.004658424368950432\n",
      "Episode 13742; Testing Loss 0.0057973968558838514; Training Loss 0.004658418302499851\n",
      "Episode 13743; Testing Loss 0.005797422546933085; Training Loss 0.0046584096849246245\n",
      "Episode 13744; Testing Loss 0.005797524908159184; Training Loss 0.004658398447816754\n",
      "Episode 13745; Testing Loss 0.005797552634812275; Training Loss 0.00465839015208201\n",
      "Episode 13746; Testing Loss 0.005797553396652584; Training Loss 0.004658381655816827\n",
      "Episode 13747; Testing Loss 0.005797535305971639; Training Loss 0.004658375012739013\n",
      "Episode 13748; Testing Loss 0.0057974594597565874; Training Loss 0.004658365005743464\n",
      "Episode 13749; Testing Loss 0.005797438431938477; Training Loss 0.004658355511113989\n",
      "Episode 13750; Testing Loss 0.005797519518787633; Training Loss 0.004658346209483457\n",
      "Episode 13751; Testing Loss 0.00579752332922313; Training Loss 0.004658336434764222\n",
      "Episode 13752; Testing Loss 0.0057974219471339785; Training Loss 0.004658326487610157\n",
      "Episode 13753; Testing Loss 0.00579736810488818; Training Loss 0.004658318838867541\n",
      "Episode 13754; Testing Loss 0.005797447651690901; Training Loss 0.00465830879386939\n",
      "Episode 13755; Testing Loss 0.005797562889675806; Training Loss 0.004658300085325239\n",
      "Episode 13756; Testing Loss 0.005797467209515204; Training Loss 0.004658291887916982\n",
      "Episode 13757; Testing Loss 0.005797261424837728; Training Loss 0.004658283392360186\n",
      "Episode 13758; Testing Loss 0.005797291172706476; Training Loss 0.0046582734300428695\n",
      "Episode 13759; Testing Loss 0.005797470312954276; Training Loss 0.004658265190496862\n",
      "Episode 13760; Testing Loss 0.0057974191337592474; Training Loss 0.004658254657632747\n",
      "Episode 13761; Testing Loss 0.00579729225612283; Training Loss 0.004658247100692931\n",
      "Episode 13762; Testing Loss 0.005797411588303328; Training Loss 0.004658236808912273\n",
      "Episode 13763; Testing Loss 0.005797549550161265; Training Loss 0.004658228723931179\n",
      "Episode 13764; Testing Loss 0.005797518749661814; Training Loss 0.004658219208542879\n",
      "Episode 13765; Testing Loss 0.005797419547235569; Training Loss 0.0046582113167235265\n",
      "Episode 13766; Testing Loss 0.005797455054615416; Training Loss 0.004658201775859767\n",
      "Episode 13767; Testing Loss 0.0057975106195651376; Training Loss 0.004658193422464407\n",
      "Episode 13768; Testing Loss 0.005797460757628405; Training Loss 0.004658182859233626\n",
      "Episode 13769; Testing Loss 0.005797335547167218; Training Loss 0.0046581738061392\n",
      "Episode 13770; Testing Loss 0.005797401987088824; Training Loss 0.004658166100977049\n",
      "Episode 13771; Testing Loss 0.005797464332293846; Training Loss 0.00465815607169091\n",
      "Episode 13772; Testing Loss 0.0057974254463743265; Training Loss 0.004658147471597036\n",
      "Episode 13773; Testing Loss 0.005797414135869932; Training Loss 0.0046581399461687245\n",
      "Episode 13774; Testing Loss 0.005797458928074035; Training Loss 0.004658132479352462\n",
      "Episode 13775; Testing Loss 0.005797443240115713; Training Loss 0.004658121570074675\n",
      "Episode 13776; Testing Loss 0.00579740998492704; Training Loss 0.004658111596771779\n",
      "Episode 13777; Testing Loss 0.005797444738710272; Training Loss 0.004658102789703189\n",
      "Episode 13778; Testing Loss 0.005797390990798182; Training Loss 0.004658094711371791\n",
      "Episode 13779; Testing Loss 0.005797350316368094; Training Loss 0.004658085071782296\n",
      "Episode 13780; Testing Loss 0.005797406135544268; Training Loss 0.004658075339175243\n",
      "Episode 13781; Testing Loss 0.005797465940748738; Training Loss 0.004658065641044586\n",
      "Episode 13782; Testing Loss 0.005797381179330141; Training Loss 0.004658057131546382\n",
      "Episode 13783; Testing Loss 0.0057973905137993055; Training Loss 0.004658047130254656\n",
      "Episode 13784; Testing Loss 0.0057974333124814256; Training Loss 0.004658040399875316\n",
      "Episode 13785; Testing Loss 0.005797414220895272; Training Loss 0.004658031161402983\n",
      "Episode 13786; Testing Loss 0.005797340602282452; Training Loss 0.004658020622453484\n",
      "Episode 13787; Testing Loss 0.005797369052132181; Training Loss 0.004658012260585667\n",
      "Episode 13788; Testing Loss 0.005797432257950081; Training Loss 0.0046580027381362575\n",
      "Episode 13789; Testing Loss 0.005797434662218866; Training Loss 0.004657993703896111\n",
      "Episode 13790; Testing Loss 0.005797387407031706; Training Loss 0.004657985412047337\n",
      "Episode 13791; Testing Loss 0.005797392683445075; Training Loss 0.004657976051682605\n",
      "Episode 13792; Testing Loss 0.005797507148465416; Training Loss 0.004657967327683995\n",
      "Episode 13793; Testing Loss 0.005797487367300705; Training Loss 0.004657959492281535\n",
      "Episode 13794; Testing Loss 0.005797383231014979; Training Loss 0.0046579504692726885\n",
      "Episode 13795; Testing Loss 0.005797355399305266; Training Loss 0.0046579403897784154\n",
      "Episode 13796; Testing Loss 0.005797417567659969; Training Loss 0.0046579298309651325\n",
      "Episode 13797; Testing Loss 0.005797420939198134; Training Loss 0.004657924921489775\n",
      "Episode 13798; Testing Loss 0.0057973000503632665; Training Loss 0.0046579145793452905\n",
      "Episode 13799; Testing Loss 0.005797239290325967; Training Loss 0.004657903766452632\n",
      "Episode 13800; Testing Loss 0.005797374836731356; Training Loss 0.004657895631051293\n",
      "Episode 13801; Testing Loss 0.005797435011144298; Training Loss 0.004657886743500245\n",
      "Episode 13802; Testing Loss 0.005797314577076324; Training Loss 0.004657875613358235\n",
      "Episode 13803; Testing Loss 0.0057972231628721565; Training Loss 0.004657867810031294\n",
      "Episode 13804; Testing Loss 0.0057973059686427745; Training Loss 0.004657857944391507\n",
      "Episode 13805; Testing Loss 0.005797405128379764; Training Loss 0.004657849277216937\n",
      "Episode 13806; Testing Loss 0.005797359294430763; Training Loss 0.00465784142551617\n",
      "Episode 13807; Testing Loss 0.005797287490192711; Training Loss 0.004657831788209636\n",
      "Episode 13808; Testing Loss 0.005797302827496589; Training Loss 0.004657821642475415\n",
      "Episode 13809; Testing Loss 0.005797313213352262; Training Loss 0.004657815945042178\n",
      "Episode 13810; Testing Loss 0.0057972705848256395; Training Loss 0.004657806023922093\n",
      "Episode 13811; Testing Loss 0.005797261295398719; Training Loss 0.0046577934515956085\n",
      "Episode 13812; Testing Loss 0.005797323471119406; Training Loss 0.004657786964696409\n",
      "Episode 13813; Testing Loss 0.005797340407457183; Training Loss 0.004657780556579548\n",
      "Episode 13814; Testing Loss 0.00579728339809397; Training Loss 0.004657768807604678\n",
      "Episode 13815; Testing Loss 0.005797335031622467; Training Loss 0.004657757624080236\n",
      "Episode 13816; Testing Loss 0.005797347299308577; Training Loss 0.004657750825393836\n",
      "Episode 13817; Testing Loss 0.0057972165622418035; Training Loss 0.004657743234781351\n",
      "Episode 13818; Testing Loss 0.005797203698952415; Training Loss 0.004657732239052824\n",
      "Episode 13819; Testing Loss 0.005797297564388739; Training Loss 0.004657721400055985\n",
      "Episode 13820; Testing Loss 0.00579725211983799; Training Loss 0.004657713084389253\n",
      "Episode 13821; Testing Loss 0.005797153888957913; Training Loss 0.004657705015459426\n",
      "Episode 13822; Testing Loss 0.005797213304923205; Training Loss 0.004657695442083735\n",
      "Episode 13823; Testing Loss 0.00579733672068689; Training Loss 0.004657685957106937\n",
      "Episode 13824; Testing Loss 0.005797337001490761; Training Loss 0.0046576765297785564\n",
      "Episode 13825; Testing Loss 0.005797208154112562; Training Loss 0.004657665999381346\n",
      "Episode 13826; Testing Loss 0.005797155273379399; Training Loss 0.004657657999123311\n",
      "Episode 13827; Testing Loss 0.005797241864822115; Training Loss 0.004657649820945369\n",
      "Episode 13828; Testing Loss 0.005797233446892085; Training Loss 0.004657640237357184\n",
      "Episode 13829; Testing Loss 0.005797158214459291; Training Loss 0.004657630780561213\n",
      "Episode 13830; Testing Loss 0.005797215557503202; Training Loss 0.004657621152023042\n",
      "Episode 13831; Testing Loss 0.005797295024283732; Training Loss 0.00465761466757265\n",
      "Episode 13832; Testing Loss 0.005797198854364543; Training Loss 0.0046576045739599205\n",
      "Episode 13833; Testing Loss 0.005797082768884788; Training Loss 0.004657594049821995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13834; Testing Loss 0.005797124764306147; Training Loss 0.004657586053526687\n",
      "Episode 13835; Testing Loss 0.00579718771334648; Training Loss 0.004657576229602905\n",
      "Episode 13836; Testing Loss 0.0057971578878680706; Training Loss 0.00465756698433472\n",
      "Episode 13837; Testing Loss 0.005797158428995254; Training Loss 0.004657558925818209\n",
      "Episode 13838; Testing Loss 0.005797184463647888; Training Loss 0.004657549468232701\n",
      "Episode 13839; Testing Loss 0.0057972185856348295; Training Loss 0.0046575407444263495\n",
      "Episode 13840; Testing Loss 0.005797200128083082; Training Loss 0.004657532296762385\n",
      "Episode 13841; Testing Loss 0.005797151107176537; Training Loss 0.004657523541724871\n",
      "Episode 13842; Testing Loss 0.005797097801049633; Training Loss 0.004657515228333314\n",
      "Episode 13843; Testing Loss 0.005797068010504597; Training Loss 0.004657504614077715\n",
      "Episode 13844; Testing Loss 0.0057970987425675895; Training Loss 0.004657493279532041\n",
      "Episode 13845; Testing Loss 0.005797172515496035; Training Loss 0.004657489498505982\n",
      "Episode 13846; Testing Loss 0.005797154327957015; Training Loss 0.004657480701020954\n",
      "Episode 13847; Testing Loss 0.005797083418678228; Training Loss 0.004657467595249359\n",
      "Episode 13848; Testing Loss 0.005797033816464949; Training Loss 0.004657459890991296\n",
      "Episode 13849; Testing Loss 0.005797075527727971; Training Loss 0.004657455016209141\n",
      "Episode 13850; Testing Loss 0.005797103796326413; Training Loss 0.004657444716953454\n",
      "Episode 13851; Testing Loss 0.005797070504034712; Training Loss 0.0046574322282901095\n",
      "Episode 13852; Testing Loss 0.005797075366819023; Training Loss 0.004657424573482663\n",
      "Episode 13853; Testing Loss 0.0057971229954519084; Training Loss 0.004657417187067488\n",
      "Episode 13854; Testing Loss 0.005797133557386423; Training Loss 0.004657407139322544\n",
      "Episode 13855; Testing Loss 0.0057971224799115645; Training Loss 0.004657395598548685\n",
      "Episode 13856; Testing Loss 0.005797102038162423; Training Loss 0.004657386880162474\n",
      "Episode 13857; Testing Loss 0.005797069995885824; Training Loss 0.004657380665271639\n",
      "Episode 13858; Testing Loss 0.0057970061942271695; Training Loss 0.004657371303456096\n",
      "Episode 13859; Testing Loss 0.005797021894042; Training Loss 0.00465735989929861\n",
      "Episode 13860; Testing Loss 0.005797117341364878; Training Loss 0.004657349770371669\n",
      "Episode 13861; Testing Loss 0.0057971408206448395; Training Loss 0.004657342501158532\n",
      "Episode 13862; Testing Loss 0.005797083513895448; Training Loss 0.004657332685989978\n",
      "Episode 13863; Testing Loss 0.005797043039023835; Training Loss 0.0046573218574481315\n",
      "Episode 13864; Testing Loss 0.005797015879345094; Training Loss 0.004657313475407818\n",
      "Episode 13865; Testing Loss 0.00579705712168712; Training Loss 0.004657304212705862\n",
      "Episode 13866; Testing Loss 0.005797086483668738; Training Loss 0.004657294700497175\n",
      "Episode 13867; Testing Loss 0.0057970024350439625; Training Loss 0.004657285882634248\n",
      "Episode 13868; Testing Loss 0.005797008398419996; Training Loss 0.004657276290907223\n",
      "Episode 13869; Testing Loss 0.005797119039099549; Training Loss 0.00465726714793997\n",
      "Episode 13870; Testing Loss 0.0057971981170995765; Training Loss 0.0046572602904084446\n",
      "Episode 13871; Testing Loss 0.005797117691761141; Training Loss 0.004657250231678803\n",
      "Episode 13872; Testing Loss 0.005797055491074891; Training Loss 0.004657239898299482\n",
      "Episode 13873; Testing Loss 0.005797079844346498; Training Loss 0.0046572300930894026\n",
      "Episode 13874; Testing Loss 0.0057970622530782775; Training Loss 0.00465722190616351\n",
      "Episode 13875; Testing Loss 0.0057969940507763594; Training Loss 0.004657212485222028\n",
      "Episode 13876; Testing Loss 0.005796998273415035; Training Loss 0.0046572018850701115\n",
      "Episode 13877; Testing Loss 0.005797074584801027; Training Loss 0.004657195902745296\n",
      "Episode 13878; Testing Loss 0.005797008000411178; Training Loss 0.004657185789640881\n",
      "Episode 13879; Testing Loss 0.005796901002079235; Training Loss 0.004657175648820564\n",
      "Episode 13880; Testing Loss 0.005796986327181662; Training Loss 0.004657166958359572\n",
      "Episode 13881; Testing Loss 0.005797085386431778; Training Loss 0.0046571582956122974\n",
      "Episode 13882; Testing Loss 0.005796986970308158; Training Loss 0.004657148756213541\n",
      "Episode 13883; Testing Loss 0.0057968946057642265; Training Loss 0.004657139918240589\n",
      "Episode 13884; Testing Loss 0.0057969490931273305; Training Loss 0.004657130443080748\n",
      "Episode 13885; Testing Loss 0.005796990849405434; Training Loss 0.004657120689361347\n",
      "Episode 13886; Testing Loss 0.005796911662693235; Training Loss 0.004657110377386564\n",
      "Episode 13887; Testing Loss 0.005796874674039705; Training Loss 0.004657104216597169\n",
      "Episode 13888; Testing Loss 0.005796864433543246; Training Loss 0.004657095742195446\n",
      "Episode 13889; Testing Loss 0.005796872123458788; Training Loss 0.0046570840719676754\n",
      "Episode 13890; Testing Loss 0.005796881952002497; Training Loss 0.004657075767455667\n",
      "Episode 13891; Testing Loss 0.005796932557620587; Training Loss 0.004657069753193095\n",
      "Episode 13892; Testing Loss 0.005796927387846844; Training Loss 0.004657059199574961\n",
      "Episode 13893; Testing Loss 0.005796897191248652; Training Loss 0.004657047915004228\n",
      "Episode 13894; Testing Loss 0.005796963309242924; Training Loss 0.0046570386516991075\n",
      "Episode 13895; Testing Loss 0.005796981284999353; Training Loss 0.004657029710678247\n",
      "Episode 13896; Testing Loss 0.005796924588007698; Training Loss 0.004657020056501918\n",
      "Episode 13897; Testing Loss 0.0057968488450696265; Training Loss 0.004657010979683497\n",
      "Episode 13898; Testing Loss 0.005796740498468383; Training Loss 0.004657002371623051\n",
      "Episode 13899; Testing Loss 0.005796787460875346; Training Loss 0.004656992112586754\n",
      "Episode 13900; Testing Loss 0.005796823388061325; Training Loss 0.004656984206117908\n",
      "Episode 13901; Testing Loss 0.005796764063426925; Training Loss 0.0046569745335011655\n",
      "Episode 13902; Testing Loss 0.005796754121676459; Training Loss 0.004656966475890035\n",
      "Episode 13903; Testing Loss 0.0057968461612847326; Training Loss 0.004656956439515449\n",
      "Episode 13904; Testing Loss 0.005796897110717188; Training Loss 0.004656947178453471\n",
      "Episode 13905; Testing Loss 0.005796834302811226; Training Loss 0.004656938620828459\n",
      "Episode 13906; Testing Loss 0.00579686528123786; Training Loss 0.004656929290548517\n",
      "Episode 13907; Testing Loss 0.005796901921071014; Training Loss 0.004656920637161168\n",
      "Episode 13908; Testing Loss 0.005796780971980395; Training Loss 0.004656910080452526\n",
      "Episode 13909; Testing Loss 0.005796690267590181; Training Loss 0.0046569025493322265\n",
      "Episode 13910; Testing Loss 0.005796799904385363; Training Loss 0.004656893124475617\n",
      "Episode 13911; Testing Loss 0.005796876260316815; Training Loss 0.0046568837527774624\n",
      "Episode 13912; Testing Loss 0.005796832255794865; Training Loss 0.004656874865273427\n",
      "Episode 13913; Testing Loss 0.005796785108955661; Training Loss 0.004656865129312207\n",
      "Episode 13914; Testing Loss 0.005796806048715716; Training Loss 0.004656857382830667\n",
      "Episode 13915; Testing Loss 0.005796740125316785; Training Loss 0.0046568473662021445\n",
      "Episode 13916; Testing Loss 0.005796660379183607; Training Loss 0.004656837736389418\n",
      "Episode 13917; Testing Loss 0.005796751892184524; Training Loss 0.004656828676141337\n",
      "Episode 13918; Testing Loss 0.005796811912683147; Training Loss 0.0046568201151192985\n",
      "Episode 13919; Testing Loss 0.005796785542974398; Training Loss 0.0046568102844669395\n",
      "Episode 13920; Testing Loss 0.005796726343972019; Training Loss 0.004656801793973981\n",
      "Episode 13921; Testing Loss 0.0057968237064778315; Training Loss 0.00465679387129787\n",
      "Episode 13922; Testing Loss 0.005796917172042811; Training Loss 0.004656783859003793\n",
      "Episode 13923; Testing Loss 0.005796832624922829; Training Loss 0.004656775477807106\n",
      "Episode 13924; Testing Loss 0.005796669736825509; Training Loss 0.004656766949299719\n",
      "Episode 13925; Testing Loss 0.005796684396054635; Training Loss 0.0046567574316997195\n",
      "Episode 13926; Testing Loss 0.0057967503905073615; Training Loss 0.004656747616864232\n",
      "Episode 13927; Testing Loss 0.0057967698564761845; Training Loss 0.004656738543156711\n",
      "Episode 13928; Testing Loss 0.005796733425227672; Training Loss 0.0046567320585613565\n",
      "Episode 13929; Testing Loss 0.005796720362469073; Training Loss 0.0046567216921816895\n",
      "Episode 13930; Testing Loss 0.005796713679421612; Training Loss 0.004656711487225559\n",
      "Episode 13931; Testing Loss 0.005796651627769009; Training Loss 0.004656703311556852\n",
      "Episode 13932; Testing Loss 0.005796625142406956; Training Loss 0.004656692937712139\n",
      "Episode 13933; Testing Loss 0.0057967066286552; Training Loss 0.004656683172195102\n",
      "Episode 13934; Testing Loss 0.005796746740822597; Training Loss 0.004656676534545468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13935; Testing Loss 0.005796682212851465; Training Loss 0.00465666641434829\n",
      "Episode 13936; Testing Loss 0.005796634189381035; Training Loss 0.004656655429722288\n",
      "Episode 13937; Testing Loss 0.005796689997952736; Training Loss 0.004656646441710852\n",
      "Episode 13938; Testing Loss 0.005796789917076567; Training Loss 0.004656638193958318\n",
      "Episode 13939; Testing Loss 0.005796736784635526; Training Loss 0.0046566287590597485\n",
      "Episode 13940; Testing Loss 0.005796717897237752; Training Loss 0.0046566197664188215\n",
      "Episode 13941; Testing Loss 0.005796701813806246; Training Loss 0.004656609976170795\n",
      "Episode 13942; Testing Loss 0.0057966834378708415; Training Loss 0.004656601487660371\n",
      "Episode 13943; Testing Loss 0.005796695693313536; Training Loss 0.004656594375702464\n",
      "Episode 13944; Testing Loss 0.00579669298589995; Training Loss 0.004656584610649444\n",
      "Episode 13945; Testing Loss 0.005796620470110176; Training Loss 0.0046565743633275826\n",
      "Episode 13946; Testing Loss 0.00579661119568496; Training Loss 0.004656564636123116\n",
      "Episode 13947; Testing Loss 0.005796686784681261; Training Loss 0.004656560260585503\n",
      "Episode 13948; Testing Loss 0.005796616013521554; Training Loss 0.0046565501022162\n",
      "Episode 13949; Testing Loss 0.005796519037491953; Training Loss 0.004656538187399766\n",
      "Episode 13950; Testing Loss 0.005796526870163225; Training Loss 0.00465653040106569\n",
      "Episode 13951; Testing Loss 0.0057967084072541065; Training Loss 0.0046565246219425135\n",
      "Episode 13952; Testing Loss 0.005796751934743268; Training Loss 0.004656515111506109\n",
      "Episode 13953; Testing Loss 0.005796660655992095; Training Loss 0.004656502364406452\n",
      "Episode 13954; Testing Loss 0.005796559303843925; Training Loss 0.00465649473949447\n",
      "Episode 13955; Testing Loss 0.005796545969133517; Training Loss 0.0046564874608082405\n",
      "Episode 13956; Testing Loss 0.005796608275332793; Training Loss 0.0046564781079917765\n",
      "Episode 13957; Testing Loss 0.005796578527391716; Training Loss 0.004656466288516983\n",
      "Episode 13958; Testing Loss 0.0057965441953763165; Training Loss 0.004656457824932269\n",
      "Episode 13959; Testing Loss 0.005796616391255972; Training Loss 0.00465644920593319\n",
      "Episode 13960; Testing Loss 0.005796703253029537; Training Loss 0.004656440261370907\n",
      "Episode 13961; Testing Loss 0.005796677288170461; Training Loss 0.0046564294563868095\n",
      "Episode 13962; Testing Loss 0.005796564685734709; Training Loss 0.004656420285522106\n",
      "Episode 13963; Testing Loss 0.005796460168682591; Training Loss 0.004656413495195975\n",
      "Episode 13964; Testing Loss 0.005796443932826067; Training Loss 0.004656403732311816\n",
      "Episode 13965; Testing Loss 0.00579656721379453; Training Loss 0.004656393151795987\n",
      "Episode 13966; Testing Loss 0.00579662725986927; Training Loss 0.0046563853208745195\n",
      "Episode 13967; Testing Loss 0.005796590643822789; Training Loss 0.004656376274452936\n",
      "Episode 13968; Testing Loss 0.005796544770528907; Training Loss 0.0046563692944355745\n",
      "Episode 13969; Testing Loss 0.005796546982823352; Training Loss 0.004656359258525621\n",
      "Episode 13970; Testing Loss 0.005796572740862342; Training Loss 0.004656349480369146\n",
      "Episode 13971; Testing Loss 0.005796560630149566; Training Loss 0.004656340870201438\n",
      "Episode 13972; Testing Loss 0.00579649855046368; Training Loss 0.0046563303603678885\n",
      "Episode 13973; Testing Loss 0.005796435112785827; Training Loss 0.00465632140201361\n",
      "Episode 13974; Testing Loss 0.005796467310257771; Training Loss 0.004656311755470896\n",
      "Episode 13975; Testing Loss 0.005796545120549874; Training Loss 0.004656302504223611\n",
      "Episode 13976; Testing Loss 0.005796592533098267; Training Loss 0.004656294307511528\n",
      "Episode 13977; Testing Loss 0.00579651277659919; Training Loss 0.0046562847770764155\n",
      "Episode 13978; Testing Loss 0.005796436984433105; Training Loss 0.004656275942061023\n",
      "Episode 13979; Testing Loss 0.005796425751228707; Training Loss 0.004656267082322503\n",
      "Episode 13980; Testing Loss 0.00579643189290126; Training Loss 0.004656258022309882\n",
      "Episode 13981; Testing Loss 0.005796347451685381; Training Loss 0.004656250163292201\n",
      "Episode 13982; Testing Loss 0.005796304792986835; Training Loss 0.004656241154543266\n",
      "Episode 13983; Testing Loss 0.005796407121526768; Training Loss 0.004656230991712593\n",
      "Episode 13984; Testing Loss 0.005796535311817615; Training Loss 0.004656223683953728\n",
      "Episode 13985; Testing Loss 0.00579649296143341; Training Loss 0.004656213494938193\n",
      "Episode 13986; Testing Loss 0.005796417164629236; Training Loss 0.00465620305121625\n",
      "Episode 13987; Testing Loss 0.0057963967299112336; Training Loss 0.004656194818387095\n",
      "Episode 13988; Testing Loss 0.005796423262607358; Training Loss 0.004656187662300144\n",
      "Episode 13989; Testing Loss 0.005796411316016665; Training Loss 0.004656179105736917\n",
      "Episode 13990; Testing Loss 0.005796452839359192; Training Loss 0.004656168255849526\n",
      "Episode 13991; Testing Loss 0.0057965149671573195; Training Loss 0.004656160018368288\n",
      "Episode 13992; Testing Loss 0.00579649183815332; Training Loss 0.004656150984095274\n",
      "Episode 13993; Testing Loss 0.005796435428068765; Training Loss 0.0046561406326123865\n",
      "Episode 13994; Testing Loss 0.00579640265758221; Training Loss 0.004656130329624574\n",
      "Episode 13995; Testing Loss 0.005796330147576126; Training Loss 0.004656122083687171\n",
      "Episode 13996; Testing Loss 0.005796321177388498; Training Loss 0.00465611268421624\n",
      "Episode 13997; Testing Loss 0.005796411319819103; Training Loss 0.004656105651829118\n",
      "Episode 13998; Testing Loss 0.005796412101075267; Training Loss 0.004656095134033306\n",
      "Episode 13999; Testing Loss 0.005796363616415607; Training Loss 0.004656086667301335\n",
      "Episode 14000; Testing Loss 0.005796380377197911; Training Loss 0.004656079595770821\n",
      "Episode 14001; Testing Loss 0.005796363803967861; Training Loss 0.0046560703494059365\n",
      "Episode 14002; Testing Loss 0.005796325549221121; Training Loss 0.004656061014621336\n",
      "Episode 14003; Testing Loss 0.005796373890111885; Training Loss 0.004656050779673873\n",
      "Episode 14004; Testing Loss 0.0057963765659147; Training Loss 0.004656043226364186\n",
      "Episode 14005; Testing Loss 0.0057963152238128; Training Loss 0.0046560340387297285\n",
      "Episode 14006; Testing Loss 0.005796274930713471; Training Loss 0.0046560229810314446\n",
      "Episode 14007; Testing Loss 0.0057963538070372; Training Loss 0.004656016149378384\n",
      "Episode 14008; Testing Loss 0.005796342047571304; Training Loss 0.004656007313544848\n",
      "Episode 14009; Testing Loss 0.005796259612191872; Training Loss 0.004655996506555552\n",
      "Episode 14010; Testing Loss 0.00579625815012286; Training Loss 0.004655988685322598\n",
      "Episode 14011; Testing Loss 0.005796380545049187; Training Loss 0.004655979598590094\n",
      "Episode 14012; Testing Loss 0.005796466355313935; Training Loss 0.004655971619133298\n",
      "Episode 14013; Testing Loss 0.0057963362848925505; Training Loss 0.004655962775683866\n",
      "Episode 14014; Testing Loss 0.00579620902256124; Training Loss 0.004655954695580946\n",
      "Episode 14015; Testing Loss 0.0057962775783828545; Training Loss 0.004655944090388516\n",
      "Episode 14016; Testing Loss 0.005796345861237017; Training Loss 0.004655934181286708\n",
      "Episode 14017; Testing Loss 0.005796191986438742; Training Loss 0.004655923816176484\n",
      "Episode 14018; Testing Loss 0.0057961505686737165; Training Loss 0.004655915807611877\n",
      "Episode 14019; Testing Loss 0.005796276814822249; Training Loss 0.0046559084149307515\n",
      "Episode 14020; Testing Loss 0.005796288495027764; Training Loss 0.004655898924530251\n",
      "Episode 14021; Testing Loss 0.005796247335701939; Training Loss 0.004655891731336687\n",
      "Episode 14022; Testing Loss 0.005796276291862817; Training Loss 0.004655883282594717\n",
      "Episode 14023; Testing Loss 0.005796327912197455; Training Loss 0.004655874703187507\n",
      "Episode 14024; Testing Loss 0.005796323078372062; Training Loss 0.004655864261366314\n",
      "Episode 14025; Testing Loss 0.0057962711079738655; Training Loss 0.004655853778276306\n",
      "Episode 14026; Testing Loss 0.005796149565932172; Training Loss 0.004655843830045984\n",
      "Episode 14027; Testing Loss 0.005796097587534502; Training Loss 0.004655836141282782\n",
      "Episode 14028; Testing Loss 0.005796222554953142; Training Loss 0.0046558283110136826\n",
      "Episode 14029; Testing Loss 0.005796237658611473; Training Loss 0.004655819363475916\n",
      "Episode 14030; Testing Loss 0.005796098695280936; Training Loss 0.00465580873504747\n",
      "Episode 14031; Testing Loss 0.005796031787179714; Training Loss 0.004655803138831415\n",
      "Episode 14032; Testing Loss 0.0057960446378575335; Training Loss 0.00465579358401052\n",
      "Episode 14033; Testing Loss 0.005796126654381447; Training Loss 0.004655781596298487\n",
      "Episode 14034; Testing Loss 0.005796229058125276; Training Loss 0.004655773445788067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14035; Testing Loss 0.005796230201651707; Training Loss 0.004655764229249364\n",
      "Episode 14036; Testing Loss 0.005796168015153539; Training Loss 0.004655754335941986\n",
      "Episode 14037; Testing Loss 0.005796207387460706; Training Loss 0.004655747594639014\n",
      "Episode 14038; Testing Loss 0.005796235518194194; Training Loss 0.004655737382863275\n",
      "Episode 14039; Testing Loss 0.005796193844461101; Training Loss 0.004655728594345042\n",
      "Episode 14040; Testing Loss 0.005796174817182805; Training Loss 0.00465572094264718\n",
      "Episode 14041; Testing Loss 0.0057961522725567234; Training Loss 0.004655712520805308\n",
      "Episode 14042; Testing Loss 0.005796111725154869; Training Loss 0.004655702755625035\n",
      "Episode 14043; Testing Loss 0.0057961140949041624; Training Loss 0.0046556923871692935\n",
      "Episode 14044; Testing Loss 0.005796109523219295; Training Loss 0.004655683021469961\n",
      "Episode 14045; Testing Loss 0.005796109937706034; Training Loss 0.004655674408753591\n",
      "Episode 14046; Testing Loss 0.005796080741579455; Training Loss 0.0046556649716559895\n",
      "Episode 14047; Testing Loss 0.005796133076649061; Training Loss 0.004655655646021261\n",
      "Episode 14048; Testing Loss 0.005796171314606525; Training Loss 0.004655647393041782\n",
      "Episode 14049; Testing Loss 0.005796151121940506; Training Loss 0.0046556391582474165\n",
      "Episode 14050; Testing Loss 0.005796054126756901; Training Loss 0.004655628043528898\n",
      "Episode 14051; Testing Loss 0.005796008449899794; Training Loss 0.004655619851902954\n",
      "Episode 14052; Testing Loss 0.005796136796140199; Training Loss 0.0046556109184944724\n",
      "Episode 14053; Testing Loss 0.005796163474753461; Training Loss 0.0046556029910333275\n",
      "Episode 14054; Testing Loss 0.005796054078324609; Training Loss 0.00465559338762414\n",
      "Episode 14055; Testing Loss 0.005796023447858729; Training Loss 0.004655583728610428\n",
      "Episode 14056; Testing Loss 0.0057960595259519115; Training Loss 0.0046555732460442445\n",
      "Episode 14057; Testing Loss 0.005796072009375051; Training Loss 0.0046555672776536754\n",
      "Episode 14058; Testing Loss 0.005795965499575882; Training Loss 0.004655559017881856\n",
      "Episode 14059; Testing Loss 0.005796003814252879; Training Loss 0.004655548332738394\n",
      "Episode 14060; Testing Loss 0.005796089213893906; Training Loss 0.004655539931070165\n",
      "Episode 14061; Testing Loss 0.005796051685994221; Training Loss 0.004655531834152729\n",
      "Episode 14062; Testing Loss 0.005795938403340407; Training Loss 0.004655522539236816\n",
      "Episode 14063; Testing Loss 0.00579598898911647; Training Loss 0.004655512684794215\n",
      "Episode 14064; Testing Loss 0.005796161203983275; Training Loss 0.004655505438954686\n",
      "Episode 14065; Testing Loss 0.005796108328378147; Training Loss 0.004655495446543441\n",
      "Episode 14066; Testing Loss 0.005795938605357508; Training Loss 0.004655486816256817\n",
      "Episode 14067; Testing Loss 0.005795976272807522; Training Loss 0.0046554777701431314\n",
      "Episode 14068; Testing Loss 0.005796091605003029; Training Loss 0.004655470541055685\n",
      "Episode 14069; Testing Loss 0.005796026152620326; Training Loss 0.004655460310215522\n",
      "Episode 14070; Testing Loss 0.005795887690870639; Training Loss 0.004655449713593277\n",
      "Episode 14071; Testing Loss 0.005795963662102029; Training Loss 0.004655444523890361\n",
      "Episode 14072; Testing Loss 0.005796095708742839; Training Loss 0.004655435154734606\n",
      "Episode 14073; Testing Loss 0.005796034281975133; Training Loss 0.004655422724733331\n",
      "Episode 14074; Testing Loss 0.005795843006890347; Training Loss 0.004655418127113473\n",
      "Episode 14075; Testing Loss 0.00579592819901644; Training Loss 0.004655410687264253\n",
      "Episode 14076; Testing Loss 0.005796122005967084; Training Loss 0.004655401668970052\n",
      "Episode 14077; Testing Loss 0.005796088880673072; Training Loss 0.004655389958368131\n",
      "Episode 14078; Testing Loss 0.005795875549574541; Training Loss 0.004655380151787941\n",
      "Episode 14079; Testing Loss 0.005795806123493953; Training Loss 0.004655374325491947\n",
      "Episode 14080; Testing Loss 0.005795959441620911; Training Loss 0.0046553639760825785\n",
      "Episode 14081; Testing Loss 0.00579600993888672; Training Loss 0.004655353979668937\n",
      "Episode 14082; Testing Loss 0.005795827695504411; Training Loss 0.0046553438488374866\n",
      "Episode 14083; Testing Loss 0.005795765169720804; Training Loss 0.004655336529673774\n",
      "Episode 14084; Testing Loss 0.005795879173749761; Training Loss 0.004655326033989259\n",
      "Episode 14085; Testing Loss 0.0057959591440161365; Training Loss 0.004655318031498308\n",
      "Episode 14086; Testing Loss 0.005795824979305471; Training Loss 0.004655307496557264\n",
      "Episode 14087; Testing Loss 0.005795719431903346; Training Loss 0.00465530005803588\n",
      "Episode 14088; Testing Loss 0.005795920458829983; Training Loss 0.004655290679904504\n",
      "Episode 14089; Testing Loss 0.005796132015202838; Training Loss 0.004655284162572379\n",
      "Episode 14090; Testing Loss 0.005796023077263249; Training Loss 0.004655273491148106\n",
      "Episode 14091; Testing Loss 0.005795764830101728; Training Loss 0.004655263797493037\n",
      "Episode 14092; Testing Loss 0.005795752968987681; Training Loss 0.004655253702774606\n",
      "Episode 14093; Testing Loss 0.00579589845518665; Training Loss 0.004655249160731128\n",
      "Episode 14094; Testing Loss 0.005795859043703046; Training Loss 0.004655238711205125\n",
      "Episode 14095; Testing Loss 0.005795728008706889; Training Loss 0.004655229256852929\n",
      "Episode 14096; Testing Loss 0.005795817970955864; Training Loss 0.004655221488333205\n",
      "Episode 14097; Testing Loss 0.005796034835573807; Training Loss 0.004655213969519987\n",
      "Episode 14098; Testing Loss 0.005795967136908589; Training Loss 0.004655204427230936\n",
      "Episode 14099; Testing Loss 0.005795764605877327; Training Loss 0.004655195739470306\n",
      "Episode 14100; Testing Loss 0.005795752574827656; Training Loss 0.004655183448445321\n",
      "Episode 14101; Testing Loss 0.005795907819553077; Training Loss 0.004655173978673593\n",
      "Episode 14102; Testing Loss 0.0057958962627846795; Training Loss 0.004655171293104169\n",
      "Episode 14103; Testing Loss 0.00579567851668991; Training Loss 0.004655162767294831\n",
      "Episode 14104; Testing Loss 0.0057955955759840996; Training Loss 0.004655151925606272\n",
      "Episode 14105; Testing Loss 0.005795775942347781; Training Loss 0.004655137724334903\n",
      "Episode 14106; Testing Loss 0.005795910314205489; Training Loss 0.004655131036308024\n",
      "Episode 14107; Testing Loss 0.005795777501935483; Training Loss 0.0046551212612049395\n",
      "Episode 14108; Testing Loss 0.005795636833565462; Training Loss 0.004655113408375097\n",
      "Episode 14109; Testing Loss 0.00579576426017098; Training Loss 0.004655102664523182\n",
      "Episode 14110; Testing Loss 0.005795931965656553; Training Loss 0.004655093468952928\n",
      "Episode 14111; Testing Loss 0.005795817841069408; Training Loss 0.004655086375537242\n",
      "Episode 14112; Testing Loss 0.005795482864157884; Training Loss 0.0046550784727461105\n",
      "Episode 14113; Testing Loss 0.005795482691446631; Training Loss 0.004655066069322896\n",
      "Episode 14114; Testing Loss 0.005795756489990269; Training Loss 0.004655059096530244\n",
      "Episode 14115; Testing Loss 0.005795834269466768; Training Loss 0.004655051854449629\n",
      "Episode 14116; Testing Loss 0.005795612251574164; Training Loss 0.004655042234467403\n",
      "Episode 14117; Testing Loss 0.005795559311457993; Training Loss 0.0046550345466852515\n",
      "Episode 14118; Testing Loss 0.005795774480660919; Training Loss 0.004655023906168533\n",
      "Episode 14119; Testing Loss 0.005795816105285752; Training Loss 0.004655014062716346\n",
      "Episode 14120; Testing Loss 0.005795622029353457; Training Loss 0.004655001449466832\n",
      "Episode 14121; Testing Loss 0.005795463934871949; Training Loss 0.004654995296611646\n",
      "Episode 14122; Testing Loss 0.005795514124283148; Training Loss 0.0046549853430001235\n",
      "Episode 14123; Testing Loss 0.005795708206825289; Training Loss 0.004654975565204734\n",
      "Episode 14124; Testing Loss 0.005795706979797501; Training Loss 0.004654966959491927\n",
      "Episode 14125; Testing Loss 0.005795615913727263; Training Loss 0.004654958819228796\n",
      "Episode 14126; Testing Loss 0.005795576461537246; Training Loss 0.0046549493707065395\n",
      "Episode 14127; Testing Loss 0.005795614146344555; Training Loss 0.004654940373923988\n",
      "Episode 14128; Testing Loss 0.005795622743276509; Training Loss 0.00465493035147013\n",
      "Episode 14129; Testing Loss 0.005795569110527001; Training Loss 0.004654922819761338\n",
      "Episode 14130; Testing Loss 0.005795473686233773; Training Loss 0.004654912812928479\n",
      "Episode 14131; Testing Loss 0.005795506329835056; Training Loss 0.004654903505317322\n",
      "Episode 14132; Testing Loss 0.005795677154837222; Training Loss 0.0046548960766957005\n",
      "Episode 14133; Testing Loss 0.005795723497582842; Training Loss 0.004654886919125805\n",
      "Episode 14134; Testing Loss 0.005795576374925638; Training Loss 0.004654877884212595\n",
      "Episode 14135; Testing Loss 0.005795476888965853; Training Loss 0.004654867290071504\n",
      "Episode 14136; Testing Loss 0.005795580435224276; Training Loss 0.004654859981054838\n",
      "Episode 14137; Testing Loss 0.005795576809291824; Training Loss 0.004654851979021916\n",
      "Episode 14138; Testing Loss 0.005795483658396177; Training Loss 0.004654841608411554\n",
      "Episode 14139; Testing Loss 0.00579549004118879; Training Loss 0.004654832972180528\n",
      "Episode 14140; Testing Loss 0.005795554972115349; Training Loss 0.004654823782877985\n",
      "Episode 14141; Testing Loss 0.005795499636430546; Training Loss 0.0046548138996234225\n",
      "Episode 14142; Testing Loss 0.005795463709238302; Training Loss 0.004654805909918753\n",
      "Episode 14143; Testing Loss 0.005795488876987884; Training Loss 0.004654796762180041\n",
      "Episode 14144; Testing Loss 0.005795507183063739; Training Loss 0.004654787200929473\n",
      "Episode 14145; Testing Loss 0.005795577104331483; Training Loss 0.004654779639400013\n",
      "Episode 14146; Testing Loss 0.005795640690682947; Training Loss 0.004654771704932092\n",
      "Episode 14147; Testing Loss 0.005795589089307512; Training Loss 0.004654761549400308\n",
      "Episode 14148; Testing Loss 0.005795462682945121; Training Loss 0.004654751145669191\n",
      "Episode 14149; Testing Loss 0.005795443236178153; Training Loss 0.004654742734608283\n",
      "Episode 14150; Testing Loss 0.005795492787911105; Training Loss 0.004654735532129977\n",
      "Episode 14151; Testing Loss 0.005795461785536427; Training Loss 0.004654725585309924\n",
      "Episode 14152; Testing Loss 0.005795438700129429; Training Loss 0.0046547162431282395\n",
      "Episode 14153; Testing Loss 0.00579542666616158; Training Loss 0.004654707203002131\n",
      "Episode 14154; Testing Loss 0.005795484417815711; Training Loss 0.004654698219255661\n",
      "Episode 14155; Testing Loss 0.005795513127506194; Training Loss 0.004654688449562243\n",
      "Episode 14156; Testing Loss 0.005795443700937076; Training Loss 0.004654679219344202\n",
      "Episode 14157; Testing Loss 0.005795413023619755; Training Loss 0.0046546693403105785\n",
      "Episode 14158; Testing Loss 0.005795397806566585; Training Loss 0.004654662684543367\n",
      "Episode 14159; Testing Loss 0.0057953249919929645; Training Loss 0.004654653595116496\n",
      "Episode 14160; Testing Loss 0.00579536200043846; Training Loss 0.004654643185315957\n",
      "Episode 14161; Testing Loss 0.005795437762960053; Training Loss 0.004654634330671672\n",
      "Episode 14162; Testing Loss 0.005795380902711175; Training Loss 0.00465462545721081\n",
      "Episode 14163; Testing Loss 0.005795343747991115; Training Loss 0.004654617288516177\n",
      "Episode 14164; Testing Loss 0.005795370932544128; Training Loss 0.0046546068701889855\n",
      "Episode 14165; Testing Loss 0.005795462737160331; Training Loss 0.004654599269080767\n",
      "Episode 14166; Testing Loss 0.005795345979499475; Training Loss 0.004654589262700255\n",
      "Episode 14167; Testing Loss 0.005795295872113458; Training Loss 0.004654580638547976\n",
      "Episode 14168; Testing Loss 0.005795395439663234; Training Loss 0.004654571497573021\n",
      "Episode 14169; Testing Loss 0.005795425313621169; Training Loss 0.004654563482765791\n",
      "Episode 14170; Testing Loss 0.0057952883512822065; Training Loss 0.004654553882935343\n",
      "Episode 14171; Testing Loss 0.005795234270940887; Training Loss 0.004654545002218052\n",
      "Episode 14172; Testing Loss 0.005795374581234296; Training Loss 0.0046545357075555805\n",
      "Episode 14173; Testing Loss 0.005795426144482731; Training Loss 0.004654526662077625\n",
      "Episode 14174; Testing Loss 0.005795357143393714; Training Loss 0.004654516774429108\n",
      "Episode 14175; Testing Loss 0.005795261990708998; Training Loss 0.004654508364475935\n",
      "Episode 14176; Testing Loss 0.005795176582801524; Training Loss 0.004654498998277944\n",
      "Episode 14177; Testing Loss 0.005795228875295362; Training Loss 0.004654490783079304\n",
      "Episode 14178; Testing Loss 0.005795355674723666; Training Loss 0.004654483336086078\n",
      "Episode 14179; Testing Loss 0.005795279413726532; Training Loss 0.004654471983437498\n",
      "Episode 14180; Testing Loss 0.005795205401897878; Training Loss 0.004654462944930999\n",
      "Episode 14181; Testing Loss 0.005795214064160991; Training Loss 0.004654453483007876\n",
      "Episode 14182; Testing Loss 0.005795289047792247; Training Loss 0.004654445252513593\n",
      "Episode 14183; Testing Loss 0.005795295919500468; Training Loss 0.0046544362475670455\n",
      "Episode 14184; Testing Loss 0.005795234024862515; Training Loss 0.004654426755952498\n",
      "Episode 14185; Testing Loss 0.0057951538057258575; Training Loss 0.004654417576306792\n",
      "Episode 14186; Testing Loss 0.005795098455890529; Training Loss 0.004654409046437279\n",
      "Episode 14187; Testing Loss 0.005795093289353791; Training Loss 0.004654399800092158\n",
      "Episode 14188; Testing Loss 0.005795128245950938; Training Loss 0.004654390984632146\n",
      "Episode 14189; Testing Loss 0.005795171774120192; Training Loss 0.00465438182972883\n",
      "Episode 14190; Testing Loss 0.005795208846434189; Training Loss 0.004654373175740343\n",
      "Episode 14191; Testing Loss 0.005795188472678764; Training Loss 0.004654363000812252\n",
      "Episode 14192; Testing Loss 0.00579516571303758; Training Loss 0.004654356472220981\n",
      "Episode 14193; Testing Loss 0.005795064185028371; Training Loss 0.0046543460313901355\n",
      "Episode 14194; Testing Loss 0.0057950159405169436; Training Loss 0.004654336571499333\n",
      "Episode 14195; Testing Loss 0.005795089364669606; Training Loss 0.004654329013281184\n",
      "Episode 14196; Testing Loss 0.005795062283584512; Training Loss 0.004654318846128551\n",
      "Episode 14197; Testing Loss 0.005794983315277643; Training Loss 0.004654311249762777\n",
      "Episode 14198; Testing Loss 0.005794976486628634; Training Loss 0.004654301217418374\n",
      "Episode 14199; Testing Loss 0.005795072454656597; Training Loss 0.004654293563713246\n",
      "Episode 14200; Testing Loss 0.005795153659926102; Training Loss 0.004654283909780749\n",
      "Episode 14201; Testing Loss 0.005795110335387547; Training Loss 0.00465427531761827\n",
      "Episode 14202; Testing Loss 0.005795018873919931; Training Loss 0.004654266925876966\n",
      "Episode 14203; Testing Loss 0.005795036636030474; Training Loss 0.004654257691526461\n",
      "Episode 14204; Testing Loss 0.005795083915108377; Training Loss 0.004654248666426267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14205; Testing Loss 0.005795055554937878; Training Loss 0.0046542383440966815\n",
      "Episode 14206; Testing Loss 0.005795021386239489; Training Loss 0.004654228308079303\n",
      "Episode 14207; Testing Loss 0.005795048920311709; Training Loss 0.004654221635069169\n",
      "Episode 14208; Testing Loss 0.005794960573749398; Training Loss 0.004654212886374501\n",
      "Episode 14209; Testing Loss 0.005794934949131478; Training Loss 0.004654201712659219\n",
      "Episode 14210; Testing Loss 0.005795006760090835; Training Loss 0.004654192661794221\n",
      "Episode 14211; Testing Loss 0.0057950411957422; Training Loss 0.0046541855055954005\n",
      "Episode 14212; Testing Loss 0.005794900034472409; Training Loss 0.004654174675971405\n",
      "Episode 14213; Testing Loss 0.005794768620939244; Training Loss 0.00465416867732766\n",
      "Episode 14214; Testing Loss 0.005794872200682915; Training Loss 0.0046541578140288226\n",
      "Episode 14215; Testing Loss 0.005794993927942622; Training Loss 0.004654149720525142\n",
      "Episode 14216; Testing Loss 0.005794937282409242; Training Loss 0.00465414046426354\n",
      "Episode 14217; Testing Loss 0.005794811573002599; Training Loss 0.004654133846427074\n",
      "Episode 14218; Testing Loss 0.005794813630412098; Training Loss 0.0046541241283985695\n",
      "Episode 14219; Testing Loss 0.005794913254348012; Training Loss 0.004654112203491902\n",
      "Episode 14220; Testing Loss 0.005794925009005778; Training Loss 0.004654104691188746\n",
      "Episode 14221; Testing Loss 0.005794856367290588; Training Loss 0.004654096151908466\n",
      "Episode 14222; Testing Loss 0.005794814932478038; Training Loss 0.004654088403551078\n",
      "Episode 14223; Testing Loss 0.005794857663312532; Training Loss 0.004654080029407345\n",
      "Episode 14224; Testing Loss 0.0057948574583305615; Training Loss 0.004654070245593612\n",
      "Episode 14225; Testing Loss 0.005794808970773083; Training Loss 0.004654059481182139\n",
      "Episode 14226; Testing Loss 0.00579478792689628; Training Loss 0.004654051472869547\n",
      "Episode 14227; Testing Loss 0.005794796969704134; Training Loss 0.004654045944050062\n",
      "Episode 14228; Testing Loss 0.005794800788815251; Training Loss 0.0046540364024510806\n",
      "Episode 14229; Testing Loss 0.005794829790079148; Training Loss 0.004654022290379348\n",
      "Episode 14230; Testing Loss 0.00579489205045693; Training Loss 0.004654015262645922\n",
      "Episode 14231; Testing Loss 0.005794817363609846; Training Loss 0.004654006367393493\n",
      "Episode 14232; Testing Loss 0.005794678811010565; Training Loss 0.004653996996482479\n",
      "Episode 14233; Testing Loss 0.005794688043515774; Training Loss 0.004653987072409862\n",
      "Episode 14234; Testing Loss 0.00579471499460789; Training Loss 0.004653980064689434\n",
      "Episode 14235; Testing Loss 0.005794671652320162; Training Loss 0.004653971412126807\n",
      "Episode 14236; Testing Loss 0.005794708261798022; Training Loss 0.004653961153320895\n",
      "Episode 14237; Testing Loss 0.005794798257589276; Training Loss 0.004653952323359349\n",
      "Episode 14238; Testing Loss 0.005794827616261744; Training Loss 0.0046539429397041655\n",
      "Episode 14239; Testing Loss 0.0057947098645402305; Training Loss 0.004653935003257857\n",
      "Episode 14240; Testing Loss 0.0057946250534585695; Training Loss 0.004653925547375109\n",
      "Episode 14241; Testing Loss 0.005794647393074425; Training Loss 0.004653915330017366\n",
      "Episode 14242; Testing Loss 0.005794764812461952; Training Loss 0.0046539067325069945\n",
      "Episode 14243; Testing Loss 0.005794728315569756; Training Loss 0.004653897845159266\n",
      "Episode 14244; Testing Loss 0.00579460040037606; Training Loss 0.004653890297144986\n",
      "Episode 14245; Testing Loss 0.005794621954724609; Training Loss 0.004653880750065698\n",
      "Episode 14246; Testing Loss 0.005794710304488257; Training Loss 0.004653871461820538\n",
      "Episode 14247; Testing Loss 0.0057947375303823495; Training Loss 0.004653863884734936\n",
      "Episode 14248; Testing Loss 0.005794651148141787; Training Loss 0.004653853700634925\n",
      "Episode 14249; Testing Loss 0.005794637120225512; Training Loss 0.00465384404665531\n",
      "Episode 14250; Testing Loss 0.005794648168464036; Training Loss 0.004653837342261894\n",
      "Episode 14251; Testing Loss 0.005794595271310204; Training Loss 0.004653828732695365\n",
      "Episode 14252; Testing Loss 0.0057945296784633355; Training Loss 0.004653818428313826\n",
      "Episode 14253; Testing Loss 0.005794593089133184; Training Loss 0.004653809072159697\n",
      "Episode 14254; Testing Loss 0.005794618572475359; Training Loss 0.004653802444559429\n",
      "Episode 14255; Testing Loss 0.005794488638388548; Training Loss 0.004653793682213319\n",
      "Episode 14256; Testing Loss 0.00579447614892832; Training Loss 0.004653784446702697\n",
      "Episode 14257; Testing Loss 0.005794628166547371; Training Loss 0.004653775025577973\n",
      "Episode 14258; Testing Loss 0.005794657012585032; Training Loss 0.004653769200842662\n",
      "Episode 14259; Testing Loss 0.005794472006858719; Training Loss 0.004653758921592944\n",
      "Episode 14260; Testing Loss 0.0057943669569252; Training Loss 0.004653747861640475\n",
      "Episode 14261; Testing Loss 0.005794491443426803; Training Loss 0.00465373843554132\n",
      "Episode 14262; Testing Loss 0.005794553234726976; Training Loss 0.004653729846275693\n",
      "Episode 14263; Testing Loss 0.00579441731063545; Training Loss 0.004653721838161097\n",
      "Episode 14264; Testing Loss 0.005794405934444417; Training Loss 0.00465371390524005\n",
      "Episode 14265; Testing Loss 0.0057945492812972186; Training Loss 0.0046537034615185\n",
      "Episode 14266; Testing Loss 0.005794580594504932; Training Loss 0.0046536956688956015\n",
      "Episode 14267; Testing Loss 0.0057944394411702385; Training Loss 0.0046536856561394556\n",
      "Episode 14268; Testing Loss 0.005794341681302275; Training Loss 0.004653678961621281\n",
      "Episode 14269; Testing Loss 0.00579438316808356; Training Loss 0.00465366971582102\n",
      "Episode 14270; Testing Loss 0.005794457395877357; Training Loss 0.004653659374438771\n",
      "Episode 14271; Testing Loss 0.005794472815025036; Training Loss 0.004653650501798225\n",
      "Episode 14272; Testing Loss 0.005794402061557039; Training Loss 0.004653641401243737\n",
      "Episode 14273; Testing Loss 0.005794430303555415; Training Loss 0.0046536317439426385\n",
      "Episode 14274; Testing Loss 0.0057945411630794565; Training Loss 0.004653623058408994\n",
      "Episode 14275; Testing Loss 0.005794526932071369; Training Loss 0.004653616273364903\n",
      "Episode 14276; Testing Loss 0.005794376797422036; Training Loss 0.004653607435827837\n",
      "Episode 14277; Testing Loss 0.005794358902367466; Training Loss 0.004653598055110291\n",
      "Episode 14278; Testing Loss 0.005794478876097481; Training Loss 0.004653589480229563\n",
      "Episode 14279; Testing Loss 0.0057944866840678195; Training Loss 0.004653580762774688\n",
      "Episode 14280; Testing Loss 0.005794354425907393; Training Loss 0.004653571601137816\n",
      "Episode 14281; Testing Loss 0.005794257735703416; Training Loss 0.004653562732846221\n",
      "Episode 14282; Testing Loss 0.005794328286843412; Training Loss 0.004653553479289554\n",
      "Episode 14283; Testing Loss 0.005794440831637662; Training Loss 0.004653544982336196\n",
      "Episode 14284; Testing Loss 0.0057944220599636526; Training Loss 0.0046535342508292705\n",
      "Episode 14285; Testing Loss 0.005794286409020565; Training Loss 0.004653526502450601\n",
      "Episode 14286; Testing Loss 0.005794257985337406; Training Loss 0.0046535175482723115\n",
      "Episode 14287; Testing Loss 0.005794350066132377; Training Loss 0.004653507377521948\n",
      "Episode 14288; Testing Loss 0.005794389042626027; Training Loss 0.00465350089718607\n",
      "Episode 14289; Testing Loss 0.00579426147386832; Training Loss 0.004653490658584555\n",
      "Episode 14290; Testing Loss 0.005794239710487137; Training Loss 0.004653482629729294\n",
      "Episode 14291; Testing Loss 0.005794443043486533; Training Loss 0.004653474085000463\n",
      "Episode 14292; Testing Loss 0.005794478689188623; Training Loss 0.004653466308431632\n",
      "Episode 14293; Testing Loss 0.005794248455553912; Training Loss 0.004653456155352542\n",
      "Episode 14294; Testing Loss 0.005794124558512771; Training Loss 0.004653447496216199\n",
      "Episode 14295; Testing Loss 0.005794275129459972; Training Loss 0.004653437632480101\n",
      "Episode 14296; Testing Loss 0.005794304618935597; Training Loss 0.004653427962106191\n",
      "Episode 14297; Testing Loss 0.005794188904960779; Training Loss 0.004653420032203119\n",
      "Episode 14298; Testing Loss 0.005794169994400077; Training Loss 0.00465341221040599\n",
      "Episode 14299; Testing Loss 0.005794321249363707; Training Loss 0.004653403448578385\n",
      "Episode 14300; Testing Loss 0.005794319508651105; Training Loss 0.004653393973537891\n",
      "Episode 14301; Testing Loss 0.005794184468969604; Training Loss 0.004653384028044309\n",
      "Episode 14302; Testing Loss 0.005794173153817102; Training Loss 0.0046533769019449535\n",
      "Episode 14303; Testing Loss 0.005794270732015556; Training Loss 0.0046533662157402\n",
      "Episode 14304; Testing Loss 0.0057942340160192654; Training Loss 0.004653357599521993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14305; Testing Loss 0.005794164790485076; Training Loss 0.004653350258963343\n",
      "Episode 14306; Testing Loss 0.005794232452297262; Training Loss 0.004653341274564489\n",
      "Episode 14307; Testing Loss 0.0057943609432661415; Training Loss 0.0046533322430046785\n",
      "Episode 14308; Testing Loss 0.005794295545995727; Training Loss 0.004653321905472039\n",
      "Episode 14309; Testing Loss 0.005794131015124662; Training Loss 0.00465331505711497\n",
      "Episode 14310; Testing Loss 0.0057941172294477435; Training Loss 0.004653306301341814\n",
      "Episode 14311; Testing Loss 0.00579424190548921; Training Loss 0.004653295981024157\n",
      "Episode 14312; Testing Loss 0.005794214155796921; Training Loss 0.0046532867195924026\n",
      "Episode 14313; Testing Loss 0.005794053595466709; Training Loss 0.004653278585200627\n",
      "Episode 14314; Testing Loss 0.005794082155226092; Training Loss 0.004653271105571476\n",
      "Episode 14315; Testing Loss 0.005794240796939717; Training Loss 0.004653260945199502\n",
      "Episode 14316; Testing Loss 0.005794164017235392; Training Loss 0.0046532503550023125\n",
      "Episode 14317; Testing Loss 0.005794004045048791; Training Loss 0.004653242316127614\n",
      "Episode 14318; Testing Loss 0.0057940113552502875; Training Loss 0.0046532333653191765\n",
      "Episode 14319; Testing Loss 0.005794137600399164; Training Loss 0.004653223778634001\n",
      "Episode 14320; Testing Loss 0.005794205499344634; Training Loss 0.004653217918373082\n",
      "Episode 14321; Testing Loss 0.005794147643285876; Training Loss 0.0046532064765512395\n",
      "Episode 14322; Testing Loss 0.005794057336460063; Training Loss 0.004653198936741058\n",
      "Episode 14323; Testing Loss 0.005793979703847549; Training Loss 0.0046531907954401545\n",
      "Episode 14324; Testing Loss 0.00579400759469154; Training Loss 0.004653181678626671\n",
      "Episode 14325; Testing Loss 0.005794074131219655; Training Loss 0.004653172613170128\n",
      "Episode 14326; Testing Loss 0.00579411216415774; Training Loss 0.004653164344345534\n",
      "Episode 14327; Testing Loss 0.005794112070635923; Training Loss 0.004653154481368596\n",
      "Episode 14328; Testing Loss 0.005794078233050608; Training Loss 0.004653144786641019\n",
      "Episode 14329; Testing Loss 0.005794063744006963; Training Loss 0.0046531376670757634\n",
      "Episode 14330; Testing Loss 0.005793977721044135; Training Loss 0.00465312777797848\n",
      "Episode 14331; Testing Loss 0.005793937904528879; Training Loss 0.004653118776613949\n",
      "Episode 14332; Testing Loss 0.005794060690436192; Training Loss 0.004653111784192996\n",
      "Episode 14333; Testing Loss 0.005794165310476761; Training Loss 0.004653102581896657\n",
      "Episode 14334; Testing Loss 0.005794042974798453; Training Loss 0.004653091139402638\n",
      "Episode 14335; Testing Loss 0.0057938940082386545; Training Loss 0.004653083784928778\n",
      "Episode 14336; Testing Loss 0.005793908403292354; Training Loss 0.00465307663962166\n",
      "Episode 14337; Testing Loss 0.005793983088423461; Training Loss 0.0046530672437176835\n",
      "Episode 14338; Testing Loss 0.0057939602303579375; Training Loss 0.004653058702301229\n",
      "Episode 14339; Testing Loss 0.005793906456141739; Training Loss 0.004653048949187574\n",
      "Episode 14340; Testing Loss 0.005793886696132137; Training Loss 0.004653040022536808\n",
      "Episode 14341; Testing Loss 0.005793872074677976; Training Loss 0.004653032099861194\n",
      "Episode 14342; Testing Loss 0.0057938881167531875; Training Loss 0.004653021133821802\n",
      "Episode 14343; Testing Loss 0.005793931041461328; Training Loss 0.004653011597701901\n",
      "Episode 14344; Testing Loss 0.005794054337278107; Training Loss 0.004653006050727987\n",
      "Episode 14345; Testing Loss 0.005794155334249321; Training Loss 0.0046529974634904455\n",
      "Episode 14346; Testing Loss 0.005794003666632626; Training Loss 0.004652986028563032\n",
      "Episode 14347; Testing Loss 0.005793853573750683; Training Loss 0.004652977927414056\n",
      "Episode 14348; Testing Loss 0.00579387515212589; Training Loss 0.004652968342419144\n",
      "Episode 14349; Testing Loss 0.005793933184781947; Training Loss 0.004652961550705399\n",
      "Episode 14350; Testing Loss 0.0057939109484841595; Training Loss 0.004652951820884145\n",
      "Episode 14351; Testing Loss 0.005793849682733072; Training Loss 0.004652942751654648\n",
      "Episode 14352; Testing Loss 0.005793901437161033; Training Loss 0.004652933890068267\n",
      "Episode 14353; Testing Loss 0.005793989674118893; Training Loss 0.004652924482422656\n",
      "Episode 14354; Testing Loss 0.005793961510147543; Training Loss 0.004652915847294161\n",
      "Episode 14355; Testing Loss 0.005793862680446777; Training Loss 0.004652905813947615\n",
      "Episode 14356; Testing Loss 0.0057939502739713464; Training Loss 0.004652898136811715\n",
      "Episode 14357; Testing Loss 0.005794025247922407; Training Loss 0.004652889199814365\n",
      "Episode 14358; Testing Loss 0.005793930563863828; Training Loss 0.004652878518855543\n",
      "Episode 14359; Testing Loss 0.005793858737789965; Training Loss 0.004652871336423946\n",
      "Episode 14360; Testing Loss 0.005793896934356275; Training Loss 0.004652863961793954\n",
      "Episode 14361; Testing Loss 0.005794024498640177; Training Loss 0.004652853508936394\n",
      "Episode 14362; Testing Loss 0.005793982384663277; Training Loss 0.004652844418892171\n",
      "Episode 14363; Testing Loss 0.005793804164712177; Training Loss 0.004652836417947528\n",
      "Episode 14364; Testing Loss 0.005793872776779769; Training Loss 0.004652826199525339\n",
      "Episode 14365; Testing Loss 0.005794019360095042; Training Loss 0.004652819621479977\n",
      "Episode 14366; Testing Loss 0.005793876792049346; Training Loss 0.004652810660760411\n",
      "Episode 14367; Testing Loss 0.005793797228708351; Training Loss 0.004652801332300321\n",
      "Episode 14368; Testing Loss 0.005793871905711966; Training Loss 0.004652792611862795\n",
      "Episode 14369; Testing Loss 0.005793863604455745; Training Loss 0.004652785674615882\n",
      "Episode 14370; Testing Loss 0.005793758105923079; Training Loss 0.004652774001922666\n",
      "Episode 14371; Testing Loss 0.005793764938435769; Training Loss 0.004652768009509972\n",
      "Episode 14372; Testing Loss 0.0057939551404766814; Training Loss 0.004652759845133474\n",
      "Episode 14373; Testing Loss 0.005793956413730523; Training Loss 0.004652750576781933\n",
      "Episode 14374; Testing Loss 0.005793772176866286; Training Loss 0.004652741550040916\n",
      "Episode 14375; Testing Loss 0.005793763579483441; Training Loss 0.004652731754701342\n",
      "Episode 14376; Testing Loss 0.005793883888411281; Training Loss 0.004652723582149964\n",
      "Episode 14377; Testing Loss 0.00579387339163832; Training Loss 0.004652716898991071\n",
      "Episode 14378; Testing Loss 0.005793760946173531; Training Loss 0.004652707637495982\n",
      "Episode 14379; Testing Loss 0.005793763517365809; Training Loss 0.004652697532173739\n",
      "Episode 14380; Testing Loss 0.0057938583980399905; Training Loss 0.0046526902098704475\n",
      "Episode 14381; Testing Loss 0.005793875081806842; Training Loss 0.004652682718691162\n",
      "Episode 14382; Testing Loss 0.0057937550949018115; Training Loss 0.004652676422983133\n",
      "Episode 14383; Testing Loss 0.00579362515841107; Training Loss 0.004652669173481742\n",
      "Episode 14384; Testing Loss 0.005793690387493092; Training Loss 0.004652655874235034\n",
      "Episode 14385; Testing Loss 0.005793801294203212; Training Loss 0.004652645495798449\n",
      "Episode 14386; Testing Loss 0.005793747239086672; Training Loss 0.004652636459324127\n",
      "Episode 14387; Testing Loss 0.005793660833646335; Training Loss 0.004652630840353306\n",
      "Episode 14388; Testing Loss 0.005793607397835838; Training Loss 0.004652622447520491\n",
      "Episode 14389; Testing Loss 0.005793663113079159; Training Loss 0.004652610694993239\n",
      "Episode 14390; Testing Loss 0.005793744590820543; Training Loss 0.004652600250756359\n",
      "Episode 14391; Testing Loss 0.005793769226401532; Training Loss 0.0046525942796535906\n",
      "Episode 14392; Testing Loss 0.005793648514040588; Training Loss 0.004652585256002877\n",
      "Episode 14393; Testing Loss 0.005793615803432227; Training Loss 0.00465257605033445\n",
      "Episode 14394; Testing Loss 0.005793714299847476; Training Loss 0.004652566267826196\n",
      "Episode 14395; Testing Loss 0.005793715462262283; Training Loss 0.004652556411096223\n",
      "Episode 14396; Testing Loss 0.005793604479280313; Training Loss 0.0046525462141248095\n",
      "Episode 14397; Testing Loss 0.005793555660437178; Training Loss 0.0046525402698425105\n",
      "Episode 14398; Testing Loss 0.0057936217501402204; Training Loss 0.004652532853082994\n",
      "Episode 14399; Testing Loss 0.005793618509572845; Training Loss 0.004652519893685241\n",
      "Episode 14400; Testing Loss 0.005793578304348277; Training Loss 0.004652514252878028\n",
      "Episode 14401; Testing Loss 0.005793687172019169; Training Loss 0.004652507007688996\n",
      "Episode 14402; Testing Loss 0.005793821363551742; Training Loss 0.004652497742430571\n",
      "Episode 14403; Testing Loss 0.005793693529229715; Training Loss 0.004652488439988347\n",
      "Episode 14404; Testing Loss 0.0057936197893886125; Training Loss 0.004652479898276426\n",
      "Episode 14405; Testing Loss 0.005793792018940764; Training Loss 0.004652467633816307\n",
      "Episode 14406; Testing Loss 0.00579382427270974; Training Loss 0.004652460024879848\n",
      "Episode 14407; Testing Loss 0.005793610499719836; Training Loss 0.0046524507586538835\n",
      "Episode 14408; Testing Loss 0.0057935155837715155; Training Loss 0.0046524435809203206\n",
      "Episode 14409; Testing Loss 0.005793656674873852; Training Loss 0.004652430905850143\n",
      "Episode 14410; Testing Loss 0.005793792387651601; Training Loss 0.004652424691989279\n",
      "Episode 14411; Testing Loss 0.005793695989421588; Training Loss 0.0046524169491222785\n",
      "Episode 14412; Testing Loss 0.00579357040555322; Training Loss 0.004652408064096082\n",
      "Episode 14413; Testing Loss 0.005793581031523977; Training Loss 0.004652400259201642\n",
      "Episode 14414; Testing Loss 0.005793634125373255; Training Loss 0.004652390104292664\n",
      "Episode 14415; Testing Loss 0.005793573464621562; Training Loss 0.004652381584530966\n",
      "Episode 14416; Testing Loss 0.0057935898447175355; Training Loss 0.004652371212201043\n",
      "Episode 14417; Testing Loss 0.005793639005225689; Training Loss 0.00465236570342409\n",
      "Episode 14418; Testing Loss 0.005793541438991413; Training Loss 0.004652357436149383\n",
      "Episode 14419; Testing Loss 0.0057934272058827; Training Loss 0.004652345916470857\n",
      "Episode 14420; Testing Loss 0.005793519204145444; Training Loss 0.004652336292124978\n",
      "Episode 14421; Testing Loss 0.0057936977117106145; Training Loss 0.004652329173797845\n",
      "Episode 14422; Testing Loss 0.005793590922504135; Training Loss 0.004652320137999773\n",
      "Episode 14423; Testing Loss 0.0057934457738922215; Training Loss 0.004652311403526737\n",
      "Episode 14424; Testing Loss 0.005793444937516611; Training Loss 0.004652302086180885\n",
      "Episode 14425; Testing Loss 0.005793554584566582; Training Loss 0.004652291997444784\n",
      "Episode 14426; Testing Loss 0.005793561574564839; Training Loss 0.0046522827215303205\n",
      "Episode 14427; Testing Loss 0.005793486164018918; Training Loss 0.004652278020637197\n",
      "Episode 14428; Testing Loss 0.005793471879006869; Training Loss 0.004652267559988356\n",
      "Episode 14429; Testing Loss 0.00579347918530318; Training Loss 0.0046522584569115965\n",
      "Episode 14430; Testing Loss 0.005793461956084617; Training Loss 0.004652250241199064\n",
      "Episode 14431; Testing Loss 0.005793438027693653; Training Loss 0.004652241276458262\n",
      "Episode 14432; Testing Loss 0.005793440897391351; Training Loss 0.004652236518438062\n",
      "Episode 14433; Testing Loss 0.00579348882550719; Training Loss 0.004652225216951973\n",
      "Episode 14434; Testing Loss 0.005793520829402558; Training Loss 0.004652217904593755\n",
      "Episode 14435; Testing Loss 0.00579344517710852; Training Loss 0.004652209649616587\n",
      "Episode 14436; Testing Loss 0.0057934095543950735; Training Loss 0.00465219906439131\n",
      "Episode 14437; Testing Loss 0.0057934289338571355; Training Loss 0.004652188944768448\n",
      "Episode 14438; Testing Loss 0.005793429864671431; Training Loss 0.0046521849777337015\n",
      "Episode 14439; Testing Loss 0.005793493231915965; Training Loss 0.0046521756652865825\n",
      "Episode 14440; Testing Loss 0.005793522952008384; Training Loss 0.004652165683200974\n",
      "Episode 14441; Testing Loss 0.0057934759358267; Training Loss 0.004652154752619229\n",
      "Episode 14442; Testing Loss 0.005793400113094245; Training Loss 0.004652151371617383\n",
      "Episode 14443; Testing Loss 0.005793373922054494; Training Loss 0.004652144598592844\n",
      "Episode 14444; Testing Loss 0.005793431938820826; Training Loss 0.004652134086056489\n",
      "Episode 14445; Testing Loss 0.005793458737612283; Training Loss 0.004652120249676565\n",
      "Episode 14446; Testing Loss 0.005793397541457659; Training Loss 0.0046521100232064586\n",
      "Episode 14447; Testing Loss 0.00579335489752265; Training Loss 0.00465210898771198\n",
      "Episode 14448; Testing Loss 0.00579336793544102; Training Loss 0.00465210097557211\n",
      "Episode 14449; Testing Loss 0.0057933730795630255; Training Loss 0.004652085107102541\n",
      "Episode 14450; Testing Loss 0.005793355748796138; Training Loss 0.004652077442182896\n",
      "Episode 14451; Testing Loss 0.00579328940599361; Training Loss 0.004652073235545289\n",
      "Episode 14452; Testing Loss 0.0057932814336275895; Training Loss 0.0046520642262486995\n",
      "Episode 14453; Testing Loss 0.005793340244554104; Training Loss 0.00465205083106276\n",
      "Episode 14454; Testing Loss 0.00579339650077868; Training Loss 0.004652042437319954\n",
      "Episode 14455; Testing Loss 0.005793381150628584; Training Loss 0.004652033308721343\n",
      "Episode 14456; Testing Loss 0.005793336062990455; Training Loss 0.004652024994692924\n",
      "Episode 14457; Testing Loss 0.005793368972561879; Training Loss 0.004652018355283417\n",
      "Episode 14458; Testing Loss 0.005793396046388326; Training Loss 0.004652008531729446\n",
      "Episode 14459; Testing Loss 0.005793333160880098; Training Loss 0.004651995418663185\n",
      "Episode 14460; Testing Loss 0.0057932950206977755; Training Loss 0.004651988433764009\n",
      "Episode 14461; Testing Loss 0.005793306489805138; Training Loss 0.004651980616449045\n",
      "Episode 14462; Testing Loss 0.005793274783061705; Training Loss 0.004651969940964197\n",
      "Episode 14463; Testing Loss 0.0057932606375911255; Training Loss 0.004651960701285948\n",
      "Episode 14464; Testing Loss 0.005793223968550649; Training Loss 0.004651951471417322\n",
      "Episode 14465; Testing Loss 0.005793243571102397; Training Loss 0.0046519431596154185\n",
      "Episode 14466; Testing Loss 0.00579323814271114; Training Loss 0.004651934078315316\n",
      "Episode 14467; Testing Loss 0.00579328446822833; Training Loss 0.004651925178961484\n",
      "Episode 14468; Testing Loss 0.005793319962726175; Training Loss 0.004651916829221344\n",
      "Episode 14469; Testing Loss 0.005793397116071392; Training Loss 0.004651907927505929\n",
      "Episode 14470; Testing Loss 0.0057934157728363675; Training Loss 0.0046518986387120076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14471; Testing Loss 0.0057933288483342955; Training Loss 0.004651889539915415\n",
      "Episode 14472; Testing Loss 0.005793178951530261; Training Loss 0.004651880997705102\n",
      "Episode 14473; Testing Loss 0.005793168253320277; Training Loss 0.004651873979593846\n",
      "Episode 14474; Testing Loss 0.005793289530954913; Training Loss 0.004651863352842974\n",
      "Episode 14475; Testing Loss 0.005793283007294671; Training Loss 0.004651858867561818\n",
      "Episode 14476; Testing Loss 0.005793070903016145; Training Loss 0.004651851038746246\n",
      "Episode 14477; Testing Loss 0.005793029407947042; Training Loss 0.00465184144578781\n",
      "Episode 14478; Testing Loss 0.005793268469336676; Training Loss 0.00465182711645021\n",
      "Episode 14479; Testing Loss 0.005793407904034861; Training Loss 0.004651826286864226\n",
      "Episode 14480; Testing Loss 0.005793394042323825; Training Loss 0.004651822027892737\n",
      "Episode 14481; Testing Loss 0.0057932641099362634; Training Loss 0.004651807579807807\n",
      "Episode 14482; Testing Loss 0.005793196502037121; Training Loss 0.004651793869391547\n",
      "Episode 14483; Testing Loss 0.005793211670547592; Training Loss 0.004651790985998907\n",
      "Episode 14484; Testing Loss 0.005793178111177122; Training Loss 0.004651784699901647\n",
      "Episode 14485; Testing Loss 0.005793121892322261; Training Loss 0.00465177401359712\n",
      "Episode 14486; Testing Loss 0.005793121611550503; Training Loss 0.004651760997152907\n",
      "Episode 14487; Testing Loss 0.005793136430845778; Training Loss 0.004651753189953101\n",
      "Episode 14488; Testing Loss 0.005793199439358627; Training Loss 0.0046517469030057145\n",
      "Episode 14489; Testing Loss 0.005793176314525993; Training Loss 0.004651735035483676\n",
      "Episode 14490; Testing Loss 0.005793154621184082; Training Loss 0.004651725419903935\n",
      "Episode 14491; Testing Loss 0.005793084693262625; Training Loss 0.004651719551351575\n",
      "Episode 14492; Testing Loss 0.005793075314982639; Training Loss 0.0046517093909814785\n",
      "Episode 14493; Testing Loss 0.005793141872968639; Training Loss 0.004651698743592793\n",
      "Episode 14494; Testing Loss 0.005793224649578501; Training Loss 0.004651690470936287\n",
      "Episode 14495; Testing Loss 0.005793130265605708; Training Loss 0.004651682717181304\n",
      "Episode 14496; Testing Loss 0.005793112863679106; Training Loss 0.004651672235095847\n",
      "Episode 14497; Testing Loss 0.005793146589629348; Training Loss 0.004651662685084665\n",
      "Episode 14498; Testing Loss 0.005793126535857739; Training Loss 0.004651653631076597\n",
      "Episode 14499; Testing Loss 0.0057931485253257566; Training Loss 0.00465164795784423\n",
      "Episode 14500; Testing Loss 0.005793085744995995; Training Loss 0.004651637602149654\n",
      "Episode 14501; Testing Loss 0.0057930428993190975; Training Loss 0.004651627022630852\n",
      "Episode 14502; Testing Loss 0.005793128010245302; Training Loss 0.00465161814464185\n",
      "Episode 14503; Testing Loss 0.005793175613499288; Training Loss 0.004651609553429394\n",
      "Episode 14504; Testing Loss 0.005793125436020453; Training Loss 0.004651600327449582\n",
      "Episode 14505; Testing Loss 0.0057930572104454734; Training Loss 0.004651592451330879\n",
      "Episode 14506; Testing Loss 0.005793141739294683; Training Loss 0.004651583128585763\n",
      "Episode 14507; Testing Loss 0.005793242733606645; Training Loss 0.004651574312073438\n",
      "Episode 14508; Testing Loss 0.005793206272502108; Training Loss 0.004651564066752891\n",
      "Episode 14509; Testing Loss 0.005793069230400939; Training Loss 0.004651555945320068\n",
      "Episode 14510; Testing Loss 0.005792963546532784; Training Loss 0.004651545773432606\n",
      "Episode 14511; Testing Loss 0.005793017311749346; Training Loss 0.004651537352763213\n",
      "Episode 14512; Testing Loss 0.005793078768393762; Training Loss 0.004651528788312538\n",
      "Episode 14513; Testing Loss 0.005793099646085951; Training Loss 0.004651519869989284\n",
      "Episode 14514; Testing Loss 0.005793091884658283; Training Loss 0.004651509970470241\n",
      "Episode 14515; Testing Loss 0.005793112619030532; Training Loss 0.0046514997757768515\n",
      "Episode 14516; Testing Loss 0.005793156197297874; Training Loss 0.004651490244135048\n",
      "Episode 14517; Testing Loss 0.0057932005517114015; Training Loss 0.004651484797832063\n",
      "Episode 14518; Testing Loss 0.005793056994740072; Training Loss 0.004651474101709126\n",
      "Episode 14519; Testing Loss 0.005793029450176997; Training Loss 0.0046514635135118925\n",
      "Episode 14520; Testing Loss 0.0057931289352928005; Training Loss 0.0046514550879374625\n",
      "Episode 14521; Testing Loss 0.005793138680827292; Training Loss 0.004651446673266561\n",
      "Episode 14522; Testing Loss 0.005793071165978527; Training Loss 0.004651436217578292\n",
      "Episode 14523; Testing Loss 0.005793021148880226; Training Loss 0.004651430128029845\n",
      "Episode 14524; Testing Loss 0.005793017680667514; Training Loss 0.004651423597835277\n",
      "Episode 14525; Testing Loss 0.005793067772429937; Training Loss 0.00465141125633243\n",
      "Episode 14526; Testing Loss 0.00579309823968841; Training Loss 0.0046514015126610025\n",
      "Episode 14527; Testing Loss 0.005793234836972969; Training Loss 0.004651393943665828\n",
      "Episode 14528; Testing Loss 0.005793253829595205; Training Loss 0.00465138246687666\n",
      "Episode 14529; Testing Loss 0.00579312022401319; Training Loss 0.0046513745078387535\n",
      "Episode 14530; Testing Loss 0.005793077949498677; Training Loss 0.004651365744987532\n",
      "Episode 14531; Testing Loss 0.005793215730875313; Training Loss 0.004651355069992341\n",
      "Episode 14532; Testing Loss 0.00579320804654969; Training Loss 0.004651345062886712\n",
      "Episode 14533; Testing Loss 0.005793010688516797; Training Loss 0.004651337001173924\n",
      "Episode 14534; Testing Loss 0.005792999190631058; Training Loss 0.004651329237570623\n",
      "Episode 14535; Testing Loss 0.005793177850205402; Training Loss 0.004651319464995519\n",
      "Episode 14536; Testing Loss 0.005793211465750661; Training Loss 0.004651309277694357\n",
      "Episode 14537; Testing Loss 0.005793123782860825; Training Loss 0.004651298245265719\n",
      "Episode 14538; Testing Loss 0.005793100306470532; Training Loss 0.004651288939885552\n",
      "Episode 14539; Testing Loss 0.005793169905141047; Training Loss 0.004651279133499083\n",
      "Episode 14540; Testing Loss 0.005793238278970577; Training Loss 0.004651269484234771\n",
      "Episode 14541; Testing Loss 0.005793272944394448; Training Loss 0.00465126045141563\n",
      "Episode 14542; Testing Loss 0.005793239199328419; Training Loss 0.004651249888416864\n",
      "Episode 14543; Testing Loss 0.00579317162149978; Training Loss 0.004651239538613872\n",
      "Episode 14544; Testing Loss 0.005793115366337263; Training Loss 0.004651230366422897\n",
      "Episode 14545; Testing Loss 0.005793049387337331; Training Loss 0.004651220914534496\n",
      "Episode 14546; Testing Loss 0.005793136189054188; Training Loss 0.004651210936704908\n",
      "Episode 14547; Testing Loss 0.005793192434818849; Training Loss 0.004651203051883731\n",
      "Episode 14548; Testing Loss 0.005793136608924439; Training Loss 0.004651191910634831\n",
      "Episode 14549; Testing Loss 0.005793109275863979; Training Loss 0.004651183214703957\n",
      "Episode 14550; Testing Loss 0.005793223903940443; Training Loss 0.0046511730195829114\n",
      "Episode 14551; Testing Loss 0.005793313873062474; Training Loss 0.0046511655192756395\n",
      "Episode 14552; Testing Loss 0.005793303053713313; Training Loss 0.004651155981312425\n",
      "Episode 14553; Testing Loss 0.0057931986352987306; Training Loss 0.004651144129056662\n",
      "Episode 14554; Testing Loss 0.005793227932320037; Training Loss 0.004651136052832679\n",
      "Episode 14555; Testing Loss 0.005793293805293779; Training Loss 0.004651128085108909\n",
      "Episode 14556; Testing Loss 0.005793162590068505; Training Loss 0.004651116673352445\n",
      "Episode 14557; Testing Loss 0.005793096177623904; Training Loss 0.004651109698077007\n",
      "Episode 14558; Testing Loss 0.005793243149345862; Training Loss 0.004651101251006723\n",
      "Episode 14559; Testing Loss 0.005793377585808775; Training Loss 0.00465109061705828\n",
      "Episode 14560; Testing Loss 0.0057932977010073505; Training Loss 0.004651078056737327\n",
      "Episode 14561; Testing Loss 0.005793163940623751; Training Loss 0.004651070840903645\n",
      "Episode 14562; Testing Loss 0.005793143288893958; Training Loss 0.00465106143239631\n",
      "Episode 14563; Testing Loss 0.005793320609884032; Training Loss 0.004651049560792199\n",
      "Episode 14564; Testing Loss 0.005793453710743405; Training Loss 0.004651040328215284\n",
      "Episode 14565; Testing Loss 0.005793431677224393; Training Loss 0.004651032127651475\n",
      "Episode 14566; Testing Loss 0.005793348537407945; Training Loss 0.0046510229335849146\n",
      "Episode 14567; Testing Loss 0.005793376013588647; Training Loss 0.004651010245239605\n",
      "Episode 14568; Testing Loss 0.005793362455844077; Training Loss 0.004651002314431015\n",
      "Episode 14569; Testing Loss 0.005793306135547512; Training Loss 0.004650996983397377\n",
      "Episode 14570; Testing Loss 0.005793340814686137; Training Loss 0.004650983682936809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14571; Testing Loss 0.00579335321879261; Training Loss 0.0046509756447761525\n",
      "Episode 14572; Testing Loss 0.0057932975374953306; Training Loss 0.004650966904271178\n",
      "Episode 14573; Testing Loss 0.005793321062298277; Training Loss 0.004650957505194021\n",
      "Episode 14574; Testing Loss 0.005793462901288781; Training Loss 0.004650945863547607\n",
      "Episode 14575; Testing Loss 0.005793486710291548; Training Loss 0.004650935052830435\n",
      "Episode 14576; Testing Loss 0.005793376470579226; Training Loss 0.004650925841804211\n",
      "Episode 14577; Testing Loss 0.005793359100209971; Training Loss 0.004650918744472281\n",
      "Episode 14578; Testing Loss 0.005793556996096857; Training Loss 0.004650908302060831\n",
      "Episode 14579; Testing Loss 0.005793580458348044; Training Loss 0.00465089887133701\n",
      "Episode 14580; Testing Loss 0.005793425504040067; Training Loss 0.004650888450516859\n",
      "Episode 14581; Testing Loss 0.00579338209847978; Training Loss 0.004650880499526427\n",
      "Episode 14582; Testing Loss 0.00579346337159943; Training Loss 0.004650870677482415\n",
      "Episode 14583; Testing Loss 0.005793518421581573; Training Loss 0.004650861284089888\n",
      "Episode 14584; Testing Loss 0.005793457177250598; Training Loss 0.004650851321124441\n",
      "Episode 14585; Testing Loss 0.005793468321900543; Training Loss 0.004650841624147175\n",
      "Episode 14586; Testing Loss 0.0057935147693978385; Training Loss 0.004650832772592242\n",
      "Episode 14587; Testing Loss 0.005793509826297645; Training Loss 0.004650823035780326\n",
      "Episode 14588; Testing Loss 0.005793473085147193; Training Loss 0.004650815419896343\n",
      "Episode 14589; Testing Loss 0.005793488661320469; Training Loss 0.00465080763408734\n",
      "Episode 14590; Testing Loss 0.0057935920491071205; Training Loss 0.004650795647065207\n",
      "Episode 14591; Testing Loss 0.005793611500730581; Training Loss 0.0046507896587138055\n",
      "Episode 14592; Testing Loss 0.005793560436476363; Training Loss 0.004650780882540537\n",
      "Episode 14593; Testing Loss 0.005793502260069692; Training Loss 0.004650769691384488\n",
      "Episode 14594; Testing Loss 0.005793480917322173; Training Loss 0.004650760480662222\n",
      "Episode 14595; Testing Loss 0.005793515138039215; Training Loss 0.0046507558389006\n",
      "Episode 14596; Testing Loss 0.005793604432036867; Training Loss 0.004650746575786329\n",
      "Episode 14597; Testing Loss 0.00579360983999161; Training Loss 0.004650735826386334\n",
      "Episode 14598; Testing Loss 0.005793443630176237; Training Loss 0.0046507274791069905\n",
      "Episode 14599; Testing Loss 0.005793398281654854; Training Loss 0.004650718634638503\n",
      "Episode 14600; Testing Loss 0.005793542169807855; Training Loss 0.004650705345852598\n",
      "Episode 14601; Testing Loss 0.005793699339901625; Training Loss 0.004650696912485143\n",
      "Episode 14602; Testing Loss 0.005793662937824216; Training Loss 0.004650690975894234\n",
      "Episode 14603; Testing Loss 0.005793601327453818; Training Loss 0.0046506812166104755\n",
      "Episode 14604; Testing Loss 0.005793642444593789; Training Loss 0.004650668601470719\n",
      "Episode 14605; Testing Loss 0.005793597008860368; Training Loss 0.004650659829913943\n",
      "Episode 14606; Testing Loss 0.005793519975037988; Training Loss 0.004650650053527924\n",
      "Episode 14607; Testing Loss 0.005793436774816289; Training Loss 0.004650641492100896\n",
      "Episode 14608; Testing Loss 0.005793503656126137; Training Loss 0.004650633846590975\n",
      "Episode 14609; Testing Loss 0.005793670564632808; Training Loss 0.004650624963177799\n",
      "Episode 14610; Testing Loss 0.005793699162555722; Training Loss 0.004650615839481496\n",
      "Episode 14611; Testing Loss 0.005793489708287334; Training Loss 0.0046506060476176935\n",
      "Episode 14612; Testing Loss 0.005793453338921494; Training Loss 0.004650595358393339\n",
      "Episode 14613; Testing Loss 0.005793653877049294; Training Loss 0.004650587314812179\n",
      "Episode 14614; Testing Loss 0.005793730336755727; Training Loss 0.004650579487521156\n",
      "Episode 14615; Testing Loss 0.005793615155636901; Training Loss 0.004650570230518321\n",
      "Episode 14616; Testing Loss 0.005793612195871608; Training Loss 0.004650560175585848\n",
      "Episode 14617; Testing Loss 0.005793713473236767; Training Loss 0.004650551395637592\n",
      "Episode 14618; Testing Loss 0.0057936367780335825; Training Loss 0.004650542266593618\n",
      "Episode 14619; Testing Loss 0.005793522933783491; Training Loss 0.0046505316715978835\n",
      "Episode 14620; Testing Loss 0.0057935577996531245; Training Loss 0.004650522159178599\n",
      "Episode 14621; Testing Loss 0.005793677115311809; Training Loss 0.004650516273684618\n",
      "Episode 14622; Testing Loss 0.005793643642998547; Training Loss 0.0046505062591616604\n",
      "Episode 14623; Testing Loss 0.00579355193821727; Training Loss 0.0046504957909874875\n",
      "Episode 14624; Testing Loss 0.0057936039649273035; Training Loss 0.004650485172690427\n",
      "Episode 14625; Testing Loss 0.005793651965346211; Training Loss 0.004650478543382734\n",
      "Episode 14626; Testing Loss 0.005793603405395035; Training Loss 0.004650470205266422\n",
      "Episode 14627; Testing Loss 0.005793566025989707; Training Loss 0.004650459470062906\n",
      "Episode 14628; Testing Loss 0.005793639880662154; Training Loss 0.004650449962162472\n",
      "Episode 14629; Testing Loss 0.00579377659969227; Training Loss 0.00465044220653551\n",
      "Episode 14630; Testing Loss 0.005793740236218465; Training Loss 0.004650433722938569\n",
      "Episode 14631; Testing Loss 0.005793619848132984; Training Loss 0.004650421996203914\n",
      "Episode 14632; Testing Loss 0.0057935394691391645; Training Loss 0.004650412694192736\n",
      "Episode 14633; Testing Loss 0.005793577966945894; Training Loss 0.0046504048675873495\n",
      "Episode 14634; Testing Loss 0.005793578110164076; Training Loss 0.0046503963584644216\n",
      "Episode 14635; Testing Loss 0.005793508654180156; Training Loss 0.004650386358401833\n",
      "Episode 14636; Testing Loss 0.005793544151979056; Training Loss 0.0046503781187880915\n",
      "Episode 14637; Testing Loss 0.005793718958907975; Training Loss 0.0046503699301957725\n",
      "Episode 14638; Testing Loss 0.005793789188238622; Training Loss 0.004650359215210748\n",
      "Episode 14639; Testing Loss 0.005793663897483654; Training Loss 0.004650348605537925\n",
      "Episode 14640; Testing Loss 0.005793535203750626; Training Loss 0.004650339410191263\n",
      "Episode 14641; Testing Loss 0.005793605810392055; Training Loss 0.004650331610473082\n",
      "Episode 14642; Testing Loss 0.005793627106510052; Training Loss 0.00465032193218243\n",
      "Episode 14643; Testing Loss 0.005793602152325715; Training Loss 0.0046503130264198165\n",
      "Episode 14644; Testing Loss 0.0057936064519084046; Training Loss 0.004650305965797298\n",
      "Episode 14645; Testing Loss 0.005793634389756482; Training Loss 0.004650295706836137\n",
      "Episode 14646; Testing Loss 0.005793660811103077; Training Loss 0.00465028493117237\n",
      "Episode 14647; Testing Loss 0.005793584557201193; Training Loss 0.004650279204085124\n",
      "Episode 14648; Testing Loss 0.005793484279934156; Training Loss 0.004650273359746671\n",
      "Episode 14649; Testing Loss 0.0057935936019297175; Training Loss 0.004650260295018422\n",
      "Episode 14650; Testing Loss 0.005793735840286606; Training Loss 0.00465025053851577\n",
      "Episode 14651; Testing Loss 0.00579366574114852; Training Loss 0.00465024082068102\n",
      "Episode 14652; Testing Loss 0.0057935213591531224; Training Loss 0.004650235613917982\n",
      "Episode 14653; Testing Loss 0.005793562120972877; Training Loss 0.004650224753994149\n",
      "Episode 14654; Testing Loss 0.0057936710756780795; Training Loss 0.0046502144407357116\n",
      "Episode 14655; Testing Loss 0.005793617416744841; Training Loss 0.004650208016143713\n",
      "Episode 14656; Testing Loss 0.005793490664975997; Training Loss 0.004650201506739344\n",
      "Episode 14657; Testing Loss 0.005793552298881939; Training Loss 0.0046501881114195225\n",
      "Episode 14658; Testing Loss 0.005793683102296907; Training Loss 0.004650179370441178\n",
      "Episode 14659; Testing Loss 0.005793678492452855; Training Loss 0.004650171171214133\n",
      "Episode 14660; Testing Loss 0.00579353807281253; Training Loss 0.004650162133300856\n",
      "Episode 14661; Testing Loss 0.005793582655052692; Training Loss 0.004650152718145403\n",
      "Episode 14662; Testing Loss 0.0057937069293786045; Training Loss 0.004650142581737322\n",
      "Episode 14663; Testing Loss 0.005793717210410898; Training Loss 0.004650134376471975\n",
      "Episode 14664; Testing Loss 0.0057935764842987365; Training Loss 0.004650124141106052\n",
      "Episode 14665; Testing Loss 0.005793538862211999; Training Loss 0.00465011575013099\n",
      "Episode 14666; Testing Loss 0.005793636853768505; Training Loss 0.004650109071451366\n",
      "Episode 14667; Testing Loss 0.005793592490474937; Training Loss 0.0046500980898184704\n",
      "Episode 14668; Testing Loss 0.005793491950509209; Training Loss 0.004650088484797917\n",
      "Episode 14669; Testing Loss 0.0057935492556838285; Training Loss 0.004650080360056924\n",
      "Episode 14670; Testing Loss 0.005793626275612189; Training Loss 0.004650072142992694\n",
      "Episode 14671; Testing Loss 0.005793592318433265; Training Loss 0.004650062210485573\n",
      "Episode 14672; Testing Loss 0.0057935401058266355; Training Loss 0.004650054194857088\n",
      "Episode 14673; Testing Loss 0.0057935850553608595; Training Loss 0.0046500445210134736\n",
      "Episode 14674; Testing Loss 0.005793640751898888; Training Loss 0.004650035060836924\n",
      "Episode 14675; Testing Loss 0.005793624475253433; Training Loss 0.004650024846493919\n",
      "Episode 14676; Testing Loss 0.005793614084172596; Training Loss 0.004650015540491278\n",
      "Episode 14677; Testing Loss 0.005793599065213231; Training Loss 0.004650007369307418\n",
      "Episode 14678; Testing Loss 0.005793580621161467; Training Loss 0.0046499985251051736\n",
      "Episode 14679; Testing Loss 0.005793548861395247; Training Loss 0.0046499887888126834\n",
      "Episode 14680; Testing Loss 0.005793517887106209; Training Loss 0.0046499813741278295\n",
      "Episode 14681; Testing Loss 0.005793471954361854; Training Loss 0.004649970044354489\n",
      "Episode 14682; Testing Loss 0.005793581713258101; Training Loss 0.004649962466107265\n",
      "Episode 14683; Testing Loss 0.005793709421933542; Training Loss 0.004649954859092262\n",
      "Episode 14684; Testing Loss 0.005793673090172626; Training Loss 0.004649944766764907\n",
      "Episode 14685; Testing Loss 0.005793540233036534; Training Loss 0.004649934597192845\n",
      "Episode 14686; Testing Loss 0.005793538962155385; Training Loss 0.004649926231725028\n",
      "Episode 14687; Testing Loss 0.005793599964430194; Training Loss 0.004649916637134095\n",
      "Episode 14688; Testing Loss 0.0057936149480767515; Training Loss 0.0046499077545439265\n",
      "Episode 14689; Testing Loss 0.005793580902310619; Training Loss 0.0046498992257782415\n",
      "Episode 14690; Testing Loss 0.005793620105040425; Training Loss 0.0046498897255066104\n",
      "Episode 14691; Testing Loss 0.005793695815912225; Training Loss 0.0046498800084250405\n",
      "Episode 14692; Testing Loss 0.005793738936633988; Training Loss 0.004649873828310941\n",
      "Episode 14693; Testing Loss 0.005793593575859455; Training Loss 0.004649863797731853\n",
      "Episode 14694; Testing Loss 0.005793486722340472; Training Loss 0.004649853072142219\n",
      "Episode 14695; Testing Loss 0.005793546168214184; Training Loss 0.004649846576481199\n",
      "Episode 14696; Testing Loss 0.005793630561787591; Training Loss 0.004649838461048732\n",
      "Episode 14697; Testing Loss 0.005793634595839597; Training Loss 0.004649830517725687\n",
      "Episode 14698; Testing Loss 0.005793658650215337; Training Loss 0.00464981992464243\n",
      "Episode 14699; Testing Loss 0.005793667958873107; Training Loss 0.0046498104336576114\n",
      "Episode 14700; Testing Loss 0.005793540225562506; Training Loss 0.004649805380184096\n",
      "Episode 14701; Testing Loss 0.005793429694869724; Training Loss 0.004649795403537689\n",
      "Episode 14702; Testing Loss 0.005793484521039668; Training Loss 0.004649781505497597\n",
      "Episode 14703; Testing Loss 0.005793651109846314; Training Loss 0.004649775465558128\n",
      "Episode 14704; Testing Loss 0.0057936740963320564; Training Loss 0.004649765647609842\n",
      "Episode 14705; Testing Loss 0.005793520318999812; Training Loss 0.00464975512426969\n",
      "Episode 14706; Testing Loss 0.00579346388257717; Training Loss 0.0046497466334963575\n",
      "Episode 14707; Testing Loss 0.005793523286208917; Training Loss 0.004649736933781775\n",
      "Episode 14708; Testing Loss 0.00579355678759465; Training Loss 0.0046497300184744055\n",
      "Episode 14709; Testing Loss 0.0057935844443242285; Training Loss 0.00464972083392714\n",
      "Episode 14710; Testing Loss 0.005793564881103096; Training Loss 0.004649710981494978\n",
      "Episode 14711; Testing Loss 0.005793453646860324; Training Loss 0.004649703317929871\n",
      "Episode 14712; Testing Loss 0.005793437574227155; Training Loss 0.004649693783366958\n",
      "Episode 14713; Testing Loss 0.005793608558297288; Training Loss 0.00464968310715466\n",
      "Episode 14714; Testing Loss 0.0057937423799429445; Training Loss 0.0046496756911814634\n",
      "Episode 14715; Testing Loss 0.005793649370097318; Training Loss 0.004649666122463066\n",
      "Episode 14716; Testing Loss 0.0057935115777688695; Training Loss 0.004649657528328539\n",
      "Episode 14717; Testing Loss 0.00579351642080073; Training Loss 0.004649647462585269\n",
      "Episode 14718; Testing Loss 0.005793570030333339; Training Loss 0.004649639327079754\n",
      "Episode 14719; Testing Loss 0.005793575879029831; Training Loss 0.004649631165005857\n",
      "Episode 14720; Testing Loss 0.005793515875134934; Training Loss 0.004649622032037751\n",
      "Episode 14721; Testing Loss 0.0057934911821840756; Training Loss 0.004649610967871786\n",
      "Episode 14722; Testing Loss 0.005793518125573602; Training Loss 0.004649603642950133\n",
      "Episode 14723; Testing Loss 0.0057935839598999275; Training Loss 0.004649596925436268\n",
      "Episode 14724; Testing Loss 0.005793636193141865; Training Loss 0.004649586857151088\n",
      "Episode 14725; Testing Loss 0.005793592913676636; Training Loss 0.004649576701762824\n",
      "Episode 14726; Testing Loss 0.005793536161185721; Training Loss 0.004649569179318227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14727; Testing Loss 0.005793517099015166; Training Loss 0.004649559147601414\n",
      "Episode 14728; Testing Loss 0.005793509643392577; Training Loss 0.004649549382358435\n",
      "Episode 14729; Testing Loss 0.005793477801499704; Training Loss 0.004649541415682936\n",
      "Episode 14730; Testing Loss 0.005793448567597124; Training Loss 0.00464953093140889\n",
      "Episode 14731; Testing Loss 0.0057935862953497124; Training Loss 0.0046495211195287855\n",
      "Episode 14732; Testing Loss 0.005793730549673258; Training Loss 0.0046495130702697285\n",
      "Episode 14733; Testing Loss 0.005793665580230849; Training Loss 0.004649504567806146\n",
      "Episode 14734; Testing Loss 0.005793477703217751; Training Loss 0.004649495352247443\n",
      "Episode 14735; Testing Loss 0.005793505304876743; Training Loss 0.004649485090821351\n",
      "Episode 14736; Testing Loss 0.005793578982798515; Training Loss 0.00464947670705629\n",
      "Episode 14737; Testing Loss 0.005793515001958305; Training Loss 0.004649468681387657\n",
      "Episode 14738; Testing Loss 0.0057934799116561245; Training Loss 0.0046494591432962125\n",
      "Episode 14739; Testing Loss 0.005793558661108287; Training Loss 0.0046494509497659944\n",
      "Episode 14740; Testing Loss 0.005793580890863071; Training Loss 0.0046494395013832335\n",
      "Episode 14741; Testing Loss 0.0057936092288467; Training Loss 0.004649433238528787\n",
      "Episode 14742; Testing Loss 0.005793640642786692; Training Loss 0.0046494250395266445\n",
      "Episode 14743; Testing Loss 0.005793514844818984; Training Loss 0.004649412642699137\n",
      "Episode 14744; Testing Loss 0.005793498282608805; Training Loss 0.004649404611636543\n",
      "Episode 14745; Testing Loss 0.005793614092916508; Training Loss 0.004649395149822757\n",
      "Episode 14746; Testing Loss 0.005793629578006966; Training Loss 0.004649386477013624\n",
      "Episode 14747; Testing Loss 0.0057935297759772585; Training Loss 0.004649378327473857\n",
      "Episode 14748; Testing Loss 0.005793550811917708; Training Loss 0.004649369877810652\n",
      "Episode 14749; Testing Loss 0.005793664922067026; Training Loss 0.004649360799326489\n",
      "Episode 14750; Testing Loss 0.005793563917630121; Training Loss 0.004649351178988423\n",
      "Episode 14751; Testing Loss 0.005793461071052935; Training Loss 0.004649343162352642\n",
      "Episode 14752; Testing Loss 0.005793521362248672; Training Loss 0.004649335421392108\n",
      "Episode 14753; Testing Loss 0.005793527637537817; Training Loss 0.004649325554740141\n",
      "Episode 14754; Testing Loss 0.005793541086975693; Training Loss 0.004649315781536773\n",
      "Episode 14755; Testing Loss 0.005793597094266656; Training Loss 0.004649307238219695\n",
      "Episode 14756; Testing Loss 0.005793623969168779; Training Loss 0.004649298340848257\n",
      "Episode 14757; Testing Loss 0.00579348375987065; Training Loss 0.004649289273880778\n",
      "Episode 14758; Testing Loss 0.005793383179884399; Training Loss 0.004649282764434249\n",
      "Episode 14759; Testing Loss 0.005793477142255116; Training Loss 0.004649271789259889\n",
      "Episode 14760; Testing Loss 0.00579352829466577; Training Loss 0.004649262549473815\n",
      "Episode 14761; Testing Loss 0.005793448334401716; Training Loss 0.004649253854798418\n",
      "Episode 14762; Testing Loss 0.005793435592445162; Training Loss 0.004649244470778717\n",
      "Episode 14763; Testing Loss 0.005793499160621658; Training Loss 0.004649234953762374\n",
      "Episode 14764; Testing Loss 0.005793537869557658; Training Loss 0.0046492261589811176\n",
      "Episode 14765; Testing Loss 0.005793528080999095; Training Loss 0.004649218722902923\n",
      "Episode 14766; Testing Loss 0.005793464712719322; Training Loss 0.004649208480445744\n",
      "Episode 14767; Testing Loss 0.005793475344212148; Training Loss 0.004649200305335805\n",
      "Episode 14768; Testing Loss 0.005793478605136718; Training Loss 0.004649192683901676\n",
      "Episode 14769; Testing Loss 0.0057934952347776; Training Loss 0.004649185096034065\n",
      "Episode 14770; Testing Loss 0.005793483991143687; Training Loss 0.0046491743541596225\n",
      "Episode 14771; Testing Loss 0.005793436558212714; Training Loss 0.004649165674160351\n",
      "Episode 14772; Testing Loss 0.005793378419383798; Training Loss 0.004649155737201684\n",
      "Episode 14773; Testing Loss 0.005793451482509753; Training Loss 0.004649147251434538\n",
      "Episode 14774; Testing Loss 0.005793626873811604; Training Loss 0.004649138873898739\n",
      "Episode 14775; Testing Loss 0.005793634275789138; Training Loss 0.004649130125073486\n",
      "Episode 14776; Testing Loss 0.005793368009808566; Training Loss 0.004649120225182357\n",
      "Episode 14777; Testing Loss 0.005793261176817996; Training Loss 0.004649113043007756\n",
      "Episode 14778; Testing Loss 0.005793491538029544; Training Loss 0.004649103674537729\n",
      "Episode 14779; Testing Loss 0.005793665317977922; Training Loss 0.0046490977409846725\n",
      "Episode 14780; Testing Loss 0.005793438231991299; Training Loss 0.004649085252530534\n",
      "Episode 14781; Testing Loss 0.005793231016927552; Training Loss 0.004649077830373685\n",
      "Episode 14782; Testing Loss 0.005793396617795779; Training Loss 0.004649068517126755\n",
      "Episode 14783; Testing Loss 0.005793555013956962; Training Loss 0.004649059734933974\n",
      "Episode 14784; Testing Loss 0.005793476414566692; Training Loss 0.004649048749695004\n",
      "Episode 14785; Testing Loss 0.005793412511934948; Training Loss 0.004649043126938144\n",
      "Episode 14786; Testing Loss 0.0057934891965229504; Training Loss 0.004649031836983499\n",
      "Episode 14787; Testing Loss 0.0057935643405752375; Training Loss 0.004649022495874994\n",
      "Episode 14788; Testing Loss 0.005793429046787311; Training Loss 0.00464901558156414\n",
      "Episode 14789; Testing Loss 0.005793332321984939; Training Loss 0.004649007094501199\n",
      "Episode 14790; Testing Loss 0.005793378236899182; Training Loss 0.004648995159975511\n",
      "Episode 14791; Testing Loss 0.005793452392094511; Training Loss 0.004648988110938327\n",
      "Episode 14792; Testing Loss 0.00579345488500416; Training Loss 0.004648980695318188\n",
      "Episode 14793; Testing Loss 0.005793457184809571; Training Loss 0.004648971435039078\n",
      "Episode 14794; Testing Loss 0.005793465291502324; Training Loss 0.004648960403645955\n",
      "Episode 14795; Testing Loss 0.0057934480675529785; Training Loss 0.004648952353834837\n",
      "Episode 14796; Testing Loss 0.005793505778649679; Training Loss 0.004648945986017357\n",
      "Episode 14797; Testing Loss 0.005793477498782781; Training Loss 0.004648936271440704\n",
      "Episode 14798; Testing Loss 0.005793364410450075; Training Loss 0.004648924929622984\n",
      "Episode 14799; Testing Loss 0.005793361011417762; Training Loss 0.004648917721966634\n",
      "Episode 14800; Testing Loss 0.005793400404796214; Training Loss 0.004648909213278774\n",
      "Episode 14801; Testing Loss 0.005793396706836702; Training Loss 0.004648898796045591\n",
      "Episode 14802; Testing Loss 0.005793398362138915; Training Loss 0.004648891903657134\n",
      "Episode 14803; Testing Loss 0.005793432975417935; Training Loss 0.004648885461364047\n",
      "Episode 14804; Testing Loss 0.00579344756938296; Training Loss 0.004648873919922153\n",
      "Episode 14805; Testing Loss 0.005793412613576866; Training Loss 0.004648862052784557\n",
      "Episode 14806; Testing Loss 0.005793351709301011; Training Loss 0.004648855005209686\n",
      "Episode 14807; Testing Loss 0.005793383051016357; Training Loss 0.004648845354267969\n",
      "Episode 14808; Testing Loss 0.005793427721925027; Training Loss 0.004648834934189873\n",
      "Episode 14809; Testing Loss 0.005793432634105483; Training Loss 0.004648826464717491\n",
      "Episode 14810; Testing Loss 0.005793361434582646; Training Loss 0.004648816856882852\n",
      "Episode 14811; Testing Loss 0.005793233676358714; Training Loss 0.004648808072225452\n",
      "Episode 14812; Testing Loss 0.005793275151325021; Training Loss 0.004648800804783253\n",
      "Episode 14813; Testing Loss 0.005793359062781609; Training Loss 0.004648791420947391\n",
      "Episode 14814; Testing Loss 0.005793379060087299; Training Loss 0.004648780982821675\n",
      "Episode 14815; Testing Loss 0.00579336811552123; Training Loss 0.004648772437167566\n",
      "Episode 14816; Testing Loss 0.00579333685013686; Training Loss 0.004648762961011528\n",
      "Episode 14817; Testing Loss 0.00579334471616209; Training Loss 0.0046487543568547365\n",
      "Episode 14818; Testing Loss 0.005793316457223755; Training Loss 0.0046487445609905455\n",
      "Episode 14819; Testing Loss 0.005793286900428149; Training Loss 0.004648736685069811\n",
      "Episode 14820; Testing Loss 0.005793277895669142; Training Loss 0.004648728137909943\n",
      "Episode 14821; Testing Loss 0.005793379021841992; Training Loss 0.004648719895689656\n",
      "Episode 14822; Testing Loss 0.005793437315983824; Training Loss 0.004648709735819271\n",
      "Episode 14823; Testing Loss 0.005793351833979054; Training Loss 0.004648701990616398\n",
      "Episode 14824; Testing Loss 0.005793203988751524; Training Loss 0.004648692107827452\n",
      "Episode 14825; Testing Loss 0.005793278939823935; Training Loss 0.004648683981517494\n",
      "Episode 14826; Testing Loss 0.005793525628277562; Training Loss 0.004648674998129147\n",
      "Episode 14827; Testing Loss 0.005793438860011804; Training Loss 0.0046486664632146586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14828; Testing Loss 0.005793326780067472; Training Loss 0.004648656121291934\n",
      "Episode 14829; Testing Loss 0.005793300284092055; Training Loss 0.00464864664771671\n",
      "Episode 14830; Testing Loss 0.005793314998148462; Training Loss 0.004648637843830952\n",
      "Episode 14831; Testing Loss 0.0057933328967126885; Training Loss 0.004648632505293222\n",
      "Episode 14832; Testing Loss 0.0057932998932064975; Training Loss 0.004648621598150929\n",
      "Episode 14833; Testing Loss 0.005793230693961221; Training Loss 0.0046486137329399555\n",
      "Episode 14834; Testing Loss 0.005793288463771969; Training Loss 0.004648605409044982\n",
      "Episode 14835; Testing Loss 0.005793305091803382; Training Loss 0.004648595196186809\n",
      "Episode 14836; Testing Loss 0.005793322980812546; Training Loss 0.004648585727158494\n",
      "Episode 14837; Testing Loss 0.005793368228192918; Training Loss 0.004648577390628618\n",
      "Episode 14838; Testing Loss 0.0057933226724050865; Training Loss 0.004648569524149197\n",
      "Episode 14839; Testing Loss 0.005793330474892451; Training Loss 0.004648559530233537\n",
      "Episode 14840; Testing Loss 0.005793362930740199; Training Loss 0.004648550258696048\n",
      "Episode 14841; Testing Loss 0.005793396741213425; Training Loss 0.004648541232370775\n",
      "Episode 14842; Testing Loss 0.005793366617978323; Training Loss 0.004648532722537829\n",
      "Episode 14843; Testing Loss 0.005793311832597805; Training Loss 0.0046485243026282335\n",
      "Episode 14844; Testing Loss 0.005793232821230158; Training Loss 0.004648515450860213\n",
      "Episode 14845; Testing Loss 0.005793265958224188; Training Loss 0.0046485061191859954\n",
      "Episode 14846; Testing Loss 0.005793291816302233; Training Loss 0.004648496523093964\n",
      "Episode 14847; Testing Loss 0.005793341164870321; Training Loss 0.004648487594000784\n",
      "Episode 14848; Testing Loss 0.005793344937527961; Training Loss 0.004648479503292676\n",
      "Episode 14849; Testing Loss 0.005793273514511877; Training Loss 0.004648469502421534\n",
      "Episode 14850; Testing Loss 0.005793278841563237; Training Loss 0.00464846028838179\n",
      "Episode 14851; Testing Loss 0.005793316170117803; Training Loss 0.004648452875784317\n",
      "Episode 14852; Testing Loss 0.005793238889786283; Training Loss 0.004648444328073504\n",
      "Episode 14853; Testing Loss 0.005793182115976872; Training Loss 0.004648436389472178\n",
      "Episode 14854; Testing Loss 0.005793316483764276; Training Loss 0.004648427635863376\n",
      "Episode 14855; Testing Loss 0.0057933787950744465; Training Loss 0.004648418633554156\n",
      "Episode 14856; Testing Loss 0.005793279669443635; Training Loss 0.004648409440530905\n",
      "Episode 14857; Testing Loss 0.0057932320340717686; Training Loss 0.004648398674815712\n",
      "Episode 14858; Testing Loss 0.005793199574522831; Training Loss 0.004648390016511454\n",
      "Episode 14859; Testing Loss 0.005793207034897321; Training Loss 0.00464838271028174\n",
      "Episode 14860; Testing Loss 0.005793272525788906; Training Loss 0.004648375550557325\n",
      "Episode 14861; Testing Loss 0.00579322938193197; Training Loss 0.004648364222342164\n",
      "Episode 14862; Testing Loss 0.005793224259833648; Training Loss 0.004648354196197952\n",
      "Episode 14863; Testing Loss 0.0057933083279231926; Training Loss 0.004648348961043795\n",
      "Episode 14864; Testing Loss 0.005793316102389507; Training Loss 0.004648339771743526\n",
      "Episode 14865; Testing Loss 0.005793189545591885; Training Loss 0.0046483282527721125\n",
      "Episode 14866; Testing Loss 0.005793133631303056; Training Loss 0.004648320785976257\n",
      "Episode 14867; Testing Loss 0.005793205107507209; Training Loss 0.004648313368918743\n",
      "Episode 14868; Testing Loss 0.005793238408148914; Training Loss 0.0046483049722606\n",
      "Episode 14869; Testing Loss 0.005793177313234419; Training Loss 0.004648296360066857\n",
      "Episode 14870; Testing Loss 0.005793192318206474; Training Loss 0.004648285948705503\n",
      "Episode 14871; Testing Loss 0.00579321780973039; Training Loss 0.004648275133369723\n",
      "Episode 14872; Testing Loss 0.005793178953466342; Training Loss 0.004648268893788057\n",
      "Episode 14873; Testing Loss 0.005793144849998285; Training Loss 0.004648261367362506\n",
      "Episode 14874; Testing Loss 0.005793179920558925; Training Loss 0.0046482505801161336\n",
      "Episode 14875; Testing Loss 0.005793269306026021; Training Loss 0.004648240502244165\n",
      "Episode 14876; Testing Loss 0.005793273732920355; Training Loss 0.0046482331809054875\n",
      "Episode 14877; Testing Loss 0.005793115599077399; Training Loss 0.004648224712450168\n",
      "Episode 14878; Testing Loss 0.0057929730216795625; Training Loss 0.004648216322277261\n",
      "Episode 14879; Testing Loss 0.0057931169321272504; Training Loss 0.004648205122013277\n",
      "Episode 14880; Testing Loss 0.005793249083083527; Training Loss 0.0046481998060882174\n",
      "Episode 14881; Testing Loss 0.0057931104452538205; Training Loss 0.004648189179166202\n",
      "Episode 14882; Testing Loss 0.005792993923063699; Training Loss 0.004648178958487662\n",
      "Episode 14883; Testing Loss 0.005793108829697186; Training Loss 0.0046481700074346745\n",
      "Episode 14884; Testing Loss 0.005793274557368488; Training Loss 0.004648161451986435\n",
      "Episode 14885; Testing Loss 0.005793244577007552; Training Loss 0.004648152494705597\n",
      "Episode 14886; Testing Loss 0.005793088490550438; Training Loss 0.004648142671473717\n",
      "Episode 14887; Testing Loss 0.005793068654183214; Training Loss 0.0046481354905433215\n",
      "Episode 14888; Testing Loss 0.005793176791262882; Training Loss 0.0046481275133651145\n",
      "Episode 14889; Testing Loss 0.005793211892899891; Training Loss 0.004648119719296914\n",
      "Episode 14890; Testing Loss 0.005793022081278466; Training Loss 0.0046481092455347515\n",
      "Episode 14891; Testing Loss 0.005792985331635865; Training Loss 0.004648099982027232\n",
      "Episode 14892; Testing Loss 0.005793112837926274; Training Loss 0.004648093211541337\n",
      "Episode 14893; Testing Loss 0.00579326915152963; Training Loss 0.004648085663387376\n",
      "Episode 14894; Testing Loss 0.005793227907736783; Training Loss 0.00464807700495844\n",
      "Episode 14895; Testing Loss 0.00579311086241197; Training Loss 0.004648065804187397\n",
      "Episode 14896; Testing Loss 0.005793135252331238; Training Loss 0.004648056453460336\n",
      "Episode 14897; Testing Loss 0.005793183890064376; Training Loss 0.004648052285860914\n",
      "Episode 14898; Testing Loss 0.005793074097325814; Training Loss 0.0046480405873159224\n",
      "Episode 14899; Testing Loss 0.005792985969426864; Training Loss 0.004648030178227303\n",
      "Episode 14900; Testing Loss 0.005793120621475525; Training Loss 0.0046480228917478465\n",
      "Episode 14901; Testing Loss 0.005793226511585285; Training Loss 0.0046480151886452055\n",
      "Episode 14902; Testing Loss 0.005793051211697027; Training Loss 0.00464800180650372\n",
      "Episode 14903; Testing Loss 0.005792907515402846; Training Loss 0.004647995490618483\n",
      "Episode 14904; Testing Loss 0.0057930587412623035; Training Loss 0.0046479851046530496\n",
      "Episode 14905; Testing Loss 0.005793192559340311; Training Loss 0.0046479768795977105\n",
      "Episode 14906; Testing Loss 0.0057931510844031675; Training Loss 0.004647966738359818\n",
      "Episode 14907; Testing Loss 0.005793044060808774; Training Loss 0.004647959549878071\n",
      "Episode 14908; Testing Loss 0.00579306816501832; Training Loss 0.004647951284148764\n",
      "Episode 14909; Testing Loss 0.005793141842468224; Training Loss 0.004647942466842018\n",
      "Episode 14910; Testing Loss 0.005793122047522598; Training Loss 0.004647933030615966\n",
      "Episode 14911; Testing Loss 0.005793070561050731; Training Loss 0.004647922942034092\n",
      "Episode 14912; Testing Loss 0.005792998165290145; Training Loss 0.004647914869513377\n",
      "Episode 14913; Testing Loss 0.005793040878302761; Training Loss 0.004647906042691765\n",
      "Episode 14914; Testing Loss 0.005793123275669138; Training Loss 0.00464789791572282\n",
      "Episode 14915; Testing Loss 0.005793032719499664; Training Loss 0.00464788844002757\n",
      "Episode 14916; Testing Loss 0.005792908675111042; Training Loss 0.0046478785224288105\n",
      "Episode 14917; Testing Loss 0.005792931632715774; Training Loss 0.004647872622264944\n",
      "Episode 14918; Testing Loss 0.005792990706162126; Training Loss 0.004647864470395057\n",
      "Episode 14919; Testing Loss 0.0057929870358725275; Training Loss 0.004647852807086317\n",
      "Episode 14920; Testing Loss 0.005792947248125899; Training Loss 0.004647845364450037\n",
      "Episode 14921; Testing Loss 0.005792962486133321; Training Loss 0.0046478384853507\n",
      "Episode 14922; Testing Loss 0.005792984630514726; Training Loss 0.004647830238752334\n",
      "Episode 14923; Testing Loss 0.005793017265436012; Training Loss 0.0046478203875711275\n",
      "Episode 14924; Testing Loss 0.005793017800658315; Training Loss 0.004647809806203366\n",
      "Episode 14925; Testing Loss 0.005793022232062321; Training Loss 0.004647799832310935\n",
      "Episode 14926; Testing Loss 0.005793052595738682; Training Loss 0.004647793549553996\n",
      "Episode 14927; Testing Loss 0.005793037350539315; Training Loss 0.004647784944647371\n",
      "Episode 14928; Testing Loss 0.005792939349595871; Training Loss 0.004647774238562543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14929; Testing Loss 0.0057928729140869885; Training Loss 0.0046477657472697425\n",
      "Episode 14930; Testing Loss 0.005792922783157533; Training Loss 0.004647758383026648\n",
      "Episode 14931; Testing Loss 0.005792977056181303; Training Loss 0.004647750087318586\n",
      "Episode 14932; Testing Loss 0.005792930293440917; Training Loss 0.004647740011442647\n",
      "Episode 14933; Testing Loss 0.005792894580941333; Training Loss 0.004647729303707805\n",
      "Episode 14934; Testing Loss 0.0057929210980432475; Training Loss 0.004647721043657747\n",
      "Episode 14935; Testing Loss 0.00579297792402317; Training Loss 0.004647713081977596\n",
      "Episode 14936; Testing Loss 0.005793017396550654; Training Loss 0.0046477023982429665\n",
      "Episode 14937; Testing Loss 0.005792992874071723; Training Loss 0.004647694106667097\n",
      "Episode 14938; Testing Loss 0.005792944419663209; Training Loss 0.004647687836209742\n",
      "Episode 14939; Testing Loss 0.005793000247120454; Training Loss 0.004647678566795579\n",
      "Episode 14940; Testing Loss 0.005792996566079097; Training Loss 0.004647669895317541\n",
      "Episode 14941; Testing Loss 0.005792869120230713; Training Loss 0.0046476594633117615\n",
      "Episode 14942; Testing Loss 0.005792838303430942; Training Loss 0.004647649491628485\n",
      "Episode 14943; Testing Loss 0.0057928786269977875; Training Loss 0.004647645607199309\n",
      "Episode 14944; Testing Loss 0.005792900788242745; Training Loss 0.004647636316934416\n",
      "Episode 14945; Testing Loss 0.0057928433871771; Training Loss 0.004647625691917515\n",
      "Episode 14946; Testing Loss 0.005792838539135808; Training Loss 0.004647614864316397\n",
      "Episode 14947; Testing Loss 0.005792945691204984; Training Loss 0.004647607062411563\n",
      "Episode 14948; Testing Loss 0.0057929878391231706; Training Loss 0.004647599019343656\n",
      "Episode 14949; Testing Loss 0.005792882446158009; Training Loss 0.00464759002387689\n",
      "Episode 14950; Testing Loss 0.005792899379206427; Training Loss 0.004647579238132636\n",
      "Episode 14951; Testing Loss 0.005792928872595889; Training Loss 0.004647572366101256\n",
      "Episode 14952; Testing Loss 0.005792786239950467; Training Loss 0.004647563274935481\n",
      "Episode 14953; Testing Loss 0.0057926471734695145; Training Loss 0.0046475551610029526\n",
      "Episode 14954; Testing Loss 0.0057928063916208306; Training Loss 0.004647544919996134\n",
      "Episode 14955; Testing Loss 0.005792975102534243; Training Loss 0.004647536975901077\n",
      "Episode 14956; Testing Loss 0.005792874038715237; Training Loss 0.0046475265663549065\n",
      "Episode 14957; Testing Loss 0.005792701081551012; Training Loss 0.00464751927021102\n",
      "Episode 14958; Testing Loss 0.005792800440780754; Training Loss 0.0046475085916163955\n",
      "Episode 14959; Testing Loss 0.005792945238979305; Training Loss 0.004647500305129294\n",
      "Episode 14960; Testing Loss 0.005792914678193611; Training Loss 0.0046474918713022345\n",
      "Episode 14961; Testing Loss 0.00579271871286098; Training Loss 0.004647483043376097\n",
      "Episode 14962; Testing Loss 0.005792698066109391; Training Loss 0.004647474170167698\n",
      "Episode 14963; Testing Loss 0.005792796955261191; Training Loss 0.004647465027243156\n",
      "Episode 14964; Testing Loss 0.005792794850007066; Training Loss 0.004647456951506821\n",
      "Episode 14965; Testing Loss 0.005792806681323907; Training Loss 0.004647449503578747\n",
      "Episode 14966; Testing Loss 0.005792847782938779; Training Loss 0.004647440871511244\n",
      "Episode 14967; Testing Loss 0.005792806952101569; Training Loss 0.004647430784032253\n",
      "Episode 14968; Testing Loss 0.005792706611115682; Training Loss 0.004647421281320664\n",
      "Episode 14969; Testing Loss 0.005792677993028751; Training Loss 0.004647412856696522\n",
      "Episode 14970; Testing Loss 0.00579273243496511; Training Loss 0.00464740341963494\n",
      "Episode 14971; Testing Loss 0.005792850332845531; Training Loss 0.004647396439704901\n",
      "Episode 14972; Testing Loss 0.005792870593721979; Training Loss 0.004647387631281981\n",
      "Episode 14973; Testing Loss 0.005792783234581638; Training Loss 0.0046473767101020995\n",
      "Episode 14974; Testing Loss 0.005792713916179511; Training Loss 0.004647370984629089\n",
      "Episode 14975; Testing Loss 0.005792780864792734; Training Loss 0.004647360738192896\n",
      "Episode 14976; Testing Loss 0.005792851575216924; Training Loss 0.004647350828623825\n",
      "Episode 14977; Testing Loss 0.005792854377122457; Training Loss 0.004647342937899616\n",
      "Episode 14978; Testing Loss 0.005792800674227; Training Loss 0.004647334158088896\n",
      "Episode 14979; Testing Loss 0.005792720950907117; Training Loss 0.0046473244088880835\n",
      "Episode 14980; Testing Loss 0.005792679215272139; Training Loss 0.004647318154863549\n",
      "Episode 14981; Testing Loss 0.005792663692847494; Training Loss 0.004647309351806038\n",
      "Episode 14982; Testing Loss 0.005792681119479372; Training Loss 0.004647298223569008\n",
      "Episode 14983; Testing Loss 0.0057927227787808; Training Loss 0.004647289452669141\n",
      "Episode 14984; Testing Loss 0.005792733923455157; Training Loss 0.004647279819962993\n",
      "Episode 14985; Testing Loss 0.0057927378030694105; Training Loss 0.004647271908897912\n",
      "Episode 14986; Testing Loss 0.005792762688057597; Training Loss 0.004647262949158136\n",
      "Episode 14987; Testing Loss 0.005792779387514073; Training Loss 0.004647253533642832\n",
      "Episode 14988; Testing Loss 0.005792735899241038; Training Loss 0.004647245950220027\n",
      "Episode 14989; Testing Loss 0.005792585471026023; Training Loss 0.004647236644478424\n",
      "Episode 14990; Testing Loss 0.005792539708020538; Training Loss 0.004647228189017653\n",
      "Episode 14991; Testing Loss 0.005792623376406536; Training Loss 0.004647220493283866\n",
      "Episode 14992; Testing Loss 0.005792667572250671; Training Loss 0.0046472108317403785\n",
      "Episode 14993; Testing Loss 0.005792588713154276; Training Loss 0.00464720313551688\n",
      "Episode 14994; Testing Loss 0.005792584839699793; Training Loss 0.004647194592741654\n",
      "Episode 14995; Testing Loss 0.005792694724980425; Training Loss 0.004647185155241063\n",
      "Episode 14996; Testing Loss 0.005792776570509792; Training Loss 0.0046471761763813985\n",
      "Episode 14997; Testing Loss 0.005792693512909801; Training Loss 0.004647165387297946\n",
      "Episode 14998; Testing Loss 0.005792604056449105; Training Loss 0.004647160203480296\n",
      "Episode 14999; Testing Loss 0.00579256585340533; Training Loss 0.00464715212519385\n",
      "Episode 15000; Testing Loss 0.0057926520060662835; Training Loss 0.004647141282386722\n",
      "Episode 15001; Testing Loss 0.005792653351210043; Training Loss 0.004647133517498488\n",
      "Episode 15002; Testing Loss 0.005792564728366266; Training Loss 0.004647125326729707\n",
      "Episode 15003; Testing Loss 0.005792613565450796; Training Loss 0.004647116767649755\n",
      "Episode 15004; Testing Loss 0.0057927531257274565; Training Loss 0.004647106774685007\n",
      "Episode 15005; Testing Loss 0.005792721184653627; Training Loss 0.004647097879396706\n",
      "Episode 15006; Testing Loss 0.005792597806383467; Training Loss 0.0046470884323132665\n",
      "Episode 15007; Testing Loss 0.005792567161175636; Training Loss 0.004647079384803311\n",
      "Episode 15008; Testing Loss 0.005792662215953904; Training Loss 0.00464707029973813\n",
      "Episode 15009; Testing Loss 0.005792692060947379; Training Loss 0.004647062535772052\n",
      "Episode 15010; Testing Loss 0.005792573908075345; Training Loss 0.004647053149433364\n",
      "Episode 15011; Testing Loss 0.005792535025103651; Training Loss 0.004647043545253771\n",
      "Episode 15012; Testing Loss 0.005792604727279942; Training Loss 0.0046470361201927084\n",
      "Episode 15013; Testing Loss 0.005792618415026362; Training Loss 0.004647027095736716\n",
      "Episode 15014; Testing Loss 0.005792600193216046; Training Loss 0.0046470163505320265\n",
      "Episode 15015; Testing Loss 0.005792597819015854; Training Loss 0.004647010556783604\n",
      "Episode 15016; Testing Loss 0.00579257317817712; Training Loss 0.004646999206951726\n",
      "Episode 15017; Testing Loss 0.005792590990915716; Training Loss 0.004646989857616545\n",
      "Episode 15018; Testing Loss 0.005792561361838924; Training Loss 0.004646983587581877\n",
      "Episode 15019; Testing Loss 0.00579260631928902; Training Loss 0.004646973527452108\n",
      "Episode 15020; Testing Loss 0.005792637543109833; Training Loss 0.004646965741630468\n",
      "Episode 15021; Testing Loss 0.005792497161598345; Training Loss 0.004646956505457147\n",
      "Episode 15022; Testing Loss 0.005792494606321444; Training Loss 0.004646946987876664\n",
      "Episode 15023; Testing Loss 0.0057926723820751775; Training Loss 0.004646939365250755\n",
      "Episode 15024; Testing Loss 0.005792676397009074; Training Loss 0.0046469292150295915\n",
      "Episode 15025; Testing Loss 0.00579253135513333; Training Loss 0.004646920170043133\n",
      "Episode 15026; Testing Loss 0.005792492988026279; Training Loss 0.004646911808267454\n",
      "Episode 15027; Testing Loss 0.005792584121603971; Training Loss 0.0046469021128483536\n",
      "Episode 15028; Testing Loss 0.005792660510885885; Training Loss 0.004646893973192526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15029; Testing Loss 0.005792540775428607; Training Loss 0.004646885626983074\n",
      "Episode 15030; Testing Loss 0.005792508428327663; Training Loss 0.004646878300857186\n",
      "Episode 15031; Testing Loss 0.005792540719047862; Training Loss 0.004646868520442948\n",
      "Episode 15032; Testing Loss 0.005792540940510388; Training Loss 0.004646859595844677\n",
      "Episode 15033; Testing Loss 0.0057924285145180875; Training Loss 0.004646852942192045\n",
      "Episode 15034; Testing Loss 0.005792349467965052; Training Loss 0.004646844771469706\n",
      "Episode 15035; Testing Loss 0.005792412639884972; Training Loss 0.004646833321604895\n",
      "Episode 15036; Testing Loss 0.005792546632582565; Training Loss 0.0046468248836983314\n",
      "Episode 15037; Testing Loss 0.005792557329561851; Training Loss 0.0046468158552903805\n",
      "Episode 15038; Testing Loss 0.005792480720942541; Training Loss 0.0046468086332654085\n",
      "Episode 15039; Testing Loss 0.005792471579405978; Training Loss 0.004646798870876806\n",
      "Episode 15040; Testing Loss 0.005792525871019876; Training Loss 0.0046467898183890615\n",
      "Episode 15041; Testing Loss 0.0057925222663121; Training Loss 0.004646781733861534\n",
      "Episode 15042; Testing Loss 0.005792436599490492; Training Loss 0.004646772713497609\n",
      "Episode 15043; Testing Loss 0.005792395769604334; Training Loss 0.004646763306915694\n",
      "Episode 15044; Testing Loss 0.005792460930029978; Training Loss 0.0046467541129658245\n",
      "Episode 15045; Testing Loss 0.005792500146800694; Training Loss 0.004646745623011772\n",
      "Episode 15046; Testing Loss 0.0057925150526585; Training Loss 0.004646737452092948\n",
      "Episode 15047; Testing Loss 0.0057923725029390884; Training Loss 0.004646728330423388\n",
      "Episode 15048; Testing Loss 0.005792345311081494; Training Loss 0.0046467200093822545\n",
      "Episode 15049; Testing Loss 0.005792383805957377; Training Loss 0.004646709587527021\n",
      "Episode 15050; Testing Loss 0.005792440895773695; Training Loss 0.00464670223110916\n",
      "Episode 15051; Testing Loss 0.005792497865549821; Training Loss 0.004646693297318365\n",
      "Episode 15052; Testing Loss 0.0057924333395316975; Training Loss 0.0046466841196289195\n",
      "Episode 15053; Testing Loss 0.005792365830696965; Training Loss 0.004646675329635503\n",
      "Episode 15054; Testing Loss 0.005792390786566303; Training Loss 0.004646666521751126\n",
      "Episode 15055; Testing Loss 0.005792423167730454; Training Loss 0.004646657351261965\n",
      "Episode 15056; Testing Loss 0.0057923569228477655; Training Loss 0.004646650463668494\n",
      "Episode 15057; Testing Loss 0.005792299283645097; Training Loss 0.004646641804813305\n",
      "Episode 15058; Testing Loss 0.005792367123801108; Training Loss 0.004646631303897868\n",
      "Episode 15059; Testing Loss 0.005792465452325619; Training Loss 0.004646623746715245\n",
      "Episode 15060; Testing Loss 0.005792468624780735; Training Loss 0.004646614733684723\n",
      "Episode 15061; Testing Loss 0.005792343914275209; Training Loss 0.004646605986117632\n",
      "Episode 15062; Testing Loss 0.00579227077459322; Training Loss 0.004646597945767062\n",
      "Episode 15063; Testing Loss 0.005792343998220599; Training Loss 0.004646588592134323\n",
      "Episode 15064; Testing Loss 0.005792430668172149; Training Loss 0.004646580711880611\n",
      "Episode 15065; Testing Loss 0.005792352310530905; Training Loss 0.004646571776613818\n",
      "Episode 15066; Testing Loss 0.005792241707949819; Training Loss 0.004646563166571963\n",
      "Episode 15067; Testing Loss 0.005792295485629702; Training Loss 0.004646552791470698\n",
      "Episode 15068; Testing Loss 0.005792432262496383; Training Loss 0.004646546197067767\n",
      "Episode 15069; Testing Loss 0.005792429008443997; Training Loss 0.004646537334397392\n",
      "Episode 15070; Testing Loss 0.005792326982753663; Training Loss 0.004646526246425555\n",
      "Episode 15071; Testing Loss 0.005792316933387918; Training Loss 0.004646518193316787\n",
      "Episode 15072; Testing Loss 0.005792382239558938; Training Loss 0.004646508953074501\n",
      "Episode 15073; Testing Loss 0.005792373794381565; Training Loss 0.004646500062196825\n",
      "Episode 15074; Testing Loss 0.005792281865083757; Training Loss 0.00464649279408632\n",
      "Episode 15075; Testing Loss 0.0057923569324640455; Training Loss 0.004646482345587778\n",
      "Episode 15076; Testing Loss 0.005792405461416204; Training Loss 0.00464647560597544\n",
      "Episode 15077; Testing Loss 0.005792255411666294; Training Loss 0.004646465613668471\n",
      "Episode 15078; Testing Loss 0.005792237872651601; Training Loss 0.00464645784611468\n",
      "Episode 15079; Testing Loss 0.00579238141832315; Training Loss 0.004646448899221989\n",
      "Episode 15080; Testing Loss 0.005792352534169865; Training Loss 0.004646438930859062\n",
      "Episode 15081; Testing Loss 0.005792199008111266; Training Loss 0.004646431788146052\n",
      "Episode 15082; Testing Loss 0.005792245280619412; Training Loss 0.004646423380775288\n",
      "Episode 15083; Testing Loss 0.00579238986159709; Training Loss 0.004646414511251488\n",
      "Episode 15084; Testing Loss 0.005792384467695732; Training Loss 0.004646405364265313\n",
      "Episode 15085; Testing Loss 0.005792278754065955; Training Loss 0.004646395718792656\n",
      "Episode 15086; Testing Loss 0.005792232436604502; Training Loss 0.004646387386869699\n",
      "Episode 15087; Testing Loss 0.005792244046134554; Training Loss 0.004646379101351499\n",
      "Episode 15088; Testing Loss 0.00579227813067429; Training Loss 0.004646369253079305\n",
      "Episode 15089; Testing Loss 0.005792321333598869; Training Loss 0.004646360491847639\n",
      "Episode 15090; Testing Loss 0.005792237377907602; Training Loss 0.004646352370261331\n",
      "Episode 15091; Testing Loss 0.0057921682696008775; Training Loss 0.004646343572453219\n",
      "Episode 15092; Testing Loss 0.005792262810193003; Training Loss 0.0046463350885297605\n",
      "Episode 15093; Testing Loss 0.005792348812960224; Training Loss 0.0046463262667905915\n",
      "Episode 15094; Testing Loss 0.005792268597578543; Training Loss 0.00464631790243347\n",
      "Episode 15095; Testing Loss 0.005792207824326872; Training Loss 0.004646309137218627\n",
      "Episode 15096; Testing Loss 0.005792257567340272; Training Loss 0.004646300200054012\n",
      "Episode 15097; Testing Loss 0.005792237401402525; Training Loss 0.004646290902338443\n",
      "Episode 15098; Testing Loss 0.005792203587327751; Training Loss 0.00464628134605853\n",
      "Episode 15099; Testing Loss 0.00579222941121647; Training Loss 0.004646272634160532\n",
      "Episode 15100; Testing Loss 0.005792183731683902; Training Loss 0.004646263961017619\n",
      "Episode 15101; Testing Loss 0.005792247068572781; Training Loss 0.004646254748944243\n",
      "Episode 15102; Testing Loss 0.0057923157675233; Training Loss 0.004646246252807775\n",
      "Episode 15103; Testing Loss 0.005792300978843359; Training Loss 0.0046462376605196\n",
      "Episode 15104; Testing Loss 0.00579226890685451; Training Loss 0.0046462280863184\n",
      "Episode 15105; Testing Loss 0.005792232515998193; Training Loss 0.004646219394826846\n",
      "Episode 15106; Testing Loss 0.005792239819056445; Training Loss 0.0046462121531949015\n",
      "Episode 15107; Testing Loss 0.005792163727073377; Training Loss 0.0046462028359592865\n",
      "Episode 15108; Testing Loss 0.005792142434513363; Training Loss 0.0046461942273272345\n",
      "Episode 15109; Testing Loss 0.0057922356385802405; Training Loss 0.004646185561920346\n",
      "Episode 15110; Testing Loss 0.005792209556158133; Training Loss 0.004646176414653699\n",
      "Episode 15111; Testing Loss 0.005792186915965316; Training Loss 0.004646169485637531\n",
      "Episode 15112; Testing Loss 0.005792244081031735; Training Loss 0.004646158890333442\n",
      "Episode 15113; Testing Loss 0.005792223616550197; Training Loss 0.004646149910428006\n",
      "Episode 15114; Testing Loss 0.005792168538005306; Training Loss 0.004646143725642671\n",
      "Episode 15115; Testing Loss 0.005792143371655578; Training Loss 0.004646134655806321\n",
      "Episode 15116; Testing Loss 0.005792216147121206; Training Loss 0.004646123674932748\n",
      "Episode 15117; Testing Loss 0.005792282186535356; Training Loss 0.00464611513370133\n",
      "Episode 15118; Testing Loss 0.005792236926924386; Training Loss 0.0046461082797726195\n",
      "Episode 15119; Testing Loss 0.005792142953206999; Training Loss 0.00464609713304373\n",
      "Episode 15120; Testing Loss 0.005792108148354975; Training Loss 0.004646088993892901\n",
      "Episode 15121; Testing Loss 0.005792183156187585; Training Loss 0.00464608101081466\n",
      "Episode 15122; Testing Loss 0.005792188051476701; Training Loss 0.004646072190844966\n",
      "Episode 15123; Testing Loss 0.005792118709678639; Training Loss 0.004646062272187865\n",
      "Episode 15124; Testing Loss 0.00579211562486133; Training Loss 0.004646054805958596\n",
      "Episode 15125; Testing Loss 0.0057921442140136175; Training Loss 0.00464604559328621\n",
      "Episode 15126; Testing Loss 0.00579217220388241; Training Loss 0.004646034775263361\n",
      "Episode 15127; Testing Loss 0.005792216701611153; Training Loss 0.004646026521616661\n",
      "Episode 15128; Testing Loss 0.005792222574583798; Training Loss 0.00464601763857856\n",
      "Episode 15129; Testing Loss 0.005792132217199934; Training Loss 0.004646008856435281\n",
      "Episode 15130; Testing Loss 0.005792045748381623; Training Loss 0.004646000108204025\n",
      "Episode 15131; Testing Loss 0.005792123160558791; Training Loss 0.00464599093407922\n",
      "Episode 15132; Testing Loss 0.005792207602225457; Training Loss 0.0046459829584571495\n",
      "Episode 15133; Testing Loss 0.005792139218629856; Training Loss 0.004645972655897441\n",
      "Episode 15134; Testing Loss 0.0057920888032866125; Training Loss 0.0046459639709543\n",
      "Episode 15135; Testing Loss 0.005792155419056982; Training Loss 0.004645955981660334\n",
      "Episode 15136; Testing Loss 0.005792176032717838; Training Loss 0.004645946735711486\n",
      "Episode 15137; Testing Loss 0.005792182501278676; Training Loss 0.004645937937123528\n",
      "Episode 15138; Testing Loss 0.005792117884886414; Training Loss 0.0046459298618872795\n",
      "Episode 15139; Testing Loss 0.005792072871446344; Training Loss 0.004645920865851682\n",
      "Episode 15140; Testing Loss 0.0057921733409852395; Training Loss 0.0046459111427815175\n",
      "Episode 15141; Testing Loss 0.005792187348946182; Training Loss 0.004645902554980643\n",
      "Episode 15142; Testing Loss 0.005792083014519596; Training Loss 0.00464589364059849\n",
      "Episode 15143; Testing Loss 0.0057921295448106975; Training Loss 0.004645884736160476\n",
      "Episode 15144; Testing Loss 0.005792164714114949; Training Loss 0.004645877338234325\n",
      "Episode 15145; Testing Loss 0.00579208977693156; Training Loss 0.004645866839241913\n",
      "Episode 15146; Testing Loss 0.005792069326768151; Training Loss 0.0046458602511159795\n",
      "Episode 15147; Testing Loss 0.005792179498888306; Training Loss 0.004645852148365418\n",
      "Episode 15148; Testing Loss 0.005792157514262223; Training Loss 0.004645842669226777\n",
      "Episode 15149; Testing Loss 0.005792064241313252; Training Loss 0.004645834602817419\n",
      "Episode 15150; Testing Loss 0.005792144649687163; Training Loss 0.004645823641223361\n",
      "Episode 15151; Testing Loss 0.005792161318910643; Training Loss 0.004645817308475578\n",
      "Episode 15152; Testing Loss 0.0057920495353360035; Training Loss 0.004645809294015933\n",
      "Episode 15153; Testing Loss 0.0057919321478242265; Training Loss 0.004645798614475557\n",
      "Episode 15154; Testing Loss 0.005792014413730442; Training Loss 0.004645791778737009\n",
      "Episode 15155; Testing Loss 0.005792238321629803; Training Loss 0.004645785050125774\n",
      "Episode 15156; Testing Loss 0.005792141337756724; Training Loss 0.0046457742407348505\n",
      "Episode 15157; Testing Loss 0.005791953200218088; Training Loss 0.004645767317409688\n",
      "Episode 15158; Testing Loss 0.005792093979375381; Training Loss 0.004645755213188006\n",
      "Episode 15159; Testing Loss 0.005792248075327155; Training Loss 0.0046457469041598426\n",
      "Episode 15160; Testing Loss 0.0057920884096570565; Training Loss 0.00464573652310383\n",
      "Episode 15161; Testing Loss 0.005791827051400161; Training Loss 0.004645730161783899\n",
      "Episode 15162; Testing Loss 0.00579192596525066; Training Loss 0.004645720083815208\n",
      "Episode 15163; Testing Loss 0.005792133468047652; Training Loss 0.004645712408127707\n",
      "Episode 15164; Testing Loss 0.005792072533301061; Training Loss 0.004645701741103937\n",
      "Episode 15165; Testing Loss 0.0057918945396511585; Training Loss 0.004645696023145355\n",
      "Episode 15166; Testing Loss 0.005791974373487191; Training Loss 0.0046456857546191025\n",
      "Episode 15167; Testing Loss 0.005792141914296184; Training Loss 0.004645676690083237\n",
      "Episode 15168; Testing Loss 0.005792112721931478; Training Loss 0.004645668030347621\n",
      "Episode 15169; Testing Loss 0.005792057249008866; Training Loss 0.00464566189890543\n",
      "Episode 15170; Testing Loss 0.005792130872007608; Training Loss 0.004645652161755754\n",
      "Episode 15171; Testing Loss 0.005792093380690116; Training Loss 0.004645643021544766\n",
      "Episode 15172; Testing Loss 0.005791934264387994; Training Loss 0.004645633329329075\n",
      "Episode 15173; Testing Loss 0.0057919697323897115; Training Loss 0.004645623205309031\n",
      "Episode 15174; Testing Loss 0.005792041051588747; Training Loss 0.004645614073411987\n",
      "Episode 15175; Testing Loss 0.005792110008180771; Training Loss 0.004645608257223059\n",
      "Episode 15176; Testing Loss 0.005792132990565498; Training Loss 0.004645596284446662\n",
      "Episode 15177; Testing Loss 0.005792104189493571; Training Loss 0.004645588520099\n",
      "Episode 15178; Testing Loss 0.005792017522001549; Training Loss 0.004645580732793758\n",
      "Episode 15179; Testing Loss 0.005792035838444837; Training Loss 0.004645572179239901\n",
      "Episode 15180; Testing Loss 0.005792024710183782; Training Loss 0.0046455633169053085\n",
      "Episode 15181; Testing Loss 0.005791997283815178; Training Loss 0.0046455531666508565\n",
      "Episode 15182; Testing Loss 0.005791994181066352; Training Loss 0.004645541985886663\n",
      "Episode 15183; Testing Loss 0.005792016983804055; Training Loss 0.0046455368822224535\n",
      "Episode 15184; Testing Loss 0.005792040996746223; Training Loss 0.004645530089839287\n",
      "Episode 15185; Testing Loss 0.005792107865561579; Training Loss 0.004645519746673537\n",
      "Episode 15186; Testing Loss 0.005792064736094625; Training Loss 0.004645508639278652\n",
      "Episode 15187; Testing Loss 0.0057919359858526675; Training Loss 0.004645501167424112\n",
      "Episode 15188; Testing Loss 0.0057919573023693756; Training Loss 0.004645493847753229\n",
      "Episode 15189; Testing Loss 0.005792120676629636; Training Loss 0.004645483961406887\n",
      "Episode 15190; Testing Loss 0.0057920389914446644; Training Loss 0.004645472614350707\n",
      "Episode 15191; Testing Loss 0.005791881474450143; Training Loss 0.004645465977243492\n",
      "Episode 15192; Testing Loss 0.0057919520427916794; Training Loss 0.00464545637907477\n",
      "Episode 15193; Testing Loss 0.0057920833037060256; Training Loss 0.004645446362629178\n",
      "Episode 15194; Testing Loss 0.00579204332320092; Training Loss 0.004645437847699953\n",
      "Episode 15195; Testing Loss 0.005791958465890831; Training Loss 0.0046454287162168454\n",
      "Episode 15196; Testing Loss 0.005791980234110166; Training Loss 0.004645421060225148\n",
      "Episode 15197; Testing Loss 0.005791970741124313; Training Loss 0.004645411057617027\n",
      "Episode 15198; Testing Loss 0.005791961307956337; Training Loss 0.004645403685518278\n",
      "Episode 15199; Testing Loss 0.005792047488075964; Training Loss 0.004645395062296635\n",
      "Episode 15200; Testing Loss 0.005792077390318403; Training Loss 0.00464538669048746\n",
      "Episode 15201; Testing Loss 0.0057919529169913045; Training Loss 0.004645376897739488\n",
      "Episode 15202; Testing Loss 0.005791849409037179; Training Loss 0.0046453677362959436\n",
      "Episode 15203; Testing Loss 0.005791911927450085; Training Loss 0.004645362097890286\n",
      "Episode 15204; Testing Loss 0.00579195889079012; Training Loss 0.004645352397259179\n",
      "Episode 15205; Testing Loss 0.005792008187086161; Training Loss 0.004645343009977172\n",
      "Episode 15206; Testing Loss 0.0057921051579530435; Training Loss 0.0046453345729582166\n",
      "Episode 15207; Testing Loss 0.005792028844998488; Training Loss 0.004645326667894387\n",
      "Episode 15208; Testing Loss 0.005791899108650691; Training Loss 0.004645319133665471\n",
      "Episode 15209; Testing Loss 0.00579190187246686; Training Loss 0.00464530952695143\n",
      "Episode 15210; Testing Loss 0.005791982407564078; Training Loss 0.004645299077862608\n",
      "Episode 15211; Testing Loss 0.005791931820834998; Training Loss 0.004645288538936704\n",
      "Episode 15212; Testing Loss 0.005791917244100821; Training Loss 0.004645283507485225\n",
      "Episode 15213; Testing Loss 0.005791922252014123; Training Loss 0.00464527524644793\n",
      "Episode 15214; Testing Loss 0.005791843216911042; Training Loss 0.004645265626142778\n",
      "Episode 15215; Testing Loss 0.005791766936226047; Training Loss 0.004645255960347056\n",
      "Episode 15216; Testing Loss 0.005791931696856058; Training Loss 0.004645247435258381\n",
      "Episode 15217; Testing Loss 0.0057920512516478236; Training Loss 0.0046452403843831504\n",
      "Episode 15218; Testing Loss 0.005791870738101124; Training Loss 0.004645228992859453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15219; Testing Loss 0.005791764309007912; Training Loss 0.004645221037754281\n",
      "Episode 15220; Testing Loss 0.0057919631096798535; Training Loss 0.004645211770764204\n",
      "Episode 15221; Testing Loss 0.005792008247634008; Training Loss 0.004645204728303216\n",
      "Episode 15222; Testing Loss 0.005791838973936359; Training Loss 0.004645193249927544\n",
      "Episode 15223; Testing Loss 0.005791745553051547; Training Loss 0.004645185950219377\n",
      "Episode 15224; Testing Loss 0.005791860765101384; Training Loss 0.0046451757099958595\n",
      "Episode 15225; Testing Loss 0.005791975021354386; Training Loss 0.004645167283502876\n",
      "Episode 15226; Testing Loss 0.005791870825295044; Training Loss 0.004645157791733648\n",
      "Episode 15227; Testing Loss 0.005791784252276226; Training Loss 0.004645149534751973\n",
      "Episode 15228; Testing Loss 0.0057918769474885915; Training Loss 0.004645141177986503\n",
      "Episode 15229; Testing Loss 0.005791871159346043; Training Loss 0.004645131538136481\n",
      "Episode 15230; Testing Loss 0.00579180573734326; Training Loss 0.004645124388806398\n",
      "Episode 15231; Testing Loss 0.0057918313993686995; Training Loss 0.004645116555010503\n",
      "Episode 15232; Testing Loss 0.005791842585219444; Training Loss 0.004645107533076209\n",
      "Episode 15233; Testing Loss 0.005791851379634015; Training Loss 0.004645098531930276\n",
      "Episode 15234; Testing Loss 0.005791889102216309; Training Loss 0.004645088723060839\n",
      "Episode 15235; Testing Loss 0.005791916748047842; Training Loss 0.004645081961568859\n",
      "Episode 15236; Testing Loss 0.0057918529286479555; Training Loss 0.004645072697932627\n",
      "Episode 15237; Testing Loss 0.005791738622593191; Training Loss 0.004645062050190647\n",
      "Episode 15238; Testing Loss 0.005791724223629774; Training Loss 0.004645055820570892\n",
      "Episode 15239; Testing Loss 0.005791837606032069; Training Loss 0.004645047576404549\n",
      "Episode 15240; Testing Loss 0.0057919374157529135; Training Loss 0.004645039664087895\n",
      "Episode 15241; Testing Loss 0.005791933985809619; Training Loss 0.004645030069626788\n",
      "Episode 15242; Testing Loss 0.005791779649194307; Training Loss 0.004645020528847856\n",
      "Episode 15243; Testing Loss 0.005791664894696094; Training Loss 0.004645011142838687\n",
      "Episode 15244; Testing Loss 0.005791817249971503; Training Loss 0.0046450036996064075\n",
      "Episode 15245; Testing Loss 0.005791904524250782; Training Loss 0.004644996911653117\n",
      "Episode 15246; Testing Loss 0.005791772760680168; Training Loss 0.004644985402452603\n",
      "Episode 15247; Testing Loss 0.005791691777959679; Training Loss 0.004644975583669952\n",
      "Episode 15248; Testing Loss 0.005791869974302254; Training Loss 0.0046449696517817845\n",
      "Episode 15249; Testing Loss 0.005791827776319817; Training Loss 0.004644959282506567\n",
      "Episode 15250; Testing Loss 0.005791648169150448; Training Loss 0.0046449552524015685\n",
      "Episode 15251; Testing Loss 0.005791808005905117; Training Loss 0.00464494243580875\n",
      "Episode 15252; Testing Loss 0.005791980685301917; Training Loss 0.004644935881282103\n",
      "Episode 15253; Testing Loss 0.005791720756601812; Training Loss 0.004644924869360261\n",
      "Episode 15254; Testing Loss 0.005791457598479788; Training Loss 0.004644921186057339\n",
      "Episode 15255; Testing Loss 0.005791651983008119; Training Loss 0.004644907926944793\n",
      "Episode 15256; Testing Loss 0.005791957481110673; Training Loss 0.004644901932100771\n",
      "Episode 15257; Testing Loss 0.00579189576128289; Training Loss 0.004644891353326091\n",
      "Episode 15258; Testing Loss 0.005791646857058651; Training Loss 0.004644883753429787\n",
      "Episode 15259; Testing Loss 0.005791606268083826; Training Loss 0.004644875309368214\n",
      "Episode 15260; Testing Loss 0.005791794264075758; Training Loss 0.004644863206571768\n",
      "Episode 15261; Testing Loss 0.0057918663586944575; Training Loss 0.004644857017150735\n",
      "Episode 15262; Testing Loss 0.005791714363105704; Training Loss 0.004644847164458028\n",
      "Episode 15263; Testing Loss 0.0057916176218221; Training Loss 0.0046448378059985775\n",
      "Episode 15264; Testing Loss 0.0057917422724381214; Training Loss 0.00464482993851613\n",
      "Episode 15265; Testing Loss 0.005791787935015568; Training Loss 0.004644822360819785\n",
      "Episode 15266; Testing Loss 0.005791641336355627; Training Loss 0.00464481107633908\n",
      "Episode 15267; Testing Loss 0.005791619283881311; Training Loss 0.0046448031875384\n",
      "Episode 15268; Testing Loss 0.005791798840260821; Training Loss 0.004644793829528934\n",
      "Episode 15269; Testing Loss 0.005791777964311687; Training Loss 0.00464478562903666\n",
      "Episode 15270; Testing Loss 0.005791567323507388; Training Loss 0.0046447763259473114\n",
      "Episode 15271; Testing Loss 0.005791598944220064; Training Loss 0.004644767846317032\n",
      "Episode 15272; Testing Loss 0.005791836753583288; Training Loss 0.00464476024502534\n",
      "Episode 15273; Testing Loss 0.005791794232655465; Training Loss 0.004644751572271749\n",
      "Episode 15274; Testing Loss 0.005791580843878643; Training Loss 0.004644743739787003\n",
      "Episode 15275; Testing Loss 0.005791513293951235; Training Loss 0.004644733393382048\n",
      "Episode 15276; Testing Loss 0.005791647513882479; Training Loss 0.004644723638754067\n",
      "Episode 15277; Testing Loss 0.005791758529027364; Training Loss 0.004644717940111464\n",
      "Episode 15278; Testing Loss 0.005791705909177046; Training Loss 0.0046447075603033095\n",
      "Episode 15279; Testing Loss 0.005791638274989873; Training Loss 0.00464469951352025\n",
      "Episode 15280; Testing Loss 0.005791654664286322; Training Loss 0.004644693276084649\n",
      "Episode 15281; Testing Loss 0.005791666724541305; Training Loss 0.004644683429022515\n",
      "Episode 15282; Testing Loss 0.005791581875595442; Training Loss 0.004644672028018184\n",
      "Episode 15283; Testing Loss 0.0057916114478498565; Training Loss 0.004644666520108182\n",
      "Episode 15284; Testing Loss 0.005791746326508167; Training Loss 0.00464465813694905\n",
      "Episode 15285; Testing Loss 0.005791716608351109; Training Loss 0.004644648406641328\n",
      "Episode 15286; Testing Loss 0.005791513562870765; Training Loss 0.004644640639135805\n",
      "Episode 15287; Testing Loss 0.0057915115329436875; Training Loss 0.004644634286046689\n",
      "Episode 15288; Testing Loss 0.005791609327696255; Training Loss 0.004644621668161664\n",
      "Episode 15289; Testing Loss 0.005791621433780358; Training Loss 0.004644612761813173\n",
      "Episode 15290; Testing Loss 0.005791596054820951; Training Loss 0.004644608066645812\n",
      "Episode 15291; Testing Loss 0.0057916650535174835; Training Loss 0.004644599291785687\n",
      "Episode 15292; Testing Loss 0.005791699571002138; Training Loss 0.00464458706921197\n",
      "Episode 15293; Testing Loss 0.005791607569574536; Training Loss 0.004644578824173766\n",
      "Episode 15294; Testing Loss 0.0057914991556722775; Training Loss 0.004644571056594964\n",
      "Episode 15295; Testing Loss 0.005791499775573781; Training Loss 0.004644565039464996\n",
      "Episode 15296; Testing Loss 0.005791505202884507; Training Loss 0.004644555616720249\n",
      "Episode 15297; Testing Loss 0.00579145426502578; Training Loss 0.004644545346282176\n",
      "Episode 15298; Testing Loss 0.005791502876440914; Training Loss 0.004644535892805171\n",
      "Episode 15299; Testing Loss 0.005791595356256697; Training Loss 0.004644528590312722\n",
      "Episode 15300; Testing Loss 0.00579160300354115; Training Loss 0.004644519154831599\n",
      "Episode 15301; Testing Loss 0.0057915676875792904; Training Loss 0.004644510412174542\n",
      "Episode 15302; Testing Loss 0.005791692048903146; Training Loss 0.004644502340621602\n",
      "Episode 15303; Testing Loss 0.005791672192707365; Training Loss 0.004644493814732754\n",
      "Episode 15304; Testing Loss 0.0057915354716121074; Training Loss 0.004644484020356624\n",
      "Episode 15305; Testing Loss 0.005791463885926055; Training Loss 0.0046444756031241985\n",
      "Episode 15306; Testing Loss 0.005791563501429727; Training Loss 0.004644466022642173\n",
      "Episode 15307; Testing Loss 0.005791613218684853; Training Loss 0.004644457940958165\n",
      "Episode 15308; Testing Loss 0.0057914715095826975; Training Loss 0.004644449027367195\n",
      "Episode 15309; Testing Loss 0.005791408325619616; Training Loss 0.0046444413691773215\n",
      "Episode 15310; Testing Loss 0.005791522359222983; Training Loss 0.004644431865138257\n",
      "Episode 15311; Testing Loss 0.0057915555137402945; Training Loss 0.004644422789136436\n",
      "Episode 15312; Testing Loss 0.005791435738794739; Training Loss 0.004644413992784009\n",
      "Episode 15313; Testing Loss 0.005791409376497278; Training Loss 0.004644407261365194\n",
      "Episode 15314; Testing Loss 0.005791416444825802; Training Loss 0.00464439912842273\n",
      "Episode 15315; Testing Loss 0.005791441907262538; Training Loss 0.004644390142224934\n",
      "Episode 15316; Testing Loss 0.005791507367920006; Training Loss 0.004644381525237028\n",
      "Episode 15317; Testing Loss 0.005791487863943768; Training Loss 0.004644373387306167\n",
      "Episode 15318; Testing Loss 0.005791406560181414; Training Loss 0.00464436436985044\n",
      "Episode 15319; Testing Loss 0.005791328808833004; Training Loss 0.004644354928458717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15320; Testing Loss 0.005791313780313486; Training Loss 0.0046443468394258966\n",
      "Episode 15321; Testing Loss 0.005791262379098687; Training Loss 0.004644338366196535\n",
      "Episode 15322; Testing Loss 0.005791311270098775; Training Loss 0.004644327437346775\n",
      "Episode 15323; Testing Loss 0.005791389101552049; Training Loss 0.004644320467979024\n",
      "Episode 15324; Testing Loss 0.005791456630401633; Training Loss 0.004644313210587736\n",
      "Episode 15325; Testing Loss 0.005791458306854024; Training Loss 0.004644304705525962\n",
      "Episode 15326; Testing Loss 0.00579144278416652; Training Loss 0.004644295460126448\n",
      "Episode 15327; Testing Loss 0.005791435982940332; Training Loss 0.004644285160320567\n",
      "Episode 15328; Testing Loss 0.005791388139026956; Training Loss 0.004644274951133411\n",
      "Episode 15329; Testing Loss 0.005791338020716261; Training Loss 0.004644268519445343\n",
      "Episode 15330; Testing Loss 0.0057912878473019075; Training Loss 0.004644261851948776\n",
      "Episode 15331; Testing Loss 0.0057913819830457776; Training Loss 0.004644250292332248\n",
      "Episode 15332; Testing Loss 0.005791402391111456; Training Loss 0.004644241538020195\n",
      "Episode 15333; Testing Loss 0.005791331257176205; Training Loss 0.00464423466792526\n",
      "Episode 15334; Testing Loss 0.005791335179474897; Training Loss 0.004644226751288144\n",
      "Episode 15335; Testing Loss 0.0057913647829276734; Training Loss 0.004644218679466269\n",
      "Episode 15336; Testing Loss 0.005791321257979919; Training Loss 0.004644206942239599\n",
      "Episode 15337; Testing Loss 0.00579132454625131; Training Loss 0.004644196565862269\n",
      "Episode 15338; Testing Loss 0.005791296975992161; Training Loss 0.004644190533509083\n",
      "Episode 15339; Testing Loss 0.0057913310881863465; Training Loss 0.0046441799811355895\n",
      "Episode 15340; Testing Loss 0.005791325361382454; Training Loss 0.004644171776152935\n",
      "Episode 15341; Testing Loss 0.0057912738569549805; Training Loss 0.004644163201420364\n",
      "Episode 15342; Testing Loss 0.00579125174611829; Training Loss 0.004644153537856525\n",
      "Episode 15343; Testing Loss 0.0057912269005227345; Training Loss 0.004644146105193764\n",
      "Episode 15344; Testing Loss 0.005791235764478396; Training Loss 0.004644137368327286\n",
      "Episode 15345; Testing Loss 0.0057912905883966935; Training Loss 0.004644127338087502\n",
      "Episode 15346; Testing Loss 0.005791298923455453; Training Loss 0.004644119736157849\n",
      "Episode 15347; Testing Loss 0.005791334222014139; Training Loss 0.004644112702148606\n",
      "Episode 15348; Testing Loss 0.005791230509188365; Training Loss 0.004644102331640461\n",
      "Episode 15349; Testing Loss 0.005791136196019014; Training Loss 0.004644094520481619\n",
      "Episode 15350; Testing Loss 0.005791226782870706; Training Loss 0.004644085143526505\n",
      "Episode 15351; Testing Loss 0.005791235052424039; Training Loss 0.004644078118312462\n",
      "Episode 15352; Testing Loss 0.005791181322842727; Training Loss 0.004644069239652263\n",
      "Episode 15353; Testing Loss 0.00579128773688864; Training Loss 0.004644060265292566\n",
      "Episode 15354; Testing Loss 0.005791394533087696; Training Loss 0.004644051412885533\n",
      "Episode 15355; Testing Loss 0.005791270689868895; Training Loss 0.004644044349249996\n",
      "Episode 15356; Testing Loss 0.005791113667926348; Training Loss 0.004644035611963196\n",
      "Episode 15357; Testing Loss 0.005791214311241111; Training Loss 0.004644027083481369\n",
      "Episode 15358; Testing Loss 0.00579143868034762; Training Loss 0.004644021134422138\n",
      "Episode 15359; Testing Loss 0.005791252979979788; Training Loss 0.004644009840536119\n",
      "Episode 15360; Testing Loss 0.005791016346609538; Training Loss 0.0046440021294803\n",
      "Episode 15361; Testing Loss 0.005791134463696935; Training Loss 0.004643991942629903\n",
      "Episode 15362; Testing Loss 0.005791320699376543; Training Loss 0.0046439842865571655\n",
      "Episode 15363; Testing Loss 0.0057913001900114355; Training Loss 0.0046439742625526085\n",
      "Episode 15364; Testing Loss 0.005791161009425013; Training Loss 0.004643965184109722\n",
      "Episode 15365; Testing Loss 0.00579113986709904; Training Loss 0.004643956411442835\n",
      "Episode 15366; Testing Loss 0.005791214097848188; Training Loss 0.004643947358843784\n",
      "Episode 15367; Testing Loss 0.0057911720956977655; Training Loss 0.004643939803178706\n",
      "Episode 15368; Testing Loss 0.0057910793298317515; Training Loss 0.004643932167001412\n",
      "Episode 15369; Testing Loss 0.0057911774483318645; Training Loss 0.004643921938660066\n",
      "Episode 15370; Testing Loss 0.005791229044857772; Training Loss 0.004643917120272431\n",
      "Episode 15371; Testing Loss 0.005791078598981126; Training Loss 0.00464390628692962\n",
      "Episode 15372; Testing Loss 0.005791037301782764; Training Loss 0.004643896910039379\n",
      "Episode 15373; Testing Loss 0.005791190926603087; Training Loss 0.004643890067809727\n",
      "Episode 15374; Testing Loss 0.0057912494251107895; Training Loss 0.004643881653043411\n",
      "Episode 15375; Testing Loss 0.005791077932656241; Training Loss 0.004643870309313249\n",
      "Episode 15376; Testing Loss 0.005791015965049148; Training Loss 0.004643862611144026\n",
      "Episode 15377; Testing Loss 0.005791080887206405; Training Loss 0.004643853918660867\n",
      "Episode 15378; Testing Loss 0.005791270804781749; Training Loss 0.0046438451584253455\n",
      "Episode 15379; Testing Loss 0.005791169070977261; Training Loss 0.004643835470116385\n",
      "Episode 15380; Testing Loss 0.005791065925028344; Training Loss 0.004643828180973085\n",
      "Episode 15381; Testing Loss 0.005791165454664852; Training Loss 0.0046438197750414325\n",
      "Episode 15382; Testing Loss 0.0057911874465214055; Training Loss 0.004643810794198728\n",
      "Episode 15383; Testing Loss 0.00579108988910284; Training Loss 0.004643801420537878\n",
      "Episode 15384; Testing Loss 0.005790995767414741; Training Loss 0.004643793077998085\n",
      "Episode 15385; Testing Loss 0.0057910243040597215; Training Loss 0.004643784553941986\n",
      "Episode 15386; Testing Loss 0.005791195339486067; Training Loss 0.004643776012027702\n",
      "Episode 15387; Testing Loss 0.005791206100115314; Training Loss 0.004643768356421628\n",
      "Episode 15388; Testing Loss 0.005791103940421854; Training Loss 0.004643759053883897\n",
      "Episode 15389; Testing Loss 0.005791052097102176; Training Loss 0.004643752630677107\n",
      "Episode 15390; Testing Loss 0.005790975028440401; Training Loss 0.004643741690959641\n",
      "Episode 15391; Testing Loss 0.005791002008689327; Training Loss 0.004643737743637886\n",
      "Episode 15392; Testing Loss 0.005791142915494227; Training Loss 0.004643728792693362\n",
      "Episode 15393; Testing Loss 0.005791169722159666; Training Loss 0.004643716789481307\n",
      "Episode 15394; Testing Loss 0.005791053038338285; Training Loss 0.004643709698244365\n",
      "Episode 15395; Testing Loss 0.0057910313128853586; Training Loss 0.004643701881904165\n",
      "Episode 15396; Testing Loss 0.005791036235411147; Training Loss 0.004643690671975459\n",
      "Episode 15397; Testing Loss 0.005791048202221779; Training Loss 0.004643683121461106\n",
      "Episode 15398; Testing Loss 0.005790987439220511; Training Loss 0.004643676707227941\n",
      "Episode 15399; Testing Loss 0.005790992517091581; Training Loss 0.004643666810476145\n",
      "Episode 15400; Testing Loss 0.00579110366729434; Training Loss 0.00464365773636452\n",
      "Episode 15401; Testing Loss 0.005791042909972235; Training Loss 0.00464365153960204\n",
      "Episode 15402; Testing Loss 0.005790877864443109; Training Loss 0.004643642127332394\n",
      "Episode 15403; Testing Loss 0.005790867263430108; Training Loss 0.00464363241779456\n",
      "Episode 15404; Testing Loss 0.005791055474299939; Training Loss 0.004643623431951575\n",
      "Episode 15405; Testing Loss 0.005791157955506753; Training Loss 0.004643615946812696\n",
      "Episode 15406; Testing Loss 0.005791039188859659; Training Loss 0.004643607168587818\n",
      "Episode 15407; Testing Loss 0.005790875402210517; Training Loss 0.00464360039334644\n",
      "Episode 15408; Testing Loss 0.005790918711853131; Training Loss 0.00464359098801993\n",
      "Episode 15409; Testing Loss 0.00579100794200641; Training Loss 0.004643581500886104\n",
      "Episode 15410; Testing Loss 0.0057909599623758665; Training Loss 0.0046435726834679605\n",
      "Episode 15411; Testing Loss 0.005790850309264537; Training Loss 0.004643564505842315\n",
      "Episode 15412; Testing Loss 0.00579092658433137; Training Loss 0.004643555757514724\n",
      "Episode 15413; Testing Loss 0.005791050322394179; Training Loss 0.004643547307516446\n",
      "Episode 15414; Testing Loss 0.005790986690925471; Training Loss 0.004643536612421461\n",
      "Episode 15415; Testing Loss 0.005790919718706677; Training Loss 0.004643532812590578\n",
      "Episode 15416; Testing Loss 0.005790957885477592; Training Loss 0.004643523900246787\n",
      "Episode 15417; Testing Loss 0.0057909854047665105; Training Loss 0.004643512016763796\n",
      "Episode 15418; Testing Loss 0.005790939821042776; Training Loss 0.004643503332319953\n",
      "Episode 15419; Testing Loss 0.0057908067093207245; Training Loss 0.004643495794758087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15420; Testing Loss 0.005790763742957081; Training Loss 0.00464348670771772\n",
      "Episode 15421; Testing Loss 0.005790819574269539; Training Loss 0.004643479002632429\n",
      "Episode 15422; Testing Loss 0.005790841830374196; Training Loss 0.0046434693092816\n",
      "Episode 15423; Testing Loss 0.005790880158905275; Training Loss 0.004643461394514598\n",
      "Episode 15424; Testing Loss 0.00579094115694573; Training Loss 0.004643453884079328\n",
      "Episode 15425; Testing Loss 0.00579093592912225; Training Loss 0.004643444550221417\n",
      "Episode 15426; Testing Loss 0.005790821036420922; Training Loss 0.004643434681633606\n",
      "Episode 15427; Testing Loss 0.005790714108745482; Training Loss 0.00464342795229716\n",
      "Episode 15428; Testing Loss 0.00579073499350409; Training Loss 0.004643419104256812\n",
      "Episode 15429; Testing Loss 0.005790802573788195; Training Loss 0.004643408457778132\n",
      "Episode 15430; Testing Loss 0.005790840832596525; Training Loss 0.004643399698483336\n",
      "Episode 15431; Testing Loss 0.005790842916069055; Training Loss 0.004643391652615717\n",
      "Episode 15432; Testing Loss 0.0057909285803530285; Training Loss 0.004643383290584231\n",
      "Episode 15433; Testing Loss 0.0057908743265563415; Training Loss 0.004643376058553968\n",
      "Episode 15434; Testing Loss 0.005790698056601074; Training Loss 0.004643366488078027\n",
      "Episode 15435; Testing Loss 0.005790686943499074; Training Loss 0.004643358306896526\n",
      "Episode 15436; Testing Loss 0.005790807826784429; Training Loss 0.004643351256770368\n",
      "Episode 15437; Testing Loss 0.005790865879470446; Training Loss 0.004643341388147651\n",
      "Episode 15438; Testing Loss 0.005790714071196996; Training Loss 0.004643330794591771\n",
      "Episode 15439; Testing Loss 0.005790655201284441; Training Loss 0.004643325801061784\n",
      "Episode 15440; Testing Loss 0.005790795377201503; Training Loss 0.004643316610344728\n",
      "Episode 15441; Testing Loss 0.0057908600318175175; Training Loss 0.004643305188715626\n",
      "Episode 15442; Testing Loss 0.005790799448456398; Training Loss 0.004643299585059194\n",
      "Episode 15443; Testing Loss 0.005790764104821239; Training Loss 0.004643289707889523\n",
      "Episode 15444; Testing Loss 0.005790776364023596; Training Loss 0.004643279671367991\n",
      "Episode 15445; Testing Loss 0.005790706426661969; Training Loss 0.004643275083301521\n",
      "Episode 15446; Testing Loss 0.0057905758236936255; Training Loss 0.004643264894259584\n",
      "Episode 15447; Testing Loss 0.0057905293053057555; Training Loss 0.00464325595684584\n",
      "Episode 15448; Testing Loss 0.00579072258101962; Training Loss 0.00464324977470108\n",
      "Episode 15449; Testing Loss 0.005790864288038412; Training Loss 0.004643241751197243\n",
      "Episode 15450; Testing Loss 0.005790757278377803; Training Loss 0.004643228762925908\n",
      "Episode 15451; Testing Loss 0.00579059599329599; Training Loss 0.004643222056566713\n",
      "Episode 15452; Testing Loss 0.0057906251652813595; Training Loss 0.004643212001891897\n",
      "Episode 15453; Testing Loss 0.005790696280096455; Training Loss 0.004643203730892997\n",
      "Episode 15454; Testing Loss 0.005790692355171496; Training Loss 0.00464319528319395\n",
      "Episode 15455; Testing Loss 0.005790628356817968; Training Loss 0.004643186792616922\n",
      "Episode 15456; Testing Loss 0.005790653521296313; Training Loss 0.004643179652238\n",
      "Episode 15457; Testing Loss 0.005790780537526873; Training Loss 0.004643170880522017\n",
      "Episode 15458; Testing Loss 0.005790680947599807; Training Loss 0.004643161066989688\n",
      "Episode 15459; Testing Loss 0.005790500961642271; Training Loss 0.004643154912591756\n",
      "Episode 15460; Testing Loss 0.0057905424368454375; Training Loss 0.004643146079457137\n",
      "Episode 15461; Testing Loss 0.005790709769043932; Training Loss 0.004643136648809275\n",
      "Episode 15462; Testing Loss 0.005790678258014303; Training Loss 0.004643127508539406\n",
      "Episode 15463; Testing Loss 0.005790619686503932; Training Loss 0.004643119018199881\n",
      "Episode 15464; Testing Loss 0.005790616242732343; Training Loss 0.0046431091888208455\n",
      "Episode 15465; Testing Loss 0.005790652228097264; Training Loss 0.00464310126583556\n",
      "Episode 15466; Testing Loss 0.00579066710900353; Training Loss 0.004643094204295537\n",
      "Episode 15467; Testing Loss 0.005790579569482704; Training Loss 0.004643085976848341\n",
      "Episode 15468; Testing Loss 0.005790555189938312; Training Loss 0.004643076393564667\n",
      "Episode 15469; Testing Loss 0.005790615082225151; Training Loss 0.004643068029720461\n",
      "Episode 15470; Testing Loss 0.005790599129595618; Training Loss 0.004643058542407751\n",
      "Episode 15471; Testing Loss 0.005790530148895489; Training Loss 0.004643050578772369\n",
      "Episode 15472; Testing Loss 0.005790639654683313; Training Loss 0.004643042181897578\n",
      "Episode 15473; Testing Loss 0.005790701575747988; Training Loss 0.004643033786128701\n",
      "Episode 15474; Testing Loss 0.005790622375867914; Training Loss 0.004643023433129245\n",
      "Episode 15475; Testing Loss 0.0057904890200098525; Training Loss 0.0046430162804791284\n",
      "Episode 15476; Testing Loss 0.005790399991288654; Training Loss 0.00464300853788684\n",
      "Episode 15477; Testing Loss 0.005790576664073257; Training Loss 0.00464299867674026\n",
      "Episode 15478; Testing Loss 0.005790757120850946; Training Loss 0.004642992164483666\n",
      "Episode 15479; Testing Loss 0.005790586464500999; Training Loss 0.004642982999484262\n",
      "Episode 15480; Testing Loss 0.005790427026234911; Training Loss 0.0046429761911760264\n",
      "Episode 15481; Testing Loss 0.005790531404113365; Training Loss 0.0046429648632541586\n",
      "Episode 15482; Testing Loss 0.005790643550985623; Training Loss 0.004642957075706898\n",
      "Episode 15483; Testing Loss 0.005790542004903964; Training Loss 0.004642947573764047\n",
      "Episode 15484; Testing Loss 0.005790418992725851; Training Loss 0.004642940561854718\n",
      "Episode 15485; Testing Loss 0.005790336741482474; Training Loss 0.004642932294999833\n",
      "Episode 15486; Testing Loss 0.005790453902047009; Training Loss 0.004642921238603835\n",
      "Episode 15487; Testing Loss 0.005790600690677761; Training Loss 0.004642914753548494\n",
      "Episode 15488; Testing Loss 0.005790610181247338; Training Loss 0.004642905751942945\n",
      "Episode 15489; Testing Loss 0.005790524919083468; Training Loss 0.004642897192096317\n",
      "Episode 15490; Testing Loss 0.005790468820475925; Training Loss 0.004642889835624514\n",
      "Episode 15491; Testing Loss 0.005790489546171372; Training Loss 0.004642884558537746\n",
      "Episode 15492; Testing Loss 0.005790414854184501; Training Loss 0.004642873090414452\n",
      "Episode 15493; Testing Loss 0.005790399575473387; Training Loss 0.004642862532406148\n",
      "Episode 15494; Testing Loss 0.005790494429733109; Training Loss 0.004642858469481\n",
      "Episode 15495; Testing Loss 0.005790537664129496; Training Loss 0.004642850353236547\n",
      "Episode 15496; Testing Loss 0.005790503561540724; Training Loss 0.004642839753720106\n",
      "Episode 15497; Testing Loss 0.00579040131834278; Training Loss 0.0046428289475744025\n",
      "Episode 15498; Testing Loss 0.00579040003294287; Training Loss 0.004642821845825681\n",
      "Episode 15499; Testing Loss 0.005790473807838955; Training Loss 0.004642813787490519\n",
      "Episode 15500; Testing Loss 0.0057904524959627056; Training Loss 0.004642804879121225\n",
      "Episode 15501; Testing Loss 0.005790424118792673; Training Loss 0.004642795634496003\n",
      "Episode 15502; Testing Loss 0.005790396367407484; Training Loss 0.004642786708386206\n",
      "Episode 15503; Testing Loss 0.005790337916712634; Training Loss 0.004642778100164845\n",
      "Episode 15504; Testing Loss 0.005790382596098057; Training Loss 0.004642769873191529\n",
      "Episode 15505; Testing Loss 0.005790464967745255; Training Loss 0.004642762408097269\n",
      "Episode 15506; Testing Loss 0.005790409305318514; Training Loss 0.0046427543421218905\n",
      "Episode 15507; Testing Loss 0.005790386982325524; Training Loss 0.0046427448716033675\n",
      "Episode 15508; Testing Loss 0.005790397922671896; Training Loss 0.004642735490460492\n",
      "Episode 15509; Testing Loss 0.00579037857072049; Training Loss 0.004642727286698759\n",
      "Episode 15510; Testing Loss 0.0057902864641988985; Training Loss 0.004642719108650857\n",
      "Episode 15511; Testing Loss 0.005790321066273408; Training Loss 0.004642710957503308\n",
      "Episode 15512; Testing Loss 0.005790360870386446; Training Loss 0.004642702293806094\n",
      "Episode 15513; Testing Loss 0.0057903669643874275; Training Loss 0.004642692546553964\n",
      "Episode 15514; Testing Loss 0.005790328929604684; Training Loss 0.004642683712883233\n",
      "Episode 15515; Testing Loss 0.0057903813567277244; Training Loss 0.004642675183663344\n",
      "Episode 15516; Testing Loss 0.005790395248280361; Training Loss 0.0046426672759343765\n",
      "Episode 15517; Testing Loss 0.005790260632393873; Training Loss 0.004642658578471758\n",
      "Episode 15518; Testing Loss 0.005790237944811128; Training Loss 0.004642649544690768\n",
      "Episode 15519; Testing Loss 0.005790341717793857; Training Loss 0.004642641378435607\n",
      "Episode 15520; Testing Loss 0.005790411953922202; Training Loss 0.004642632409749713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15521; Testing Loss 0.005790378901979921; Training Loss 0.004642625309159451\n",
      "Episode 15522; Testing Loss 0.005790242204935552; Training Loss 0.004642618329886118\n",
      "Episode 15523; Testing Loss 0.0057902490975413065; Training Loss 0.0046426098951731455\n",
      "Episode 15524; Testing Loss 0.0057903072430264295; Training Loss 0.0046425998412539665\n",
      "Episode 15525; Testing Loss 0.005790346482233446; Training Loss 0.0046425907069476335\n",
      "Episode 15526; Testing Loss 0.00579030883622649; Training Loss 0.004642584051801351\n",
      "Episode 15527; Testing Loss 0.005790224508678534; Training Loss 0.004642574610342702\n",
      "Episode 15528; Testing Loss 0.005790160628259494; Training Loss 0.004642564855041534\n",
      "Episode 15529; Testing Loss 0.005790139292990339; Training Loss 0.004642558408330808\n",
      "Episode 15530; Testing Loss 0.0057902149781556; Training Loss 0.004642550731622568\n",
      "Episode 15531; Testing Loss 0.005790261622118411; Training Loss 0.004642542770687959\n",
      "Episode 15532; Testing Loss 0.005790191293176049; Training Loss 0.004642533637897338\n",
      "Episode 15533; Testing Loss 0.0057901310028479844; Training Loss 0.00464252332201022\n",
      "Episode 15534; Testing Loss 0.005790253549801063; Training Loss 0.0046425150264141745\n",
      "Episode 15535; Testing Loss 0.005790417148228501; Training Loss 0.004642510938243494\n",
      "Episode 15536; Testing Loss 0.005790287187298025; Training Loss 0.004642501182050014\n",
      "Episode 15537; Testing Loss 0.005790016897747816; Training Loss 0.004642491083017559\n",
      "Episode 15538; Testing Loss 0.005789943885274615; Training Loss 0.004642483052882243\n",
      "Episode 15539; Testing Loss 0.005790208728585925; Training Loss 0.004642474094033424\n",
      "Episode 15540; Testing Loss 0.005790338514640174; Training Loss 0.0046424682496630995\n",
      "Episode 15541; Testing Loss 0.00579014504614479; Training Loss 0.004642456208444796\n",
      "Episode 15542; Testing Loss 0.005790033581242539; Training Loss 0.004642448186299087\n",
      "Episode 15543; Testing Loss 0.0057902302033992934; Training Loss 0.004642436291377723\n",
      "Episode 15544; Testing Loss 0.005790324250097864; Training Loss 0.00464242965499122\n",
      "Episode 15545; Testing Loss 0.0057901240336964; Training Loss 0.004642419851064891\n",
      "Episode 15546; Testing Loss 0.005790025441740345; Training Loss 0.004642411889020863\n",
      "Episode 15547; Testing Loss 0.005790102130754443; Training Loss 0.004642403077252209\n",
      "Episode 15548; Testing Loss 0.005790162746994995; Training Loss 0.004642394956979681\n",
      "Episode 15549; Testing Loss 0.005790070773047708; Training Loss 0.004642384857977757\n",
      "Episode 15550; Testing Loss 0.005790008569759942; Training Loss 0.004642378723726416\n",
      "Episode 15551; Testing Loss 0.005790070464439997; Training Loss 0.0046423690848040235\n",
      "Episode 15552; Testing Loss 0.0057901245443848825; Training Loss 0.004642360957373242\n",
      "Episode 15553; Testing Loss 0.00579001147936229; Training Loss 0.0046423514721829285\n",
      "Episode 15554; Testing Loss 0.0057899808027324345; Training Loss 0.00464234304404309\n",
      "Episode 15555; Testing Loss 0.005790061563566598; Training Loss 0.004642334388849525\n",
      "Episode 15556; Testing Loss 0.005790092516850562; Training Loss 0.00464232738182258\n",
      "Episode 15557; Testing Loss 0.0057900062634967395; Training Loss 0.004642318736929831\n",
      "Episode 15558; Testing Loss 0.005789986830494244; Training Loss 0.004642309171315908\n",
      "Episode 15559; Testing Loss 0.005790011146784333; Training Loss 0.004642300121685144\n",
      "Episode 15560; Testing Loss 0.005790033399282554; Training Loss 0.004642291779684096\n",
      "Episode 15561; Testing Loss 0.005790011860019536; Training Loss 0.004642282824718202\n",
      "Episode 15562; Testing Loss 0.005789911819392227; Training Loss 0.0046422762765118255\n",
      "Episode 15563; Testing Loss 0.005790003206101078; Training Loss 0.004642267093624511\n",
      "Episode 15564; Testing Loss 0.0057901230688514466; Training Loss 0.004642258546131913\n",
      "Episode 15565; Testing Loss 0.005790012910336465; Training Loss 0.00464225159125287\n",
      "Episode 15566; Testing Loss 0.0057898340938322395; Training Loss 0.004642242821603862\n",
      "Episode 15567; Testing Loss 0.005789952172124895; Training Loss 0.004642232291925143\n",
      "Episode 15568; Testing Loss 0.005790073482735308; Training Loss 0.004642225584617014\n",
      "Episode 15569; Testing Loss 0.005789961708196029; Training Loss 0.004642217647489608\n",
      "Episode 15570; Testing Loss 0.005789898163739579; Training Loss 0.004642206664836498\n",
      "Episode 15571; Testing Loss 0.00578990138031379; Training Loss 0.0046421994530243295\n",
      "Episode 15572; Testing Loss 0.005789933410264275; Training Loss 0.00464219113029269\n",
      "Episode 15573; Testing Loss 0.005789868370358139; Training Loss 0.004642180870189045\n",
      "Episode 15574; Testing Loss 0.0057898890528291; Training Loss 0.004642175459892346\n",
      "Episode 15575; Testing Loss 0.005789959329408203; Training Loss 0.004642165384919115\n",
      "Episode 15576; Testing Loss 0.00578994951481446; Training Loss 0.004642156744982872\n",
      "Episode 15577; Testing Loss 0.005789745861271378; Training Loss 0.00464214781828808\n",
      "Episode 15578; Testing Loss 0.005789734803909701; Training Loss 0.004642139332056073\n",
      "Episode 15579; Testing Loss 0.005789901974758237; Training Loss 0.004642132601972564\n",
      "Episode 15580; Testing Loss 0.005789953299662654; Training Loss 0.0046421244757116386\n",
      "Episode 15581; Testing Loss 0.005789825695469768; Training Loss 0.004642114058055507\n",
      "Episode 15582; Testing Loss 0.00578971315902813; Training Loss 0.004642106544115792\n",
      "Episode 15583; Testing Loss 0.005789769944792141; Training Loss 0.0046420985579120715\n",
      "Episode 15584; Testing Loss 0.005789834538411833; Training Loss 0.004642089537926066\n",
      "Episode 15585; Testing Loss 0.005789863772021141; Training Loss 0.004642082531877422\n",
      "Episode 15586; Testing Loss 0.005789806106642398; Training Loss 0.004642073694296595\n",
      "Episode 15587; Testing Loss 0.005789779936635319; Training Loss 0.0046420643902915755\n",
      "Episode 15588; Testing Loss 0.005789885630829216; Training Loss 0.004642055489602235\n",
      "Episode 15589; Testing Loss 0.005789867702031497; Training Loss 0.004642047333236359\n",
      "Episode 15590; Testing Loss 0.005789739212266661; Training Loss 0.004642038287165095\n",
      "Episode 15591; Testing Loss 0.005789760540352884; Training Loss 0.004642029427766423\n",
      "Episode 15592; Testing Loss 0.005789825882756091; Training Loss 0.004642020796132499\n",
      "Episode 15593; Testing Loss 0.0057898116386847056; Training Loss 0.0046420134742772776\n",
      "Episode 15594; Testing Loss 0.005789727404658633; Training Loss 0.0046420041613545555\n",
      "Episode 15595; Testing Loss 0.0057897201778375484; Training Loss 0.004641994439344133\n",
      "Episode 15596; Testing Loss 0.005789772822402711; Training Loss 0.004641987475665424\n",
      "Episode 15597; Testing Loss 0.005789735618287453; Training Loss 0.00464197843621997\n",
      "Episode 15598; Testing Loss 0.00578969631450585; Training Loss 0.004641969539628532\n",
      "Episode 15599; Testing Loss 0.005789673738254827; Training Loss 0.0046419622962585675\n",
      "Episode 15600; Testing Loss 0.005789703936212645; Training Loss 0.0046419533922679515\n",
      "Episode 15601; Testing Loss 0.005789796693355454; Training Loss 0.004641945464066227\n",
      "Episode 15602; Testing Loss 0.0057897963032919955; Training Loss 0.00464193697075233\n",
      "Episode 15603; Testing Loss 0.005789646490817491; Training Loss 0.004641928009634772\n",
      "Episode 15604; Testing Loss 0.005789565107684137; Training Loss 0.004641923327261466\n",
      "Episode 15605; Testing Loss 0.0057896871257750665; Training Loss 0.004641912074486629\n",
      "Episode 15606; Testing Loss 0.005789758664626734; Training Loss 0.004641904342651156\n",
      "Episode 15607; Testing Loss 0.0057896787727576915; Training Loss 0.00464189846658558\n",
      "Episode 15608; Testing Loss 0.005789631176500295; Training Loss 0.004641888692946509\n",
      "Episode 15609; Testing Loss 0.005789671814919123; Training Loss 0.004641876538077739\n",
      "Episode 15610; Testing Loss 0.005789691045025658; Training Loss 0.004641869376250436\n",
      "Episode 15611; Testing Loss 0.005789664289082854; Training Loss 0.004641861467260728\n",
      "Episode 15612; Testing Loss 0.0057896201606877406; Training Loss 0.004641853577299861\n",
      "Episode 15613; Testing Loss 0.005789513985343467; Training Loss 0.00464184363466087\n",
      "Episode 15614; Testing Loss 0.005789509752993467; Training Loss 0.0046418354518603395\n",
      "Episode 15615; Testing Loss 0.00578958722231133; Training Loss 0.0046418273747566696\n",
      "Episode 15616; Testing Loss 0.005789636119530448; Training Loss 0.00464181794315123\n",
      "Episode 15617; Testing Loss 0.00578961965031276; Training Loss 0.004641810665497987\n",
      "Episode 15618; Testing Loss 0.005789599142973957; Training Loss 0.004641801651367957\n",
      "Episode 15619; Testing Loss 0.005789523305050908; Training Loss 0.004641793434492161\n",
      "Episode 15620; Testing Loss 0.005789514086716222; Training Loss 0.00464178504315092\n",
      "Episode 15621; Testing Loss 0.00578963620282953; Training Loss 0.0046417762090773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15622; Testing Loss 0.005789610658401266; Training Loss 0.004641768847648709\n",
      "Episode 15623; Testing Loss 0.005789449249094114; Training Loss 0.004641758559073534\n",
      "Episode 15624; Testing Loss 0.005789406101957391; Training Loss 0.004641751763493849\n",
      "Episode 15625; Testing Loss 0.005789541010823963; Training Loss 0.0046417436057258465\n",
      "Episode 15626; Testing Loss 0.005789669802390042; Training Loss 0.004641736326924056\n",
      "Episode 15627; Testing Loss 0.005789535045571058; Training Loss 0.004641725793571499\n",
      "Episode 15628; Testing Loss 0.005789445680016771; Training Loss 0.004641717027226758\n",
      "Episode 15629; Testing Loss 0.005789543727661248; Training Loss 0.004641709342623772\n",
      "Episode 15630; Testing Loss 0.0057895475908293715; Training Loss 0.004641701312663565\n",
      "Episode 15631; Testing Loss 0.005789384813308921; Training Loss 0.004641690709073073\n",
      "Episode 15632; Testing Loss 0.005789364266438082; Training Loss 0.004641682898713135\n",
      "Episode 15633; Testing Loss 0.005789470169104001; Training Loss 0.004641676838340376\n",
      "Episode 15634; Testing Loss 0.005789458815054273; Training Loss 0.0046416661955058485\n",
      "Episode 15635; Testing Loss 0.005789410739782907; Training Loss 0.004641656916980061\n",
      "Episode 15636; Testing Loss 0.005789432998400039; Training Loss 0.004641647208521186\n",
      "Episode 15637; Testing Loss 0.005789474704626954; Training Loss 0.004641637657509163\n",
      "Episode 15638; Testing Loss 0.005789363775745871; Training Loss 0.004641629764783424\n",
      "Episode 15639; Testing Loss 0.005789277560584467; Training Loss 0.00464162144703417\n",
      "Episode 15640; Testing Loss 0.005789384671098813; Training Loss 0.004641613133123689\n",
      "Episode 15641; Testing Loss 0.0057895254346685635; Training Loss 0.0046416042447659615\n",
      "Episode 15642; Testing Loss 0.005789397168600124; Training Loss 0.004641593982735602\n",
      "Episode 15643; Testing Loss 0.005789227553704892; Training Loss 0.0046415881286117925\n",
      "Episode 15644; Testing Loss 0.005789312371708115; Training Loss 0.004641577666831624\n",
      "Episode 15645; Testing Loss 0.005789428456634002; Training Loss 0.004641568454041541\n",
      "Episode 15646; Testing Loss 0.005789393186385184; Training Loss 0.004641559855719943\n",
      "Episode 15647; Testing Loss 0.005789258748232187; Training Loss 0.004641552021798228\n",
      "Episode 15648; Testing Loss 0.005789313597965193; Training Loss 0.004641541440176128\n",
      "Episode 15649; Testing Loss 0.005789377259407671; Training Loss 0.0046415360325386176\n",
      "Episode 15650; Testing Loss 0.005789237464121143; Training Loss 0.004641525975848383\n",
      "Episode 15651; Testing Loss 0.005789136899953153; Training Loss 0.004641517645795579\n",
      "Episode 15652; Testing Loss 0.0057893134047317715; Training Loss 0.004641508548547614\n",
      "Episode 15653; Testing Loss 0.005789381460061579; Training Loss 0.004641499552272686\n",
      "Episode 15654; Testing Loss 0.005789257193236676; Training Loss 0.0046414904214883475\n",
      "Episode 15655; Testing Loss 0.005789279133865931; Training Loss 0.004641483199279333\n",
      "Episode 15656; Testing Loss 0.005789290434262565; Training Loss 0.004641475531576282\n",
      "Episode 15657; Testing Loss 0.005789178611310136; Training Loss 0.0046414642456723216\n",
      "Episode 15658; Testing Loss 0.005789159282953287; Training Loss 0.0046414568892660215\n",
      "Episode 15659; Testing Loss 0.005789252958999677; Training Loss 0.0046414482019823365\n",
      "Episode 15660; Testing Loss 0.005789186315998674; Training Loss 0.004641438546519883\n",
      "Episode 15661; Testing Loss 0.005789122227371473; Training Loss 0.0046414300443781776\n",
      "Episode 15662; Testing Loss 0.005789086283702385; Training Loss 0.0046414209079576945\n",
      "Episode 15663; Testing Loss 0.0057892232314440744; Training Loss 0.004641414284702514\n",
      "Episode 15664; Testing Loss 0.0057894090758997254; Training Loss 0.004641406818980999\n",
      "Episode 15665; Testing Loss 0.005789235324642689; Training Loss 0.004641395668356169\n",
      "Episode 15666; Testing Loss 0.005788991747091197; Training Loss 0.004641388250889739\n",
      "Episode 15667; Testing Loss 0.005789039715512848; Training Loss 0.0046413775463489855\n",
      "Episode 15668; Testing Loss 0.005789204868818851; Training Loss 0.004641373022453106\n",
      "Episode 15669; Testing Loss 0.005789169866559941; Training Loss 0.004641363840937489\n",
      "Episode 15670; Testing Loss 0.005789031170366125; Training Loss 0.004641352785673177\n",
      "Episode 15671; Testing Loss 0.005789079316378056; Training Loss 0.004641344983151448\n",
      "Episode 15672; Testing Loss 0.005789278370420022; Training Loss 0.004641337920942347\n",
      "Episode 15673; Testing Loss 0.005789281775669484; Training Loss 0.004641330120816367\n",
      "Episode 15674; Testing Loss 0.005789084902852409; Training Loss 0.004641320846588782\n",
      "Episode 15675; Testing Loss 0.005789008847726082; Training Loss 0.004641311400988357\n",
      "Episode 15676; Testing Loss 0.005789117963571772; Training Loss 0.004641301269966378\n",
      "Episode 15677; Testing Loss 0.005789060235894951; Training Loss 0.00464129470944739\n",
      "Episode 15678; Testing Loss 0.005788857970212002; Training Loss 0.00464128568944894\n",
      "Episode 15679; Testing Loss 0.005788949687889584; Training Loss 0.004641274485660615\n",
      "Episode 15680; Testing Loss 0.005789202035227394; Training Loss 0.004641266921039771\n",
      "Episode 15681; Testing Loss 0.005789178264117713; Training Loss 0.004641258553920156\n",
      "Episode 15682; Testing Loss 0.00578903630454764; Training Loss 0.004641249251427606\n",
      "Episode 15683; Testing Loss 0.005788996984656563; Training Loss 0.004641239720116684\n",
      "Episode 15684; Testing Loss 0.005789042013877711; Training Loss 0.004641232011571646\n",
      "Episode 15685; Testing Loss 0.005788946465375123; Training Loss 0.0046412233033002815\n",
      "Episode 15686; Testing Loss 0.005788848077259376; Training Loss 0.004641216492585939\n",
      "Episode 15687; Testing Loss 0.005788967462773653; Training Loss 0.004641206389089706\n",
      "Episode 15688; Testing Loss 0.005789054076680142; Training Loss 0.004641196994599488\n",
      "Episode 15689; Testing Loss 0.00578896349189419; Training Loss 0.004641189610066457\n",
      "Episode 15690; Testing Loss 0.0057888799505904235; Training Loss 0.004641181238259027\n",
      "Episode 15691; Testing Loss 0.00578899129465875; Training Loss 0.004641171258530455\n",
      "Episode 15692; Testing Loss 0.005789048006407758; Training Loss 0.004641164062814589\n",
      "Episode 15693; Testing Loss 0.0057889326615207055; Training Loss 0.004641155064713379\n",
      "Episode 15694; Testing Loss 0.005788877223756794; Training Loss 0.00464114679658185\n",
      "Episode 15695; Testing Loss 0.005788972440582563; Training Loss 0.004641136456803956\n",
      "Episode 15696; Testing Loss 0.005788973902407337; Training Loss 0.004641130378337974\n",
      "Episode 15697; Testing Loss 0.005788822462357584; Training Loss 0.004641121139801187\n",
      "Episode 15698; Testing Loss 0.0057887650603002825; Training Loss 0.004641112474386762\n",
      "Episode 15699; Testing Loss 0.0057889760519991085; Training Loss 0.00464110298760707\n",
      "Episode 15700; Testing Loss 0.005789085836654127; Training Loss 0.0046410948064284745\n",
      "Episode 15701; Testing Loss 0.005788917738924781; Training Loss 0.0046410872160024385\n",
      "Episode 15702; Testing Loss 0.005788872682089792; Training Loss 0.004641080033894339\n",
      "Episode 15703; Testing Loss 0.005788909655289211; Training Loss 0.00464107136323987\n",
      "Episode 15704; Testing Loss 0.005788852486648137; Training Loss 0.004641058885796747\n",
      "Episode 15705; Testing Loss 0.005788816722381456; Training Loss 0.004641051999584457\n",
      "Episode 15706; Testing Loss 0.005788872004556694; Training Loss 0.004641043710864125\n",
      "Episode 15707; Testing Loss 0.005788793831096524; Training Loss 0.004641034088129921\n",
      "Episode 15708; Testing Loss 0.005788834112250843; Training Loss 0.004641025832392069\n",
      "Episode 15709; Testing Loss 0.0057888970811337745; Training Loss 0.00464101646986899\n",
      "Episode 15710; Testing Loss 0.005788825759321645; Training Loss 0.004641008432703095\n",
      "Episode 15711; Testing Loss 0.005788784729777989; Training Loss 0.004641000883185371\n",
      "Episode 15712; Testing Loss 0.005788829663065863; Training Loss 0.004640991885297974\n",
      "Episode 15713; Testing Loss 0.005788852149843576; Training Loss 0.0046409833822711214\n",
      "Episode 15714; Testing Loss 0.00578875065978886; Training Loss 0.004640972949543305\n",
      "Episode 15715; Testing Loss 0.005788791799638752; Training Loss 0.004640967648875544\n",
      "Episode 15716; Testing Loss 0.005788827155483066; Training Loss 0.004640958942159271\n",
      "Episode 15717; Testing Loss 0.0057887749167200315; Training Loss 0.004640947905060872\n",
      "Episode 15718; Testing Loss 0.005788718643287337; Training Loss 0.004640941605705501\n",
      "Episode 15719; Testing Loss 0.005788781869919918; Training Loss 0.004640934328830162\n",
      "Episode 15720; Testing Loss 0.005788848265271387; Training Loss 0.004640926212376004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15721; Testing Loss 0.005788831986869446; Training Loss 0.004640918051211553\n",
      "Episode 15722; Testing Loss 0.005788772012415376; Training Loss 0.0046409088587278675\n",
      "Episode 15723; Testing Loss 0.005788770685162545; Training Loss 0.0046408975763792175\n",
      "Episode 15724; Testing Loss 0.005788778940419762; Training Loss 0.004640888174202064\n",
      "Episode 15725; Testing Loss 0.005788682823466584; Training Loss 0.0046408804257908765\n",
      "Episode 15726; Testing Loss 0.00578861443931018; Training Loss 0.004640871954114298\n",
      "Episode 15727; Testing Loss 0.005788690634155163; Training Loss 0.00464086393679659\n",
      "Episode 15728; Testing Loss 0.005788803813222822; Training Loss 0.004640855108713521\n",
      "Episode 15729; Testing Loss 0.005788695987778685; Training Loss 0.00464084652062173\n",
      "Episode 15730; Testing Loss 0.00578852858361528; Training Loss 0.0046408408349905075\n",
      "Episode 15731; Testing Loss 0.0057887255307458915; Training Loss 0.004640829090076589\n",
      "Episode 15732; Testing Loss 0.005788957213589553; Training Loss 0.004640823154850504\n",
      "Episode 15733; Testing Loss 0.005788739856751187; Training Loss 0.00464081078852603\n",
      "Episode 15734; Testing Loss 0.005788477208470342; Training Loss 0.004640806848264112\n",
      "Episode 15735; Testing Loss 0.005788599939575802; Training Loss 0.004640794293436386\n",
      "Episode 15736; Testing Loss 0.005788720527980475; Training Loss 0.004640787657122631\n",
      "Episode 15737; Testing Loss 0.005788616204880102; Training Loss 0.004640777018363615\n",
      "Episode 15738; Testing Loss 0.005788556180218166; Training Loss 0.004640768775217522\n",
      "Episode 15739; Testing Loss 0.005788707704652187; Training Loss 0.004640761472831821\n",
      "Episode 15740; Testing Loss 0.005788742808061615; Training Loss 0.004640752677352586\n",
      "Episode 15741; Testing Loss 0.005788623498433077; Training Loss 0.0046407443737028695\n",
      "Episode 15742; Testing Loss 0.005788584030988308; Training Loss 0.004640736147075068\n",
      "Episode 15743; Testing Loss 0.005788621819371255; Training Loss 0.004640727794849874\n",
      "Episode 15744; Testing Loss 0.0057886412193569395; Training Loss 0.004640719593950835\n",
      "Episode 15745; Testing Loss 0.005788509448170385; Training Loss 0.004640709244741538\n",
      "Episode 15746; Testing Loss 0.005788525666649596; Training Loss 0.004640703281198956\n",
      "Episode 15747; Testing Loss 0.00578863583563544; Training Loss 0.004640694354595121\n",
      "Episode 15748; Testing Loss 0.005788660312864701; Training Loss 0.0046406839400470815\n",
      "Episode 15749; Testing Loss 0.005788579896684947; Training Loss 0.004640676441649858\n",
      "Episode 15750; Testing Loss 0.005788539158271482; Training Loss 0.004640669984313524\n",
      "Episode 15751; Testing Loss 0.005788617447405407; Training Loss 0.004640661718187391\n",
      "Episode 15752; Testing Loss 0.005788672560151356; Training Loss 0.004640652988433376\n",
      "Episode 15753; Testing Loss 0.0057886242623859; Training Loss 0.0046406428573693564\n",
      "Episode 15754; Testing Loss 0.005788511565813202; Training Loss 0.004640633737698484\n",
      "Episode 15755; Testing Loss 0.005788535299408366; Training Loss 0.004640623252469107\n",
      "Episode 15756; Testing Loss 0.005788529766201881; Training Loss 0.004640617026484218\n",
      "Episode 15757; Testing Loss 0.005788444739690542; Training Loss 0.004640606892921261\n",
      "Episode 15758; Testing Loss 0.005788516241837548; Training Loss 0.00464059849302767\n",
      "Episode 15759; Testing Loss 0.0057886700592249185; Training Loss 0.004640590255880283\n",
      "Episode 15760; Testing Loss 0.005788600324383805; Training Loss 0.004640582398621177\n",
      "Episode 15761; Testing Loss 0.005788453106859562; Training Loss 0.004640572329296349\n",
      "Episode 15762; Testing Loss 0.0057884253329004084; Training Loss 0.004640564037332403\n",
      "Episode 15763; Testing Loss 0.005788504284710927; Training Loss 0.004640557962126388\n",
      "Episode 15764; Testing Loss 0.005788507948698213; Training Loss 0.00464054685561774\n",
      "Episode 15765; Testing Loss 0.005788525494694713; Training Loss 0.004640537884825596\n",
      "Episode 15766; Testing Loss 0.005788523184657992; Training Loss 0.004640530540038125\n",
      "Episode 15767; Testing Loss 0.0057885128823529225; Training Loss 0.004640521789014158\n",
      "Episode 15768; Testing Loss 0.005788501398969902; Training Loss 0.004640513434204663\n",
      "Episode 15769; Testing Loss 0.0057884862088129615; Training Loss 0.004640504042601806\n",
      "Episode 15770; Testing Loss 0.005788494888089133; Training Loss 0.00464049689319048\n",
      "Episode 15771; Testing Loss 0.005788506614028443; Training Loss 0.004640487244196436\n",
      "Episode 15772; Testing Loss 0.00578846191644432; Training Loss 0.0046404793815125345\n",
      "Episode 15773; Testing Loss 0.005788372952613916; Training Loss 0.004640472518569325\n",
      "Episode 15774; Testing Loss 0.005788385143222174; Training Loss 0.004640463696165345\n",
      "Episode 15775; Testing Loss 0.005788451063409073; Training Loss 0.004640454222131083\n",
      "Episode 15776; Testing Loss 0.005788420243940339; Training Loss 0.004640444411468452\n",
      "Episode 15777; Testing Loss 0.005788403408760137; Training Loss 0.004640437962711836\n",
      "Episode 15778; Testing Loss 0.005788410457406581; Training Loss 0.004640429067352365\n",
      "Episode 15779; Testing Loss 0.005788484015673015; Training Loss 0.004640418971236082\n",
      "Episode 15780; Testing Loss 0.005788514173695061; Training Loss 0.004640410732297726\n",
      "Episode 15781; Testing Loss 0.005788438095112442; Training Loss 0.004640402065531849\n",
      "Episode 15782; Testing Loss 0.00578845003455775; Training Loss 0.004640396019138431\n",
      "Episode 15783; Testing Loss 0.005788392169399701; Training Loss 0.004640385339183026\n",
      "Episode 15784; Testing Loss 0.005788408251595086; Training Loss 0.004640378215642199\n",
      "Episode 15785; Testing Loss 0.005788510236590157; Training Loss 0.004640371675908657\n",
      "Episode 15786; Testing Loss 0.0057883626214545675; Training Loss 0.00464036286176858\n",
      "Episode 15787; Testing Loss 0.005788335002840975; Training Loss 0.004640355826733832\n",
      "Episode 15788; Testing Loss 0.005788500081362731; Training Loss 0.004640345438829191\n",
      "Episode 15789; Testing Loss 0.0057884817911877705; Training Loss 0.004640337050792127\n",
      "Episode 15790; Testing Loss 0.005788275932035125; Training Loss 0.004640328433007692\n",
      "Episode 15791; Testing Loss 0.005788288333006479; Training Loss 0.004640320770040498\n",
      "Episode 15792; Testing Loss 0.005788543676272391; Training Loss 0.004640312569041787\n",
      "Episode 15793; Testing Loss 0.005788513958906611; Training Loss 0.004640303204385247\n",
      "Episode 15794; Testing Loss 0.005788263351266673; Training Loss 0.004640294481612975\n",
      "Episode 15795; Testing Loss 0.0057882053196545175; Training Loss 0.004640288634635382\n",
      "Episode 15796; Testing Loss 0.005788395093450985; Training Loss 0.004640278290569698\n",
      "Episode 15797; Testing Loss 0.005788501648825461; Training Loss 0.00464027049317099\n",
      "Episode 15798; Testing Loss 0.005788360137593812; Training Loss 0.004640261799851598\n",
      "Episode 15799; Testing Loss 0.005788257626796849; Training Loss 0.004640254772594079\n",
      "Episode 15800; Testing Loss 0.0057883527498950585; Training Loss 0.004640243941112902\n",
      "Episode 15801; Testing Loss 0.005788471356569475; Training Loss 0.00464023651206615\n",
      "Episode 15802; Testing Loss 0.005788342109189482; Training Loss 0.004640226430144532\n",
      "Episode 15803; Testing Loss 0.0057882094586587; Training Loss 0.004640218362966856\n",
      "Episode 15804; Testing Loss 0.005788376977821899; Training Loss 0.004640209117078683\n",
      "Episode 15805; Testing Loss 0.005788526524787426; Training Loss 0.004640202359348877\n",
      "Episode 15806; Testing Loss 0.005788353999888137; Training Loss 0.004640191081183199\n",
      "Episode 15807; Testing Loss 0.005788194136871225; Training Loss 0.00464018484485177\n",
      "Episode 15808; Testing Loss 0.005788223554677872; Training Loss 0.004640175632966856\n",
      "Episode 15809; Testing Loss 0.005788433160511504; Training Loss 0.004640167233800429\n",
      "Episode 15810; Testing Loss 0.005788435746774547; Training Loss 0.0046401591357480435\n",
      "Episode 15811; Testing Loss 0.0057882726674314995; Training Loss 0.004640151334552483\n",
      "Episode 15812; Testing Loss 0.005788258727946592; Training Loss 0.00464014263203312\n",
      "Episode 15813; Testing Loss 0.005788405811250155; Training Loss 0.004640133615387876\n",
      "Episode 15814; Testing Loss 0.005788391326502925; Training Loss 0.004640125057268851\n",
      "Episode 15815; Testing Loss 0.005788313798500629; Training Loss 0.004640116223478928\n",
      "Episode 15816; Testing Loss 0.005788296548329616; Training Loss 0.00464010904290184\n",
      "Episode 15817; Testing Loss 0.005788386996215751; Training Loss 0.00464010120633263\n",
      "Episode 15818; Testing Loss 0.005788329977335202; Training Loss 0.0046400906871350575\n",
      "Episode 15819; Testing Loss 0.005788251446766549; Training Loss 0.00464008286239547\n",
      "Episode 15820; Testing Loss 0.005788314926643767; Training Loss 0.004640073116032261\n",
      "Episode 15821; Testing Loss 0.005788334877859472; Training Loss 0.004640065086249956\n",
      "Episode 15822; Testing Loss 0.005788306882964607; Training Loss 0.0046400578884424875\n",
      "Episode 15823; Testing Loss 0.005788261752252876; Training Loss 0.004640049374714615\n",
      "Episode 15824; Testing Loss 0.005788279826957864; Training Loss 0.004640040233415008\n",
      "Episode 15825; Testing Loss 0.005788277667682827; Training Loss 0.00464003107492408\n",
      "Episode 15826; Testing Loss 0.005788137337218805; Training Loss 0.004640022844704489\n",
      "Episode 15827; Testing Loss 0.005788151350236735; Training Loss 0.004640014520955989\n",
      "Episode 15828; Testing Loss 0.0057882809606554465; Training Loss 0.004640005354294686\n",
      "Episode 15829; Testing Loss 0.005788323728004993; Training Loss 0.004639997461167108\n",
      "Episode 15830; Testing Loss 0.005788203666311776; Training Loss 0.004639988992045023\n",
      "Episode 15831; Testing Loss 0.005788197566763677; Training Loss 0.004639981072678568\n",
      "Episode 15832; Testing Loss 0.005788259437805694; Training Loss 0.004639971890709937\n",
      "Episode 15833; Testing Loss 0.005788224453210786; Training Loss 0.004639966825953376\n",
      "Episode 15834; Testing Loss 0.00578818162343558; Training Loss 0.0046399571453581865\n",
      "Episode 15835; Testing Loss 0.005788228558054879; Training Loss 0.004639950318747654\n",
      "Episode 15836; Testing Loss 0.005788345377828996; Training Loss 0.004639941732844205\n",
      "Episode 15837; Testing Loss 0.005788321407409087; Training Loss 0.004639930418700539\n",
      "Episode 15838; Testing Loss 0.005788117020127013; Training Loss 0.004639925549700154\n",
      "Episode 15839; Testing Loss 0.005788082518495882; Training Loss 0.004639919420781468\n",
      "Episode 15840; Testing Loss 0.0057882152413390375; Training Loss 0.004639906440652493\n",
      "Episode 15841; Testing Loss 0.0057882927611304855; Training Loss 0.0046398995009243015\n",
      "Episode 15842; Testing Loss 0.005788225365542712; Training Loss 0.0046398921669834\n",
      "Episode 15843; Testing Loss 0.005788211732364136; Training Loss 0.00463988468017503\n",
      "Episode 15844; Testing Loss 0.005788208273942446; Training Loss 0.004639876484970265\n",
      "Episode 15845; Testing Loss 0.005788208562383995; Training Loss 0.004639866094310004\n",
      "Episode 15846; Testing Loss 0.005788173866330177; Training Loss 0.00463985559178748\n",
      "Episode 15847; Testing Loss 0.0057881318405558005; Training Loss 0.004639848084243708\n",
      "Episode 15848; Testing Loss 0.00578808459814881; Training Loss 0.004639840535881488\n",
      "Episode 15849; Testing Loss 0.005788104243189295; Training Loss 0.004639829534678026\n",
      "Episode 15850; Testing Loss 0.005788157217288665; Training Loss 0.004639823176630514\n",
      "Episode 15851; Testing Loss 0.005788205802220584; Training Loss 0.004639816170699101\n",
      "Episode 15852; Testing Loss 0.00578819329413134; Training Loss 0.004639807908258325\n",
      "Episode 15853; Testing Loss 0.005788191831396247; Training Loss 0.004639799048641926\n",
      "Episode 15854; Testing Loss 0.0057881996960602875; Training Loss 0.004639789136443848\n",
      "Episode 15855; Testing Loss 0.005788146640648949; Training Loss 0.0046397781497255625\n",
      "Episode 15856; Testing Loss 0.0057880625373581095; Training Loss 0.004639773589228692\n",
      "Episode 15857; Testing Loss 0.00578799536206042; Training Loss 0.00463976682920356\n",
      "Episode 15858; Testing Loss 0.005788081302084848; Training Loss 0.004639755091266554\n",
      "Episode 15859; Testing Loss 0.005788111933161598; Training Loss 0.004639747622034049\n",
      "Episode 15860; Testing Loss 0.005788075530751376; Training Loss 0.0046397402533259425\n",
      "Episode 15861; Testing Loss 0.005788187186877735; Training Loss 0.004639731838336925\n",
      "Episode 15862; Testing Loss 0.005788238520168802; Training Loss 0.004639724696551955\n",
      "Episode 15863; Testing Loss 0.0057881109869130365; Training Loss 0.004639713709163084\n",
      "Episode 15864; Testing Loss 0.005787981145484423; Training Loss 0.004639704955619031\n",
      "Episode 15865; Testing Loss 0.005788021263603242; Training Loss 0.0046396970959186115\n",
      "Episode 15866; Testing Loss 0.005788133128171072; Training Loss 0.004639690776704757\n",
      "Episode 15867; Testing Loss 0.005788098060597876; Training Loss 0.004639681768410482\n",
      "Episode 15868; Testing Loss 0.005787999206089779; Training Loss 0.004639671747556209\n",
      "Episode 15869; Testing Loss 0.005788045040854727; Training Loss 0.004639664228552207\n",
      "Episode 15870; Testing Loss 0.005788077804793657; Training Loss 0.004639656279029195\n",
      "Episode 15871; Testing Loss 0.00578800974112302; Training Loss 0.004639646768046361\n",
      "Episode 15872; Testing Loss 0.00578809995070295; Training Loss 0.00463963774833558\n",
      "Episode 15873; Testing Loss 0.005788180468945187; Training Loss 0.004639629451337849\n",
      "Episode 15874; Testing Loss 0.005788071851563135; Training Loss 0.0046396230769893704\n",
      "Episode 15875; Testing Loss 0.005787924712528049; Training Loss 0.004639614263817575\n",
      "Episode 15876; Testing Loss 0.00578791662704475; Training Loss 0.004639603670589169\n",
      "Episode 15877; Testing Loss 0.005788018323713926; Training Loss 0.004639596322850725\n",
      "Episode 15878; Testing Loss 0.005788116853166882; Training Loss 0.004639590558332873\n",
      "Episode 15879; Testing Loss 0.005788071842680104; Training Loss 0.004639581525073894\n",
      "Episode 15880; Testing Loss 0.005788008276003845; Training Loss 0.004639571948264849\n",
      "Episode 15881; Testing Loss 0.005788055870784676; Training Loss 0.004639561534091859\n",
      "Episode 15882; Testing Loss 0.005788087127462425; Training Loss 0.004639552167749484\n",
      "Episode 15883; Testing Loss 0.005787971493952779; Training Loss 0.0046395471620908416\n",
      "Episode 15884; Testing Loss 0.00578788586815918; Training Loss 0.00463954039270283\n",
      "Episode 15885; Testing Loss 0.00578787676626737; Training Loss 0.004639528218314506\n",
      "Episode 15886; Testing Loss 0.005788020592075989; Training Loss 0.00463952009742587\n",
      "Episode 15887; Testing Loss 0.00578814786717058; Training Loss 0.004639515190936839\n",
      "Episode 15888; Testing Loss 0.005787964228695668; Training Loss 0.004639505251897711\n",
      "Episode 15889; Testing Loss 0.00578795957529313; Training Loss 0.004639496508465674\n",
      "Episode 15890; Testing Loss 0.005788140896600425; Training Loss 0.004639489971911221\n",
      "Episode 15891; Testing Loss 0.005788029738485127; Training Loss 0.004639477311839939\n",
      "Episode 15892; Testing Loss 0.005787854510144719; Training Loss 0.004639472032990053\n",
      "Episode 15893; Testing Loss 0.005787913864845682; Training Loss 0.004639463050402432\n",
      "Episode 15894; Testing Loss 0.0057880111233757315; Training Loss 0.004639452877005613\n",
      "Episode 15895; Testing Loss 0.005787944593144916; Training Loss 0.004639445116327787\n",
      "Episode 15896; Testing Loss 0.005787896362078316; Training Loss 0.0046394380764087995\n",
      "Episode 15897; Testing Loss 0.00578800528114872; Training Loss 0.004639429628129469\n",
      "Episode 15898; Testing Loss 0.005788065033032846; Training Loss 0.0046394220862421304\n",
      "Episode 15899; Testing Loss 0.005787919490725883; Training Loss 0.004639412106629091\n",
      "Episode 15900; Testing Loss 0.005787947283714596; Training Loss 0.004639402353472048\n",
      "Episode 15901; Testing Loss 0.005788080799063039; Training Loss 0.004639398636268598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15902; Testing Loss 0.005787904374616396; Training Loss 0.004639388829280526\n",
      "Episode 15903; Testing Loss 0.005787738703944025; Training Loss 0.0046393803174972015\n",
      "Episode 15904; Testing Loss 0.005787858325046805; Training Loss 0.004639370056346803\n",
      "Episode 15905; Testing Loss 0.005788066366233867; Training Loss 0.004639363874185817\n",
      "Episode 15906; Testing Loss 0.005788025206201253; Training Loss 0.004639354765967003\n",
      "Episode 15907; Testing Loss 0.005787886898453438; Training Loss 0.004639347635365618\n",
      "Episode 15908; Testing Loss 0.005787920441953877; Training Loss 0.004639337296827864\n",
      "Episode 15909; Testing Loss 0.005788031697426777; Training Loss 0.004639327966714099\n",
      "Episode 15910; Testing Loss 0.00578792643268999; Training Loss 0.004639321396837838\n",
      "Episode 15911; Testing Loss 0.005787723749748131; Training Loss 0.004639313745770665\n",
      "Episode 15912; Testing Loss 0.005787690667095606; Training Loss 0.004639304493182346\n",
      "Episode 15913; Testing Loss 0.005787891086587871; Training Loss 0.00463929650346228\n",
      "Episode 15914; Testing Loss 0.005787889241064928; Training Loss 0.004639291091237769\n",
      "Episode 15915; Testing Loss 0.005787725768662639; Training Loss 0.004639281389221622\n",
      "Episode 15916; Testing Loss 0.00578773737859942; Training Loss 0.004639271756695182\n",
      "Episode 15917; Testing Loss 0.0057879271634104015; Training Loss 0.0046392604932329155\n",
      "Episode 15918; Testing Loss 0.005787974958449183; Training Loss 0.004639252993683818\n",
      "Episode 15919; Testing Loss 0.005787840711517635; Training Loss 0.0046392441665769665\n",
      "Episode 15920; Testing Loss 0.005787716172102556; Training Loss 0.0046392366114222545\n",
      "Episode 15921; Testing Loss 0.005787823477032143; Training Loss 0.004639228063256349\n",
      "Episode 15922; Testing Loss 0.005787842176132159; Training Loss 0.004639220173220661\n",
      "Episode 15923; Testing Loss 0.005787810779306192; Training Loss 0.00463921313799737\n",
      "Episode 15924; Testing Loss 0.005787796437126798; Training Loss 0.004639201916780816\n",
      "Episode 15925; Testing Loss 0.005787795998547833; Training Loss 0.004639196004820606\n",
      "Episode 15926; Testing Loss 0.005787714450766755; Training Loss 0.004639188597381773\n",
      "Episode 15927; Testing Loss 0.005787721111997201; Training Loss 0.004639176984572303\n",
      "Episode 15928; Testing Loss 0.00578784390780219; Training Loss 0.004639169249461689\n",
      "Episode 15929; Testing Loss 0.005787884891967351; Training Loss 0.004639163304094116\n",
      "Episode 15930; Testing Loss 0.00578785774218069; Training Loss 0.004639153099536122\n",
      "Episode 15931; Testing Loss 0.005787776933790665; Training Loss 0.00463914350699091\n",
      "Episode 15932; Testing Loss 0.005787763478342688; Training Loss 0.004639137971339499\n",
      "Episode 15933; Testing Loss 0.005787769846014459; Training Loss 0.0046391289602533\n",
      "Episode 15934; Testing Loss 0.0057877529341144865; Training Loss 0.0046391200446197186\n",
      "Episode 15935; Testing Loss 0.005787741650093041; Training Loss 0.004639113309143751\n",
      "Episode 15936; Testing Loss 0.005787762569666967; Training Loss 0.004639103705807148\n",
      "Episode 15937; Testing Loss 0.00578779668835728; Training Loss 0.004639093366619183\n",
      "Episode 15938; Testing Loss 0.005787759972161184; Training Loss 0.004639085668327862\n",
      "Episode 15939; Testing Loss 0.005787728739850516; Training Loss 0.0046390794216156096\n",
      "Episode 15940; Testing Loss 0.005787717516521794; Training Loss 0.004639069808012722\n",
      "Episode 15941; Testing Loss 0.005787771413105637; Training Loss 0.0046390603182375825\n",
      "Episode 15942; Testing Loss 0.005787779015940578; Training Loss 0.004639050746747409\n",
      "Episode 15943; Testing Loss 0.005787728662620294; Training Loss 0.004639042596288708\n",
      "Episode 15944; Testing Loss 0.005787719008584885; Training Loss 0.004639034629503426\n",
      "Episode 15945; Testing Loss 0.005787731246736972; Training Loss 0.004639026798705541\n",
      "Episode 15946; Testing Loss 0.0057877763156376286; Training Loss 0.004639018327977901\n",
      "Episode 15947; Testing Loss 0.00578775366613746; Training Loss 0.0046390090388522\n",
      "Episode 15948; Testing Loss 0.00578760865076204; Training Loss 0.004639001923154461\n",
      "Episode 15949; Testing Loss 0.005787607443943544; Training Loss 0.00463899407231971\n",
      "Episode 15950; Testing Loss 0.005787750384994091; Training Loss 0.004638984790877631\n",
      "Episode 15951; Testing Loss 0.005787746340849367; Training Loss 0.0046389775013169245\n",
      "Episode 15952; Testing Loss 0.005787635876287439; Training Loss 0.004638968048882796\n",
      "Episode 15953; Testing Loss 0.005787580616369019; Training Loss 0.004638961137406612\n",
      "Episode 15954; Testing Loss 0.005787698898869958; Training Loss 0.004638953343059655\n",
      "Episode 15955; Testing Loss 0.005787706284829429; Training Loss 0.004638945193086498\n",
      "Episode 15956; Testing Loss 0.005787591401000797; Training Loss 0.004638935577899527\n",
      "Episode 15957; Testing Loss 0.005787564511909439; Training Loss 0.004638926233584729\n",
      "Episode 15958; Testing Loss 0.005787705028957282; Training Loss 0.004638917438609584\n",
      "Episode 15959; Testing Loss 0.005787697659958411; Training Loss 0.004638910031926402\n",
      "Episode 15960; Testing Loss 0.0057876098120738005; Training Loss 0.004638903452070569\n",
      "Episode 15961; Testing Loss 0.00578767057265482; Training Loss 0.0046388937125205025\n",
      "Episode 15962; Testing Loss 0.005787716436102916; Training Loss 0.0046388846338873595\n",
      "Episode 15963; Testing Loss 0.0057876512517927095; Training Loss 0.004638877044733841\n",
      "Episode 15964; Testing Loss 0.005787546311826072; Training Loss 0.004638869232788888\n",
      "Episode 15965; Testing Loss 0.005787514901913825; Training Loss 0.004638860229231687\n",
      "Episode 15966; Testing Loss 0.0057876613905559595; Training Loss 0.004638852358393995\n",
      "Episode 15967; Testing Loss 0.0057877203285313554; Training Loss 0.004638844805695004\n",
      "Episode 15968; Testing Loss 0.005787599674233652; Training Loss 0.004638834518870618\n",
      "Episode 15969; Testing Loss 0.005787558143577833; Training Loss 0.004638826266231133\n",
      "Episode 15970; Testing Loss 0.005787708341281927; Training Loss 0.004638817528768691\n",
      "Episode 15971; Testing Loss 0.005787724624817999; Training Loss 0.004638810119094796\n",
      "Episode 15972; Testing Loss 0.005787615806805581; Training Loss 0.004638802163186779\n",
      "Episode 15973; Testing Loss 0.005787497935464303; Training Loss 0.004638794377909421\n",
      "Episode 15974; Testing Loss 0.0057875676881153195; Training Loss 0.004638784981028813\n",
      "Episode 15975; Testing Loss 0.005787668599155575; Training Loss 0.004638776520561216\n",
      "Episode 15976; Testing Loss 0.005787581600033662; Training Loss 0.00463876987262999\n",
      "Episode 15977; Testing Loss 0.005787465353332594; Training Loss 0.00463876290892704\n",
      "Episode 15978; Testing Loss 0.005787554335587467; Training Loss 0.0046387511245896155\n",
      "Episode 15979; Testing Loss 0.005787617318800233; Training Loss 0.0046387452615269855\n",
      "Episode 15980; Testing Loss 0.005787566473188227; Training Loss 0.004638737208818861\n",
      "Episode 15981; Testing Loss 0.005787530751307493; Training Loss 0.004638730135378402\n",
      "Episode 15982; Testing Loss 0.00578762822039416; Training Loss 0.004638721149267115\n",
      "Episode 15983; Testing Loss 0.0057876470484778154; Training Loss 0.00463871152592641\n",
      "Episode 15984; Testing Loss 0.005787480522406988; Training Loss 0.0046387005719913865\n",
      "Episode 15985; Testing Loss 0.00578740523083412; Training Loss 0.00463869716549507\n",
      "Episode 15986; Testing Loss 0.005787537224851412; Training Loss 0.0046386888760846685\n",
      "Episode 15987; Testing Loss 0.005787562953277166; Training Loss 0.0046386784485044135\n",
      "Episode 15988; Testing Loss 0.005787431458401624; Training Loss 0.00463866919936722\n",
      "Episode 15989; Testing Loss 0.0057873543316577245; Training Loss 0.0046386634048414125\n",
      "Episode 15990; Testing Loss 0.005787579038241881; Training Loss 0.004638654253046936\n",
      "Episode 15991; Testing Loss 0.005787688750828638; Training Loss 0.004638647017719662\n",
      "Episode 15992; Testing Loss 0.005787518121228414; Training Loss 0.004638635152329654\n",
      "Episode 15993; Testing Loss 0.005787376582076203; Training Loss 0.004638625935982044\n",
      "Episode 15994; Testing Loss 0.005787429975842312; Training Loss 0.004638621129174066\n",
      "Episode 15995; Testing Loss 0.00578747067975107; Training Loss 0.004638614622174602\n",
      "Episode 15996; Testing Loss 0.005787487686336502; Training Loss 0.004638603117505363\n",
      "Episode 15997; Testing Loss 0.005787424730463856; Training Loss 0.004638592959422453\n",
      "Episode 15998; Testing Loss 0.005787425826494495; Training Loss 0.004638586970852944\n",
      "Episode 15999; Testing Loss 0.00578749652505466; Training Loss 0.004638580039134538\n",
      "Episode 16000; Testing Loss 0.005787449832537837; Training Loss 0.00463857095189921\n",
      "Episode 16001; Testing Loss 0.005787394025348305; Training Loss 0.004638562510134314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16002; Testing Loss 0.005787468521071676; Training Loss 0.004638551021751949\n",
      "Episode 16003; Testing Loss 0.005787504128778104; Training Loss 0.004638545649368765\n",
      "Episode 16004; Testing Loss 0.0057873322430668965; Training Loss 0.0046385377382999245\n",
      "Episode 16005; Testing Loss 0.005787284674629766; Training Loss 0.004638528222095976\n",
      "Episode 16006; Testing Loss 0.005787505105849673; Training Loss 0.0046385201647095396\n",
      "Episode 16007; Testing Loss 0.005787541316588541; Training Loss 0.004638512988215313\n",
      "Episode 16008; Testing Loss 0.005787426483662203; Training Loss 0.004638508454410446\n",
      "Episode 16009; Testing Loss 0.005787447064895854; Training Loss 0.004638496913522147\n",
      "Episode 16010; Testing Loss 0.005787589823476594; Training Loss 0.004638487478686563\n",
      "Episode 16011; Testing Loss 0.00578743664485267; Training Loss 0.00463847786014381\n",
      "Episode 16012; Testing Loss 0.005787262948955839; Training Loss 0.004638471516927839\n",
      "Episode 16013; Testing Loss 0.005787309750776687; Training Loss 0.004638464563407268\n",
      "Episode 16014; Testing Loss 0.0057874648543749675; Training Loss 0.0046384557414597336\n",
      "Episode 16015; Testing Loss 0.005787396879717123; Training Loss 0.004638446278192951\n",
      "Episode 16016; Testing Loss 0.0057871614034353345; Training Loss 0.004638440165487729\n",
      "Episode 16017; Testing Loss 0.005787220400821443; Training Loss 0.004638431099341757\n",
      "Episode 16018; Testing Loss 0.005787471949785131; Training Loss 0.004638424999021605\n",
      "Episode 16019; Testing Loss 0.00578746374366845; Training Loss 0.00463841405509627\n",
      "Episode 16020; Testing Loss 0.005787356288844116; Training Loss 0.004638405006938916\n",
      "Episode 16021; Testing Loss 0.005787354926696736; Training Loss 0.004638396351940289\n",
      "Episode 16022; Testing Loss 0.005787376946168145; Training Loss 0.004638387589357475\n",
      "Episode 16023; Testing Loss 0.005787324410449756; Training Loss 0.00463837762643477\n",
      "Episode 16024; Testing Loss 0.005787217265920645; Training Loss 0.004638372130501389\n",
      "Episode 16025; Testing Loss 0.005787228713589362; Training Loss 0.0046383647740044776\n",
      "Episode 16026; Testing Loss 0.005787353695027114; Training Loss 0.004638356126750295\n",
      "Episode 16027; Testing Loss 0.00578735563215309; Training Loss 0.00463834666855258\n",
      "Episode 16028; Testing Loss 0.005787289724650305; Training Loss 0.004638338996997619\n",
      "Episode 16029; Testing Loss 0.005787330545630109; Training Loss 0.004638330069935544\n",
      "Episode 16030; Testing Loss 0.005787411623139633; Training Loss 0.004638323258658525\n",
      "Episode 16031; Testing Loss 0.005787321836949029; Training Loss 0.004638312888731982\n",
      "Episode 16032; Testing Loss 0.00578719280844708; Training Loss 0.004638303389949107\n",
      "Episode 16033; Testing Loss 0.005787241516263704; Training Loss 0.004638295820916423\n",
      "Episode 16034; Testing Loss 0.005787240613865794; Training Loss 0.004638287978114709\n",
      "Episode 16035; Testing Loss 0.005787130415861573; Training Loss 0.004638279801423552\n",
      "Episode 16036; Testing Loss 0.005787031958665171; Training Loss 0.0046382712611292365\n",
      "Episode 16037; Testing Loss 0.005787094638158498; Training Loss 0.004638262818068664\n",
      "Episode 16038; Testing Loss 0.005787278376658648; Training Loss 0.004638255559979003\n",
      "Episode 16039; Testing Loss 0.005787260930139749; Training Loss 0.00463824596193688\n",
      "Episode 16040; Testing Loss 0.005787188459792051; Training Loss 0.0046382381596940135\n",
      "Episode 16041; Testing Loss 0.005787278133119176; Training Loss 0.004638228823665111\n",
      "Episode 16042; Testing Loss 0.005787408560679067; Training Loss 0.004638222699417196\n",
      "Episode 16043; Testing Loss 0.005787302352100375; Training Loss 0.004638212093648635\n",
      "Episode 16044; Testing Loss 0.005787138725868915; Training Loss 0.0046382042096962325\n",
      "Episode 16045; Testing Loss 0.005787242327053817; Training Loss 0.004638195246446996\n",
      "Episode 16046; Testing Loss 0.005787293306108188; Training Loss 0.004638187353365431\n",
      "Episode 16047; Testing Loss 0.005787130884307044; Training Loss 0.004638177792709392\n",
      "Episode 16048; Testing Loss 0.005787092178810603; Training Loss 0.004638171585709162\n",
      "Episode 16049; Testing Loss 0.005787234804894896; Training Loss 0.0046381623929632404\n",
      "Episode 16050; Testing Loss 0.0057872483881117225; Training Loss 0.00463815370248674\n",
      "Episode 16051; Testing Loss 0.005787075251041461; Training Loss 0.004638145563934022\n",
      "Episode 16052; Testing Loss 0.005787073380078504; Training Loss 0.004638137284387527\n",
      "Episode 16053; Testing Loss 0.00578728786184523; Training Loss 0.004638128782776052\n",
      "Episode 16054; Testing Loss 0.005787283751404874; Training Loss 0.004638121180593923\n",
      "Episode 16055; Testing Loss 0.005787028614541298; Training Loss 0.0046381117242150025\n",
      "Episode 16056; Testing Loss 0.005786960241380972; Training Loss 0.00463810574795118\n",
      "Episode 16057; Testing Loss 0.005787217193510948; Training Loss 0.004638097056592626\n",
      "Episode 16058; Testing Loss 0.005787235722716103; Training Loss 0.004638089009690747\n",
      "Episode 16059; Testing Loss 0.005787071853697662; Training Loss 0.00463808231541401\n",
      "Episode 16060; Testing Loss 0.00578704871450207; Training Loss 0.004638070848409645\n",
      "Episode 16061; Testing Loss 0.0057871610287436506; Training Loss 0.0046380650827278005\n",
      "Episode 16062; Testing Loss 0.005787068649039194; Training Loss 0.0046380605255759615\n",
      "Episode 16063; Testing Loss 0.00578689057775106; Training Loss 0.004638048635958695\n",
      "Episode 16064; Testing Loss 0.005786986804763715; Training Loss 0.004638039402737894\n",
      "Episode 16065; Testing Loss 0.005787246704502316; Training Loss 0.004638034158555048\n",
      "Episode 16066; Testing Loss 0.005787330390128892; Training Loss 0.004638026226702599\n",
      "Episode 16067; Testing Loss 0.0057871526939996; Training Loss 0.004638015827338145\n",
      "Episode 16068; Testing Loss 0.005786927966452222; Training Loss 0.004638008543907879\n",
      "Episode 16069; Testing Loss 0.005786972046352382; Training Loss 0.004637999920907784\n",
      "Episode 16070; Testing Loss 0.005787059869612204; Training Loss 0.004637992402058003\n",
      "Episode 16071; Testing Loss 0.005786935133390117; Training Loss 0.004637982359934658\n",
      "Episode 16072; Testing Loss 0.005786881662367768; Training Loss 0.0046379766102724275\n",
      "Episode 16073; Testing Loss 0.005787079284913587; Training Loss 0.004637969593174616\n",
      "Episode 16074; Testing Loss 0.0057872399232552; Training Loss 0.004637960398083536\n",
      "Episode 16075; Testing Loss 0.0057871059595892265; Training Loss 0.0046379502613452025\n",
      "Episode 16076; Testing Loss 0.005786973575721638; Training Loss 0.004637943991201493\n",
      "Episode 16077; Testing Loss 0.005787000991838176; Training Loss 0.00463793405666368\n",
      "Episode 16078; Testing Loss 0.005787060352090111; Training Loss 0.004637923997664056\n",
      "Episode 16079; Testing Loss 0.0057870161148706745; Training Loss 0.004637916773548574\n",
      "Episode 16080; Testing Loss 0.00578686739797072; Training Loss 0.0046379090385026064\n",
      "Episode 16081; Testing Loss 0.00578690450110959; Training Loss 0.004637898329056296\n",
      "Episode 16082; Testing Loss 0.005787096529700777; Training Loss 0.004637890325768888\n",
      "Episode 16083; Testing Loss 0.005787166908724895; Training Loss 0.004637882580877152\n",
      "Episode 16084; Testing Loss 0.0057869813736030305; Training Loss 0.004637874793138151\n",
      "Episode 16085; Testing Loss 0.005786888591003889; Training Loss 0.004637866833897507\n",
      "Episode 16086; Testing Loss 0.005787050393656246; Training Loss 0.004637856172047709\n",
      "Episode 16087; Testing Loss 0.005787075888643959; Training Loss 0.004637850101492316\n",
      "Episode 16088; Testing Loss 0.005786851180965823; Training Loss 0.0046378428416107\n",
      "Episode 16089; Testing Loss 0.005786779371866549; Training Loss 0.0046378348776334516\n",
      "Episode 16090; Testing Loss 0.005786888751688665; Training Loss 0.004637824474915298\n",
      "Episode 16091; Testing Loss 0.005786965144013098; Training Loss 0.004637816064605132\n",
      "Episode 16092; Testing Loss 0.005786881894081705; Training Loss 0.004637806608310906\n",
      "Episode 16093; Testing Loss 0.005786853219139472; Training Loss 0.0046377982697725015\n",
      "Episode 16094; Testing Loss 0.005786939982197289; Training Loss 0.00463779016768693\n",
      "Episode 16095; Testing Loss 0.00578697684159472; Training Loss 0.004637781693797185\n",
      "Episode 16096; Testing Loss 0.005786896531956627; Training Loss 0.004637772526371608\n",
      "Episode 16097; Testing Loss 0.0057868152586570114; Training Loss 0.004637766056398367\n",
      "Episode 16098; Testing Loss 0.005786862761924809; Training Loss 0.00463775623797539\n",
      "Episode 16099; Testing Loss 0.005786990469798559; Training Loss 0.00463774859852283\n",
      "Episode 16100; Testing Loss 0.0057870176741075235; Training Loss 0.0046377412029787075\n",
      "Episode 16101; Testing Loss 0.00578689731150204; Training Loss 0.004637734575075746\n",
      "Episode 16102; Testing Loss 0.005786924713773169; Training Loss 0.004637724551768482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16103; Testing Loss 0.0057869188172173505; Training Loss 0.004637714882195003\n",
      "Episode 16104; Testing Loss 0.005786744842046833; Training Loss 0.004637708872184794\n",
      "Episode 16105; Testing Loss 0.005786635348608123; Training Loss 0.004637703220760161\n",
      "Episode 16106; Testing Loss 0.005786733539477026; Training Loss 0.004637691323059965\n",
      "Episode 16107; Testing Loss 0.005786807205948975; Training Loss 0.004637683726162744\n",
      "Episode 16108; Testing Loss 0.005786793038295968; Training Loss 0.004637677119254797\n",
      "Episode 16109; Testing Loss 0.005786731280624636; Training Loss 0.004637669454644923\n",
      "Episode 16110; Testing Loss 0.005786866047146895; Training Loss 0.004637659862254841\n",
      "Episode 16111; Testing Loss 0.005787063118705352; Training Loss 0.0046376526290438135\n",
      "Episode 16112; Testing Loss 0.0057869302601489345; Training Loss 0.0046376412865310294\n",
      "Episode 16113; Testing Loss 0.0057866565969056215; Training Loss 0.004637634267870221\n",
      "Episode 16114; Testing Loss 0.005786549249546992; Training Loss 0.004637627558329364\n",
      "Episode 16115; Testing Loss 0.005786700055962033; Training Loss 0.0046376173673471\n",
      "Episode 16116; Testing Loss 0.005786792999050235; Training Loss 0.004637608863737723\n",
      "Episode 16117; Testing Loss 0.005786699597756216; Training Loss 0.0046376014284900055\n",
      "Episode 16118; Testing Loss 0.005786699331221841; Training Loss 0.004637591459523279\n",
      "Episode 16119; Testing Loss 0.005786816147988746; Training Loss 0.00463758432470296\n",
      "Episode 16120; Testing Loss 0.005786792813763433; Training Loss 0.004637579178749132\n",
      "Episode 16121; Testing Loss 0.00578664081279132; Training Loss 0.004637568876387458\n",
      "Episode 16122; Testing Loss 0.005786649662676925; Training Loss 0.004637559585736636\n",
      "Episode 16123; Testing Loss 0.005786802646924828; Training Loss 0.004637552173558077\n",
      "Episode 16124; Testing Loss 0.005786868485468829; Training Loss 0.004637542765542183\n",
      "Episode 16125; Testing Loss 0.0057867360704944885; Training Loss 0.004637533346306313\n",
      "Episode 16126; Testing Loss 0.00578662043020596; Training Loss 0.0046375249578727825\n",
      "Episode 16127; Testing Loss 0.0057866730044774965; Training Loss 0.004637518238002934\n",
      "Episode 16128; Testing Loss 0.00578679736710448; Training Loss 0.004637510668750142\n",
      "Episode 16129; Testing Loss 0.005786795983932476; Training Loss 0.004637502080058319\n",
      "Episode 16130; Testing Loss 0.005786669402565877; Training Loss 0.004637492644111104\n",
      "Episode 16131; Testing Loss 0.005786647889573755; Training Loss 0.004637484564745778\n",
      "Episode 16132; Testing Loss 0.005786719662672329; Training Loss 0.004637478672573802\n",
      "Episode 16133; Testing Loss 0.005786664897133779; Training Loss 0.0046374693450758194\n",
      "Episode 16134; Testing Loss 0.005786525532377318; Training Loss 0.004637460115762174\n",
      "Episode 16135; Testing Loss 0.005786608675034708; Training Loss 0.004637452677358244\n",
      "Episode 16136; Testing Loss 0.005786712140869951; Training Loss 0.004637446054026311\n",
      "Episode 16137; Testing Loss 0.005786704388505587; Training Loss 0.004637435641717143\n",
      "Episode 16138; Testing Loss 0.0057865783775564355; Training Loss 0.004637428451043976\n",
      "Episode 16139; Testing Loss 0.005786505532636007; Training Loss 0.0046374206242106135\n",
      "Episode 16140; Testing Loss 0.005786596767143914; Training Loss 0.004637410458735664\n",
      "Episode 16141; Testing Loss 0.0057866954977574105; Training Loss 0.004637401241175956\n",
      "Episode 16142; Testing Loss 0.005786699586020026; Training Loss 0.004637393847955192\n",
      "Episode 16143; Testing Loss 0.005786632628710624; Training Loss 0.004637385552756988\n",
      "Episode 16144; Testing Loss 0.005786614051488532; Training Loss 0.004637376019160674\n",
      "Episode 16145; Testing Loss 0.00578654060479343; Training Loss 0.004637367916172567\n",
      "Episode 16146; Testing Loss 0.005786509140939581; Training Loss 0.004637361032675869\n",
      "Episode 16147; Testing Loss 0.005786600120370237; Training Loss 0.004637353081903244\n",
      "Episode 16148; Testing Loss 0.005786611250790917; Training Loss 0.004637344311589682\n",
      "Episode 16149; Testing Loss 0.005786552047314503; Training Loss 0.004637336438955539\n",
      "Episode 16150; Testing Loss 0.0057865782843760805; Training Loss 0.004637327085956566\n",
      "Episode 16151; Testing Loss 0.005786582241920819; Training Loss 0.004637321103101696\n",
      "Episode 16152; Testing Loss 0.005786478531085905; Training Loss 0.004637312086914512\n",
      "Episode 16153; Testing Loss 0.005786462890792451; Training Loss 0.004637301857783922\n",
      "Episode 16154; Testing Loss 0.005786536366874013; Training Loss 0.004637293891284843\n",
      "Episode 16155; Testing Loss 0.005786576497908593; Training Loss 0.004637285815595284\n",
      "Episode 16156; Testing Loss 0.00578648265730568; Training Loss 0.004637276294582353\n",
      "Episode 16157; Testing Loss 0.005786415023097834; Training Loss 0.004637269827156596\n",
      "Episode 16158; Testing Loss 0.0057866100610340825; Training Loss 0.0046372615667581\n",
      "Episode 16159; Testing Loss 0.005786597057966312; Training Loss 0.004637252170752887\n",
      "Episode 16160; Testing Loss 0.005786489220992041; Training Loss 0.004637245593485903\n",
      "Episode 16161; Testing Loss 0.005786515644048532; Training Loss 0.00463723570096765\n",
      "Episode 16162; Testing Loss 0.005786534310728935; Training Loss 0.004637229851821092\n",
      "Episode 16163; Testing Loss 0.005786391482419492; Training Loss 0.004637220204365487\n",
      "Episode 16164; Testing Loss 0.0057863313887175855; Training Loss 0.004637215889659286\n",
      "Episode 16165; Testing Loss 0.005786540933481724; Training Loss 0.00463720758568957\n",
      "Episode 16166; Testing Loss 0.00578662638612859; Training Loss 0.004637196984904221\n",
      "Episode 16167; Testing Loss 0.005786413548446949; Training Loss 0.0046371882776661405\n",
      "Episode 16168; Testing Loss 0.005786347282093672; Training Loss 0.0046371817693883705\n",
      "Episode 16169; Testing Loss 0.0057864920149576695; Training Loss 0.004637169827605303\n",
      "Episode 16170; Testing Loss 0.005786546735825791; Training Loss 0.00463716551267334\n",
      "Episode 16171; Testing Loss 0.0057863227248080035; Training Loss 0.004637157934190909\n",
      "Episode 16172; Testing Loss 0.005786245986473234; Training Loss 0.004637148007435921\n",
      "Episode 16173; Testing Loss 0.005786387188817068; Training Loss 0.004637139470527972\n",
      "Episode 16174; Testing Loss 0.005786510610363948; Training Loss 0.004637131501647506\n",
      "Episode 16175; Testing Loss 0.005786343191697616; Training Loss 0.004637121623946716\n",
      "Episode 16176; Testing Loss 0.0057862915683167815; Training Loss 0.004637116320767844\n",
      "Episode 16177; Testing Loss 0.005786528263562488; Training Loss 0.004637104908063484\n",
      "Episode 16178; Testing Loss 0.005786551288026743; Training Loss 0.00463709829335422\n",
      "Episode 16179; Testing Loss 0.005786290589772751; Training Loss 0.00463708763690402\n",
      "Episode 16180; Testing Loss 0.005786155231883412; Training Loss 0.004637082013242681\n",
      "Episode 16181; Testing Loss 0.005786319924832036; Training Loss 0.004637072291964713\n",
      "Episode 16182; Testing Loss 0.005786417279876317; Training Loss 0.004637065103548618\n",
      "Episode 16183; Testing Loss 0.00578627830021283; Training Loss 0.004637055277460832\n",
      "Episode 16184; Testing Loss 0.005786246628056697; Training Loss 0.004637046517067459\n",
      "Episode 16185; Testing Loss 0.005786359773145154; Training Loss 0.0046370398309822135\n",
      "Episode 16186; Testing Loss 0.005786401352667694; Training Loss 0.004637030998499787\n",
      "Episode 16187; Testing Loss 0.00578629564277302; Training Loss 0.004637023293016028\n",
      "Episode 16188; Testing Loss 0.005786285755393034; Training Loss 0.004637015929936312\n",
      "Episode 16189; Testing Loss 0.005786408489631203; Training Loss 0.004637007465836558\n",
      "Episode 16190; Testing Loss 0.005786388247700789; Training Loss 0.004636998108985922\n",
      "Episode 16191; Testing Loss 0.005786266879323939; Training Loss 0.004636988901386655\n",
      "Episode 16192; Testing Loss 0.005786267923104317; Training Loss 0.004636983176635868\n",
      "Episode 16193; Testing Loss 0.005786318791048179; Training Loss 0.004636975526181726\n",
      "Episode 16194; Testing Loss 0.005786194807132116; Training Loss 0.004636964656993368\n",
      "Episode 16195; Testing Loss 0.005786247399244309; Training Loss 0.004636957712765766\n",
      "Episode 16196; Testing Loss 0.005786442295828927; Training Loss 0.004636951919264316\n",
      "Episode 16197; Testing Loss 0.005786325665982042; Training Loss 0.004636940377223107\n",
      "Episode 16198; Testing Loss 0.005786141297112343; Training Loss 0.004636935582588753\n",
      "Episode 16199; Testing Loss 0.005786215881700161; Training Loss 0.00463692519782888\n",
      "Episode 16200; Testing Loss 0.005786203565786716; Training Loss 0.004636915543893072\n",
      "Episode 16201; Testing Loss 0.005786222680635982; Training Loss 0.004636907642250545\n",
      "Episode 16202; Testing Loss 0.005786237151366207; Training Loss 0.004636899097377094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16203; Testing Loss 0.005786204138076068; Training Loss 0.004636891884495921\n",
      "Episode 16204; Testing Loss 0.005786210023366792; Training Loss 0.004636882015522872\n",
      "Episode 16205; Testing Loss 0.0057861365380615665; Training Loss 0.004636875900869187\n",
      "Episode 16206; Testing Loss 0.005786045520265828; Training Loss 0.004636868180552692\n",
      "Episode 16207; Testing Loss 0.005786082800955225; Training Loss 0.0046368578156155615\n",
      "Episode 16208; Testing Loss 0.005786267081534362; Training Loss 0.0046368511032998895\n",
      "Episode 16209; Testing Loss 0.005786399680619906; Training Loss 0.00463684355249594\n",
      "Episode 16210; Testing Loss 0.005786239771140183; Training Loss 0.00463683401238902\n",
      "Episode 16211; Testing Loss 0.005786060730423397; Training Loss 0.004636826950412806\n",
      "Episode 16212; Testing Loss 0.005786182577257834; Training Loss 0.004636818045416057\n",
      "Episode 16213; Testing Loss 0.005786183874360171; Training Loss 0.004636808749448971\n",
      "Episode 16214; Testing Loss 0.005786048677133044; Training Loss 0.004636803907671506\n",
      "Episode 16215; Testing Loss 0.005786037756103185; Training Loss 0.0046367943497560935\n",
      "Episode 16216; Testing Loss 0.005786184592815552; Training Loss 0.004636785279787622\n",
      "Episode 16217; Testing Loss 0.00578610063253184; Training Loss 0.004636775681979273\n",
      "Episode 16218; Testing Loss 0.005786058005171946; Training Loss 0.004636768867937948\n",
      "Episode 16219; Testing Loss 0.005786173681065206; Training Loss 0.004636760011654926\n",
      "Episode 16220; Testing Loss 0.005786243302557109; Training Loss 0.00463675329846048\n",
      "Episode 16221; Testing Loss 0.0057860707305033045; Training Loss 0.0046367428610479015\n",
      "Episode 16222; Testing Loss 0.0057859794712864; Training Loss 0.004636737385888022\n",
      "Episode 16223; Testing Loss 0.005786101894861472; Training Loss 0.004636729109453198\n",
      "Episode 16224; Testing Loss 0.005786195839212358; Training Loss 0.004636721225147294\n",
      "Episode 16225; Testing Loss 0.005786120448962432; Training Loss 0.004636711009591261\n",
      "Episode 16226; Testing Loss 0.005786067622552075; Training Loss 0.004636702983908294\n",
      "Episode 16227; Testing Loss 0.005786129133805646; Training Loss 0.004636696257764002\n",
      "Episode 16228; Testing Loss 0.0057861501063092135; Training Loss 0.00463668757911758\n",
      "Episode 16229; Testing Loss 0.005786037155680933; Training Loss 0.00463667711494871\n",
      "Episode 16230; Testing Loss 0.005786035339872075; Training Loss 0.004636668938715796\n",
      "Episode 16231; Testing Loss 0.005786058225116836; Training Loss 0.004636662179936719\n",
      "Episode 16232; Testing Loss 0.005786005106185847; Training Loss 0.0046366522225353675\n",
      "Episode 16233; Testing Loss 0.005786059849506634; Training Loss 0.004636645791111259\n",
      "Episode 16234; Testing Loss 0.005786149162480205; Training Loss 0.004636637200318449\n",
      "Episode 16235; Testing Loss 0.005786061006022863; Training Loss 0.004636629713563159\n",
      "Episode 16236; Testing Loss 0.005785948006626354; Training Loss 0.004636622026721409\n",
      "Episode 16237; Testing Loss 0.005786040030497179; Training Loss 0.004636612400690307\n",
      "Episode 16238; Testing Loss 0.005786186243716376; Training Loss 0.0046366042107047695\n",
      "Episode 16239; Testing Loss 0.005786099360392627; Training Loss 0.004636595670691867\n",
      "Episode 16240; Testing Loss 0.005785975532885646; Training Loss 0.004636587802657607\n",
      "Episode 16241; Testing Loss 0.00578598551988574; Training Loss 0.004636580192262114\n",
      "Episode 16242; Testing Loss 0.005785970272086127; Training Loss 0.004636570833828088\n",
      "Episode 16243; Testing Loss 0.005785964784719219; Training Loss 0.004636565061785844\n",
      "Episode 16244; Testing Loss 0.00578601069879446; Training Loss 0.004636555341288863\n",
      "Episode 16245; Testing Loss 0.005785977016610735; Training Loss 0.004636545784759629\n",
      "Episode 16246; Testing Loss 0.005785902422641628; Training Loss 0.004636539801235195\n",
      "Episode 16247; Testing Loss 0.005785889669110674; Training Loss 0.00463653128507849\n",
      "Episode 16248; Testing Loss 0.005786013345161233; Training Loss 0.004636521628329848\n",
      "Episode 16249; Testing Loss 0.005785982464803887; Training Loss 0.004636512961274252\n",
      "Episode 16250; Testing Loss 0.005785937098232415; Training Loss 0.004636507348334195\n",
      "Episode 16251; Testing Loss 0.0057859977283692635; Training Loss 0.004636497456999375\n",
      "Episode 16252; Testing Loss 0.005786014032103996; Training Loss 0.004636492131948208\n",
      "Episode 16253; Testing Loss 0.0057858108461411656; Training Loss 0.00463648223627721\n",
      "Episode 16254; Testing Loss 0.005785758708852484; Training Loss 0.004636475687730043\n",
      "Episode 16255; Testing Loss 0.005786073400360001; Training Loss 0.004636467737161307\n",
      "Episode 16256; Testing Loss 0.005786223104616264; Training Loss 0.004636461235396903\n",
      "Episode 16257; Testing Loss 0.005785994989049557; Training Loss 0.004636450090057358\n",
      "Episode 16258; Testing Loss 0.0057857438013833385; Training Loss 0.004636444302990279\n",
      "Episode 16259; Testing Loss 0.0057857994591535905; Training Loss 0.004636434613085618\n",
      "Episode 16260; Testing Loss 0.00578597325130266; Training Loss 0.004636428403376439\n",
      "Episode 16261; Testing Loss 0.005785816808439953; Training Loss 0.004636415939080304\n",
      "Episode 16262; Testing Loss 0.005785611652514078; Training Loss 0.0046364119092609036\n",
      "Episode 16263; Testing Loss 0.0057857501299430625; Training Loss 0.004636403368634221\n",
      "Episode 16264; Testing Loss 0.005786047948130679; Training Loss 0.004636395721627782\n",
      "Episode 16265; Testing Loss 0.005786022617228779; Training Loss 0.004636385569576401\n",
      "Episode 16266; Testing Loss 0.005785827036343121; Training Loss 0.004636378472564722\n",
      "Episode 16267; Testing Loss 0.00578585045226801; Training Loss 0.0046363686506847345\n",
      "Episode 16268; Testing Loss 0.005786021687512404; Training Loss 0.004636359291296077\n",
      "Episode 16269; Testing Loss 0.005785953865200311; Training Loss 0.004636350237099022\n",
      "Episode 16270; Testing Loss 0.005785673680767257; Training Loss 0.0046363444811359815\n",
      "Episode 16271; Testing Loss 0.00578570980531302; Training Loss 0.004636334572001282\n",
      "Episode 16272; Testing Loss 0.005785885684945816; Training Loss 0.004636326281966197\n",
      "Episode 16273; Testing Loss 0.005785883765044396; Training Loss 0.004636318396693044\n",
      "Episode 16274; Testing Loss 0.0057856688017244974; Training Loss 0.004636310351242689\n",
      "Episode 16275; Testing Loss 0.00578568176686401; Training Loss 0.004636301669133986\n",
      "Episode 16276; Testing Loss 0.005785882156831931; Training Loss 0.004636291721858022\n",
      "Episode 16277; Testing Loss 0.005785929628513808; Training Loss 0.004636284810173007\n",
      "Episode 16278; Testing Loss 0.005785715917971194; Training Loss 0.004636276366864677\n",
      "Episode 16279; Testing Loss 0.005785699069859823; Training Loss 0.0046362669657202425\n",
      "Episode 16280; Testing Loss 0.005785742556656215; Training Loss 0.004636260128533709\n",
      "Episode 16281; Testing Loss 0.005785667014486088; Training Loss 0.004636250047725086\n",
      "Episode 16282; Testing Loss 0.005785655233006362; Training Loss 0.00463624224810523\n",
      "Episode 16283; Testing Loss 0.005785747385213545; Training Loss 0.004636235570775417\n",
      "Episode 16284; Testing Loss 0.005785857080136663; Training Loss 0.004636227231743954\n",
      "Episode 16285; Testing Loss 0.005785782267471788; Training Loss 0.004636218474083133\n",
      "Episode 16286; Testing Loss 0.005785641306259127; Training Loss 0.004636209385759617\n",
      "Episode 16287; Testing Loss 0.0057856719354912; Training Loss 0.004636202333507213\n",
      "Episode 16288; Testing Loss 0.005785633917523927; Training Loss 0.004636194453589681\n",
      "Episode 16289; Testing Loss 0.005785610005022068; Training Loss 0.0046361876218568195\n",
      "Episode 16290; Testing Loss 0.005785698527041012; Training Loss 0.00463617796560905\n",
      "Episode 16291; Testing Loss 0.005785695261939484; Training Loss 0.004636169406202353\n",
      "Episode 16292; Testing Loss 0.005785553893941063; Training Loss 0.004636160321751515\n",
      "Episode 16293; Testing Loss 0.005785564788000943; Training Loss 0.00463615415787682\n",
      "Episode 16294; Testing Loss 0.005785671776926148; Training Loss 0.004636144556648399\n",
      "Episode 16295; Testing Loss 0.005785726613162097; Training Loss 0.00463613855366137\n",
      "Episode 16296; Testing Loss 0.005785722908736474; Training Loss 0.004636130432945454\n",
      "Episode 16297; Testing Loss 0.005785653707004315; Training Loss 0.004636120350374081\n",
      "Episode 16298; Testing Loss 0.005785590866804776; Training Loss 0.004636114812939711\n",
      "Episode 16299; Testing Loss 0.00578557538344662; Training Loss 0.004636104311287532\n",
      "Episode 16300; Testing Loss 0.005785602640846475; Training Loss 0.004636095900580655\n",
      "Episode 16301; Testing Loss 0.005785634516552577; Training Loss 0.004636090196451482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16302; Testing Loss 0.005785636665068398; Training Loss 0.004636078704759813\n",
      "Episode 16303; Testing Loss 0.005785650225815596; Training Loss 0.0046360707365863995\n",
      "Episode 16304; Testing Loss 0.005785594109291947; Training Loss 0.004636065257618405\n",
      "Episode 16305; Testing Loss 0.005785533598689662; Training Loss 0.004636056427812457\n",
      "Episode 16306; Testing Loss 0.005785585465403393; Training Loss 0.0046360460287245145\n",
      "Episode 16307; Testing Loss 0.005785660326037605; Training Loss 0.0046360379087319915\n",
      "Episode 16308; Testing Loss 0.005785647361948482; Training Loss 0.004636031173636201\n",
      "Episode 16309; Testing Loss 0.005785588062422405; Training Loss 0.004636022383954943\n",
      "Episode 16310; Testing Loss 0.005785582760086368; Training Loss 0.0046360145542289925\n",
      "Episode 16311; Testing Loss 0.005785604341520135; Training Loss 0.004636006808744022\n",
      "Episode 16312; Testing Loss 0.005785526590193033; Training Loss 0.004635997030994646\n",
      "Episode 16313; Testing Loss 0.005785482647192289; Training Loss 0.0046359897839092935\n",
      "Episode 16314; Testing Loss 0.00578546705209542; Training Loss 0.004635981536588706\n",
      "Episode 16315; Testing Loss 0.005785503586432431; Training Loss 0.004635972540590749\n",
      "Episode 16316; Testing Loss 0.005785556086507575; Training Loss 0.004635964778559938\n",
      "Episode 16317; Testing Loss 0.005785562268883711; Training Loss 0.004635956291142528\n",
      "Episode 16318; Testing Loss 0.0057855658192425335; Training Loss 0.004635947513698541\n",
      "Episode 16319; Testing Loss 0.005785528791942502; Training Loss 0.004635940662684393\n",
      "Episode 16320; Testing Loss 0.0057854684973083804; Training Loss 0.004635932109437778\n",
      "Episode 16321; Testing Loss 0.005785446450792701; Training Loss 0.00463592351850645\n",
      "Episode 16322; Testing Loss 0.005785525645935448; Training Loss 0.004635915488105961\n",
      "Episode 16323; Testing Loss 0.005785574680023707; Training Loss 0.004635907717819297\n",
      "Episode 16324; Testing Loss 0.0057854935914608266; Training Loss 0.00463589877870662\n",
      "Episode 16325; Testing Loss 0.0057853817808893575; Training Loss 0.004635889756324806\n",
      "Episode 16326; Testing Loss 0.00578543649841113; Training Loss 0.004635883414385665\n",
      "Episode 16327; Testing Loss 0.005785411149191836; Training Loss 0.004635874931179791\n",
      "Episode 16328; Testing Loss 0.00578535320630216; Training Loss 0.004635864737296821\n",
      "Episode 16329; Testing Loss 0.005785389237177434; Training Loss 0.004635857712608368\n",
      "Episode 16330; Testing Loss 0.005785384736264368; Training Loss 0.004635848860073744\n",
      "Episode 16331; Testing Loss 0.005785416256501142; Training Loss 0.0046358404309407965\n",
      "Episode 16332; Testing Loss 0.005785454102621908; Training Loss 0.0046358342233498205\n",
      "Episode 16333; Testing Loss 0.005785445340082622; Training Loss 0.004635826149149467\n",
      "Episode 16334; Testing Loss 0.005785437292218301; Training Loss 0.004635815733163242\n",
      "Episode 16335; Testing Loss 0.0057853963177074445; Training Loss 0.004635808260192505\n",
      "Episode 16336; Testing Loss 0.005785358061129541; Training Loss 0.0046358004156352\n",
      "Episode 16337; Testing Loss 0.0057852958275110625; Training Loss 0.004635791812588771\n",
      "Episode 16338; Testing Loss 0.005785393851091154; Training Loss 0.004635782500007186\n",
      "Episode 16339; Testing Loss 0.005785417410387031; Training Loss 0.004635774239667323\n",
      "Episode 16340; Testing Loss 0.005785390894061394; Training Loss 0.004635765954035525\n",
      "Episode 16341; Testing Loss 0.005785303764315398; Training Loss 0.004635758929340796\n",
      "Episode 16342; Testing Loss 0.005785343954130697; Training Loss 0.00463575090555725\n",
      "Episode 16343; Testing Loss 0.005785426466026188; Training Loss 0.004635742414394593\n",
      "Episode 16344; Testing Loss 0.005785357812181581; Training Loss 0.00463573408009972\n",
      "Episode 16345; Testing Loss 0.005785264991996892; Training Loss 0.004635726318729311\n",
      "Episode 16346; Testing Loss 0.005785316508539074; Training Loss 0.00463571763477056\n",
      "Episode 16347; Testing Loss 0.005785435325053888; Training Loss 0.0046357095237348185\n",
      "Episode 16348; Testing Loss 0.005785340034531091; Training Loss 0.0046357012346802656\n",
      "Episode 16349; Testing Loss 0.005785211627050742; Training Loss 0.004635693415087949\n",
      "Episode 16350; Testing Loss 0.0057852593703946265; Training Loss 0.004635684820246371\n",
      "Episode 16351; Testing Loss 0.005785356555969864; Training Loss 0.0046356760217063486\n",
      "Episode 16352; Testing Loss 0.0057852970065201; Training Loss 0.00463566970597727\n",
      "Episode 16353; Testing Loss 0.005785136950856321; Training Loss 0.004635661278950889\n",
      "Episode 16354; Testing Loss 0.005785181739647959; Training Loss 0.0046356520927483685\n",
      "Episode 16355; Testing Loss 0.005785364680961539; Training Loss 0.004635644734520781\n",
      "Episode 16356; Testing Loss 0.005785411690197465; Training Loss 0.004635636280363401\n",
      "Episode 16357; Testing Loss 0.005785297268679209; Training Loss 0.004635628138343595\n",
      "Episode 16358; Testing Loss 0.00578527548095342; Training Loss 0.004635620257920015\n",
      "Episode 16359; Testing Loss 0.005785300892233058; Training Loss 0.004635611721360789\n",
      "Episode 16360; Testing Loss 0.0057853483259984895; Training Loss 0.004635602835721264\n",
      "Episode 16361; Testing Loss 0.005785254798960184; Training Loss 0.004635595033372394\n",
      "Episode 16362; Testing Loss 0.005785123785056477; Training Loss 0.004635587374139696\n",
      "Episode 16363; Testing Loss 0.00578510612569546; Training Loss 0.004635578591957475\n",
      "Episode 16364; Testing Loss 0.0057852649115615; Training Loss 0.0046355725209794455\n",
      "Episode 16365; Testing Loss 0.0057851983288492725; Training Loss 0.004635563187574402\n",
      "Episode 16366; Testing Loss 0.005785026069485389; Training Loss 0.004635558322595068\n",
      "Episode 16367; Testing Loss 0.005785154377879046; Training Loss 0.004635546767772684\n",
      "Episode 16368; Testing Loss 0.005785370964365787; Training Loss 0.004635539912620766\n",
      "Episode 16369; Testing Loss 0.005785131376510383; Training Loss 0.004635527862888563\n",
      "Episode 16370; Testing Loss 0.005784961361168687; Training Loss 0.004635522223239486\n",
      "Episode 16371; Testing Loss 0.0057851595189366365; Training Loss 0.004635512185656981\n",
      "Episode 16372; Testing Loss 0.005785287311945201; Training Loss 0.004635504028255872\n",
      "Episode 16373; Testing Loss 0.005785147742660856; Training Loss 0.004635497114198335\n",
      "Episode 16374; Testing Loss 0.0057851549089014895; Training Loss 0.004635488257532719\n",
      "Episode 16375; Testing Loss 0.005785254699981488; Training Loss 0.004635481568683264\n",
      "Episode 16376; Testing Loss 0.005785173911414709; Training Loss 0.004635472210388193\n",
      "Episode 16377; Testing Loss 0.005785095100326821; Training Loss 0.0046354622080246035\n",
      "Episode 16378; Testing Loss 0.0057850914517241425; Training Loss 0.0046354544019319155\n",
      "Episode 16379; Testing Loss 0.005785135208482757; Training Loss 0.00463544686527141\n",
      "Episode 16380; Testing Loss 0.005785181462441081; Training Loss 0.004635437911877144\n",
      "Episode 16381; Testing Loss 0.005785153405842286; Training Loss 0.004635431099732387\n",
      "Episode 16382; Testing Loss 0.0057850650350595775; Training Loss 0.004635422144747922\n",
      "Episode 16383; Testing Loss 0.005785041972380735; Training Loss 0.004635414238675548\n",
      "Episode 16384; Testing Loss 0.0057851202629369625; Training Loss 0.004635405610970947\n",
      "Episode 16385; Testing Loss 0.005785161430304298; Training Loss 0.004635396862048209\n",
      "Episode 16386; Testing Loss 0.005785031753952101; Training Loss 0.0046353878877387865\n",
      "Episode 16387; Testing Loss 0.0057849400023628865; Training Loss 0.004635382220560496\n",
      "Episode 16388; Testing Loss 0.005785075368289211; Training Loss 0.004635372924720755\n",
      "Episode 16389; Testing Loss 0.005785118138525954; Training Loss 0.004635363811849743\n",
      "Episode 16390; Testing Loss 0.005785053788276166; Training Loss 0.0046353558541787744\n",
      "Episode 16391; Testing Loss 0.005784996699872429; Training Loss 0.004635348938939669\n",
      "Episode 16392; Testing Loss 0.0057850998512969706; Training Loss 0.004635339605494133\n",
      "Episode 16393; Testing Loss 0.005785090748466283; Training Loss 0.004635331469651251\n",
      "Episode 16394; Testing Loss 0.005785033692363804; Training Loss 0.004635324106054943\n",
      "Episode 16395; Testing Loss 0.0057849056727368965; Training Loss 0.004635314621324704\n",
      "Episode 16396; Testing Loss 0.005784985145357448; Training Loss 0.004635307614643722\n",
      "Episode 16397; Testing Loss 0.00578511468218826; Training Loss 0.004635301097506336\n",
      "Episode 16398; Testing Loss 0.005784964060294116; Training Loss 0.004635289762704016\n",
      "Episode 16399; Testing Loss 0.0057849129216572415; Training Loss 0.004635282217002742\n",
      "Episode 16400; Testing Loss 0.005785045493251519; Training Loss 0.004635274308126317\n",
      "Episode 16401; Testing Loss 0.005785054578704331; Training Loss 0.00463526726339556\n",
      "Episode 16402; Testing Loss 0.005784935426458708; Training Loss 0.004635257928769808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16403; Testing Loss 0.00578489300030247; Training Loss 0.004635250307791701\n",
      "Episode 16404; Testing Loss 0.005785056778710121; Training Loss 0.00463524154659036\n",
      "Episode 16405; Testing Loss 0.005785057936244728; Training Loss 0.004635233546051948\n",
      "Episode 16406; Testing Loss 0.0057848959439521725; Training Loss 0.0046352247365219765\n",
      "Episode 16407; Testing Loss 0.005784821901193366; Training Loss 0.004635217326606665\n",
      "Episode 16408; Testing Loss 0.005784955743039523; Training Loss 0.004635210585568079\n",
      "Episode 16409; Testing Loss 0.005784952336990843; Training Loss 0.004635201787370717\n",
      "Episode 16410; Testing Loss 0.005784748767013158; Training Loss 0.004635193405096071\n",
      "Episode 16411; Testing Loss 0.005784796603542717; Training Loss 0.00463518500169053\n",
      "Episode 16412; Testing Loss 0.005784994969604657; Training Loss 0.0046351784196709545\n",
      "Episode 16413; Testing Loss 0.0057849487328254165; Training Loss 0.004635169016431142\n",
      "Episode 16414; Testing Loss 0.005784730160925553; Training Loss 0.004635160731682366\n",
      "Episode 16415; Testing Loss 0.005784769693320967; Training Loss 0.00463515410348378\n",
      "Episode 16416; Testing Loss 0.005784919410210616; Training Loss 0.004635146562790696\n",
      "Episode 16417; Testing Loss 0.00578486979157033; Training Loss 0.004635135990217175\n",
      "Episode 16418; Testing Loss 0.005784797421537; Training Loss 0.004635130907209829\n",
      "Episode 16419; Testing Loss 0.005784866682691654; Training Loss 0.004635122235685081\n",
      "Episode 16420; Testing Loss 0.005784839682218777; Training Loss 0.004635114089548103\n",
      "Episode 16421; Testing Loss 0.00578478779715481; Training Loss 0.004635105652489937\n",
      "Episode 16422; Testing Loss 0.005784795536598654; Training Loss 0.00463509671740307\n",
      "Episode 16423; Testing Loss 0.005784819647878662; Training Loss 0.0046350871979311346\n",
      "Episode 16424; Testing Loss 0.005784790173399105; Training Loss 0.004635077609435226\n",
      "Episode 16425; Testing Loss 0.005784747113394149; Training Loss 0.004635070862827283\n",
      "Episode 16426; Testing Loss 0.005784756941474157; Training Loss 0.004635064017932818\n",
      "Episode 16427; Testing Loss 0.005784679129721669; Training Loss 0.004635052763656658\n",
      "Episode 16428; Testing Loss 0.0057847128110744064; Training Loss 0.004635047399704287\n",
      "Episode 16429; Testing Loss 0.005784838822902248; Training Loss 0.004635041446201969\n",
      "Episode 16430; Testing Loss 0.005784752459903502; Training Loss 0.004635032207271863\n",
      "Episode 16431; Testing Loss 0.005784664403959967; Training Loss 0.004635023877742239\n",
      "Episode 16432; Testing Loss 0.005784726895458365; Training Loss 0.00463501394882437\n",
      "Episode 16433; Testing Loss 0.005784841620499819; Training Loss 0.00463500600017188\n",
      "Episode 16434; Testing Loss 0.005784697746298082; Training Loss 0.0046349971739047795\n",
      "Episode 16435; Testing Loss 0.005784608125628197; Training Loss 0.004634989678520897\n",
      "Episode 16436; Testing Loss 0.005784659357966326; Training Loss 0.0046349804728069805\n",
      "Episode 16437; Testing Loss 0.005784825871419918; Training Loss 0.004634973080439358\n",
      "Episode 16438; Testing Loss 0.005784874250970886; Training Loss 0.00463496561541957\n",
      "Episode 16439; Testing Loss 0.005784656202281278; Training Loss 0.0046349570918992\n",
      "Episode 16440; Testing Loss 0.0057845136918780425; Training Loss 0.004634949016538368\n",
      "Episode 16441; Testing Loss 0.005784647155849093; Training Loss 0.0046349417921699365\n",
      "Episode 16442; Testing Loss 0.005784709657212563; Training Loss 0.00463493440745624\n",
      "Episode 16443; Testing Loss 0.005784600111830031; Training Loss 0.00463492334965474\n",
      "Episode 16444; Testing Loss 0.005784542438781799; Training Loss 0.00463491789650728\n",
      "Episode 16445; Testing Loss 0.005784556548075897; Training Loss 0.0046349079387890315\n",
      "Episode 16446; Testing Loss 0.005784657689260578; Training Loss 0.004634898309024207\n",
      "Episode 16447; Testing Loss 0.0057847033827304615; Training Loss 0.004634889917123047\n",
      "Episode 16448; Testing Loss 0.005784627016903954; Training Loss 0.0046348811870402405\n",
      "Episode 16449; Testing Loss 0.005784645916786997; Training Loss 0.00463487233719621\n",
      "Episode 16450; Testing Loss 0.005784551678307528; Training Loss 0.004634865386841019\n",
      "Episode 16451; Testing Loss 0.005784578588327669; Training Loss 0.004634858108948684\n",
      "Episode 16452; Testing Loss 0.00578465001469797; Training Loss 0.004634848400362021\n",
      "Episode 16453; Testing Loss 0.005784722606070852; Training Loss 0.0046348396671912\n",
      "Episode 16454; Testing Loss 0.00578465478725134; Training Loss 0.004634831357271635\n",
      "Episode 16455; Testing Loss 0.0057845916827728334; Training Loss 0.004634822533877196\n",
      "Episode 16456; Testing Loss 0.005784572954570125; Training Loss 0.004634813924510955\n",
      "Episode 16457; Testing Loss 0.005784622181681791; Training Loss 0.004634807684908874\n",
      "Episode 16458; Testing Loss 0.005784535138621658; Training Loss 0.004634796911375727\n",
      "Episode 16459; Testing Loss 0.005784583001950336; Training Loss 0.004634788401625653\n",
      "Episode 16460; Testing Loss 0.005784631891757243; Training Loss 0.00463478130332709\n",
      "Episode 16461; Testing Loss 0.005784580072830002; Training Loss 0.004634773124961053\n",
      "Episode 16462; Testing Loss 0.005784621657382431; Training Loss 0.004634764552776129\n",
      "Episode 16463; Testing Loss 0.005784597163175173; Training Loss 0.004634756891141507\n",
      "Episode 16464; Testing Loss 0.005784513033786342; Training Loss 0.004634749336108145\n",
      "Episode 16465; Testing Loss 0.005784563867850268; Training Loss 0.004634739159121045\n",
      "Episode 16466; Testing Loss 0.005784624757496678; Training Loss 0.00463473190720851\n",
      "Episode 16467; Testing Loss 0.005784590794172816; Training Loss 0.004634722461244198\n",
      "Episode 16468; Testing Loss 0.005784433736513082; Training Loss 0.0046347148258531435\n",
      "Episode 16469; Testing Loss 0.005784445445138034; Training Loss 0.004634707309524153\n",
      "Episode 16470; Testing Loss 0.005784555631961502; Training Loss 0.004634696599113294\n",
      "Episode 16471; Testing Loss 0.005784662023694402; Training Loss 0.004634689734626794\n",
      "Episode 16472; Testing Loss 0.005784576121480761; Training Loss 0.00463468129502781\n",
      "Episode 16473; Testing Loss 0.005784543937832404; Training Loss 0.004634672786749733\n",
      "Episode 16474; Testing Loss 0.005784576973878331; Training Loss 0.004634664770925303\n",
      "Episode 16475; Testing Loss 0.005784601245025159; Training Loss 0.0046346577937284965\n",
      "Episode 16476; Testing Loss 0.005784528302696902; Training Loss 0.004634647683408725\n",
      "Episode 16477; Testing Loss 0.005784548998166695; Training Loss 0.004634638158257888\n",
      "Episode 16478; Testing Loss 0.005784601877629745; Training Loss 0.004634630442677647\n",
      "Episode 16479; Testing Loss 0.0057845772875409355; Training Loss 0.004634623146366024\n",
      "Episode 16480; Testing Loss 0.005784621617738962; Training Loss 0.004634613298199815\n",
      "Episode 16481; Testing Loss 0.005784615794696043; Training Loss 0.004634605207867731\n",
      "Episode 16482; Testing Loss 0.005784547573139811; Training Loss 0.0046345980522643675\n",
      "Episode 16483; Testing Loss 0.005784539784651573; Training Loss 0.004634589608523411\n",
      "Episode 16484; Testing Loss 0.005784644386925709; Training Loss 0.004634580026695508\n",
      "Episode 16485; Testing Loss 0.005784599836743752; Training Loss 0.004634571430532098\n",
      "Episode 16486; Testing Loss 0.005784567878293091; Training Loss 0.004634562241756262\n",
      "Episode 16487; Testing Loss 0.005784588330701089; Training Loss 0.0046345567852520775\n",
      "Episode 16488; Testing Loss 0.005784523509785048; Training Loss 0.004634546642971531\n",
      "Episode 16489; Testing Loss 0.005784585794261512; Training Loss 0.004634536963854423\n",
      "Episode 16490; Testing Loss 0.00578466586854689; Training Loss 0.004634529516232655\n",
      "Episode 16491; Testing Loss 0.005784659113467089; Training Loss 0.004634522464161419\n",
      "Episode 16492; Testing Loss 0.005784658182816453; Training Loss 0.004634513190265475\n",
      "Episode 16493; Testing Loss 0.005784554479121421; Training Loss 0.004634505546449349\n",
      "Episode 16494; Testing Loss 0.005784518996970153; Training Loss 0.004634497621984111\n",
      "Episode 16495; Testing Loss 0.005784673554896626; Training Loss 0.004634488336566923\n",
      "Episode 16496; Testing Loss 0.005784670241950871; Training Loss 0.004634480087385653\n",
      "Episode 16497; Testing Loss 0.005784543489153145; Training Loss 0.0046344727379918\n",
      "Episode 16498; Testing Loss 0.005784515212241998; Training Loss 0.004634462851912903\n",
      "Episode 16499; Testing Loss 0.0057846890681642294; Training Loss 0.004634456698358542\n",
      "Episode 16500; Testing Loss 0.0057847314563394025; Training Loss 0.0046344470524792535\n",
      "Episode 16501; Testing Loss 0.005784640692962664; Training Loss 0.00463444097231603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16502; Testing Loss 0.005784755261358; Training Loss 0.0046344318692305405\n",
      "Episode 16503; Testing Loss 0.005784776676434776; Training Loss 0.004634422514664336\n",
      "Episode 16504; Testing Loss 0.005784579897600207; Training Loss 0.004634413202441308\n",
      "Episode 16505; Testing Loss 0.0057844387154042; Training Loss 0.004634409193867805\n",
      "Episode 16506; Testing Loss 0.005784479001302708; Training Loss 0.004634398574118104\n",
      "Episode 16507; Testing Loss 0.005784672039834867; Training Loss 0.004634389057792895\n",
      "Episode 16508; Testing Loss 0.005784767807794986; Training Loss 0.004634381600772136\n",
      "Episode 16509; Testing Loss 0.005784675428061771; Training Loss 0.004634372317483717\n",
      "Episode 16510; Testing Loss 0.005784646079000684; Training Loss 0.004634364018230117\n",
      "Episode 16511; Testing Loss 0.005784788652477401; Training Loss 0.004634356085516992\n",
      "Episode 16512; Testing Loss 0.00578477086914047; Training Loss 0.004634346963868828\n",
      "Episode 16513; Testing Loss 0.005784608743436041; Training Loss 0.004634339672380879\n",
      "Episode 16514; Testing Loss 0.005784622062247679; Training Loss 0.0046343315879785706\n",
      "Episode 16515; Testing Loss 0.005784726733586685; Training Loss 0.004634323420628231\n",
      "Episode 16516; Testing Loss 0.0057847630731073275; Training Loss 0.0046343149278383865\n",
      "Episode 16517; Testing Loss 0.005784690331605818; Training Loss 0.004634305780109698\n",
      "Episode 16518; Testing Loss 0.005784587384847101; Training Loss 0.004634300742608523\n",
      "Episode 16519; Testing Loss 0.005784549256649131; Training Loss 0.004634290069158374\n",
      "Episode 16520; Testing Loss 0.005784617941972969; Training Loss 0.004634280039263\n",
      "Episode 16521; Testing Loss 0.005784762545842571; Training Loss 0.004634277283322783\n",
      "Episode 16522; Testing Loss 0.005784766992482503; Training Loss 0.00463426797293738\n",
      "Episode 16523; Testing Loss 0.005784662764600928; Training Loss 0.004634255367346732\n",
      "Episode 16524; Testing Loss 0.005784619803527921; Training Loss 0.004634249440858781\n",
      "Episode 16525; Testing Loss 0.00578463143157536; Training Loss 0.004634239355665415\n",
      "Episode 16526; Testing Loss 0.005784711534998191; Training Loss 0.004634230109494903\n",
      "Episode 16527; Testing Loss 0.005784805330679486; Training Loss 0.0046342245032679075\n",
      "Episode 16528; Testing Loss 0.005784827772602472; Training Loss 0.004634215964528998\n",
      "Episode 16529; Testing Loss 0.005784705124849271; Training Loss 0.004634206102262376\n",
      "Episode 16530; Testing Loss 0.00578464002583321; Training Loss 0.004634198164725507\n",
      "Episode 16531; Testing Loss 0.005784715948068238; Training Loss 0.004634189324836846\n",
      "Episode 16532; Testing Loss 0.0057847480727980126; Training Loss 0.004634181272900652\n",
      "Episode 16533; Testing Loss 0.00578468078137632; Training Loss 0.0046341731063681574\n",
      "Episode 16534; Testing Loss 0.0057846527141130685; Training Loss 0.004634165566815092\n",
      "Episode 16535; Testing Loss 0.005784742628598132; Training Loss 0.004634156725898127\n",
      "Episode 16536; Testing Loss 0.005784769426612396; Training Loss 0.004634147329608782\n",
      "Episode 16537; Testing Loss 0.005784712045607051; Training Loss 0.004634137200494322\n",
      "Episode 16538; Testing Loss 0.0057847430092525225; Training Loss 0.0046341319227836434\n",
      "Episode 16539; Testing Loss 0.005784808777889583; Training Loss 0.00463412334906529\n",
      "Episode 16540; Testing Loss 0.005784795916954508; Training Loss 0.004634112491508691\n",
      "Episode 16541; Testing Loss 0.005784787239784558; Training Loss 0.004634105288757366\n",
      "Episode 16542; Testing Loss 0.0057847508554925075; Training Loss 0.004634097169187379\n",
      "Episode 16543; Testing Loss 0.005784684539938959; Training Loss 0.004634088852440136\n",
      "Episode 16544; Testing Loss 0.005784783195965641; Training Loss 0.004634081929238204\n",
      "Episode 16545; Testing Loss 0.0057849061143990885; Training Loss 0.004634071222999478\n",
      "Episode 16546; Testing Loss 0.005784832189578142; Training Loss 0.0046340637194368485\n",
      "Episode 16547; Testing Loss 0.00578477618242334; Training Loss 0.0046340562653738255\n",
      "Episode 16548; Testing Loss 0.005784815384457179; Training Loss 0.00463404718129702\n",
      "Episode 16549; Testing Loss 0.005784772245635169; Training Loss 0.004634039517822895\n",
      "Episode 16550; Testing Loss 0.005784773964006904; Training Loss 0.004634030485160393\n",
      "Episode 16551; Testing Loss 0.005784744651967012; Training Loss 0.004634022464647702\n",
      "Episode 16552; Testing Loss 0.005784610369643275; Training Loss 0.004634014823613916\n",
      "Episode 16553; Testing Loss 0.005784700659831923; Training Loss 0.0046340067959797036\n",
      "Episode 16554; Testing Loss 0.005784960762387413; Training Loss 0.004633998500640509\n",
      "Episode 16555; Testing Loss 0.005784928986458033; Training Loss 0.00463399066218631\n",
      "Episode 16556; Testing Loss 0.00578468806285648; Training Loss 0.004633982037762785\n",
      "Episode 16557; Testing Loss 0.005784745060039318; Training Loss 0.004633973491065845\n",
      "Episode 16558; Testing Loss 0.0057849725317485595; Training Loss 0.004633967692460831\n",
      "Episode 16559; Testing Loss 0.005784843227070796; Training Loss 0.0046339570918149\n",
      "Episode 16560; Testing Loss 0.005784574606479765; Training Loss 0.0046339490763563996\n",
      "Episode 16561; Testing Loss 0.005784672004423937; Training Loss 0.0046339393659563495\n",
      "Episode 16562; Testing Loss 0.0057848959735086825; Training Loss 0.004633930712679958\n",
      "Episode 16563; Testing Loss 0.00578492265052954; Training Loss 0.004633921515623675\n",
      "Episode 16564; Testing Loss 0.005784725966601093; Training Loss 0.004633915672021841\n",
      "Episode 16565; Testing Loss 0.005784757508557485; Training Loss 0.004633905924171959\n",
      "Episode 16566; Testing Loss 0.0057848679520727765; Training Loss 0.004633898610957168\n",
      "Episode 16567; Testing Loss 0.005784838402730292; Training Loss 0.0046338902690506644\n",
      "Episode 16568; Testing Loss 0.005784757135004652; Training Loss 0.004633883337212543\n",
      "Episode 16569; Testing Loss 0.005784801842178432; Training Loss 0.004633873791232003\n",
      "Episode 16570; Testing Loss 0.005784811218600893; Training Loss 0.004633864767108579\n",
      "Episode 16571; Testing Loss 0.005784808948836666; Training Loss 0.004633858179768589\n",
      "Episode 16572; Testing Loss 0.005784779919146697; Training Loss 0.004633848537247519\n",
      "Episode 16573; Testing Loss 0.005784819427289449; Training Loss 0.004633840793220882\n",
      "Episode 16574; Testing Loss 0.005784833023738006; Training Loss 0.004633833701085195\n",
      "Episode 16575; Testing Loss 0.005784802306916265; Training Loss 0.004633825414823427\n",
      "Episode 16576; Testing Loss 0.005784800521597255; Training Loss 0.004633816056931752\n",
      "Episode 16577; Testing Loss 0.005784780360771327; Training Loss 0.004633808393692227\n",
      "Episode 16578; Testing Loss 0.005784715137984508; Training Loss 0.0046337989176237875\n",
      "Episode 16579; Testing Loss 0.005784768778173477; Training Loss 0.004633791567353352\n",
      "Episode 16580; Testing Loss 0.00578484973512125; Training Loss 0.0046337850411246615\n",
      "Episode 16581; Testing Loss 0.005784848664113496; Training Loss 0.004633776718338407\n",
      "Episode 16582; Testing Loss 0.005784737736568668; Training Loss 0.004633767560033916\n",
      "Episode 16583; Testing Loss 0.005784724762375346; Training Loss 0.004633760503595065\n",
      "Episode 16584; Testing Loss 0.00578479290448211; Training Loss 0.004633751605714239\n",
      "Episode 16585; Testing Loss 0.005784822749146647; Training Loss 0.004633743017899796\n",
      "Episode 16586; Testing Loss 0.005784754368508185; Training Loss 0.004633735108535078\n",
      "Episode 16587; Testing Loss 0.005784802116125979; Training Loss 0.004633727201619389\n",
      "Episode 16588; Testing Loss 0.005784861648132138; Training Loss 0.004633719450092831\n",
      "Episode 16589; Testing Loss 0.005784785915670747; Training Loss 0.004633711264728504\n",
      "Episode 16590; Testing Loss 0.00578484743821115; Training Loss 0.0046337017426301644\n",
      "Episode 16591; Testing Loss 0.00578486395980903; Training Loss 0.004633695893239666\n",
      "Episode 16592; Testing Loss 0.005784713047058738; Training Loss 0.004633686258051095\n",
      "Episode 16593; Testing Loss 0.005784646248531097; Training Loss 0.004633678176843451\n",
      "Episode 16594; Testing Loss 0.005784820576943315; Training Loss 0.004633670276087721\n",
      "Episode 16595; Testing Loss 0.005784856311308234; Training Loss 0.0046336651019689215\n",
      "Episode 16596; Testing Loss 0.005784701324628322; Training Loss 0.004633654449846472\n",
      "Episode 16597; Testing Loss 0.005784658612180656; Training Loss 0.004633645195000062\n",
      "Episode 16598; Testing Loss 0.005784737001598309; Training Loss 0.00463363811513488\n",
      "Episode 16599; Testing Loss 0.005784754763843607; Training Loss 0.004633629945080446\n",
      "Episode 16600; Testing Loss 0.005784654196488707; Training Loss 0.004633621504724288\n",
      "Episode 16601; Testing Loss 0.005784700438608077; Training Loss 0.0046336130173942345\n",
      "Episode 16602; Testing Loss 0.005784735554489207; Training Loss 0.004633607993533274\n",
      "Episode 16603; Testing Loss 0.005784616400960505; Training Loss 0.004633596404203591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16604; Testing Loss 0.0057845553348841; Training Loss 0.004633589590997941\n",
      "Episode 16605; Testing Loss 0.0057847147824375686; Training Loss 0.004633584009828127\n",
      "Episode 16606; Testing Loss 0.005784726333607762; Training Loss 0.004633575036003896\n",
      "Episode 16607; Testing Loss 0.005784564256680278; Training Loss 0.004633564881156885\n",
      "Episode 16608; Testing Loss 0.005784514940129323; Training Loss 0.004633557756760161\n",
      "Episode 16609; Testing Loss 0.005784740197347858; Training Loss 0.004633549280858123\n",
      "Episode 16610; Testing Loss 0.005784721922515445; Training Loss 0.004633541075838243\n",
      "Episode 16611; Testing Loss 0.00578452841748572; Training Loss 0.0046335315270397415\n",
      "Episode 16612; Testing Loss 0.005784499936133272; Training Loss 0.0046335260696631604\n",
      "Episode 16613; Testing Loss 0.005784658166588743; Training Loss 0.004633518009347118\n",
      "Episode 16614; Testing Loss 0.005784644906405543; Training Loss 0.004633508737600851\n",
      "Episode 16615; Testing Loss 0.0057844966978596105; Training Loss 0.004633500114039909\n",
      "Episode 16616; Testing Loss 0.005784542323882445; Training Loss 0.004633491812869713\n",
      "Episode 16617; Testing Loss 0.005784682394157962; Training Loss 0.004633483904069226\n",
      "Episode 16618; Testing Loss 0.00578464661381963; Training Loss 0.004633474766965498\n",
      "Episode 16619; Testing Loss 0.005784507641029533; Training Loss 0.004633467476841725\n",
      "Episode 16620; Testing Loss 0.005784541546344031; Training Loss 0.004633458755782938\n",
      "Episode 16621; Testing Loss 0.005784695254004343; Training Loss 0.0046334511274881305\n",
      "Episode 16622; Testing Loss 0.0057846050228600844; Training Loss 0.004633442439178435\n",
      "Episode 16623; Testing Loss 0.0057844082188247435; Training Loss 0.004633436155386559\n",
      "Episode 16624; Testing Loss 0.005784495777304687; Training Loss 0.00463342600702755\n",
      "Episode 16625; Testing Loss 0.005784555460024813; Training Loss 0.004633421785011571\n",
      "Episode 16626; Testing Loss 0.00578438759473652; Training Loss 0.004633410830828004\n",
      "Episode 16627; Testing Loss 0.005784404523125527; Training Loss 0.004633402748557545\n",
      "Episode 16628; Testing Loss 0.005784590701008122; Training Loss 0.004633395075808646\n",
      "Episode 16629; Testing Loss 0.0057846547419632055; Training Loss 0.00463338670557837\n",
      "Episode 16630; Testing Loss 0.0057844762851915225; Training Loss 0.004633377517059139\n",
      "Episode 16631; Testing Loss 0.005784383676846445; Training Loss 0.004633369948820702\n",
      "Episode 16632; Testing Loss 0.005784423221112907; Training Loss 0.004633361949454916\n",
      "Episode 16633; Testing Loss 0.005784473241683334; Training Loss 0.004633352702282492\n",
      "Episode 16634; Testing Loss 0.005784459818747945; Training Loss 0.00463334503655436\n",
      "Episode 16635; Testing Loss 0.005784474071698916; Training Loss 0.004633337404827532\n",
      "Episode 16636; Testing Loss 0.005784486139536985; Training Loss 0.004633328489951237\n",
      "Episode 16637; Testing Loss 0.005784477027140222; Training Loss 0.004633321050204683\n",
      "Episode 16638; Testing Loss 0.005784380383312282; Training Loss 0.004633315447687055\n",
      "Episode 16639; Testing Loss 0.005784450401420276; Training Loss 0.004633306212553694\n",
      "Episode 16640; Testing Loss 0.005784496321414314; Training Loss 0.0046332998418711\n",
      "Episode 16641; Testing Loss 0.005784334950316989; Training Loss 0.004633290320536517\n",
      "Episode 16642; Testing Loss 0.005784218947691359; Training Loss 0.004633282573086998\n",
      "Episode 16643; Testing Loss 0.005784382846199654; Training Loss 0.004633272860990537\n",
      "Episode 16644; Testing Loss 0.0057844965380267725; Training Loss 0.004633267478990503\n",
      "Episode 16645; Testing Loss 0.005784277003187812; Training Loss 0.0046332563050131914\n",
      "Episode 16646; Testing Loss 0.005784233066194308; Training Loss 0.004633250524576578\n",
      "Episode 16647; Testing Loss 0.005784386554712502; Training Loss 0.004633240380254943\n",
      "Episode 16648; Testing Loss 0.005784474993095672; Training Loss 0.004633234805436075\n",
      "Episode 16649; Testing Loss 0.005784263184477447; Training Loss 0.004633226669302047\n",
      "Episode 16650; Testing Loss 0.005784205059139539; Training Loss 0.004633221318707439\n",
      "Episode 16651; Testing Loss 0.0057844245171157155; Training Loss 0.004633210391544942\n",
      "Episode 16652; Testing Loss 0.005784427314730158; Training Loss 0.004633201724429318\n",
      "Episode 16653; Testing Loss 0.005784194088758893; Training Loss 0.00463319449435858\n",
      "Episode 16654; Testing Loss 0.005784125426381898; Training Loss 0.004633186441998137\n",
      "Episode 16655; Testing Loss 0.005784298193350059; Training Loss 0.004633176692585034\n",
      "Episode 16656; Testing Loss 0.005784370296183496; Training Loss 0.00463317048195991\n",
      "Episode 16657; Testing Loss 0.005784197893623908; Training Loss 0.004633161954034697\n",
      "Episode 16658; Testing Loss 0.005784091477920541; Training Loss 0.004633154376235156\n",
      "Episode 16659; Testing Loss 0.005784197592407942; Training Loss 0.004633144167795968\n",
      "Episode 16660; Testing Loss 0.00578428851598422; Training Loss 0.004633136163441384\n",
      "Episode 16661; Testing Loss 0.005784149924474814; Training Loss 0.004633128112187541\n",
      "Episode 16662; Testing Loss 0.005784214864202452; Training Loss 0.004633120818659125\n",
      "Episode 16663; Testing Loss 0.005784363080408203; Training Loss 0.004633114731980759\n",
      "Episode 16664; Testing Loss 0.005784277172421502; Training Loss 0.004633104639604053\n",
      "Episode 16665; Testing Loss 0.00578412951217109; Training Loss 0.0046330978143206926\n",
      "Episode 16666; Testing Loss 0.00578419220087711; Training Loss 0.004633088787993179\n",
      "Episode 16667; Testing Loss 0.005784173627296911; Training Loss 0.004633082464188317\n",
      "Episode 16668; Testing Loss 0.0057839850727730395; Training Loss 0.0046330742657486506\n",
      "Episode 16669; Testing Loss 0.005783942592333891; Training Loss 0.004633067227433255\n",
      "Episode 16670; Testing Loss 0.005784166020200001; Training Loss 0.004633058297690365\n",
      "Episode 16671; Testing Loss 0.005784256655517062; Training Loss 0.004633052221958913\n",
      "Episode 16672; Testing Loss 0.005784110125472153; Training Loss 0.00463303913916458\n",
      "Episode 16673; Testing Loss 0.005783976406258913; Training Loss 0.0046330332458936856\n",
      "Episode 16674; Testing Loss 0.005784051913001622; Training Loss 0.004633025970587439\n",
      "Episode 16675; Testing Loss 0.00578425218825107; Training Loss 0.004633018473857849\n",
      "Episode 16676; Testing Loss 0.005784215290464936; Training Loss 0.004633008609142829\n",
      "Episode 16677; Testing Loss 0.005784037861142751; Training Loss 0.004633000863389326\n",
      "Episode 16678; Testing Loss 0.0057840225494832755; Training Loss 0.004632994769124412\n",
      "Episode 16679; Testing Loss 0.005784098462119669; Training Loss 0.00463298453906418\n",
      "Episode 16680; Testing Loss 0.005784055684635989; Training Loss 0.004632976580730565\n",
      "Episode 16681; Testing Loss 0.005783885874709721; Training Loss 0.004632968745759538\n",
      "Episode 16682; Testing Loss 0.005783831684244737; Training Loss 0.0046329617809450115\n",
      "Episode 16683; Testing Loss 0.00578403136039784; Training Loss 0.00463295182188157\n",
      "Episode 16684; Testing Loss 0.00578419153444629; Training Loss 0.0046329454359432505\n",
      "Episode 16685; Testing Loss 0.005784058439577073; Training Loss 0.004632936049789971\n",
      "Episode 16686; Testing Loss 0.005784012096340629; Training Loss 0.004632927692794626\n",
      "Episode 16687; Testing Loss 0.00578409176026595; Training Loss 0.0046329223113572345\n",
      "Episode 16688; Testing Loss 0.005784132585830704; Training Loss 0.004632914453002456\n",
      "Episode 16689; Testing Loss 0.005783978871058818; Training Loss 0.004632902086636234\n",
      "Episode 16690; Testing Loss 0.005783921956447086; Training Loss 0.0046328953554595305\n",
      "Episode 16691; Testing Loss 0.005783918692479299; Training Loss 0.004632888785577762\n",
      "Episode 16692; Testing Loss 0.005783935031005825; Training Loss 0.004632880419757827\n",
      "Episode 16693; Testing Loss 0.005783940228144692; Training Loss 0.0046328710560655725\n",
      "Episode 16694; Testing Loss 0.005783918924927853; Training Loss 0.004632864802311306\n",
      "Episode 16695; Testing Loss 0.0057838552377425835; Training Loss 0.004632858258296339\n",
      "Episode 16696; Testing Loss 0.005783824562634127; Training Loss 0.004632846383216058\n",
      "Episode 16697; Testing Loss 0.005783939214732069; Training Loss 0.004632837857686728\n",
      "Episode 16698; Testing Loss 0.005784003206277191; Training Loss 0.004632830616595099\n",
      "Episode 16699; Testing Loss 0.00578388770809565; Training Loss 0.004632822080235493\n",
      "Episode 16700; Testing Loss 0.005783830088971802; Training Loss 0.0046328128652657185\n",
      "Episode 16701; Testing Loss 0.005783857393707198; Training Loss 0.004632807486826614\n",
      "Episode 16702; Testing Loss 0.005783756031171481; Training Loss 0.00463279750760086\n",
      "Episode 16703; Testing Loss 0.005783817608972279; Training Loss 0.004632789650995658\n",
      "Episode 16704; Testing Loss 0.005783987844938439; Training Loss 0.004632782572106525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16705; Testing Loss 0.005783969047808259; Training Loss 0.004632773548354665\n",
      "Episode 16706; Testing Loss 0.005783820859503788; Training Loss 0.004632766302776005\n",
      "Episode 16707; Testing Loss 0.0057838679511553704; Training Loss 0.0046327571588566\n",
      "Episode 16708; Testing Loss 0.005783951333918189; Training Loss 0.004632751232929129\n",
      "Episode 16709; Testing Loss 0.005783817722856909; Training Loss 0.00463274137908649\n",
      "Episode 16710; Testing Loss 0.005783673741421149; Training Loss 0.00463273345052106\n",
      "Episode 16711; Testing Loss 0.005783813595450231; Training Loss 0.00463272780555804\n",
      "Episode 16712; Testing Loss 0.005783803629138701; Training Loss 0.004632718709341605\n",
      "Episode 16713; Testing Loss 0.005783687049124115; Training Loss 0.004632710295411486\n",
      "Episode 16714; Testing Loss 0.005783765772732071; Training Loss 0.004632700270645268\n",
      "Episode 16715; Testing Loss 0.00578388059896896; Training Loss 0.004632696568673097\n",
      "Episode 16716; Testing Loss 0.0057836810830295; Training Loss 0.004632686906247006\n",
      "Episode 16717; Testing Loss 0.005783556688600209; Training Loss 0.004632677677038696\n",
      "Episode 16718; Testing Loss 0.005783732375381454; Training Loss 0.004632669045570665\n",
      "Episode 16719; Testing Loss 0.00578384328039461; Training Loss 0.004632663134166317\n",
      "Episode 16720; Testing Loss 0.005783654569643519; Training Loss 0.004632653436171909\n",
      "Episode 16721; Testing Loss 0.005783566091599861; Training Loss 0.0046326458514612\n",
      "Episode 16722; Testing Loss 0.005783736934449532; Training Loss 0.004632638357013483\n",
      "Episode 16723; Testing Loss 0.005783800700358084; Training Loss 0.004632629816585374\n",
      "Episode 16724; Testing Loss 0.005783668969354236; Training Loss 0.004632620069453254\n",
      "Episode 16725; Testing Loss 0.0057836542531912605; Training Loss 0.0046326126529295415\n",
      "Episode 16726; Testing Loss 0.005783752346097745; Training Loss 0.0046326042395168215\n",
      "Episode 16727; Testing Loss 0.005783685231594681; Training Loss 0.0046325967435901965\n",
      "Episode 16728; Testing Loss 0.005783692828127673; Training Loss 0.004632587563760417\n",
      "Episode 16729; Testing Loss 0.005783763232787946; Training Loss 0.004632581737692247\n",
      "Episode 16730; Testing Loss 0.005783620059475766; Training Loss 0.004632572125895402\n",
      "Episode 16731; Testing Loss 0.0057835012452639774; Training Loss 0.004632565961939275\n",
      "Episode 16732; Testing Loss 0.005783691059653202; Training Loss 0.0046325589288080124\n",
      "Episode 16733; Testing Loss 0.005783828690479527; Training Loss 0.0046325517964143845\n",
      "Episode 16734; Testing Loss 0.005783640871557578; Training Loss 0.004632540011756192\n",
      "Episode 16735; Testing Loss 0.00578343915052559; Training Loss 0.004632532930012687\n",
      "Episode 16736; Testing Loss 0.005783562721077415; Training Loss 0.004632526611015582\n",
      "Episode 16737; Testing Loss 0.005783657090933648; Training Loss 0.004632518416973504\n",
      "Episode 16738; Testing Loss 0.005783598378865586; Training Loss 0.004632507278296681\n",
      "Episode 16739; Testing Loss 0.005783543707276814; Training Loss 0.004632501942549464\n",
      "Episode 16740; Testing Loss 0.005783655114456002; Training Loss 0.004632492089126829\n",
      "Episode 16741; Testing Loss 0.005783688508386119; Training Loss 0.004632482798658259\n",
      "Episode 16742; Testing Loss 0.005783501783704821; Training Loss 0.004632475663311383\n",
      "Episode 16743; Testing Loss 0.005783339546613785; Training Loss 0.00463246976973933\n",
      "Episode 16744; Testing Loss 0.005783360398528927; Training Loss 0.004632459683165979\n",
      "Episode 16745; Testing Loss 0.005783537398257606; Training Loss 0.004632449391333839\n",
      "Episode 16746; Testing Loss 0.00578359965096397; Training Loss 0.004632442158623377\n",
      "Episode 16747; Testing Loss 0.005783539904548285; Training Loss 0.004632431897283469\n",
      "Episode 16748; Testing Loss 0.005783475344257657; Training Loss 0.004632423883119086\n",
      "Episode 16749; Testing Loss 0.005783555828540915; Training Loss 0.004632415655949109\n",
      "Episode 16750; Testing Loss 0.005783553536387983; Training Loss 0.004632406249690337\n",
      "Episode 16751; Testing Loss 0.005783432482679924; Training Loss 0.004632397531449194\n",
      "Episode 16752; Testing Loss 0.005783320814799691; Training Loss 0.004632390317573038\n",
      "Episode 16753; Testing Loss 0.005783362635175785; Training Loss 0.004632381588343247\n",
      "Episode 16754; Testing Loss 0.005783394698328706; Training Loss 0.004632373854490262\n",
      "Episode 16755; Testing Loss 0.005783290537300352; Training Loss 0.004632364104185328\n",
      "Episode 16756; Testing Loss 0.005783263161629826; Training Loss 0.004632355219376984\n",
      "Episode 16757; Testing Loss 0.005783377075930698; Training Loss 0.004632346246593005\n",
      "Episode 16758; Testing Loss 0.005783430462215525; Training Loss 0.004632338649323995\n",
      "Episode 16759; Testing Loss 0.005783259681743658; Training Loss 0.004632329328270001\n",
      "Episode 16760; Testing Loss 0.005783148643186295; Training Loss 0.00463232027188339\n",
      "Episode 16761; Testing Loss 0.005783087949483772; Training Loss 0.004632311927739496\n",
      "Episode 16762; Testing Loss 0.0057830500994565095; Training Loss 0.004632303628237101\n",
      "Episode 16763; Testing Loss 0.005782984007072029; Training Loss 0.004632294071045243\n",
      "Episode 16764; Testing Loss 0.005782989843595893; Training Loss 0.004632285831555349\n",
      "Episode 16765; Testing Loss 0.005782977346535355; Training Loss 0.004632278551485824\n",
      "Episode 16766; Testing Loss 0.0057830142775670695; Training Loss 0.00463226867455444\n",
      "Episode 16767; Testing Loss 0.005783059351805154; Training Loss 0.004632260318979308\n",
      "Episode 16768; Testing Loss 0.0057830740300770745; Training Loss 0.004632252136361972\n",
      "Episode 16769; Testing Loss 0.0057830531637158376; Training Loss 0.004632243424271407\n",
      "Episode 16770; Testing Loss 0.005782989562434161; Training Loss 0.004632235880579192\n",
      "Episode 16771; Testing Loss 0.005782905374122063; Training Loss 0.004632227526081284\n",
      "Episode 16772; Testing Loss 0.005782903839860455; Training Loss 0.004632218959091903\n",
      "Episode 16773; Testing Loss 0.005782923834214339; Training Loss 0.0046322103310416874\n",
      "Episode 16774; Testing Loss 0.005782921359517707; Training Loss 0.00463220132213826\n",
      "Episode 16775; Testing Loss 0.00578291709556703; Training Loss 0.004632193806937184\n",
      "Episode 16776; Testing Loss 0.0057829295850764465; Training Loss 0.00463218489762241\n",
      "Episode 16777; Testing Loss 0.005782900439243285; Training Loss 0.004632175162848697\n",
      "Episode 16778; Testing Loss 0.005782789334041342; Training Loss 0.0046321664964209365\n",
      "Episode 16779; Testing Loss 0.005782691381071938; Training Loss 0.004632158271983952\n",
      "Episode 16780; Testing Loss 0.005782680415963571; Training Loss 0.00463215035977606\n",
      "Episode 16781; Testing Loss 0.005782678483740785; Training Loss 0.004632141677617113\n",
      "Episode 16782; Testing Loss 0.005782642044623863; Training Loss 0.004632132201073592\n",
      "Episode 16783; Testing Loss 0.005782617712396575; Training Loss 0.00463212343529687\n",
      "Episode 16784; Testing Loss 0.005782596147782887; Training Loss 0.004632115293043393\n",
      "Episode 16785; Testing Loss 0.005782716347687333; Training Loss 0.004632108803729879\n",
      "Episode 16786; Testing Loss 0.005782807672353132; Training Loss 0.004632099886493301\n",
      "Episode 16787; Testing Loss 0.005782644405197109; Training Loss 0.004632090043990192\n",
      "Episode 16788; Testing Loss 0.0057824928235521685; Training Loss 0.004632081452534503\n",
      "Episode 16789; Testing Loss 0.005782485960101584; Training Loss 0.004632073821591208\n",
      "Episode 16790; Testing Loss 0.005782538873446293; Training Loss 0.004632064918068063\n",
      "Episode 16791; Testing Loss 0.0057825474360344905; Training Loss 0.004632057404076724\n",
      "Episode 16792; Testing Loss 0.005782532857537043; Training Loss 0.004632048007037459\n",
      "Episode 16793; Testing Loss 0.005782496571490466; Training Loss 0.004632038835904297\n",
      "Episode 16794; Testing Loss 0.00578252531705607; Training Loss 0.004632031066329042\n",
      "Episode 16795; Testing Loss 0.005782530160069714; Training Loss 0.0046320215962502775\n",
      "Episode 16796; Testing Loss 0.005782552837266873; Training Loss 0.004632012931932596\n",
      "Episode 16797; Testing Loss 0.005782481659362064; Training Loss 0.004632004912266084\n",
      "Episode 16798; Testing Loss 0.005782446398385169; Training Loss 0.004631997438675615\n",
      "Episode 16799; Testing Loss 0.005782566306711917; Training Loss 0.004631988494648211\n",
      "Episode 16800; Testing Loss 0.005782567304442809; Training Loss 0.004631982211821189\n",
      "Episode 16801; Testing Loss 0.005782398055265827; Training Loss 0.004631971823593608\n",
      "Episode 16802; Testing Loss 0.005782352218324403; Training Loss 0.004631963412695297\n",
      "Episode 16803; Testing Loss 0.005782447049381969; Training Loss 0.00463195421728665\n",
      "Episode 16804; Testing Loss 0.005782477902643343; Training Loss 0.0046319457925832076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16805; Testing Loss 0.005782445048804479; Training Loss 0.004631937699565048\n",
      "Episode 16806; Testing Loss 0.005782475031503818; Training Loss 0.004631928718175332\n",
      "Episode 16807; Testing Loss 0.005782435522577478; Training Loss 0.004631920216115302\n",
      "Episode 16808; Testing Loss 0.005782314208595282; Training Loss 0.0046319132736644215\n",
      "Episode 16809; Testing Loss 0.005782232681364578; Training Loss 0.0046319040357580155\n",
      "Episode 16810; Testing Loss 0.005782353926117263; Training Loss 0.004631894921626671\n",
      "Episode 16811; Testing Loss 0.005782426947935408; Training Loss 0.00463188751208496\n",
      "Episode 16812; Testing Loss 0.005782313503660672; Training Loss 0.004631878304824148\n",
      "Episode 16813; Testing Loss 0.005782204204576335; Training Loss 0.004631870301667285\n",
      "Episode 16814; Testing Loss 0.005782254883692308; Training Loss 0.004631861505718965\n",
      "Episode 16815; Testing Loss 0.0057823318768426175; Training Loss 0.004631853441768735\n",
      "Episode 16816; Testing Loss 0.005782294388000219; Training Loss 0.004631844748456208\n",
      "Episode 16817; Testing Loss 0.005782210914194576; Training Loss 0.004631835747000719\n",
      "Episode 16818; Testing Loss 0.005782267285550919; Training Loss 0.004631826841304357\n",
      "Episode 16819; Testing Loss 0.005782299777328797; Training Loss 0.004631819244795896\n",
      "Episode 16820; Testing Loss 0.005782218918733402; Training Loss 0.004631811322081186\n",
      "Episode 16821; Testing Loss 0.005782211542333444; Training Loss 0.004631802525535245\n",
      "Episode 16822; Testing Loss 0.005782203851237679; Training Loss 0.0046317940144834395\n",
      "Episode 16823; Testing Loss 0.005782189067662708; Training Loss 0.004631787293520438\n",
      "Episode 16824; Testing Loss 0.00578218911333367; Training Loss 0.004631778058842361\n",
      "Episode 16825; Testing Loss 0.005782115463012262; Training Loss 0.00463177006950244\n",
      "Episode 16826; Testing Loss 0.005782042599868055; Training Loss 0.004631761259960028\n",
      "Episode 16827; Testing Loss 0.005782041920015924; Training Loss 0.0046317525029106795\n",
      "Episode 16828; Testing Loss 0.005782085845893053; Training Loss 0.004631743564727797\n",
      "Episode 16829; Testing Loss 0.005782143078029927; Training Loss 0.0046317365573947476\n",
      "Episode 16830; Testing Loss 0.005782142035274991; Training Loss 0.004631727676285053\n",
      "Episode 16831; Testing Loss 0.005782009459682716; Training Loss 0.004631719105504646\n",
      "Episode 16832; Testing Loss 0.00578196128880494; Training Loss 0.004631710619020887\n",
      "Episode 16833; Testing Loss 0.005781960714907811; Training Loss 0.004631702694444034\n",
      "Episode 16834; Testing Loss 0.005782005921969217; Training Loss 0.004631695640141067\n",
      "Episode 16835; Testing Loss 0.005782014480649326; Training Loss 0.004631686481293991\n",
      "Episode 16836; Testing Loss 0.005781892558854931; Training Loss 0.004631677708812861\n",
      "Episode 16837; Testing Loss 0.00578185203827823; Training Loss 0.004631669625080688\n",
      "Episode 16838; Testing Loss 0.0057819240745696; Training Loss 0.004631661510900876\n",
      "Episode 16839; Testing Loss 0.005781948416180482; Training Loss 0.004631652326945862\n",
      "Episode 16840; Testing Loss 0.00578197045170928; Training Loss 0.004631643967316033\n",
      "Episode 16841; Testing Loss 0.005781943316284377; Training Loss 0.004631635822056758\n",
      "Episode 16842; Testing Loss 0.005781842446377507; Training Loss 0.004631628150232713\n",
      "Episode 16843; Testing Loss 0.005781894444406251; Training Loss 0.004631620152964294\n",
      "Episode 16844; Testing Loss 0.005781958361483031; Training Loss 0.004631611696836412\n",
      "Episode 16845; Testing Loss 0.005781874264268822; Training Loss 0.004631603219066021\n",
      "Episode 16846; Testing Loss 0.00578173605704327; Training Loss 0.004631595552109898\n",
      "Episode 16847; Testing Loss 0.005781767163163256; Training Loss 0.004631586718535366\n",
      "Episode 16848; Testing Loss 0.0057818668810843; Training Loss 0.004631578891894409\n",
      "Episode 16849; Testing Loss 0.005781770822061662; Training Loss 0.004631569461317688\n",
      "Episode 16850; Testing Loss 0.005781656375300142; Training Loss 0.0046315616850892\n",
      "Episode 16851; Testing Loss 0.005781669457476607; Training Loss 0.0046315532291255025\n",
      "Episode 16852; Testing Loss 0.005781751939795848; Training Loss 0.004631546053583649\n",
      "Episode 16853; Testing Loss 0.005781644600409171; Training Loss 0.00463153753146682\n",
      "Episode 16854; Testing Loss 0.005781654956385301; Training Loss 0.004631528045118056\n",
      "Episode 16855; Testing Loss 0.0057817070833003515; Training Loss 0.0046315204008642915\n",
      "Episode 16856; Testing Loss 0.005781636262925817; Training Loss 0.0046315116929903175\n",
      "Episode 16857; Testing Loss 0.005781675214827085; Training Loss 0.004631503716784097\n",
      "Episode 16858; Testing Loss 0.005781779222547114; Training Loss 0.0046314958836035265\n",
      "Episode 16859; Testing Loss 0.0057815998883541135; Training Loss 0.004631487400497078\n",
      "Episode 16860; Testing Loss 0.00578141692328408; Training Loss 0.00463148087768981\n",
      "Episode 16861; Testing Loss 0.005781578921385739; Training Loss 0.0046314709770653665\n",
      "Episode 16862; Testing Loss 0.00578177467410063; Training Loss 0.004631464404279248\n",
      "Episode 16863; Testing Loss 0.005781682866076581; Training Loss 0.004631455564849887\n",
      "Episode 16864; Testing Loss 0.005781550085711981; Training Loss 0.004631447533694311\n",
      "Episode 16865; Testing Loss 0.005781469598935558; Training Loss 0.004631438505077962\n",
      "Episode 16866; Testing Loss 0.005781519493705607; Training Loss 0.004631429385424996\n",
      "Episode 16867; Testing Loss 0.005781657424109327; Training Loss 0.0046314226647841475\n",
      "Episode 16868; Testing Loss 0.005781510793184794; Training Loss 0.0046314127134106774\n",
      "Episode 16869; Testing Loss 0.0057813586385341885; Training Loss 0.004631405751014969\n",
      "Episode 16870; Testing Loss 0.005781465814686958; Training Loss 0.004631395973456281\n",
      "Episode 16871; Testing Loss 0.005781577298221328; Training Loss 0.004631388660933845\n",
      "Episode 16872; Testing Loss 0.005781462723099935; Training Loss 0.004631379608393355\n",
      "Episode 16873; Testing Loss 0.0057813360528191785; Training Loss 0.00463137100599143\n",
      "Episode 16874; Testing Loss 0.005781328261587035; Training Loss 0.004631363541349388\n",
      "Episode 16875; Testing Loss 0.005781335230770795; Training Loss 0.004631354401555691\n",
      "Episode 16876; Testing Loss 0.005781381871840267; Training Loss 0.004631346424514409\n",
      "Episode 16877; Testing Loss 0.005781368300859473; Training Loss 0.004631338875956118\n",
      "Episode 16878; Testing Loss 0.005781357631675129; Training Loss 0.0046313309543227115\n",
      "Episode 16879; Testing Loss 0.005781392240269667; Training Loss 0.004631322239662237\n",
      "Episode 16880; Testing Loss 0.005781242899886206; Training Loss 0.004631313720729451\n",
      "Episode 16881; Testing Loss 0.005781126226087011; Training Loss 0.0046313079202894404\n",
      "Episode 16882; Testing Loss 0.005781134956948433; Training Loss 0.004631298072165689\n",
      "Episode 16883; Testing Loss 0.005781321752186976; Training Loss 0.004631288709365085\n",
      "Episode 16884; Testing Loss 0.005781403045777495; Training Loss 0.004631281322221032\n",
      "Episode 16885; Testing Loss 0.005781265264613638; Training Loss 0.004631272714380951\n",
      "Episode 16886; Testing Loss 0.005781196389924125; Training Loss 0.004631264603118571\n",
      "Episode 16887; Testing Loss 0.005781247017854726; Training Loss 0.004631255921963833\n",
      "Episode 16888; Testing Loss 0.005781269174886415; Training Loss 0.0046312479753019015\n",
      "Episode 16889; Testing Loss 0.005781150268712249; Training Loss 0.004631239462187831\n",
      "Episode 16890; Testing Loss 0.005781103823530371; Training Loss 0.004631231353672273\n",
      "Episode 16891; Testing Loss 0.005781153015739573; Training Loss 0.00463122341505706\n",
      "Episode 16892; Testing Loss 0.005781119393404784; Training Loss 0.004631215171688172\n",
      "Episode 16893; Testing Loss 0.005781101366664069; Training Loss 0.004631209378704914\n",
      "Episode 16894; Testing Loss 0.005781144330462945; Training Loss 0.004631198523657325\n",
      "Episode 16895; Testing Loss 0.005781110171424169; Training Loss 0.0046311921050972635\n",
      "Episode 16896; Testing Loss 0.005780997564681401; Training Loss 0.004631184431737348\n",
      "Episode 16897; Testing Loss 0.005780981722272024; Training Loss 0.004631173706613328\n",
      "Episode 16898; Testing Loss 0.005781057443320608; Training Loss 0.004631165925790315\n",
      "Episode 16899; Testing Loss 0.005781060608788849; Training Loss 0.004631159186821419\n",
      "Episode 16900; Testing Loss 0.00578100478038933; Training Loss 0.004631149320317442\n",
      "Episode 16901; Testing Loss 0.005780927865998457; Training Loss 0.004631140644653717\n",
      "Episode 16902; Testing Loss 0.005780947227903724; Training Loss 0.004631134484779398\n",
      "Episode 16903; Testing Loss 0.005780926744658084; Training Loss 0.0046311246092936515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16904; Testing Loss 0.005780893113376251; Training Loss 0.004631116370120326\n",
      "Episode 16905; Testing Loss 0.005780959176030767; Training Loss 0.0046311122359942725\n",
      "Episode 16906; Testing Loss 0.005781033896080808; Training Loss 0.004631103521598067\n",
      "Episode 16907; Testing Loss 0.005780985263970817; Training Loss 0.0046310923030619755\n",
      "Episode 16908; Testing Loss 0.005780854526364388; Training Loss 0.0046310845574944305\n",
      "Episode 16909; Testing Loss 0.005780841888892072; Training Loss 0.004631079350305171\n",
      "Episode 16910; Testing Loss 0.005780917666659607; Training Loss 0.004631070159119226\n",
      "Episode 16911; Testing Loss 0.005780891890601921; Training Loss 0.004631061110291246\n",
      "Episode 16912; Testing Loss 0.005780809714436401; Training Loss 0.00463105355296692\n",
      "Episode 16913; Testing Loss 0.005780770639155816; Training Loss 0.004631045490835714\n",
      "Episode 16914; Testing Loss 0.0057808679320937514; Training Loss 0.004631036669686222\n",
      "Episode 16915; Testing Loss 0.00578092433816225; Training Loss 0.004631028373681289\n",
      "Episode 16916; Testing Loss 0.005780800062790521; Training Loss 0.004631019576526123\n",
      "Episode 16917; Testing Loss 0.005780632032759285; Training Loss 0.004631012153754477\n",
      "Episode 16918; Testing Loss 0.0057805948276707965; Training Loss 0.004631003626676565\n",
      "Episode 16919; Testing Loss 0.0057807659822223445; Training Loss 0.004630994616577131\n",
      "Episode 16920; Testing Loss 0.00578083805105706; Training Loss 0.00463098707487064\n",
      "Episode 16921; Testing Loss 0.0057806861627912876; Training Loss 0.004630979186375665\n",
      "Episode 16922; Testing Loss 0.005780641468227281; Training Loss 0.004630971019121398\n",
      "Episode 16923; Testing Loss 0.005780724350138644; Training Loss 0.004630961917385957\n",
      "Episode 16924; Testing Loss 0.005780701302296052; Training Loss 0.004630953881931413\n",
      "Episode 16925; Testing Loss 0.005780583935490145; Training Loss 0.004630944762096113\n",
      "Episode 16926; Testing Loss 0.005780539933310967; Training Loss 0.004630937229818628\n",
      "Episode 16927; Testing Loss 0.005780614316223725; Training Loss 0.004630929164246606\n",
      "Episode 16928; Testing Loss 0.005780619432683654; Training Loss 0.004630921250345081\n",
      "Episode 16929; Testing Loss 0.00578052561348268; Training Loss 0.0046309123741138545\n",
      "Episode 16930; Testing Loss 0.005780480392144541; Training Loss 0.004630904628476719\n",
      "Episode 16931; Testing Loss 0.005780585026518231; Training Loss 0.004630895998290534\n",
      "Episode 16932; Testing Loss 0.005780588191651379; Training Loss 0.004630888053113577\n",
      "Episode 16933; Testing Loss 0.0057804260937263; Training Loss 0.004630879226554382\n",
      "Episode 16934; Testing Loss 0.005780337730068986; Training Loss 0.0046308718593001535\n",
      "Episode 16935; Testing Loss 0.005780378668815563; Training Loss 0.0046308622713449295\n",
      "Episode 16936; Testing Loss 0.005780454239241596; Training Loss 0.004630854114159414\n",
      "Episode 16937; Testing Loss 0.005780480497980317; Training Loss 0.004630846757868427\n",
      "Episode 16938; Testing Loss 0.005780424674887988; Training Loss 0.004630839561089785\n",
      "Episode 16939; Testing Loss 0.005780447476438028; Training Loss 0.0046308291515489534\n",
      "Episode 16940; Testing Loss 0.005780387333098247; Training Loss 0.004630820924378563\n",
      "Episode 16941; Testing Loss 0.00578030706687068; Training Loss 0.004630814235820573\n",
      "Episode 16942; Testing Loss 0.00578026434058675; Training Loss 0.004630805472017298\n",
      "Episode 16943; Testing Loss 0.0057803661036175125; Training Loss 0.0046307967648610065\n",
      "Episode 16944; Testing Loss 0.005780385207615724; Training Loss 0.00463078865354258\n",
      "Episode 16945; Testing Loss 0.0057803127806713124; Training Loss 0.0046307805568696955\n",
      "Episode 16946; Testing Loss 0.005780306885518322; Training Loss 0.004630771827391133\n",
      "Episode 16947; Testing Loss 0.005780374903142574; Training Loss 0.004630764078422081\n",
      "Episode 16948; Testing Loss 0.0057802999822392995; Training Loss 0.004630755885193767\n",
      "Episode 16949; Testing Loss 0.00578017508752078; Training Loss 0.004630747633610188\n",
      "Episode 16950; Testing Loss 0.005780216045044114; Training Loss 0.0046307390404564325\n",
      "Episode 16951; Testing Loss 0.005780242715301597; Training Loss 0.004630730812397095\n",
      "Episode 16952; Testing Loss 0.005780243308228506; Training Loss 0.004630723354157482\n",
      "Episode 16953; Testing Loss 0.005780198868778765; Training Loss 0.0046307163631163235\n",
      "Episode 16954; Testing Loss 0.005780318424259173; Training Loss 0.004630708468097106\n",
      "Episode 16955; Testing Loss 0.005780253045548495; Training Loss 0.004630699415289749\n",
      "Episode 16956; Testing Loss 0.0057802389026609564; Training Loss 0.004630690349042812\n",
      "Episode 16957; Testing Loss 0.005780166633103202; Training Loss 0.004630683858978477\n",
      "Episode 16958; Testing Loss 0.005780053843162127; Training Loss 0.004630674790831812\n",
      "Episode 16959; Testing Loss 0.005780084644059221; Training Loss 0.0046306662051059325\n",
      "Episode 16960; Testing Loss 0.0057802151273963; Training Loss 0.0046306584734463665\n",
      "Episode 16961; Testing Loss 0.005780199365164641; Training Loss 0.004630650238354963\n",
      "Episode 16962; Testing Loss 0.005780032639250337; Training Loss 0.004630643143973104\n",
      "Episode 16963; Testing Loss 0.005779945466215448; Training Loss 0.00463063390955079\n",
      "Episode 16964; Testing Loss 0.005780088031994852; Training Loss 0.004630625284042372\n",
      "Episode 16965; Testing Loss 0.005780148157389488; Training Loss 0.004630618920178747\n",
      "Episode 16966; Testing Loss 0.005779902674897194; Training Loss 0.004630609736501641\n",
      "Episode 16967; Testing Loss 0.0057797875563098886; Training Loss 0.004630602838895469\n",
      "Episode 16968; Testing Loss 0.005779916176358874; Training Loss 0.00463059323326971\n",
      "Episode 16969; Testing Loss 0.005780092978430259; Training Loss 0.00463058741213756\n",
      "Episode 16970; Testing Loss 0.00577997847689556; Training Loss 0.004630576694941652\n",
      "Episode 16971; Testing Loss 0.005779944975947245; Training Loss 0.0046305701514271075\n",
      "Episode 16972; Testing Loss 0.005780124568943074; Training Loss 0.0046305606675139145\n",
      "Episode 16973; Testing Loss 0.005780181125776428; Training Loss 0.004630554419176794\n",
      "Episode 16974; Testing Loss 0.005779999119085376; Training Loss 0.0046305451194720015\n",
      "Episode 16975; Testing Loss 0.00577990388902828; Training Loss 0.004630537528457109\n",
      "Episode 16976; Testing Loss 0.005780048702529941; Training Loss 0.004630527650173969\n",
      "Episode 16977; Testing Loss 0.005779994067152211; Training Loss 0.004630520128853892\n",
      "Episode 16978; Testing Loss 0.005779741998131392; Training Loss 0.004630512091819749\n",
      "Episode 16979; Testing Loss 0.0057797360368603565; Training Loss 0.004630503922616246\n",
      "Episode 16980; Testing Loss 0.005779913924408109; Training Loss 0.0046304953645991305\n",
      "Episode 16981; Testing Loss 0.005779938317550536; Training Loss 0.004630487691664012\n",
      "Episode 16982; Testing Loss 0.005779781974428323; Training Loss 0.0046304788240784395\n",
      "Episode 16983; Testing Loss 0.005779748053429106; Training Loss 0.004630471385632959\n",
      "Episode 16984; Testing Loss 0.005779912484283006; Training Loss 0.004630461663131828\n",
      "Episode 16985; Testing Loss 0.00577996662864178; Training Loss 0.004630454109858988\n",
      "Episode 16986; Testing Loss 0.005779767126027199; Training Loss 0.004630445136078815\n",
      "Episode 16987; Testing Loss 0.005779631888997819; Training Loss 0.004630438109147003\n",
      "Episode 16988; Testing Loss 0.005779746988536498; Training Loss 0.004630429015038488\n",
      "Episode 16989; Testing Loss 0.0057798266146600625; Training Loss 0.00463042083141575\n",
      "Episode 16990; Testing Loss 0.005779790321259521; Training Loss 0.004630412173388377\n",
      "Episode 16991; Testing Loss 0.005779691373699938; Training Loss 0.004630405912021907\n",
      "Episode 16992; Testing Loss 0.005779814268285128; Training Loss 0.004630396003737871\n",
      "Episode 16993; Testing Loss 0.005779827422823205; Training Loss 0.004630390009048478\n",
      "Episode 16994; Testing Loss 0.0057796746465768965; Training Loss 0.004630379481098358\n",
      "Episode 16995; Testing Loss 0.005779582436264924; Training Loss 0.004630371647281057\n",
      "Episode 16996; Testing Loss 0.005779701261932232; Training Loss 0.0046303625184313775\n",
      "Episode 16997; Testing Loss 0.005779773891179257; Training Loss 0.004630355325800446\n",
      "Episode 16998; Testing Loss 0.005779635230514211; Training Loss 0.004630347630525325\n",
      "Episode 16999; Testing Loss 0.005779646501183056; Training Loss 0.0046303404772749255\n",
      "Episode 17000; Testing Loss 0.005779632745651976; Training Loss 0.004630331022365071\n",
      "Episode 17001; Testing Loss 0.0057795718553017485; Training Loss 0.0046303228355001\n",
      "Episode 17002; Testing Loss 0.005779583140635793; Training Loss 0.004630314984513183\n",
      "Episode 17003; Testing Loss 0.005779571322341592; Training Loss 0.0046303068876409875\n",
      "Episode 17004; Testing Loss 0.005779529177798106; Training Loss 0.004630298676544173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17005; Testing Loss 0.005779470366305514; Training Loss 0.004630290449153059\n",
      "Episode 17006; Testing Loss 0.005779506644156127; Training Loss 0.004630282788202728\n",
      "Episode 17007; Testing Loss 0.005779513539885338; Training Loss 0.00463027362002923\n",
      "Episode 17008; Testing Loss 0.0057795873612055005; Training Loss 0.004630266264911251\n",
      "Episode 17009; Testing Loss 0.005779647790134724; Training Loss 0.004630258639765714\n",
      "Episode 17010; Testing Loss 0.005779498999198614; Training Loss 0.004630249573949777\n",
      "Episode 17011; Testing Loss 0.005779319915403567; Training Loss 0.004630242784585342\n",
      "Episode 17012; Testing Loss 0.0057794412197582435; Training Loss 0.00463023367579449\n",
      "Episode 17013; Testing Loss 0.005779593637030759; Training Loss 0.004630226327892129\n",
      "Episode 17014; Testing Loss 0.005779527707601218; Training Loss 0.004630217337665002\n",
      "Episode 17015; Testing Loss 0.00577935849771331; Training Loss 0.004630209473860339\n",
      "Episode 17016; Testing Loss 0.005779387907115813; Training Loss 0.004630201046699821\n",
      "Episode 17017; Testing Loss 0.005779491008836098; Training Loss 0.004630193545691243\n",
      "Episode 17018; Testing Loss 0.005779514165874531; Training Loss 0.00463018565648772\n",
      "Episode 17019; Testing Loss 0.0057794116759878195; Training Loss 0.004630176458391293\n",
      "Episode 17020; Testing Loss 0.005779311357562678; Training Loss 0.004630168471384029\n",
      "Episode 17021; Testing Loss 0.005779339856657938; Training Loss 0.0046301605783475586\n",
      "Episode 17022; Testing Loss 0.005779375935632234; Training Loss 0.004630153728976964\n",
      "Episode 17023; Testing Loss 0.00577928386156354; Training Loss 0.004630144559379698\n",
      "Episode 17024; Testing Loss 0.00577926582736095; Training Loss 0.004630136009676902\n",
      "Episode 17025; Testing Loss 0.005779334829411198; Training Loss 0.004630127479124496\n",
      "Episode 17026; Testing Loss 0.0057793553934187706; Training Loss 0.004630119692492458\n",
      "Episode 17027; Testing Loss 0.0057793148721325075; Training Loss 0.004630114229450112\n",
      "Episode 17028; Testing Loss 0.005779221801097935; Training Loss 0.004630103817433137\n",
      "Episode 17029; Testing Loss 0.0057791970304302085; Training Loss 0.00463009807778937\n",
      "Episode 17030; Testing Loss 0.005779330227152549; Training Loss 0.004630089991793062\n",
      "Episode 17031; Testing Loss 0.0057793539514436245; Training Loss 0.004630079908666413\n",
      "Episode 17032; Testing Loss 0.005779152225900057; Training Loss 0.004630072755333887\n",
      "Episode 17033; Testing Loss 0.005779021889164024; Training Loss 0.004630065215105854\n",
      "Episode 17034; Testing Loss 0.005779235419374291; Training Loss 0.004630054042492331\n",
      "Episode 17035; Testing Loss 0.005779381186700519; Training Loss 0.004630047215593413\n",
      "Episode 17036; Testing Loss 0.00577919068773367; Training Loss 0.004630040358053857\n",
      "Episode 17037; Testing Loss 0.0057791010984089815; Training Loss 0.0046300306258771325\n",
      "Episode 17038; Testing Loss 0.005779151283602704; Training Loss 0.004630021582332002\n",
      "Episode 17039; Testing Loss 0.005779191943824968; Training Loss 0.004630013411097731\n",
      "Episode 17040; Testing Loss 0.005779180668073034; Training Loss 0.004630006861501422\n",
      "Episode 17041; Testing Loss 0.005779129110912481; Training Loss 0.0046299976382740745\n",
      "Episode 17042; Testing Loss 0.00577918595356819; Training Loss 0.004629989599294968\n",
      "Episode 17043; Testing Loss 0.005779185885950257; Training Loss 0.004629981834790239\n",
      "Episode 17044; Testing Loss 0.0057790651473684776; Training Loss 0.00462997392168976\n",
      "Episode 17045; Testing Loss 0.005779039435036916; Training Loss 0.004629965379556816\n",
      "Episode 17046; Testing Loss 0.005779133001909731; Training Loss 0.00462995735701987\n",
      "Episode 17047; Testing Loss 0.005779161043161613; Training Loss 0.004629949274427284\n",
      "Episode 17048; Testing Loss 0.005779129416958806; Training Loss 0.004629941249623929\n",
      "Episode 17049; Testing Loss 0.005779080714756679; Training Loss 0.004629932779264829\n",
      "Episode 17050; Testing Loss 0.005779032769966493; Training Loss 0.0046299245258069775\n",
      "Episode 17051; Testing Loss 0.005778973233297945; Training Loss 0.004629916384004821\n",
      "Episode 17052; Testing Loss 0.005778969120101764; Training Loss 0.0046299080526718064\n",
      "Episode 17053; Testing Loss 0.005779039751467767; Training Loss 0.004629900419503942\n",
      "Episode 17054; Testing Loss 0.005779050512435828; Training Loss 0.004629891754609788\n",
      "Episode 17055; Testing Loss 0.005779009233770183; Training Loss 0.004629883951102627\n",
      "Episode 17056; Testing Loss 0.005778969096624584; Training Loss 0.004629876561728922\n",
      "Episode 17057; Testing Loss 0.005779003345559998; Training Loss 0.004629867940674548\n",
      "Episode 17058; Testing Loss 0.005778930688917438; Training Loss 0.004629861886267016\n",
      "Episode 17059; Testing Loss 0.005778837962713465; Training Loss 0.004629852673606936\n",
      "Episode 17060; Testing Loss 0.0057788702914908716; Training Loss 0.004629844435014872\n",
      "Episode 17061; Testing Loss 0.005779005810493116; Training Loss 0.004629837614044334\n",
      "Episode 17062; Testing Loss 0.005779001979677737; Training Loss 0.004629828657998169\n",
      "Episode 17063; Testing Loss 0.0057787596084450926; Training Loss 0.004629820831745228\n",
      "Episode 17064; Testing Loss 0.005778667570751547; Training Loss 0.004629813723749112\n",
      "Episode 17065; Testing Loss 0.005778903123748771; Training Loss 0.004629802430912408\n",
      "Episode 17066; Testing Loss 0.005779071368005476; Training Loss 0.004629797569924263\n",
      "Episode 17067; Testing Loss 0.005778842883764152; Training Loss 0.004629788468852859\n",
      "Episode 17068; Testing Loss 0.0057787384081527966; Training Loss 0.004629782138175057\n",
      "Episode 17069; Testing Loss 0.00577892601670697; Training Loss 0.004629771894782284\n",
      "Episode 17070; Testing Loss 0.005778935888030663; Training Loss 0.004629765221425985\n",
      "Episode 17071; Testing Loss 0.005778737206737354; Training Loss 0.0046297562415174446\n",
      "Episode 17072; Testing Loss 0.005778657643813915; Training Loss 0.0046297495162795676\n",
      "Episode 17073; Testing Loss 0.005778738343687792; Training Loss 0.004629740164106519\n",
      "Episode 17074; Testing Loss 0.005778896009593072; Training Loss 0.0046297330310333005\n",
      "Episode 17075; Testing Loss 0.0057788500471170874; Training Loss 0.00462972383982599\n",
      "Episode 17076; Testing Loss 0.0057786953617500565; Training Loss 0.004629716325008187\n",
      "Episode 17077; Testing Loss 0.005778709547464545; Training Loss 0.004629707820695669\n",
      "Episode 17078; Testing Loss 0.005778839825809201; Training Loss 0.004629700226610405\n",
      "Episode 17079; Testing Loss 0.005778800625769082; Training Loss 0.0046296921889299025\n",
      "Episode 17080; Testing Loss 0.005778581928560859; Training Loss 0.004629683596302854\n",
      "Episode 17081; Testing Loss 0.005778596381817453; Training Loss 0.004629675289848393\n",
      "Episode 17082; Testing Loss 0.005778804545935875; Training Loss 0.004629667706668542\n",
      "Episode 17083; Testing Loss 0.00577878967392134; Training Loss 0.004629659764818351\n",
      "Episode 17084; Testing Loss 0.0057785574070624525; Training Loss 0.004629651323853116\n",
      "Episode 17085; Testing Loss 0.005778562149376473; Training Loss 0.0046296440035356315\n",
      "Episode 17086; Testing Loss 0.005778783959409032; Training Loss 0.004629635729858806\n",
      "Episode 17087; Testing Loss 0.005778700252317178; Training Loss 0.0046296284349842745\n",
      "Episode 17088; Testing Loss 0.005778378493792509; Training Loss 0.004629619200643387\n",
      "Episode 17089; Testing Loss 0.005778380784933555; Training Loss 0.0046296113718412085\n",
      "Episode 17090; Testing Loss 0.005778678720689875; Training Loss 0.004629603247899015\n",
      "Episode 17091; Testing Loss 0.005778727701424925; Training Loss 0.004629595059214822\n",
      "Episode 17092; Testing Loss 0.005778476249254721; Training Loss 0.0046295861551172565\n",
      "Episode 17093; Testing Loss 0.005778446518586157; Training Loss 0.004629579687034967\n",
      "Episode 17094; Testing Loss 0.005778729389456427; Training Loss 0.004629572156776632\n",
      "Episode 17095; Testing Loss 0.005778678173535754; Training Loss 0.0046295638148196126\n",
      "Episode 17096; Testing Loss 0.005778408429459215; Training Loss 0.004629555310560406\n",
      "Episode 17097; Testing Loss 0.005778440006936267; Training Loss 0.0046295478010599115\n",
      "Episode 17098; Testing Loss 0.005778693864081443; Training Loss 0.004629539738105095\n",
      "Episode 17099; Testing Loss 0.005778618370011747; Training Loss 0.004629530685683313\n",
      "Episode 17100; Testing Loss 0.005778317360702559; Training Loss 0.004629523058246094\n",
      "Episode 17101; Testing Loss 0.0057782804600505795; Training Loss 0.00462951552453116\n",
      "Episode 17102; Testing Loss 0.005778466186147027; Training Loss 0.0046295046386263\n",
      "Episode 17103; Testing Loss 0.005778612862044918; Training Loss 0.004629498735784379\n",
      "Episode 17104; Testing Loss 0.005778505197379753; Training Loss 0.004629490539539403\n",
      "Episode 17105; Testing Loss 0.005778394185638115; Training Loss 0.004629481902454976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17106; Testing Loss 0.005778431922464362; Training Loss 0.004629474254551671\n",
      "Episode 17107; Testing Loss 0.00577844840722349; Training Loss 0.004629465302399936\n",
      "Episode 17108; Testing Loss 0.005778332138088396; Training Loss 0.004629456794566323\n",
      "Episode 17109; Testing Loss 0.005778332243527391; Training Loss 0.004629452028634921\n",
      "Episode 17110; Testing Loss 0.0057784411851218844; Training Loss 0.004629442200485497\n",
      "Episode 17111; Testing Loss 0.005778423181905216; Training Loss 0.004629433367074395\n",
      "Episode 17112; Testing Loss 0.0057782705802416285; Training Loss 0.004629428026648603\n",
      "Episode 17113; Testing Loss 0.005778213189523156; Training Loss 0.004629421820554462\n",
      "Episode 17114; Testing Loss 0.0057783685407625405; Training Loss 0.004629408915628486\n",
      "Episode 17115; Testing Loss 0.0057784844771382494; Training Loss 0.004629401804557916\n",
      "Episode 17116; Testing Loss 0.0057783373016018205; Training Loss 0.004629393224808975\n",
      "Episode 17117; Testing Loss 0.005778172559408603; Training Loss 0.0046293872609906046\n",
      "Episode 17118; Testing Loss 0.005778280675391623; Training Loss 0.004629376577639121\n",
      "Episode 17119; Testing Loss 0.0057783761235389915; Training Loss 0.004629369059880508\n",
      "Episode 17120; Testing Loss 0.005778265954313169; Training Loss 0.004629360254440768\n",
      "Episode 17121; Testing Loss 0.005778124944575742; Training Loss 0.004629353600480706\n",
      "Episode 17122; Testing Loss 0.0057782490436862075; Training Loss 0.004629344037307794\n",
      "Episode 17123; Testing Loss 0.005778357179241953; Training Loss 0.004629336780883833\n",
      "Episode 17124; Testing Loss 0.005778246036243465; Training Loss 0.004629327612450684\n",
      "Episode 17125; Testing Loss 0.005778133951705464; Training Loss 0.004629320905661966\n",
      "Episode 17126; Testing Loss 0.005778224336046296; Training Loss 0.00462931151120446\n",
      "Episode 17127; Testing Loss 0.005778325767914918; Training Loss 0.0046293043928485305\n",
      "Episode 17128; Testing Loss 0.005778127146026016; Training Loss 0.004629295696043771\n",
      "Episode 17129; Testing Loss 0.005778065905825039; Training Loss 0.004629287857740782\n",
      "Episode 17130; Testing Loss 0.005778189852796718; Training Loss 0.004629280302073257\n",
      "Episode 17131; Testing Loss 0.005778191249662547; Training Loss 0.004629271181037135\n",
      "Episode 17132; Testing Loss 0.005778167020684458; Training Loss 0.00462926270757558\n",
      "Episode 17133; Testing Loss 0.005778131104456902; Training Loss 0.004629255256762628\n",
      "Episode 17134; Testing Loss 0.005778107764350663; Training Loss 0.004629247481986856\n",
      "Episode 17135; Testing Loss 0.005778108369905696; Training Loss 0.004629239768456911\n",
      "Episode 17136; Testing Loss 0.00577812324736879; Training Loss 0.004629231410630939\n",
      "Episode 17137; Testing Loss 0.0057780846740676396; Training Loss 0.004629222370759794\n",
      "Episode 17138; Testing Loss 0.005778082568602349; Training Loss 0.00462921487780568\n",
      "Episode 17139; Testing Loss 0.005778134698639429; Training Loss 0.004629207050445639\n",
      "Episode 17140; Testing Loss 0.005778077646221264; Training Loss 0.00462919807143969\n",
      "Episode 17141; Testing Loss 0.0057779432261572975; Training Loss 0.004629190150347784\n",
      "Episode 17142; Testing Loss 0.005777968943780041; Training Loss 0.004629181648547121\n",
      "Episode 17143; Testing Loss 0.005778054462114656; Training Loss 0.004629175848314598\n",
      "Episode 17144; Testing Loss 0.005777959589321149; Training Loss 0.004629165528370973\n",
      "Episode 17145; Testing Loss 0.005777953409778601; Training Loss 0.004629158363986336\n",
      "Episode 17146; Testing Loss 0.005778011075705867; Training Loss 0.004629150604119229\n",
      "Episode 17147; Testing Loss 0.005777930634201125; Training Loss 0.004629142965213757\n",
      "Episode 17148; Testing Loss 0.00577781290433879; Training Loss 0.004629135280086719\n",
      "Episode 17149; Testing Loss 0.00577794691405191; Training Loss 0.00462912573592676\n",
      "Episode 17150; Testing Loss 0.00577804126117444; Training Loss 0.004629117769461561\n",
      "Episode 17151; Testing Loss 0.005777979778844889; Training Loss 0.004629109128244809\n",
      "Episode 17152; Testing Loss 0.0057778481615272615; Training Loss 0.004629102005755229\n",
      "Episode 17153; Testing Loss 0.005777902437454314; Training Loss 0.004629095156881065\n",
      "Episode 17154; Testing Loss 0.005777885668214368; Training Loss 0.0046290865941618995\n",
      "Episode 17155; Testing Loss 0.005777863641388062; Training Loss 0.004629077782766283\n",
      "Episode 17156; Testing Loss 0.005777973808149111; Training Loss 0.004629069781060467\n",
      "Episode 17157; Testing Loss 0.005778008055576357; Training Loss 0.004629061765280113\n",
      "Episode 17158; Testing Loss 0.005777909463463786; Training Loss 0.004629053659482897\n",
      "Episode 17159; Testing Loss 0.005777706525149448; Training Loss 0.004629046671108204\n",
      "Episode 17160; Testing Loss 0.0057777538858930985; Training Loss 0.0046290385909628605\n",
      "Episode 17161; Testing Loss 0.005777959325884515; Training Loss 0.004629031614404495\n",
      "Episode 17162; Testing Loss 0.0057778707473123084; Training Loss 0.004629023487072932\n",
      "Episode 17163; Testing Loss 0.005777653931477673; Training Loss 0.004629015114172588\n",
      "Episode 17164; Testing Loss 0.005777712355590588; Training Loss 0.004629005714275263\n",
      "Episode 17165; Testing Loss 0.00577786718129517; Training Loss 0.0046289981824538485\n",
      "Episode 17166; Testing Loss 0.005777833896864079; Training Loss 0.004628989764918163\n",
      "Episode 17167; Testing Loss 0.005777690591725687; Training Loss 0.0046289817856600265\n",
      "Episode 17168; Testing Loss 0.005777706047661219; Training Loss 0.004628973279821851\n",
      "Episode 17169; Testing Loss 0.005777829712878081; Training Loss 0.004628964931373443\n",
      "Episode 17170; Testing Loss 0.005777792490318813; Training Loss 0.004628956593263752\n",
      "Episode 17171; Testing Loss 0.005777655979339542; Training Loss 0.004628948589390019\n",
      "Episode 17172; Testing Loss 0.005777645626084081; Training Loss 0.004628941488482081\n",
      "Episode 17173; Testing Loss 0.005777691034492143; Training Loss 0.0046289323635392\n",
      "Episode 17174; Testing Loss 0.005777718014202035; Training Loss 0.004628924073211086\n",
      "Episode 17175; Testing Loss 0.0057776959503443946; Training Loss 0.004628916918861005\n",
      "Episode 17176; Testing Loss 0.005777752056502896; Training Loss 0.004628908752865664\n",
      "Episode 17177; Testing Loss 0.005777682674860828; Training Loss 0.004628901776723137\n",
      "Episode 17178; Testing Loss 0.005777546927461221; Training Loss 0.004628892958451154\n",
      "Episode 17179; Testing Loss 0.005777607101846946; Training Loss 0.004628884690634222\n",
      "Episode 17180; Testing Loss 0.005777721357147141; Training Loss 0.004628876566224327\n",
      "Episode 17181; Testing Loss 0.0057776942925142375; Training Loss 0.0046288682538712335\n",
      "Episode 17182; Testing Loss 0.0057775869962562185; Training Loss 0.004628860595248232\n",
      "Episode 17183; Testing Loss 0.005777560141831193; Training Loss 0.004628852369111744\n",
      "Episode 17184; Testing Loss 0.005777614992302267; Training Loss 0.004628845291234236\n",
      "Episode 17185; Testing Loss 0.005777531623761168; Training Loss 0.004628837263220763\n",
      "Episode 17186; Testing Loss 0.005777594694182601; Training Loss 0.0046288287437476466\n",
      "Episode 17187; Testing Loss 0.005777644901291853; Training Loss 0.004628821164912005\n",
      "Episode 17188; Testing Loss 0.005777536192204329; Training Loss 0.004628813790431566\n",
      "Episode 17189; Testing Loss 0.0057775621327344285; Training Loss 0.004628805466551406\n",
      "Episode 17190; Testing Loss 0.005777664026489402; Training Loss 0.004628798370433407\n",
      "Episode 17191; Testing Loss 0.00577745623340021; Training Loss 0.0046287892832892474\n",
      "Episode 17192; Testing Loss 0.00577728066748216; Training Loss 0.004628782811036657\n",
      "Episode 17193; Testing Loss 0.005777396856937605; Training Loss 0.004628773883399503\n",
      "Episode 17194; Testing Loss 0.005777526969305183; Training Loss 0.004628766558909177\n",
      "Episode 17195; Testing Loss 0.005777460765738867; Training Loss 0.004628757314272096\n",
      "Episode 17196; Testing Loss 0.005777346619130504; Training Loss 0.00462874933614468\n",
      "Episode 17197; Testing Loss 0.005777405171954081; Training Loss 0.0046287405012292475\n",
      "Episode 17198; Testing Loss 0.005777555985141483; Training Loss 0.0046287341587569605\n",
      "Episode 17199; Testing Loss 0.005777527781323991; Training Loss 0.0046287268247717315\n",
      "Episode 17200; Testing Loss 0.0057773336553282; Training Loss 0.004628717943670438\n",
      "Episode 17201; Testing Loss 0.005777355046291724; Training Loss 0.004628708499082712\n",
      "Episode 17202; Testing Loss 0.005777448195643299; Training Loss 0.004628702354313555\n",
      "Episode 17203; Testing Loss 0.005777341656496282; Training Loss 0.00462869290233686\n",
      "Episode 17204; Testing Loss 0.005777296963131706; Training Loss 0.004628685330723344\n",
      "Episode 17205; Testing Loss 0.00577742733866627; Training Loss 0.004628676278661725\n",
      "Episode 17206; Testing Loss 0.005777457300998529; Training Loss 0.004628668252854444\n",
      "Episode 17207; Testing Loss 0.0057774072788612145; Training Loss 0.004628660330457584\n",
      "Episode 17208; Testing Loss 0.0057772579605813975; Training Loss 0.00462865260079514\n",
      "Episode 17209; Testing Loss 0.005777292438797482; Training Loss 0.004628644730046451\n",
      "Episode 17210; Testing Loss 0.0057773724296140486; Training Loss 0.004628637005094846\n",
      "Episode 17211; Testing Loss 0.005777279664874607; Training Loss 0.004628628145636034\n",
      "Episode 17212; Testing Loss 0.005777226604632457; Training Loss 0.004628620741676511\n",
      "Episode 17213; Testing Loss 0.005777277615449441; Training Loss 0.004628611876035809\n",
      "Episode 17214; Testing Loss 0.005777356530393343; Training Loss 0.004628604632659402\n",
      "Episode 17215; Testing Loss 0.005777360315320407; Training Loss 0.00462859608083627\n",
      "Episode 17216; Testing Loss 0.005777222636745225; Training Loss 0.004628588503010263\n",
      "Episode 17217; Testing Loss 0.005777144829947334; Training Loss 0.004628581412313948\n",
      "Episode 17218; Testing Loss 0.005777258235412601; Training Loss 0.004628573375784229\n",
      "Episode 17219; Testing Loss 0.005777310382987271; Training Loss 0.004628565850610692\n",
      "Episode 17220; Testing Loss 0.005777248488772106; Training Loss 0.004628556675276665\n",
      "Episode 17221; Testing Loss 0.0057770940111001885; Training Loss 0.004628549852665065\n",
      "Episode 17222; Testing Loss 0.00577723816064056; Training Loss 0.004628540732856303\n",
      "Episode 17223; Testing Loss 0.005777388931026603; Training Loss 0.004628533847929976\n",
      "Episode 17224; Testing Loss 0.005777209607814979; Training Loss 0.004628524675457582\n",
      "Episode 17225; Testing Loss 0.005777103178186875; Training Loss 0.004628517007757025\n",
      "Episode 17226; Testing Loss 0.005777225792460647; Training Loss 0.004628509137112629\n",
      "Episode 17227; Testing Loss 0.0057772225270477035; Training Loss 0.004628501239493852\n",
      "Episode 17228; Testing Loss 0.00577712747173807; Training Loss 0.0046284932674442005\n",
      "Episode 17229; Testing Loss 0.00577715538968635; Training Loss 0.004628485365084479\n",
      "Episode 17230; Testing Loss 0.005777235837597162; Training Loss 0.0046284779225466526\n",
      "Episode 17231; Testing Loss 0.0057772764265267415; Training Loss 0.004628469897906212\n",
      "Episode 17232; Testing Loss 0.005777191351256607; Training Loss 0.004628461385661225\n",
      "Episode 17233; Testing Loss 0.005777081889651249; Training Loss 0.004628453836624602\n",
      "Episode 17234; Testing Loss 0.005777051384725534; Training Loss 0.004628445539645053\n",
      "Episode 17235; Testing Loss 0.005777075981765562; Training Loss 0.004628437832257048\n",
      "Episode 17236; Testing Loss 0.0057770915435334; Training Loss 0.004628430337886225\n",
      "Episode 17237; Testing Loss 0.005777036372892709; Training Loss 0.004628421979955958\n",
      "Episode 17238; Testing Loss 0.005776948363542055; Training Loss 0.004628414046528983\n",
      "Episode 17239; Testing Loss 0.005776989408626473; Training Loss 0.00462840575268375\n",
      "Episode 17240; Testing Loss 0.005777065878487279; Training Loss 0.00462839710691994\n",
      "Episode 17241; Testing Loss 0.005777084419435714; Training Loss 0.004628388832043562\n",
      "Episode 17242; Testing Loss 0.00577709729267184; Training Loss 0.0046283815860324925\n",
      "Episode 17243; Testing Loss 0.005777124176386394; Training Loss 0.004628375118189051\n",
      "Episode 17244; Testing Loss 0.005777045787207477; Training Loss 0.00462836637668413\n",
      "Episode 17245; Testing Loss 0.005777064941490218; Training Loss 0.004628356946017228\n",
      "Episode 17246; Testing Loss 0.005777088454447104; Training Loss 0.004628349363627295\n",
      "Episode 17247; Testing Loss 0.005777015953263016; Training Loss 0.004628341208131899\n",
      "Episode 17248; Testing Loss 0.005776984978142452; Training Loss 0.004628332989004946\n",
      "Episode 17249; Testing Loss 0.00577704897444254; Training Loss 0.004628325049449133\n",
      "Episode 17250; Testing Loss 0.005777082136461176; Training Loss 0.004628317063930351\n",
      "Episode 17251; Testing Loss 0.005777012644538248; Training Loss 0.0046283089212181975\n",
      "Episode 17252; Testing Loss 0.005776964693512557; Training Loss 0.004628300959440943\n",
      "Episode 17253; Testing Loss 0.005776949889280013; Training Loss 0.004628292771053688\n",
      "Episode 17254; Testing Loss 0.005776993073928058; Training Loss 0.0046282849643229425\n",
      "Episode 17255; Testing Loss 0.0057769675251241725; Training Loss 0.004628276473141523\n",
      "Episode 17256; Testing Loss 0.005776864649199844; Training Loss 0.004628270678839255\n",
      "Episode 17257; Testing Loss 0.005776966246861309; Training Loss 0.004628262960766939\n",
      "Episode 17258; Testing Loss 0.005777103679222345; Training Loss 0.004628255073868129\n",
      "Episode 17259; Testing Loss 0.005776889964877162; Training Loss 0.004628245960765403\n",
      "Episode 17260; Testing Loss 0.005776663092108023; Training Loss 0.00462824002672272\n",
      "Episode 17261; Testing Loss 0.005776837176755803; Training Loss 0.0046282287816378975\n",
      "Episode 17262; Testing Loss 0.005777100758629551; Training Loss 0.004628225089751743\n",
      "Episode 17263; Testing Loss 0.005776916204973844; Training Loss 0.0046282130943383504\n",
      "Episode 17264; Testing Loss 0.0057766487156268535; Training Loss 0.004628210191115819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17265; Testing Loss 0.0057768261183473646; Training Loss 0.004628198427053695\n",
      "Episode 17266; Testing Loss 0.005776971331933262; Training Loss 0.004628192705445311\n",
      "Episode 17267; Testing Loss 0.005776831136678125; Training Loss 0.004628181170479591\n",
      "Episode 17268; Testing Loss 0.005776702782330736; Training Loss 0.004628178522848941\n",
      "Episode 17269; Testing Loss 0.005776838846149207; Training Loss 0.004628168977150887\n",
      "Episode 17270; Testing Loss 0.005776977176947909; Training Loss 0.004628159770933639\n",
      "Episode 17271; Testing Loss 0.005776832804818647; Training Loss 0.004628152533772401\n",
      "Episode 17272; Testing Loss 0.0057766518184969706; Training Loss 0.00462814528429509\n",
      "Episode 17273; Testing Loss 0.005776726663150338; Training Loss 0.004628134287370591\n",
      "Episode 17274; Testing Loss 0.005776917498982618; Training Loss 0.004628128269715709\n",
      "Episode 17275; Testing Loss 0.005776834030758791; Training Loss 0.004628119331401957\n",
      "Episode 17276; Testing Loss 0.005776637045642815; Training Loss 0.004628111004069413\n",
      "Episode 17277; Testing Loss 0.005776670860494468; Training Loss 0.0046281020827253164\n",
      "Episode 17278; Testing Loss 0.005776828244039847; Training Loss 0.004628093908903716\n",
      "Episode 17279; Testing Loss 0.0057768624115958605; Training Loss 0.004628086741468714\n",
      "Episode 17280; Testing Loss 0.005776668893235472; Training Loss 0.0046280777053648815\n",
      "Episode 17281; Testing Loss 0.005776605146454955; Training Loss 0.004628070788077256\n",
      "Episode 17282; Testing Loss 0.005776716830357321; Training Loss 0.004628061493864944\n",
      "Episode 17283; Testing Loss 0.005776787126468834; Training Loss 0.004628053794468817\n",
      "Episode 17284; Testing Loss 0.005776824534919434; Training Loss 0.004628046863543186\n",
      "Episode 17285; Testing Loss 0.005776759825615523; Training Loss 0.0046280388459995405\n",
      "Episode 17286; Testing Loss 0.005776748399636164; Training Loss 0.004628029577669456\n",
      "Episode 17287; Testing Loss 0.005776740041165306; Training Loss 0.004628022838616717\n",
      "Episode 17288; Testing Loss 0.0057766211491824425; Training Loss 0.004628014394862133\n",
      "Episode 17289; Testing Loss 0.005776565250234013; Training Loss 0.004628005888422348\n",
      "Episode 17290; Testing Loss 0.005776645797377907; Training Loss 0.004628000215905843\n",
      "Episode 17291; Testing Loss 0.0057765964548433905; Training Loss 0.0046279908523957835\n",
      "Episode 17292; Testing Loss 0.005776549690947723; Training Loss 0.004627982499591659\n",
      "Episode 17293; Testing Loss 0.005776683648736765; Training Loss 0.004627974669053805\n",
      "Episode 17294; Testing Loss 0.00577678625160072; Training Loss 0.0046279667656189795\n",
      "Episode 17295; Testing Loss 0.005776706772502241; Training Loss 0.0046279575329553685\n",
      "Episode 17296; Testing Loss 0.00577650015131122; Training Loss 0.004627949605170564\n",
      "Episode 17297; Testing Loss 0.005776489416115539; Training Loss 0.004627942759977011\n",
      "Episode 17298; Testing Loss 0.005776624446523317; Training Loss 0.004627933404382569\n",
      "Episode 17299; Testing Loss 0.005776615256137099; Training Loss 0.004627925827247795\n",
      "Episode 17300; Testing Loss 0.005776590634337248; Training Loss 0.0046279170179171865\n",
      "Episode 17301; Testing Loss 0.005776561434250505; Training Loss 0.004627909711195655\n",
      "Episode 17302; Testing Loss 0.005776544457283374; Training Loss 0.0046279024007547425\n",
      "Episode 17303; Testing Loss 0.005776516426976811; Training Loss 0.004627892980376726\n",
      "Episode 17304; Testing Loss 0.005776551452332143; Training Loss 0.0046278859329272745\n",
      "Episode 17305; Testing Loss 0.005776591906505056; Training Loss 0.004627879926077835\n",
      "Episode 17306; Testing Loss 0.005776600756636224; Training Loss 0.004627869348412668\n",
      "Episode 17307; Testing Loss 0.0057765236658160945; Training Loss 0.004627860520950942\n",
      "Episode 17308; Testing Loss 0.005776468297528384; Training Loss 0.004627853273266816\n",
      "Episode 17309; Testing Loss 0.005776593446943187; Training Loss 0.004627845056096603\n",
      "Episode 17310; Testing Loss 0.005776611952984545; Training Loss 0.004627837326205102\n",
      "Episode 17311; Testing Loss 0.0057764629637348415; Training Loss 0.004627829919881711\n",
      "Episode 17312; Testing Loss 0.005776447350679336; Training Loss 0.0046278222284250456\n",
      "Episode 17313; Testing Loss 0.005776589742205219; Training Loss 0.0046278131461964635\n",
      "Episode 17314; Testing Loss 0.00577661699004309; Training Loss 0.0046278078094332846\n",
      "Episode 17315; Testing Loss 0.005776388498175603; Training Loss 0.004627799025080263\n",
      "Episode 17316; Testing Loss 0.005776316568935745; Training Loss 0.004627790193996641\n",
      "Episode 17317; Testing Loss 0.005776520135396404; Training Loss 0.004627781427232282\n",
      "Episode 17318; Testing Loss 0.005776704482335814; Training Loss 0.004627775956544744\n",
      "Episode 17319; Testing Loss 0.005776620524805232; Training Loss 0.004627767032744827\n",
      "Episode 17320; Testing Loss 0.005776540925288766; Training Loss 0.0046277590487321055\n",
      "Episode 17321; Testing Loss 0.005776571358336176; Training Loss 0.0046277497668843525\n",
      "Episode 17322; Testing Loss 0.005776624787327204; Training Loss 0.004627743759154537\n",
      "Episode 17323; Testing Loss 0.005776503973012348; Training Loss 0.004627734990206417\n",
      "Episode 17324; Testing Loss 0.005776349691810454; Training Loss 0.004627725113141109\n",
      "Episode 17325; Testing Loss 0.005776409967451484; Training Loss 0.004627716650385318\n",
      "Episode 17326; Testing Loss 0.005776598601053764; Training Loss 0.004627710123203514\n",
      "Episode 17327; Testing Loss 0.005776547088599548; Training Loss 0.004627701538365597\n",
      "Episode 17328; Testing Loss 0.005776369769535933; Training Loss 0.004627693384232186\n",
      "Episode 17329; Testing Loss 0.005776383726967068; Training Loss 0.004627683668948798\n",
      "Episode 17330; Testing Loss 0.005776486781865613; Training Loss 0.004627677889926416\n",
      "Episode 17331; Testing Loss 0.005776450544627427; Training Loss 0.004627668693586\n",
      "Episode 17332; Testing Loss 0.005776368115216687; Training Loss 0.004627660709179108\n",
      "Episode 17333; Testing Loss 0.005776480954952259; Training Loss 0.004627652498438174\n",
      "Episode 17334; Testing Loss 0.005776573700791965; Training Loss 0.004627645111531172\n",
      "Episode 17335; Testing Loss 0.00577644696357733; Training Loss 0.0046276361504004986\n",
      "Episode 17336; Testing Loss 0.00577627714323397; Training Loss 0.004627629504971174\n",
      "Episode 17337; Testing Loss 0.005776409433697184; Training Loss 0.004627619628726485\n",
      "Episode 17338; Testing Loss 0.005776549468549364; Training Loss 0.004627612000156304\n",
      "Episode 17339; Testing Loss 0.005776454817063655; Training Loss 0.004627604029982383\n",
      "Episode 17340; Testing Loss 0.0057764139349473055; Training Loss 0.004627596233575524\n",
      "Episode 17341; Testing Loss 0.005776475399470075; Training Loss 0.004627587333716795\n",
      "Episode 17342; Testing Loss 0.0057764831311756994; Training Loss 0.004627579258087077\n",
      "Episode 17343; Testing Loss 0.005776361650036588; Training Loss 0.00462757195686393\n",
      "Episode 17344; Testing Loss 0.005776246492626536; Training Loss 0.004627563851076902\n",
      "Episode 17345; Testing Loss 0.005776303267053923; Training Loss 0.004627556689372012\n",
      "Episode 17346; Testing Loss 0.005776473553472858; Training Loss 0.004627548267514157\n",
      "Episode 17347; Testing Loss 0.0057764184278763665; Training Loss 0.004627539886948894\n",
      "Episode 17348; Testing Loss 0.005776239067158042; Training Loss 0.004627532736063136\n",
      "Episode 17349; Testing Loss 0.00577633908230464; Training Loss 0.004627523458940214\n",
      "Episode 17350; Testing Loss 0.005776516938384785; Training Loss 0.0046275166320544694\n",
      "Episode 17351; Testing Loss 0.005776406231660651; Training Loss 0.004627506271971839\n",
      "Episode 17352; Testing Loss 0.005776184880070703; Training Loss 0.0046275000627645875\n",
      "Episode 17353; Testing Loss 0.00577623513838292; Training Loss 0.0046274920993775465\n",
      "Episode 17354; Testing Loss 0.00577651239456878; Training Loss 0.004627486837078037\n",
      "Episode 17355; Testing Loss 0.00577648192932729; Training Loss 0.004627478574916896\n",
      "Episode 17356; Testing Loss 0.005776177350842435; Training Loss 0.004627467885236763\n",
      "Episode 17357; Testing Loss 0.0057761394367569245; Training Loss 0.004627466008922288\n",
      "Episode 17358; Testing Loss 0.005776467665292452; Training Loss 0.0046274559445193784\n",
      "Episode 17359; Testing Loss 0.0057765234318960185; Training Loss 0.0046274460442686845\n",
      "Episode 17360; Testing Loss 0.005776125277695371; Training Loss 0.0046274366837788464\n",
      "Episode 17361; Testing Loss 0.005776063834953732; Training Loss 0.004627430233229931\n",
      "Episode 17362; Testing Loss 0.005776409813123957; Training Loss 0.004627420741002295\n",
      "Episode 17363; Testing Loss 0.005776477399063559; Training Loss 0.00462741478118747\n",
      "Episode 17364; Testing Loss 0.005776164088073532; Training Loss 0.004627403552270079\n",
      "Episode 17365; Testing Loss 0.005776009218805613; Training Loss 0.004627398250795328\n",
      "Episode 17366; Testing Loss 0.005776255928821094; Training Loss 0.004627387921118805\n",
      "Episode 17367; Testing Loss 0.005776441169206; Training Loss 0.004627381992017575\n",
      "Episode 17368; Testing Loss 0.005776226805991701; Training Loss 0.004627373619211932\n",
      "Episode 17369; Testing Loss 0.005776134746228286; Training Loss 0.0046273647918806905\n",
      "Episode 17370; Testing Loss 0.005776280175023899; Training Loss 0.004627356349325194\n",
      "Episode 17371; Testing Loss 0.0057763136371166614; Training Loss 0.0046273508609604745\n",
      "Episode 17372; Testing Loss 0.005776165672808357; Training Loss 0.00462734099597631\n",
      "Episode 17373; Testing Loss 0.0057761692686107155; Training Loss 0.004627332044606135\n",
      "Episode 17374; Testing Loss 0.005776316083541957; Training Loss 0.00462732455971578\n",
      "Episode 17375; Testing Loss 0.005776359629272418; Training Loss 0.004627317365304703\n",
      "Episode 17376; Testing Loss 0.005776270311250721; Training Loss 0.004627308482520941\n",
      "Episode 17377; Testing Loss 0.00577618285729778; Training Loss 0.004627299841874581\n",
      "Episode 17378; Testing Loss 0.0057761676936637155; Training Loss 0.004627292565245254\n",
      "Episode 17379; Testing Loss 0.005776173542517746; Training Loss 0.00462728515730422\n",
      "Episode 17380; Testing Loss 0.005776189222077676; Training Loss 0.004627275828927967\n",
      "Episode 17381; Testing Loss 0.005776217240307058; Training Loss 0.004627267686853429\n",
      "Episode 17382; Testing Loss 0.005776223801050499; Training Loss 0.0046272598082887995\n",
      "Episode 17383; Testing Loss 0.0057762565298756195; Training Loss 0.004627252319807498\n",
      "Episode 17384; Testing Loss 0.005776294976371566; Training Loss 0.004627243861244057\n",
      "Episode 17385; Testing Loss 0.00577621080337888; Training Loss 0.004627235501382241\n",
      "Episode 17386; Testing Loss 0.0057760584039672795; Training Loss 0.0046272281393366236\n",
      "Episode 17387; Testing Loss 0.0057760793630705805; Training Loss 0.0046272206169730885\n",
      "Episode 17388; Testing Loss 0.0057761568217590195; Training Loss 0.004627212303269535\n",
      "Episode 17389; Testing Loss 0.005776164083586245; Training Loss 0.004627203542856368\n",
      "Episode 17390; Testing Loss 0.005776114759744799; Training Loss 0.004627196847731085\n",
      "Episode 17391; Testing Loss 0.00577606330948712; Training Loss 0.004627188432968287\n",
      "Episode 17392; Testing Loss 0.005776134693196644; Training Loss 0.004627180246774353\n",
      "Episode 17393; Testing Loss 0.0057762501216789665; Training Loss 0.004627172720183648\n",
      "Episode 17394; Testing Loss 0.005776205646723737; Training Loss 0.0046271646532363014\n",
      "Episode 17395; Testing Loss 0.005776077126710065; Training Loss 0.004627156018399961\n",
      "Episode 17396; Testing Loss 0.005776080002291211; Training Loss 0.004627147164023228\n",
      "Episode 17397; Testing Loss 0.005776191450778928; Training Loss 0.004627139680451514\n",
      "Episode 17398; Testing Loss 0.005776173004814432; Training Loss 0.004627131571845315\n",
      "Episode 17399; Testing Loss 0.005776032579560125; Training Loss 0.004627122645776026\n",
      "Episode 17400; Testing Loss 0.005776087709144216; Training Loss 0.004627116210843391\n",
      "Episode 17401; Testing Loss 0.005776219997064869; Training Loss 0.004627107443463613\n",
      "Episode 17402; Testing Loss 0.005776003721184392; Training Loss 0.004627097020589292\n",
      "Episode 17403; Testing Loss 0.005775882577119425; Training Loss 0.004627091218253982\n",
      "Episode 17404; Testing Loss 0.005776039398919551; Training Loss 0.004627081157445929\n",
      "Episode 17405; Testing Loss 0.005776099356903073; Training Loss 0.004627073869376046\n",
      "Episode 17406; Testing Loss 0.005776095028326919; Training Loss 0.0046270648533419945\n",
      "Episode 17407; Testing Loss 0.005776014600145928; Training Loss 0.004627058282691851\n",
      "Episode 17408; Testing Loss 0.005775884732423961; Training Loss 0.004627050781773754\n",
      "Episode 17409; Testing Loss 0.005775889954085663; Training Loss 0.004627040691980345\n",
      "Episode 17410; Testing Loss 0.005776013703204041; Training Loss 0.004627034417128531\n",
      "Episode 17411; Testing Loss 0.005776083648793773; Training Loss 0.004627025715561797\n",
      "Episode 17412; Testing Loss 0.005775911832235754; Training Loss 0.004627015751962828\n",
      "Episode 17413; Testing Loss 0.005775879110364101; Training Loss 0.004627008558136304\n",
      "Episode 17414; Testing Loss 0.005775930091042067; Training Loss 0.0046269998253109955\n",
      "Episode 17415; Testing Loss 0.005776006739153349; Training Loss 0.004626990773390318\n",
      "Episode 17416; Testing Loss 0.005776036018059559; Training Loss 0.00462698322129799\n",
      "Episode 17417; Testing Loss 0.005775964518406984; Training Loss 0.004626974796046504\n",
      "Episode 17418; Testing Loss 0.005775942037081855; Training Loss 0.004626965849610804\n",
      "Episode 17419; Testing Loss 0.005775987208581285; Training Loss 0.004626959639499365\n",
      "Episode 17420; Testing Loss 0.005775908238652743; Training Loss 0.004626951200518471\n",
      "Episode 17421; Testing Loss 0.005775873259344752; Training Loss 0.004626942151653661\n",
      "Episode 17422; Testing Loss 0.005776032204803089; Training Loss 0.00462693326826687\n",
      "Episode 17423; Testing Loss 0.005776031194477707; Training Loss 0.004626925010599798\n",
      "Episode 17424; Testing Loss 0.005775890069251332; Training Loss 0.004626917014763115\n",
      "Episode 17425; Testing Loss 0.005775863145103394; Training Loss 0.004626910722364992\n",
      "Episode 17426; Testing Loss 0.005776046987180271; Training Loss 0.004626903126725223\n",
      "Episode 17427; Testing Loss 0.0057759707569571645; Training Loss 0.004626893806775058\n",
      "Episode 17428; Testing Loss 0.005775857307138238; Training Loss 0.004626884688089378\n",
      "Episode 17429; Testing Loss 0.005775884979068511; Training Loss 0.004626881460632188\n",
      "Episode 17430; Testing Loss 0.005776111798780032; Training Loss 0.004626870302878452\n",
      "Episode 17431; Testing Loss 0.005776103660300694; Training Loss 0.004626864242121569\n",
      "Episode 17432; Testing Loss 0.005775827806982513; Training Loss 0.004626856991087069\n",
      "Episode 17433; Testing Loss 0.005775742118859135; Training Loss 0.0046268479094778705\n",
      "Episode 17434; Testing Loss 0.00577590178224736; Training Loss 0.004626836483545423\n",
      "Episode 17435; Testing Loss 0.005776046903920381; Training Loss 0.004626831244936373\n",
      "Episode 17436; Testing Loss 0.005776011444998768; Training Loss 0.004626821138108282\n",
      "Episode 17437; Testing Loss 0.005775891466708489; Training Loss 0.004626812762540602\n",
      "Episode 17438; Testing Loss 0.0057758164997936115; Training Loss 0.004626807923730449\n",
      "Episode 17439; Testing Loss 0.005775928880579432; Training Loss 0.004626797336758465\n",
      "Episode 17440; Testing Loss 0.005776018833234519; Training Loss 0.004626787674252104\n",
      "Episode 17441; Testing Loss 0.0057759485346251084; Training Loss 0.004626780251375885\n",
      "Episode 17442; Testing Loss 0.005775824171211792; Training Loss 0.004626772702808065\n",
      "Episode 17443; Testing Loss 0.005775931545685872; Training Loss 0.004626762537015761\n",
      "Episode 17444; Testing Loss 0.0057760245169610975; Training Loss 0.004626756198975045\n",
      "Episode 17445; Testing Loss 0.005775911653997312; Training Loss 0.004626746867228745\n",
      "Episode 17446; Testing Loss 0.005775771658858234; Training Loss 0.004626740159274692\n",
      "Episode 17447; Testing Loss 0.005775874412418233; Training Loss 0.004626731460234626\n",
      "Episode 17448; Testing Loss 0.005776005028962562; Training Loss 0.004626724175824089\n",
      "Episode 17449; Testing Loss 0.005775916814196737; Training Loss 0.004626714660129787\n",
      "Episode 17450; Testing Loss 0.005775795486468092; Training Loss 0.0046267063170440485\n",
      "Episode 17451; Testing Loss 0.005775859464776671; Training Loss 0.004626697106585052\n",
      "Episode 17452; Testing Loss 0.005775917012017132; Training Loss 0.004626688655506195\n",
      "Episode 17453; Testing Loss 0.005775831898011669; Training Loss 0.004626681792848225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17454; Testing Loss 0.005775790987189766; Training Loss 0.004626676123086818\n",
      "Episode 17455; Testing Loss 0.005775851950370294; Training Loss 0.004626664742665535\n",
      "Episode 17456; Testing Loss 0.005775871761020822; Training Loss 0.004626658652447827\n",
      "Episode 17457; Testing Loss 0.005775757586749255; Training Loss 0.004626651249497598\n",
      "Episode 17458; Testing Loss 0.005775737470979268; Training Loss 0.004626641543210658\n",
      "Episode 17459; Testing Loss 0.0057758577289191825; Training Loss 0.004626632267572978\n",
      "Episode 17460; Testing Loss 0.005775963818114077; Training Loss 0.004626625733933576\n",
      "Episode 17461; Testing Loss 0.005775937054979902; Training Loss 0.004626615839852467\n",
      "Episode 17462; Testing Loss 0.005775788386415048; Training Loss 0.004626609380716457\n",
      "Episode 17463; Testing Loss 0.005775737114353341; Training Loss 0.004626602787264483\n",
      "Episode 17464; Testing Loss 0.005775886073528002; Training Loss 0.004626591784083489\n",
      "Episode 17465; Testing Loss 0.005775953137078532; Training Loss 0.004626585370281266\n",
      "Episode 17466; Testing Loss 0.005775924254685849; Training Loss 0.0046265764802246164\n",
      "Episode 17467; Testing Loss 0.005775841484701722; Training Loss 0.004626567125625141\n",
      "Episode 17468; Testing Loss 0.005775768797320918; Training Loss 0.004626561937357571\n",
      "Episode 17469; Testing Loss 0.005775810622636793; Training Loss 0.004626552237282102\n",
      "Episode 17470; Testing Loss 0.005775847775645227; Training Loss 0.004626542236095449\n",
      "Episode 17471; Testing Loss 0.005775865094324623; Training Loss 0.004626535735775336\n",
      "Episode 17472; Testing Loss 0.005775816499058455; Training Loss 0.004626527187189072\n",
      "Episode 17473; Testing Loss 0.005775792821695964; Training Loss 0.004626518327838018\n",
      "Episode 17474; Testing Loss 0.005775850520930416; Training Loss 0.004626511096053182\n",
      "Episode 17475; Testing Loss 0.005775854417773604; Training Loss 0.004626502103400481\n",
      "Episode 17476; Testing Loss 0.005775831971498662; Training Loss 0.004626493432667739\n",
      "Episode 17477; Testing Loss 0.0057758501961502; Training Loss 0.0046264862774275965\n",
      "Episode 17478; Testing Loss 0.005775883149691266; Training Loss 0.004626477462126817\n",
      "Episode 17479; Testing Loss 0.0057757490560017485; Training Loss 0.004626470001897757\n",
      "Episode 17480; Testing Loss 0.005775623618661695; Training Loss 0.004626463043862779\n",
      "Episode 17481; Testing Loss 0.005775737058375901; Training Loss 0.004626452789131332\n",
      "Episode 17482; Testing Loss 0.005775873459175045; Training Loss 0.004626446293371039\n",
      "Episode 17483; Testing Loss 0.005775832679059257; Training Loss 0.004626440756300983\n",
      "Episode 17484; Testing Loss 0.005775785301945331; Training Loss 0.004626429545663137\n",
      "Episode 17485; Testing Loss 0.0057757822079102725; Training Loss 0.004626420823554621\n",
      "Episode 17486; Testing Loss 0.005775734091209714; Training Loss 0.004626413142357433\n",
      "Episode 17487; Testing Loss 0.005775684811811836; Training Loss 0.00462640482058667\n",
      "Episode 17488; Testing Loss 0.0057757473805076; Training Loss 0.004626396947243749\n",
      "Episode 17489; Testing Loss 0.005775801618623303; Training Loss 0.00462638844941342\n",
      "Episode 17490; Testing Loss 0.0057757656077034305; Training Loss 0.004626379222342293\n",
      "Episode 17491; Testing Loss 0.005775675216332166; Training Loss 0.004626372989538779\n",
      "Episode 17492; Testing Loss 0.0057757869651169965; Training Loss 0.004626363727325673\n",
      "Episode 17493; Testing Loss 0.005775899836897794; Training Loss 0.004626356843351168\n",
      "Episode 17494; Testing Loss 0.005775730453988643; Training Loss 0.004626346776775293\n",
      "Episode 17495; Testing Loss 0.005775606869299199; Training Loss 0.004626340200198341\n",
      "Episode 17496; Testing Loss 0.0057757711719465255; Training Loss 0.004626332853599098\n",
      "Episode 17497; Testing Loss 0.005775748747446089; Training Loss 0.0046263243114188475\n",
      "Episode 17498; Testing Loss 0.005775666504903196; Training Loss 0.004626314733357767\n",
      "Episode 17499; Testing Loss 0.00577565195524667; Training Loss 0.004626309326270181\n",
      "Episode 17500; Testing Loss 0.00577578117171501; Training Loss 0.004626299295955807\n",
      "Episode 17501; Testing Loss 0.005775738550663485; Training Loss 0.004626291170348852\n",
      "Episode 17502; Testing Loss 0.005775606687679277; Training Loss 0.004626283471398485\n",
      "Episode 17503; Testing Loss 0.005775622945096657; Training Loss 0.004626274361640081\n",
      "Episode 17504; Testing Loss 0.005775817615080446; Training Loss 0.004626267642019909\n",
      "Episode 17505; Testing Loss 0.005775902066131038; Training Loss 0.0046262605870218635\n",
      "Episode 17506; Testing Loss 0.0057756611440639575; Training Loss 0.004626249217196234\n",
      "Episode 17507; Testing Loss 0.005775507446995522; Training Loss 0.004626244293680886\n",
      "Episode 17508; Testing Loss 0.005775705690275055; Training Loss 0.004626234479735047\n",
      "Episode 17509; Testing Loss 0.005775702431686987; Training Loss 0.004626226207625036\n",
      "Episode 17510; Testing Loss 0.005775510717715437; Training Loss 0.004626220966001783\n",
      "Episode 17511; Testing Loss 0.005775587552668429; Training Loss 0.0046262101965218695\n",
      "Episode 17512; Testing Loss 0.005775727667277609; Training Loss 0.00462620273104642\n",
      "Episode 17513; Testing Loss 0.005775651948711635; Training Loss 0.004626193468835104\n",
      "Episode 17514; Testing Loss 0.005775558349305474; Training Loss 0.004626187133256817\n",
      "Episode 17515; Testing Loss 0.005775766040890606; Training Loss 0.004626177697762329\n",
      "Episode 17516; Testing Loss 0.005775884454385099; Training Loss 0.004626170703477622\n",
      "Episode 17517; Testing Loss 0.005775679344951809; Training Loss 0.004626161337833581\n",
      "Episode 17518; Testing Loss 0.005775481581311842; Training Loss 0.0046261556616570285\n",
      "Episode 17519; Testing Loss 0.005775575246689958; Training Loss 0.0046261456524535624\n",
      "Episode 17520; Testing Loss 0.005775790170749062; Training Loss 0.004626139082395355\n",
      "Episode 17521; Testing Loss 0.005775688057553504; Training Loss 0.004626128682646316\n",
      "Episode 17522; Testing Loss 0.005775555157015628; Training Loss 0.004626122869101645\n",
      "Episode 17523; Testing Loss 0.005775688725736773; Training Loss 0.004626113682912899\n",
      "Episode 17524; Testing Loss 0.0057757793367071925; Training Loss 0.004626107113180054\n",
      "Episode 17525; Testing Loss 0.005775680724945588; Training Loss 0.004626096131109583\n",
      "Episode 17526; Testing Loss 0.00577552964416373; Training Loss 0.004626090352664261\n",
      "Episode 17527; Testing Loss 0.005775638337105936; Training Loss 0.004626081079456157\n",
      "Episode 17528; Testing Loss 0.005775675779766067; Training Loss 0.004626073404779455\n",
      "Episode 17529; Testing Loss 0.005775549569183165; Training Loss 0.004626065130104546\n",
      "Episode 17530; Testing Loss 0.005775470113830506; Training Loss 0.004626056517765011\n",
      "Episode 17531; Testing Loss 0.005775557549674794; Training Loss 0.00462604853817954\n",
      "Episode 17532; Testing Loss 0.005775698474667033; Training Loss 0.004626041290888135\n",
      "Episode 17533; Testing Loss 0.005775729717646284; Training Loss 0.004626033079775698\n",
      "Episode 17534; Testing Loss 0.0057755467042436065; Training Loss 0.004626025172449098\n",
      "Episode 17535; Testing Loss 0.005775464695923871; Training Loss 0.0046260167885951815\n",
      "Episode 17536; Testing Loss 0.005775629833423709; Training Loss 0.004626007588808949\n",
      "Episode 17537; Testing Loss 0.005775631881306381; Training Loss 0.004626000150616359\n",
      "Episode 17538; Testing Loss 0.005775463080875951; Training Loss 0.004625991120032663\n",
      "Episode 17539; Testing Loss 0.005775370979406578; Training Loss 0.004625985093208726\n",
      "Episode 17540; Testing Loss 0.0057755861015214086; Training Loss 0.004625974956188886\n",
      "Episode 17541; Testing Loss 0.00577574678138593; Training Loss 0.004625968165987832\n",
      "Episode 17542; Testing Loss 0.005775521300202365; Training Loss 0.004625960208805543\n",
      "Episode 17543; Testing Loss 0.005775509836649006; Training Loss 0.004625950932283726\n",
      "Episode 17544; Testing Loss 0.005775611665363256; Training Loss 0.00462594258279574\n",
      "Episode 17545; Testing Loss 0.005775581926926702; Training Loss 0.004625933944875313\n",
      "Episode 17546; Testing Loss 0.005775543480061247; Training Loss 0.004625926026795585\n",
      "Episode 17547; Testing Loss 0.005775610478307399; Training Loss 0.004625919563393225\n",
      "Episode 17548; Testing Loss 0.005775550537417604; Training Loss 0.004625911360407047\n",
      "Episode 17549; Testing Loss 0.005775458125393368; Training Loss 0.004625901880814704\n",
      "Episode 17550; Testing Loss 0.005775490887591889; Training Loss 0.004625894015792764\n",
      "Episode 17551; Testing Loss 0.005775534104981515; Training Loss 0.004625885906969942\n",
      "Episode 17552; Testing Loss 0.0057754985082594905; Training Loss 0.004625877974384031\n",
      "Episode 17553; Testing Loss 0.005775414205064869; Training Loss 0.004625869463037176\n",
      "Episode 17554; Testing Loss 0.005775510132735125; Training Loss 0.0046258608743692635\n",
      "Episode 17555; Testing Loss 0.005775572632878994; Training Loss 0.004625854327554501\n",
      "Episode 17556; Testing Loss 0.005775577035165097; Training Loss 0.0046258460066406\n",
      "Episode 17557; Testing Loss 0.005775469317190099; Training Loss 0.004625837208682235\n",
      "Episode 17558; Testing Loss 0.005775450543961036; Training Loss 0.004625828370275708\n",
      "Episode 17559; Testing Loss 0.005775539484153415; Training Loss 0.004625821306179536\n",
      "Episode 17560; Testing Loss 0.005775431123918194; Training Loss 0.004625813419447992\n",
      "Episode 17561; Testing Loss 0.005775424579032315; Training Loss 0.004625805652008732\n",
      "Episode 17562; Testing Loss 0.005775392818924919; Training Loss 0.004625797604188964\n",
      "Episode 17563; Testing Loss 0.0057754378665901985; Training Loss 0.004625788215653135\n",
      "Episode 17564; Testing Loss 0.005775544638297385; Training Loss 0.004625780079690206\n",
      "Episode 17565; Testing Loss 0.0057755416587723105; Training Loss 0.0046257722436179385\n",
      "Episode 17566; Testing Loss 0.005775399807667247; Training Loss 0.004625766950114808\n",
      "Episode 17567; Testing Loss 0.005775478732173188; Training Loss 0.004625756097469601\n",
      "Episode 17568; Testing Loss 0.005775506919221844; Training Loss 0.004625751518057384\n",
      "Episode 17569; Testing Loss 0.005775308745886266; Training Loss 0.004625742846851049\n",
      "Episode 17570; Testing Loss 0.005775216486228921; Training Loss 0.00462573515256495\n",
      "Episode 17571; Testing Loss 0.005775455899344074; Training Loss 0.004625724930897893\n",
      "Episode 17572; Testing Loss 0.005775630663655246; Training Loss 0.004625718490954485\n",
      "Episode 17573; Testing Loss 0.005775422098258992; Training Loss 0.004625709829923115\n",
      "Episode 17574; Testing Loss 0.005775370272055481; Training Loss 0.004625701710161172\n",
      "Episode 17575; Testing Loss 0.005775498615257444; Training Loss 0.004625693050888661\n",
      "Episode 17576; Testing Loss 0.005775566906970859; Training Loss 0.00462568603777319\n",
      "Episode 17577; Testing Loss 0.005775514558590868; Training Loss 0.004625675983586139\n",
      "Episode 17578; Testing Loss 0.005775418991824795; Training Loss 0.0046256707316879335\n",
      "Episode 17579; Testing Loss 0.005775262838361463; Training Loss 0.004625664739467033\n",
      "Episode 17580; Testing Loss 0.005775311862329068; Training Loss 0.004625655630652549\n",
      "Episode 17581; Testing Loss 0.005775470170178035; Training Loss 0.004625645696500595\n",
      "Episode 17582; Testing Loss 0.00577549486061743; Training Loss 0.0046256365781540045\n",
      "Episode 17583; Testing Loss 0.005775446883579744; Training Loss 0.004625632101203995\n",
      "Episode 17584; Testing Loss 0.0057755140339513554; Training Loss 0.004625620917155599\n",
      "Episode 17585; Testing Loss 0.005775511624028943; Training Loss 0.004625612548094522\n",
      "Episode 17586; Testing Loss 0.005775349241775451; Training Loss 0.0046256045874528764\n",
      "Episode 17587; Testing Loss 0.005775271280702462; Training Loss 0.0046255971619710325\n",
      "Episode 17588; Testing Loss 0.005775332698048735; Training Loss 0.004625589231839685\n",
      "Episode 17589; Testing Loss 0.005775342116558543; Training Loss 0.004625580661418506\n",
      "Episode 17590; Testing Loss 0.005775340329251958; Training Loss 0.0046255728571712815\n",
      "Episode 17591; Testing Loss 0.005775385304961478; Training Loss 0.004625564491307458\n",
      "Episode 17592; Testing Loss 0.005775420707977664; Training Loss 0.0046255554946379555\n",
      "Episode 17593; Testing Loss 0.005775394140236761; Training Loss 0.004625547519642511\n",
      "Episode 17594; Testing Loss 0.005775336658798752; Training Loss 0.004625540254746657\n",
      "Episode 17595; Testing Loss 0.005775416396432611; Training Loss 0.0046255318651195615\n",
      "Episode 17596; Testing Loss 0.005775395865152838; Training Loss 0.004625523944945399\n",
      "Episode 17597; Testing Loss 0.0057752370194926; Training Loss 0.004625517885908553\n",
      "Episode 17598; Testing Loss 0.005775107678774642; Training Loss 0.004625510543508466\n",
      "Episode 17599; Testing Loss 0.005775228701282834; Training Loss 0.004625499295330402\n",
      "Episode 17600; Testing Loss 0.0057754502072358445; Training Loss 0.004625492415941714\n",
      "Episode 17601; Testing Loss 0.00577541940314498; Training Loss 0.00462548286709209\n",
      "Episode 17602; Testing Loss 0.005775258669739254; Training Loss 0.004625475803402896\n",
      "Episode 17603; Testing Loss 0.005775319288302279; Training Loss 0.0046254696380606105\n",
      "Episode 17604; Testing Loss 0.005775383853077994; Training Loss 0.004625460704540441\n",
      "Episode 17605; Testing Loss 0.005775333164545358; Training Loss 0.0046254505691733176\n",
      "Episode 17606; Testing Loss 0.005775237265203943; Training Loss 0.0046254453635363405\n",
      "Episode 17607; Testing Loss 0.005775245217317854; Training Loss 0.0046254344502542726\n",
      "Episode 17608; Testing Loss 0.005775210774334781; Training Loss 0.00462542711479209\n",
      "Episode 17609; Testing Loss 0.005775257482639274; Training Loss 0.004625420456505377\n",
      "Episode 17610; Testing Loss 0.0057751998657161385; Training Loss 0.004625412685289483\n",
      "Episode 17611; Testing Loss 0.005775125274777759; Training Loss 0.00462540368349464\n",
      "Episode 17612; Testing Loss 0.005775242669262025; Training Loss 0.004625394557515338\n",
      "Episode 17613; Testing Loss 0.0057754442309763615; Training Loss 0.00462538913326322\n",
      "Episode 17614; Testing Loss 0.005775394826008174; Training Loss 0.004625379365134619\n",
      "Episode 17615; Testing Loss 0.005775115166770247; Training Loss 0.004625371133347196\n",
      "Episode 17616; Testing Loss 0.005775082970004913; Training Loss 0.004625363619796553\n",
      "Episode 17617; Testing Loss 0.005775320193682137; Training Loss 0.004625355588458369\n",
      "Episode 17618; Testing Loss 0.005775259792363961; Training Loss 0.00462534667799516\n",
      "Episode 17619; Testing Loss 0.0057751507821197975; Training Loss 0.004625338135416827\n",
      "Episode 17620; Testing Loss 0.005775102736133672; Training Loss 0.004625330275499327\n",
      "Episode 17621; Testing Loss 0.0057752219577537145; Training Loss 0.004625322303623744\n",
      "Episode 17622; Testing Loss 0.005775391533058796; Training Loss 0.00462531515402936\n",
      "Episode 17623; Testing Loss 0.005775308759243687; Training Loss 0.004625306001046442\n",
      "Episode 17624; Testing Loss 0.005775112637489508; Training Loss 0.004625298261277827\n",
      "Episode 17625; Testing Loss 0.005775078524524228; Training Loss 0.004625290238822788\n",
      "Episode 17626; Testing Loss 0.0057751822861236346; Training Loss 0.00462528197104424\n",
      "Episode 17627; Testing Loss 0.00577526672721254; Training Loss 0.004625274415882368\n",
      "Episode 17628; Testing Loss 0.005775101057592055; Training Loss 0.004625268426864404\n",
      "Episode 17629; Testing Loss 0.005775134967515948; Training Loss 0.004625256964417621\n",
      "Episode 17630; Testing Loss 0.005775246250373074; Training Loss 0.0046252516297762315\n",
      "Episode 17631; Testing Loss 0.005775218319468209; Training Loss 0.004625244615543709\n",
      "Episode 17632; Testing Loss 0.005775151128202342; Training Loss 0.004625234648623323\n",
      "Episode 17633; Testing Loss 0.00577518366778332; Training Loss 0.004625231241645579\n",
      "Episode 17634; Testing Loss 0.005775339256060833; Training Loss 0.004625222519357569\n",
      "Episode 17635; Testing Loss 0.005775284851343992; Training Loss 0.004625210336668437\n",
      "Episode 17636; Testing Loss 0.005774960406476984; Training Loss 0.004625203429618455\n",
      "Episode 17637; Testing Loss 0.005774949003158124; Training Loss 0.004625195129872382\n",
      "Episode 17638; Testing Loss 0.005775243641702469; Training Loss 0.004625186768787913\n",
      "Episode 17639; Testing Loss 0.0057752671813414095; Training Loss 0.0046251791660677985\n",
      "Episode 17640; Testing Loss 0.005775082858920172; Training Loss 0.004625170535832982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17641; Testing Loss 0.005775057491688142; Training Loss 0.004625162921480493\n",
      "Episode 17642; Testing Loss 0.0057751479975833245; Training Loss 0.0046251533838092265\n",
      "Episode 17643; Testing Loss 0.005775115803324956; Training Loss 0.004625146083594745\n",
      "Episode 17644; Testing Loss 0.005775083472192263; Training Loss 0.004625139852648952\n",
      "Episode 17645; Testing Loss 0.005775106282995031; Training Loss 0.004625129989283139\n",
      "Episode 17646; Testing Loss 0.005775050045961586; Training Loss 0.004625123634873482\n",
      "Episode 17647; Testing Loss 0.005774948938329195; Training Loss 0.004625115807601509\n",
      "Episode 17648; Testing Loss 0.0057750048805438395; Training Loss 0.0046251059060023495\n",
      "Episode 17649; Testing Loss 0.005775132453126725; Training Loss 0.004625098294601989\n",
      "Episode 17650; Testing Loss 0.00577520339644238; Training Loss 0.004625091941142371\n",
      "Episode 17651; Testing Loss 0.005775153327842455; Training Loss 0.004625083391849352\n",
      "Episode 17652; Testing Loss 0.005775073452612846; Training Loss 0.004625073970209673\n",
      "Episode 17653; Testing Loss 0.005775066342696323; Training Loss 0.004625066299155096\n",
      "Episode 17654; Testing Loss 0.005774994454796565; Training Loss 0.004625058854929143\n",
      "Episode 17655; Testing Loss 0.005774943273898007; Training Loss 0.004625049612488006\n",
      "Episode 17656; Testing Loss 0.005775073888777104; Training Loss 0.004625041346338788\n",
      "Episode 17657; Testing Loss 0.0057751278998574; Training Loss 0.004625032772471217\n",
      "Episode 17658; Testing Loss 0.005775004073399396; Training Loss 0.00462502392020935\n",
      "Episode 17659; Testing Loss 0.00577497452193512; Training Loss 0.0046250162838289205\n",
      "Episode 17660; Testing Loss 0.005775089765781825; Training Loss 0.004625008056878405\n",
      "Episode 17661; Testing Loss 0.005775139834544252; Training Loss 0.004625000639225604\n",
      "Episode 17662; Testing Loss 0.0057750257004090325; Training Loss 0.004624991928775803\n",
      "Episode 17663; Testing Loss 0.005774934391635702; Training Loss 0.004624983306171143\n",
      "Episode 17664; Testing Loss 0.00577498354628999; Training Loss 0.00462497431529072\n",
      "Episode 17665; Testing Loss 0.005775127656444751; Training Loss 0.004624968435601904\n",
      "Episode 17666; Testing Loss 0.005775000454150241; Training Loss 0.00462495843116783\n",
      "Episode 17667; Testing Loss 0.00577494936462616; Training Loss 0.0046249511478463606\n",
      "Episode 17668; Testing Loss 0.005774903093673892; Training Loss 0.004624943823057289\n",
      "Episode 17669; Testing Loss 0.0057749419211017405; Training Loss 0.00462493483614882\n",
      "Episode 17670; Testing Loss 0.0057749792961619; Training Loss 0.004624927129905639\n",
      "Episode 17671; Testing Loss 0.005775030637272981; Training Loss 0.00462491830982192\n",
      "Episode 17672; Testing Loss 0.005774902386849041; Training Loss 0.004624909970686228\n",
      "Episode 17673; Testing Loss 0.005774946778303933; Training Loss 0.004624902113391764\n",
      "Episode 17674; Testing Loss 0.005775041492080567; Training Loss 0.004624893561554795\n",
      "Episode 17675; Testing Loss 0.005775075994698363; Training Loss 0.004624885587259246\n",
      "Episode 17676; Testing Loss 0.005775021447510515; Training Loss 0.0046248765959185055\n",
      "Episode 17677; Testing Loss 0.0057749195472444285; Training Loss 0.004624869215910168\n",
      "Episode 17678; Testing Loss 0.005774891899500185; Training Loss 0.004624861273260314\n",
      "Episode 17679; Testing Loss 0.005774964624646048; Training Loss 0.0046248534172304324\n",
      "Episode 17680; Testing Loss 0.005775066460807655; Training Loss 0.004624846727932281\n",
      "Episode 17681; Testing Loss 0.005774909389130349; Training Loss 0.004624837439699251\n",
      "Episode 17682; Testing Loss 0.005774811905407369; Training Loss 0.00462483060684192\n",
      "Episode 17683; Testing Loss 0.00577498880174018; Training Loss 0.004624821681544981\n",
      "Episode 17684; Testing Loss 0.005775196926423603; Training Loss 0.004624815094091803\n",
      "Episode 17685; Testing Loss 0.005775014434260039; Training Loss 0.004624804435485888\n",
      "Episode 17686; Testing Loss 0.005774729961013711; Training Loss 0.004624798628148091\n",
      "Episode 17687; Testing Loss 0.005774885945069895; Training Loss 0.004624787846716599\n",
      "Episode 17688; Testing Loss 0.005775115561045489; Training Loss 0.0046247806211427815\n",
      "Episode 17689; Testing Loss 0.005775079281945855; Training Loss 0.004624771453779457\n",
      "Episode 17690; Testing Loss 0.005774846914678077; Training Loss 0.0046247641101565245\n",
      "Episode 17691; Testing Loss 0.005774839926644776; Training Loss 0.0046247557355585415\n",
      "Episode 17692; Testing Loss 0.00577505905381425; Training Loss 0.004624747551771641\n",
      "Episode 17693; Testing Loss 0.005775029797952678; Training Loss 0.004624739444078393\n",
      "Episode 17694; Testing Loss 0.005774904974521002; Training Loss 0.004624731466165235\n",
      "Episode 17695; Testing Loss 0.0057749199329663195; Training Loss 0.0046247222236883686\n",
      "Episode 17696; Testing Loss 0.00577506284680404; Training Loss 0.004624716242945464\n",
      "Episode 17697; Testing Loss 0.00577496290994143; Training Loss 0.004624706908291977\n",
      "Episode 17698; Testing Loss 0.0057747953163845575; Training Loss 0.004624699547526967\n",
      "Episode 17699; Testing Loss 0.005774952085477322; Training Loss 0.004624689837581451\n",
      "Episode 17700; Testing Loss 0.005775139199628734; Training Loss 0.004624682788266756\n",
      "Episode 17701; Testing Loss 0.005775028891699321; Training Loss 0.004624674957794445\n",
      "Episode 17702; Testing Loss 0.005774850234955661; Training Loss 0.004624667047482835\n",
      "Episode 17703; Testing Loss 0.005774912161450851; Training Loss 0.004624660084859295\n",
      "Episode 17704; Testing Loss 0.005775156387761566; Training Loss 0.0046246518679597\n",
      "Episode 17705; Testing Loss 0.005774949389362342; Training Loss 0.0046246418055465\n",
      "Episode 17706; Testing Loss 0.0057747214324068655; Training Loss 0.004624634634141444\n",
      "Episode 17707; Testing Loss 0.0057747992135541645; Training Loss 0.004624626819675893\n",
      "Episode 17708; Testing Loss 0.005774960546506955; Training Loss 0.004624619172780656\n",
      "Episode 17709; Testing Loss 0.005774911841792212; Training Loss 0.004624608079858137\n",
      "Episode 17710; Testing Loss 0.0057748499662545645; Training Loss 0.004624605230567013\n",
      "Episode 17711; Testing Loss 0.005775009547309885; Training Loss 0.004624593936337623\n",
      "Episode 17712; Testing Loss 0.005775039201674348; Training Loss 0.004624586780748937\n",
      "Episode 17713; Testing Loss 0.005774799570986167; Training Loss 0.004624578932221012\n",
      "Episode 17714; Testing Loss 0.0057746777356412; Training Loss 0.004624571439163493\n",
      "Episode 17715; Testing Loss 0.00577487699531446; Training Loss 0.004624560578732796\n",
      "Episode 17716; Testing Loss 0.005774998601146321; Training Loss 0.0046245544028857555\n",
      "Episode 17717; Testing Loss 0.005774879878578652; Training Loss 0.004624546622396246\n",
      "Episode 17718; Testing Loss 0.005774781978593362; Training Loss 0.00462453558262647\n",
      "Episode 17719; Testing Loss 0.005774839825234544; Training Loss 0.004624529596317107\n",
      "Episode 17720; Testing Loss 0.005774952318512189; Training Loss 0.0046245229390867535\n",
      "Episode 17721; Testing Loss 0.005774937046347889; Training Loss 0.004624512925000041\n",
      "Episode 17722; Testing Loss 0.005774784055688384; Training Loss 0.004624503871857535\n",
      "Episode 17723; Testing Loss 0.005774820668968823; Training Loss 0.004624496448277699\n",
      "Episode 17724; Testing Loss 0.005775059626914892; Training Loss 0.004624487973709216\n",
      "Episode 17725; Testing Loss 0.005775015606905342; Training Loss 0.004624480097703959\n",
      "Episode 17726; Testing Loss 0.0057747752869873395; Training Loss 0.004624472608544073\n",
      "Episode 17727; Testing Loss 0.005774755450903497; Training Loss 0.004624462411718541\n",
      "Episode 17728; Testing Loss 0.005774913763600777; Training Loss 0.004624455254428709\n",
      "Episode 17729; Testing Loss 0.005774868945666552; Training Loss 0.00462444672127103\n",
      "Episode 17730; Testing Loss 0.005774769261175985; Training Loss 0.004624438340693595\n",
      "Episode 17731; Testing Loss 0.00577481703508262; Training Loss 0.00462442988244305\n",
      "Episode 17732; Testing Loss 0.005774944302614672; Training Loss 0.004624423027702092\n",
      "Episode 17733; Testing Loss 0.0057749240495585195; Training Loss 0.004624413556188329\n",
      "Episode 17734; Testing Loss 0.005774756276238589; Training Loss 0.004624406395922733\n",
      "Episode 17735; Testing Loss 0.005774842359262758; Training Loss 0.004624398025533486\n",
      "Episode 17736; Testing Loss 0.005775004194095883; Training Loss 0.004624390382119447\n",
      "Episode 17737; Testing Loss 0.005774937152922675; Training Loss 0.0046243804504015554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17738; Testing Loss 0.005774786898513886; Training Loss 0.0046243741293918155\n",
      "Episode 17739; Testing Loss 0.005774800450255165; Training Loss 0.004624365473699292\n",
      "Episode 17740; Testing Loss 0.005774885780601046; Training Loss 0.00462435625532942\n",
      "Episode 17741; Testing Loss 0.005774938358511992; Training Loss 0.004624349727431192\n",
      "Episode 17742; Testing Loss 0.0057748279850391; Training Loss 0.004624340755745604\n",
      "Episode 17743; Testing Loss 0.005774722557466177; Training Loss 0.004624332584940433\n",
      "Episode 17744; Testing Loss 0.005774782889482456; Training Loss 0.0046243254313132354\n",
      "Episode 17745; Testing Loss 0.005774808184147796; Training Loss 0.004624317151057716\n",
      "Episode 17746; Testing Loss 0.005774784123904997; Training Loss 0.004624307051329197\n",
      "Episode 17747; Testing Loss 0.0057747993370471365; Training Loss 0.004624299381027341\n",
      "Episode 17748; Testing Loss 0.005774934501154792; Training Loss 0.004624290954487495\n",
      "Episode 17749; Testing Loss 0.005774949403725716; Training Loss 0.00462428429963294\n",
      "Episode 17750; Testing Loss 0.005774751549864659; Training Loss 0.004624275477380456\n",
      "Episode 17751; Testing Loss 0.005774686315596521; Training Loss 0.004624266063804651\n",
      "Episode 17752; Testing Loss 0.005774868221124052; Training Loss 0.0046242581947100935\n",
      "Episode 17753; Testing Loss 0.005774875052352895; Training Loss 0.004624249628720751\n",
      "Episode 17754; Testing Loss 0.005774708762014528; Training Loss 0.0046242407416296405\n",
      "Episode 17755; Testing Loss 0.005774713709535183; Training Loss 0.004624233353632656\n",
      "Episode 17756; Testing Loss 0.005774821208334615; Training Loss 0.004624224860103118\n",
      "Episode 17757; Testing Loss 0.005774877516354267; Training Loss 0.004624216191588606\n",
      "Episode 17758; Testing Loss 0.005774747690496158; Training Loss 0.004624211844198779\n",
      "Episode 17759; Testing Loss 0.005774768042829823; Training Loss 0.004624200290079528\n",
      "Episode 17760; Testing Loss 0.005774857465714503; Training Loss 0.004624194322985704\n",
      "Episode 17761; Testing Loss 0.005774742494850656; Training Loss 0.004624187536811042\n",
      "Episode 17762; Testing Loss 0.00577464316726866; Training Loss 0.004624178909202609\n",
      "Episode 17763; Testing Loss 0.005774705580622882; Training Loss 0.004624167871175577\n",
      "Episode 17764; Testing Loss 0.005774778440475938; Training Loss 0.004624160072335927\n",
      "Episode 17765; Testing Loss 0.0057748424164262265; Training Loss 0.0046241559822574685\n",
      "Episode 17766; Testing Loss 0.005774847798040854; Training Loss 0.0046241434811889245\n",
      "Episode 17767; Testing Loss 0.005774719759568489; Training Loss 0.004624133780426897\n",
      "Episode 17768; Testing Loss 0.005774675426205427; Training Loss 0.004624126880482989\n",
      "Episode 17769; Testing Loss 0.005774724980004716; Training Loss 0.004624117489679616\n",
      "Episode 17770; Testing Loss 0.005774790411187392; Training Loss 0.004624109853890931\n",
      "Episode 17771; Testing Loss 0.0057749091669158795; Training Loss 0.004624103320694592\n",
      "Episode 17772; Testing Loss 0.005774974643210948; Training Loss 0.004624094461228126\n",
      "Episode 17773; Testing Loss 0.005774832684163799; Training Loss 0.004624085497588504\n",
      "Episode 17774; Testing Loss 0.0057746889843468445; Training Loss 0.0046240773304760995\n",
      "Episode 17775; Testing Loss 0.005774713408306521; Training Loss 0.004624068296314925\n",
      "Episode 17776; Testing Loss 0.005774854523936339; Training Loss 0.004624061501914912\n",
      "Episode 17777; Testing Loss 0.005774779916532234; Training Loss 0.004624052969086047\n",
      "Episode 17778; Testing Loss 0.005774547442304707; Training Loss 0.0046240460732754965\n",
      "Episode 17779; Testing Loss 0.005774615184637123; Training Loss 0.0046240363883154265\n",
      "Episode 17780; Testing Loss 0.005774876826310592; Training Loss 0.004624029623075069\n",
      "Episode 17781; Testing Loss 0.005774797542392898; Training Loss 0.004624018918977596\n",
      "Episode 17782; Testing Loss 0.005774668310541388; Training Loss 0.0046240111976912635\n",
      "Episode 17783; Testing Loss 0.005774779687871608; Training Loss 0.004624002488411296\n",
      "Episode 17784; Testing Loss 0.005774852937917624; Training Loss 0.004623994119651864\n",
      "Episode 17785; Testing Loss 0.005774781339091405; Training Loss 0.004623985928193391\n",
      "Episode 17786; Testing Loss 0.0057747047553747535; Training Loss 0.0046239778425339\n",
      "Episode 17787; Testing Loss 0.005774726535424531; Training Loss 0.0046239693284064335\n",
      "Episode 17788; Testing Loss 0.005774854195023485; Training Loss 0.004623961678054231\n",
      "Episode 17789; Testing Loss 0.005774818486033371; Training Loss 0.004623952570351605\n",
      "Episode 17790; Testing Loss 0.0057746553671781066; Training Loss 0.004623944920007242\n",
      "Episode 17791; Testing Loss 0.005774661560478158; Training Loss 0.004623936672102728\n",
      "Episode 17792; Testing Loss 0.0057746918423327975; Training Loss 0.004623928328314392\n",
      "Episode 17793; Testing Loss 0.005774724135719318; Training Loss 0.004623919917808957\n",
      "Episode 17794; Testing Loss 0.005774789603752299; Training Loss 0.004623911950570786\n",
      "Episode 17795; Testing Loss 0.005774742727352981; Training Loss 0.004623904838870931\n",
      "Episode 17796; Testing Loss 0.0057746309530336834; Training Loss 0.004623895846172879\n",
      "Episode 17797; Testing Loss 0.0057746228148596816; Training Loss 0.004623889281942758\n",
      "Episode 17798; Testing Loss 0.005774736437481636; Training Loss 0.0046238815328867346\n",
      "Episode 17799; Testing Loss 0.005774624355991705; Training Loss 0.004623872298663767\n",
      "Episode 17800; Testing Loss 0.005774469169086225; Training Loss 0.004623865654294556\n",
      "Episode 17801; Testing Loss 0.005774632696003089; Training Loss 0.004623856218512494\n",
      "Episode 17802; Testing Loss 0.005774715385342636; Training Loss 0.004623848128200322\n",
      "Episode 17803; Testing Loss 0.005774626466641204; Training Loss 0.004623838741579227\n",
      "Episode 17804; Testing Loss 0.005774618560529702; Training Loss 0.004623832742658984\n",
      "Episode 17805; Testing Loss 0.005774804383447398; Training Loss 0.004623823293297403\n",
      "Episode 17806; Testing Loss 0.005774756292554652; Training Loss 0.004623814775626469\n",
      "Episode 17807; Testing Loss 0.005774535724839988; Training Loss 0.004623806922412776\n",
      "Episode 17808; Testing Loss 0.005774584978454322; Training Loss 0.004623798224587307\n",
      "Episode 17809; Testing Loss 0.0057747594049509035; Training Loss 0.0046237906951763835\n",
      "Episode 17810; Testing Loss 0.00577468898263126; Training Loss 0.00462378136372015\n",
      "Episode 17811; Testing Loss 0.005774620765886419; Training Loss 0.004623773124588592\n",
      "Episode 17812; Testing Loss 0.005774664770642819; Training Loss 0.004623765310991134\n",
      "Episode 17813; Testing Loss 0.005774726071765134; Training Loss 0.004623756882437244\n",
      "Episode 17814; Testing Loss 0.005774755368591171; Training Loss 0.0046237498935561465\n",
      "Episode 17815; Testing Loss 0.005774761342523177; Training Loss 0.004623743583326717\n",
      "Episode 17816; Testing Loss 0.005774771561893426; Training Loss 0.004623733419958372\n",
      "Episode 17817; Testing Loss 0.005774615731461744; Training Loss 0.00462372524078242\n",
      "Episode 17818; Testing Loss 0.005774455828506692; Training Loss 0.004623718506518829\n",
      "Episode 17819; Testing Loss 0.0057745654641886066; Training Loss 0.004623708337143882\n",
      "Episode 17820; Testing Loss 0.005774653872225569; Training Loss 0.004623701533541254\n",
      "Episode 17821; Testing Loss 0.005774677603859874; Training Loss 0.0046236945588528445\n",
      "Episode 17822; Testing Loss 0.005774620179208133; Training Loss 0.00462368342829854\n",
      "Episode 17823; Testing Loss 0.00577459282115981; Training Loss 0.004623676819039177\n",
      "Episode 17824; Testing Loss 0.005774621720129819; Training Loss 0.004623669719161114\n",
      "Episode 17825; Testing Loss 0.005774662548836531; Training Loss 0.004623659374442757\n",
      "Episode 17826; Testing Loss 0.005774594565152773; Training Loss 0.004623651686917331\n",
      "Episode 17827; Testing Loss 0.0057745674754270905; Training Loss 0.0046236457725367\n",
      "Episode 17828; Testing Loss 0.005774647196423553; Training Loss 0.004623635976046027\n",
      "Episode 17829; Testing Loss 0.005774601734862772; Training Loss 0.004623627919968469\n",
      "Episode 17830; Testing Loss 0.005774449808787616; Training Loss 0.004623620227034669\n",
      "Episode 17831; Testing Loss 0.005774489245118131; Training Loss 0.004623610349892343\n",
      "Episode 17832; Testing Loss 0.005774680874035349; Training Loss 0.004623603042016354\n",
      "Episode 17833; Testing Loss 0.005774730741332501; Training Loss 0.004623596149672233\n",
      "Episode 17834; Testing Loss 0.005774608473123736; Training Loss 0.004623587532074825\n",
      "Episode 17835; Testing Loss 0.005774540623127454; Training Loss 0.0046235787641103745\n",
      "Episode 17836; Testing Loss 0.005774594955008917; Training Loss 0.0046235701739465755\n",
      "Episode 17837; Testing Loss 0.005774576554672136; Training Loss 0.004623563556934084\n",
      "Episode 17838; Testing Loss 0.00577445887479092; Training Loss 0.004623554952364884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17839; Testing Loss 0.005774445455008891; Training Loss 0.004623545778299337\n",
      "Episode 17840; Testing Loss 0.005774603055879613; Training Loss 0.004623537278688199\n",
      "Episode 17841; Testing Loss 0.005774688415494989; Training Loss 0.004623529038383547\n",
      "Episode 17842; Testing Loss 0.005774603739164679; Training Loss 0.004623519982549748\n",
      "Episode 17843; Testing Loss 0.005774454384058184; Training Loss 0.004623512586805358\n",
      "Episode 17844; Testing Loss 0.00577453778152231; Training Loss 0.004623503754378585\n",
      "Episode 17845; Testing Loss 0.005774608438866466; Training Loss 0.004623495882788698\n",
      "Episode 17846; Testing Loss 0.005774601374445019; Training Loss 0.004623487998990782\n",
      "Episode 17847; Testing Loss 0.00577459977535052; Training Loss 0.004623479194259935\n",
      "Episode 17848; Testing Loss 0.005774603082471279; Training Loss 0.004623471268505272\n",
      "Episode 17849; Testing Loss 0.005774530634208894; Training Loss 0.004623462954296094\n",
      "Episode 17850; Testing Loss 0.005774551211002986; Training Loss 0.00462345507918911\n",
      "Episode 17851; Testing Loss 0.0057745717934029545; Training Loss 0.004623447371193564\n",
      "Episode 17852; Testing Loss 0.005774563025684964; Training Loss 0.004623438987990965\n",
      "Episode 17853; Testing Loss 0.005774462623135645; Training Loss 0.004623430738642539\n",
      "Episode 17854; Testing Loss 0.005774398963170416; Training Loss 0.004623424326924429\n",
      "Episode 17855; Testing Loss 0.0057745790038848084; Training Loss 0.004623416960345361\n",
      "Episode 17856; Testing Loss 0.005774584319427193; Training Loss 0.004623408191952518\n",
      "Episode 17857; Testing Loss 0.005774469656203692; Training Loss 0.004623398795681723\n",
      "Episode 17858; Testing Loss 0.005774472138197986; Training Loss 0.004623396734027844\n",
      "Episode 17859; Testing Loss 0.005774698844718928; Training Loss 0.0046233856672509704\n",
      "Episode 17860; Testing Loss 0.005774716042759057; Training Loss 0.004623378567471361\n",
      "Episode 17861; Testing Loss 0.0057743173362213105; Training Loss 0.0046233700559929975\n",
      "Episode 17862; Testing Loss 0.00577419510427018; Training Loss 0.004623364310911078\n",
      "Episode 17863; Testing Loss 0.005774478018806778; Training Loss 0.004623351080812173\n",
      "Episode 17864; Testing Loss 0.005774681895090381; Training Loss 0.0046233458595926705\n",
      "Episode 17865; Testing Loss 0.005774555643770267; Training Loss 0.004623337733982131\n",
      "Episode 17866; Testing Loss 0.005774440362794899; Training Loss 0.004623327750116667\n",
      "Episode 17867; Testing Loss 0.005774485779641448; Training Loss 0.004623318896483012\n",
      "Episode 17868; Testing Loss 0.005774554024571205; Training Loss 0.004623310277666489\n",
      "Episode 17869; Testing Loss 0.005774496119299124; Training Loss 0.004623302134314094\n",
      "Episode 17870; Testing Loss 0.005774387430651427; Training Loss 0.004623294666041277\n",
      "Episode 17871; Testing Loss 0.005774350542148277; Training Loss 0.004623287127294647\n",
      "Episode 17872; Testing Loss 0.0057744247287769635; Training Loss 0.004623278394676437\n",
      "Episode 17873; Testing Loss 0.005774463951944191; Training Loss 0.004623270802907914\n",
      "Episode 17874; Testing Loss 0.005774317867610501; Training Loss 0.004623262044119648\n",
      "Episode 17875; Testing Loss 0.005774327374144773; Training Loss 0.004623253330538174\n",
      "Episode 17876; Testing Loss 0.005774544236187211; Training Loss 0.004623244883532334\n",
      "Episode 17877; Testing Loss 0.005774599506087194; Training Loss 0.004623237602286489\n",
      "Episode 17878; Testing Loss 0.005774400589285724; Training Loss 0.004623229069403173\n",
      "Episode 17879; Testing Loss 0.005774333797275734; Training Loss 0.004623221135168093\n",
      "Episode 17880; Testing Loss 0.005774429934147257; Training Loss 0.004623212420373513\n",
      "Episode 17881; Testing Loss 0.005774491459686136; Training Loss 0.004623204251446036\n",
      "Episode 17882; Testing Loss 0.005774332640852731; Training Loss 0.0046231954146924124\n",
      "Episode 17883; Testing Loss 0.005774318576820342; Training Loss 0.004623187760834603\n",
      "Episode 17884; Testing Loss 0.005774509146595157; Training Loss 0.004623180740776237\n",
      "Episode 17885; Testing Loss 0.005774521727341755; Training Loss 0.004623171742583143\n",
      "Episode 17886; Testing Loss 0.005774341604533271; Training Loss 0.004623165769167983\n",
      "Episode 17887; Testing Loss 0.005774364153092291; Training Loss 0.004623155102487294\n",
      "Episode 17888; Testing Loss 0.005774440551796543; Training Loss 0.004623147944371773\n",
      "Episode 17889; Testing Loss 0.0057743871598769985; Training Loss 0.004623139435181418\n",
      "Episode 17890; Testing Loss 0.005774285773794308; Training Loss 0.004623131738509085\n",
      "Episode 17891; Testing Loss 0.005774425068990481; Training Loss 0.004623122603685631\n",
      "Episode 17892; Testing Loss 0.005774508249693477; Training Loss 0.004623115241416046\n",
      "Episode 17893; Testing Loss 0.005774391607884191; Training Loss 0.004623105807220286\n",
      "Episode 17894; Testing Loss 0.0057742895173722485; Training Loss 0.004623099110463684\n",
      "Episode 17895; Testing Loss 0.005774412317327086; Training Loss 0.004623089562428105\n",
      "Episode 17896; Testing Loss 0.0057745677304854495; Training Loss 0.004623083959518751\n",
      "Episode 17897; Testing Loss 0.00577443419816616; Training Loss 0.004623073562865801\n",
      "Episode 17898; Testing Loss 0.005774226825297565; Training Loss 0.0046230699505031055\n",
      "Episode 17899; Testing Loss 0.005774365516610658; Training Loss 0.004623058007988811\n",
      "Episode 17900; Testing Loss 0.005774504125275766; Training Loss 0.004623055434937957\n",
      "Episode 17901; Testing Loss 0.005774322151096121; Training Loss 0.004623046743296246\n",
      "Episode 17902; Testing Loss 0.005774161538674839; Training Loss 0.004623039768705406\n",
      "Episode 17903; Testing Loss 0.0057743494705583725; Training Loss 0.004623028099300725\n",
      "Episode 17904; Testing Loss 0.005774528090988101; Training Loss 0.0046230193977085405\n",
      "Episode 17905; Testing Loss 0.005774468570195349; Training Loss 0.00462301221886846\n",
      "Episode 17906; Testing Loss 0.005774342870304345; Training Loss 0.004623002585087337\n",
      "Episode 17907; Testing Loss 0.005774341700377498; Training Loss 0.004622994588176803\n",
      "Episode 17908; Testing Loss 0.005774361512737907; Training Loss 0.004622986751745243\n",
      "Episode 17909; Testing Loss 0.005774303470627428; Training Loss 0.004622977372284072\n",
      "Episode 17910; Testing Loss 0.005774203908283093; Training Loss 0.004622972351379006\n",
      "Episode 17911; Testing Loss 0.005774298226474397; Training Loss 0.004622962856287684\n",
      "Episode 17912; Testing Loss 0.005774352251447894; Training Loss 0.0046229543023656484\n",
      "Episode 17913; Testing Loss 0.005774223700413386; Training Loss 0.004622946714538115\n",
      "Episode 17914; Testing Loss 0.005774129416508226; Training Loss 0.004622939372696747\n",
      "Episode 17915; Testing Loss 0.0057742130189446005; Training Loss 0.0046229290142008415\n",
      "Episode 17916; Testing Loss 0.00577433148230838; Training Loss 0.004622922231300752\n",
      "Episode 17917; Testing Loss 0.005774408393254271; Training Loss 0.004622916109246207\n",
      "Episode 17918; Testing Loss 0.005774410202073584; Training Loss 0.004622905912770079\n",
      "Episode 17919; Testing Loss 0.005774265633327608; Training Loss 0.004622896113207977\n",
      "Episode 17920; Testing Loss 0.005774186120568517; Training Loss 0.004622890499093659\n",
      "Episode 17921; Testing Loss 0.005774230412869494; Training Loss 0.004622882546635726\n",
      "Episode 17922; Testing Loss 0.005774289474532071; Training Loss 0.004622872926052602\n",
      "Episode 17923; Testing Loss 0.005774250644792746; Training Loss 0.004622864996936958\n",
      "Episode 17924; Testing Loss 0.005774255182633372; Training Loss 0.004622857681614697\n",
      "Episode 17925; Testing Loss 0.005774331442532139; Training Loss 0.004622849678059443\n",
      "Episode 17926; Testing Loss 0.005774391590554921; Training Loss 0.00462284125284395\n",
      "Episode 17927; Testing Loss 0.005774322531998663; Training Loss 0.004622831276623861\n",
      "Episode 17928; Testing Loss 0.005774235597500357; Training Loss 0.004622825190736544\n",
      "Episode 17929; Testing Loss 0.005774114281897644; Training Loss 0.004622817258982765\n",
      "Episode 17930; Testing Loss 0.005774095943402143; Training Loss 0.0046228078641367765\n",
      "Episode 17931; Testing Loss 0.005774299452320771; Training Loss 0.004622799446856016\n",
      "Episode 17932; Testing Loss 0.005774329177537602; Training Loss 0.004622791201884813\n",
      "Episode 17933; Testing Loss 0.0057741659033280985; Training Loss 0.00462278290273476\n",
      "Episode 17934; Testing Loss 0.005774195640374047; Training Loss 0.004622774999498708\n",
      "Episode 17935; Testing Loss 0.005774293782502346; Training Loss 0.004622766932614137\n",
      "Episode 17936; Testing Loss 0.00577427646640663; Training Loss 0.004622757420244272\n",
      "Episode 17937; Testing Loss 0.0057742694908524345; Training Loss 0.004622749999315419\n",
      "Episode 17938; Testing Loss 0.005774269028709164; Training Loss 0.004622741513482716\n",
      "Episode 17939; Testing Loss 0.005774291063229243; Training Loss 0.004622734476091204\n",
      "Episode 17940; Testing Loss 0.005774219088250705; Training Loss 0.004622725511523191\n",
      "Episode 17941; Testing Loss 0.005774215248064172; Training Loss 0.004622717255025089\n",
      "Episode 17942; Testing Loss 0.005774196284577613; Training Loss 0.00462270964904057\n",
      "Episode 17943; Testing Loss 0.005774147962749587; Training Loss 0.0046227016783266335\n",
      "Episode 17944; Testing Loss 0.005774193018857775; Training Loss 0.004622692881786469\n",
      "Episode 17945; Testing Loss 0.005774268210886258; Training Loss 0.004622686318474858\n",
      "Episode 17946; Testing Loss 0.005774183202707752; Training Loss 0.004622677954953467\n",
      "Episode 17947; Testing Loss 0.005774173485963542; Training Loss 0.004622672953058529\n",
      "Episode 17948; Testing Loss 0.005774285309511302; Training Loss 0.004622662056850053\n",
      "Episode 17949; Testing Loss 0.0057742500076867365; Training Loss 0.004622657704599406\n",
      "Episode 17950; Testing Loss 0.005773964702995301; Training Loss 0.004622650426278967\n",
      "Episode 17951; Testing Loss 0.005773871880238577; Training Loss 0.004622643073490944\n",
      "Episode 17952; Testing Loss 0.005774122831253116; Training Loss 0.004622631324934194\n",
      "Episode 17953; Testing Loss 0.005774248500719498; Training Loss 0.00462262336710632\n",
      "Episode 17954; Testing Loss 0.005774156913037197; Training Loss 0.004622617647478693\n",
      "Episode 17955; Testing Loss 0.005774101176333872; Training Loss 0.0046226064380753\n",
      "Episode 17956; Testing Loss 0.00577420908066681; Training Loss 0.004622601135998572\n",
      "Episode 17957; Testing Loss 0.005774140896987756; Training Loss 0.004622594682309149\n",
      "Episode 17958; Testing Loss 0.005773972841087907; Training Loss 0.004622586205397037\n",
      "Episode 17959; Testing Loss 0.00577393659306032; Training Loss 0.004622576437207671\n",
      "Episode 17960; Testing Loss 0.005774113017274686; Training Loss 0.004622565843776017\n",
      "Episode 17961; Testing Loss 0.005774285080042217; Training Loss 0.004622559952702722\n",
      "Episode 17962; Testing Loss 0.005774189213283105; Training Loss 0.004622549839809747\n",
      "Episode 17963; Testing Loss 0.005774045474981793; Training Loss 0.00462254265539668\n",
      "Episode 17964; Testing Loss 0.00577417005313656; Training Loss 0.0046225324221542095\n",
      "Episode 17965; Testing Loss 0.005774268252841013; Training Loss 0.004622528923063266\n",
      "Episode 17966; Testing Loss 0.005774067344840083; Training Loss 0.0046225193714406875\n",
      "Episode 17967; Testing Loss 0.005773888169557869; Training Loss 0.004622510652644615\n",
      "Episode 17968; Testing Loss 0.005774007163905097; Training Loss 0.00462250116475896\n",
      "Episode 17969; Testing Loss 0.005774242012789166; Training Loss 0.004622494173805105\n",
      "Episode 17970; Testing Loss 0.00577420847120181; Training Loss 0.0046224840152226734\n",
      "Episode 17971; Testing Loss 0.0057740536731392175; Training Loss 0.004622476233263254\n",
      "Episode 17972; Testing Loss 0.005774072225258754; Training Loss 0.0046224700825234005\n",
      "Episode 17973; Testing Loss 0.00577414095449768; Training Loss 0.004622462452552066\n",
      "Episode 17974; Testing Loss 0.00577407898737505; Training Loss 0.004622451997134028\n",
      "Episode 17975; Testing Loss 0.005774010210847939; Training Loss 0.004622444698464254\n",
      "Episode 17976; Testing Loss 0.005774133681914957; Training Loss 0.004622436069741995\n",
      "Episode 17977; Testing Loss 0.005774188757754743; Training Loss 0.0046224279138140185\n",
      "Episode 17978; Testing Loss 0.005774013998272609; Training Loss 0.004622419293827849\n",
      "Episode 17979; Testing Loss 0.005773940737091383; Training Loss 0.004622411441375528\n",
      "Episode 17980; Testing Loss 0.005774070296615352; Training Loss 0.004622404264298587\n",
      "Episode 17981; Testing Loss 0.005774087093358005; Training Loss 0.0046223954607889734\n",
      "Episode 17982; Testing Loss 0.0057739790283543755; Training Loss 0.004622388762075346\n",
      "Episode 17983; Testing Loss 0.005774072098928683; Training Loss 0.004622378767474682\n",
      "Episode 17984; Testing Loss 0.005774095001026065; Training Loss 0.004622372529765572\n",
      "Episode 17985; Testing Loss 0.0057738990899583245; Training Loss 0.004622364164560778\n",
      "Episode 17986; Testing Loss 0.005773848663684919; Training Loss 0.004622356422142606\n",
      "Episode 17987; Testing Loss 0.005774040246145167; Training Loss 0.004622347252994164\n",
      "Episode 17988; Testing Loss 0.005774132306780437; Training Loss 0.004622339192392623\n",
      "Episode 17989; Testing Loss 0.005774037346343319; Training Loss 0.00462233073672736\n",
      "Episode 17990; Testing Loss 0.005773883318324883; Training Loss 0.004622323086703556\n",
      "Episode 17991; Testing Loss 0.005773940622265064; Training Loss 0.004622314527925253\n",
      "Episode 17992; Testing Loss 0.005774078397552062; Training Loss 0.004622307793209678\n",
      "Episode 17993; Testing Loss 0.005773966831837438; Training Loss 0.004622298551927495\n",
      "Episode 17994; Testing Loss 0.005773891728320112; Training Loss 0.004622290804505674\n",
      "Episode 17995; Testing Loss 0.00577397908484193; Training Loss 0.004622282444261235\n",
      "Episode 17996; Testing Loss 0.005773969373662861; Training Loss 0.0046222739041283075\n",
      "Episode 17997; Testing Loss 0.005773945036057124; Training Loss 0.004622265835260115\n",
      "Episode 17998; Testing Loss 0.005773905809870002; Training Loss 0.004622258047537868\n",
      "Episode 17999; Testing Loss 0.0057739346428414395; Training Loss 0.004622250391516301\n",
      "Episode 18000; Testing Loss 0.00577403818398696; Training Loss 0.004622243882504491\n",
      "Episode 18001; Testing Loss 0.005773917677950291; Training Loss 0.0046222351061085435\n",
      "Episode 18002; Testing Loss 0.00577377279124207; Training Loss 0.004622226877683056\n",
      "Episode 18003; Testing Loss 0.005773926490496398; Training Loss 0.004622218134510608\n",
      "Episode 18004; Testing Loss 0.0057740361606548945; Training Loss 0.004622211102123226\n",
      "Episode 18005; Testing Loss 0.005773895566623212; Training Loss 0.004622201733246267\n",
      "Episode 18006; Testing Loss 0.005773877560208841; Training Loss 0.00462219581032872\n",
      "Episode 18007; Testing Loss 0.005773916242022037; Training Loss 0.0046221878237893395\n",
      "Episode 18008; Testing Loss 0.005773964212749498; Training Loss 0.004622177885628481\n",
      "Episode 18009; Testing Loss 0.005773901787589662; Training Loss 0.00462217379069814\n",
      "Episode 18010; Testing Loss 0.0057739203376984275; Training Loss 0.004622165115819793\n",
      "Episode 18011; Testing Loss 0.005773939536741386; Training Loss 0.004622154137960128\n",
      "Episode 18012; Testing Loss 0.005773819075050726; Training Loss 0.004622146785460576\n",
      "Episode 18013; Testing Loss 0.005773808682356357; Training Loss 0.0046221383864063275\n",
      "Episode 18014; Testing Loss 0.005773905985486981; Training Loss 0.004622128910671602\n",
      "Episode 18015; Testing Loss 0.0057739843159830655; Training Loss 0.004622122113152461\n",
      "Episode 18016; Testing Loss 0.005773851627720742; Training Loss 0.0046221136115250595\n",
      "Episode 18017; Testing Loss 0.005773764540249605; Training Loss 0.004622105441854238\n",
      "Episode 18018; Testing Loss 0.005773891644556651; Training Loss 0.004622098998415315\n",
      "Episode 18019; Testing Loss 0.005773951071260916; Training Loss 0.004622090684160566\n",
      "Episode 18020; Testing Loss 0.005773817832214308; Training Loss 0.0046220824997895435\n",
      "Episode 18021; Testing Loss 0.005773854511370503; Training Loss 0.004622074596954166\n",
      "Episode 18022; Testing Loss 0.005773932262531598; Training Loss 0.004622066425280491\n",
      "Episode 18023; Testing Loss 0.005773886446887667; Training Loss 0.004622057483085833\n",
      "Episode 18024; Testing Loss 0.005773833854502479; Training Loss 0.004622050306384964\n",
      "Episode 18025; Testing Loss 0.005773786382419704; Training Loss 0.004622041925811155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18026; Testing Loss 0.00577381829933972; Training Loss 0.0046220337152419804\n",
      "Episode 18027; Testing Loss 0.005773829206846089; Training Loss 0.004622026606617756\n",
      "Episode 18028; Testing Loss 0.0057738041593881; Training Loss 0.004622018571108865\n",
      "Episode 18029; Testing Loss 0.005773834142393351; Training Loss 0.0046220090684695615\n",
      "Episode 18030; Testing Loss 0.005773772717511072; Training Loss 0.00462200152473988\n",
      "Episode 18031; Testing Loss 0.005773773921961817; Training Loss 0.004621993466045474\n",
      "Episode 18032; Testing Loss 0.005773826029222722; Training Loss 0.0046219861602673174\n",
      "Episode 18033; Testing Loss 0.005773911150357947; Training Loss 0.004621979436132179\n",
      "Episode 18034; Testing Loss 0.005773886333288139; Training Loss 0.0046219697548136466\n",
      "Episode 18035; Testing Loss 0.005773756871325181; Training Loss 0.004621961773860003\n",
      "Episode 18036; Testing Loss 0.005773700787594408; Training Loss 0.004621955338743689\n",
      "Episode 18037; Testing Loss 0.005773782854748791; Training Loss 0.004621945093525941\n",
      "Episode 18038; Testing Loss 0.005773852355432884; Training Loss 0.004621937799352916\n",
      "Episode 18039; Testing Loss 0.005773861988180195; Training Loss 0.004621931356858369\n",
      "Episode 18040; Testing Loss 0.00577383438488537; Training Loss 0.004621921446904548\n",
      "Episode 18041; Testing Loss 0.005773732132809052; Training Loss 0.004621913593753302\n",
      "Episode 18042; Testing Loss 0.005773640446446624; Training Loss 0.00462190630608637\n",
      "Episode 18043; Testing Loss 0.005773627492604067; Training Loss 0.004621898636885502\n",
      "Episode 18044; Testing Loss 0.00577378540501171; Training Loss 0.004621890220107778\n",
      "Episode 18045; Testing Loss 0.005773861149928507; Training Loss 0.004621882140572627\n",
      "Episode 18046; Testing Loss 0.005773743179055557; Training Loss 0.004621872247183154\n",
      "Episode 18047; Testing Loss 0.00577368431346399; Training Loss 0.004621866644424222\n",
      "Episode 18048; Testing Loss 0.0057737006030092065; Training Loss 0.00462185767481664\n",
      "Episode 18049; Testing Loss 0.005773770420115848; Training Loss 0.00462184896715032\n",
      "Episode 18050; Testing Loss 0.005773751123121976; Training Loss 0.004621841158288394\n",
      "Episode 18051; Testing Loss 0.005773669392083264; Training Loss 0.004621832651665293\n",
      "Episode 18052; Testing Loss 0.005773715377025239; Training Loss 0.004621824942040618\n",
      "Episode 18053; Testing Loss 0.005773685722219779; Training Loss 0.004621816239891741\n",
      "Episode 18054; Testing Loss 0.005773635377284123; Training Loss 0.004621809483282877\n",
      "Episode 18055; Testing Loss 0.005773711461570569; Training Loss 0.004621802591584604\n",
      "Episode 18056; Testing Loss 0.00577377600542361; Training Loss 0.004621793780018455\n",
      "Episode 18057; Testing Loss 0.005773645006486008; Training Loss 0.004621785230513973\n",
      "Episode 18058; Testing Loss 0.005773541490596221; Training Loss 0.004621779412380512\n",
      "Episode 18059; Testing Loss 0.0057736484307916456; Training Loss 0.0046217686767558415\n",
      "Episode 18060; Testing Loss 0.005773854726001958; Training Loss 0.004621762197770742\n",
      "Episode 18061; Testing Loss 0.005773743939692186; Training Loss 0.004621752806779101\n",
      "Episode 18062; Testing Loss 0.005773533914504544; Training Loss 0.004621745557194949\n",
      "Episode 18063; Testing Loss 0.005773532464476157; Training Loss 0.004621738020367197\n",
      "Episode 18064; Testing Loss 0.005773721836974877; Training Loss 0.004621728087301155\n",
      "Episode 18065; Testing Loss 0.00577380160797848; Training Loss 0.004621721882665714\n",
      "Episode 18066; Testing Loss 0.005773642301474563; Training Loss 0.004621714183385192\n",
      "Episode 18067; Testing Loss 0.005773612507862818; Training Loss 0.004621703485470667\n",
      "Episode 18068; Testing Loss 0.0057736670352476795; Training Loss 0.004621699973429196\n",
      "Episode 18069; Testing Loss 0.005773554175648383; Training Loss 0.004621693241655263\n",
      "Episode 18070; Testing Loss 0.005773438981749103; Training Loss 0.004621684659266561\n",
      "Episode 18071; Testing Loss 0.005773483283302568; Training Loss 0.004621674078055012\n",
      "Episode 18072; Testing Loss 0.0057736554798767225; Training Loss 0.004621665766753792\n",
      "Episode 18073; Testing Loss 0.005773743843323844; Training Loss 0.004621659959839163\n",
      "Episode 18074; Testing Loss 0.005773710374780797; Training Loss 0.004621649791779858\n",
      "Episode 18075; Testing Loss 0.005773632085447009; Training Loss 0.004621641143874534\n",
      "Episode 18076; Testing Loss 0.005773604064403906; Training Loss 0.004621635192300498\n",
      "Episode 18077; Testing Loss 0.005773598916901713; Training Loss 0.004621627117355564\n",
      "Episode 18078; Testing Loss 0.005773532211453836; Training Loss 0.004621617767215794\n",
      "Episode 18079; Testing Loss 0.005773518999764571; Training Loss 0.004621609228364394\n",
      "Episode 18080; Testing Loss 0.0057735950096155925; Training Loss 0.00462160173975559\n",
      "Episode 18081; Testing Loss 0.005773641241894264; Training Loss 0.004621592969992105\n",
      "Episode 18082; Testing Loss 0.005773638950984767; Training Loss 0.004621584293665297\n",
      "Episode 18083; Testing Loss 0.005773675468046091; Training Loss 0.004621575664704413\n",
      "Episode 18084; Testing Loss 0.005773638567443119; Training Loss 0.00462156936578165\n",
      "Episode 18085; Testing Loss 0.005773514370141668; Training Loss 0.004621561422231649\n",
      "Episode 18086; Testing Loss 0.005773488579639993; Training Loss 0.004621551258498273\n",
      "Episode 18087; Testing Loss 0.005773598471412237; Training Loss 0.004621545434661961\n",
      "Episode 18088; Testing Loss 0.005773686887486921; Training Loss 0.004621537490267584\n",
      "Episode 18089; Testing Loss 0.0057735698024622955; Training Loss 0.004621527455227038\n",
      "Episode 18090; Testing Loss 0.005773461302807586; Training Loss 0.0046215203496621105\n",
      "Episode 18091; Testing Loss 0.0057734557674357905; Training Loss 0.004621513196618919\n",
      "Episode 18092; Testing Loss 0.005773602891806096; Training Loss 0.004621502806525708\n",
      "Episode 18093; Testing Loss 0.005773617804388364; Training Loss 0.004621497920041775\n",
      "Episode 18094; Testing Loss 0.0057735821214195005; Training Loss 0.0046214883711827295\n",
      "Episode 18095; Testing Loss 0.005773589218915503; Training Loss 0.004621479942782309\n",
      "Episode 18096; Testing Loss 0.005773501455894814; Training Loss 0.004621473369672254\n",
      "Episode 18097; Testing Loss 0.005773512789452026; Training Loss 0.004621465763453283\n",
      "Episode 18098; Testing Loss 0.005773618982254086; Training Loss 0.004621455380343898\n",
      "Episode 18099; Testing Loss 0.005773574893256857; Training Loss 0.004621448278868203\n",
      "Episode 18100; Testing Loss 0.005773505659105023; Training Loss 0.004621443402654869\n",
      "Episode 18101; Testing Loss 0.00577351506024101; Training Loss 0.004621432466240012\n",
      "Episode 18102; Testing Loss 0.0057735434690851795; Training Loss 0.004621423138103722\n",
      "Episode 18103; Testing Loss 0.005773532845659929; Training Loss 0.004621416196759631\n",
      "Episode 18104; Testing Loss 0.005773389767056297; Training Loss 0.004621409190133648\n",
      "Episode 18105; Testing Loss 0.0057733998963152375; Training Loss 0.004621399386982282\n",
      "Episode 18106; Testing Loss 0.005773450955013419; Training Loss 0.004621392444106115\n",
      "Episode 18107; Testing Loss 0.005773519827706569; Training Loss 0.0046213856489663524\n",
      "Episode 18108; Testing Loss 0.005773453960321773; Training Loss 0.0046213762902964795\n",
      "Episode 18109; Testing Loss 0.005773417811441401; Training Loss 0.004621367287864501\n",
      "Episode 18110; Testing Loss 0.005773481811666386; Training Loss 0.004621359495047963\n",
      "Episode 18111; Testing Loss 0.005773538325931995; Training Loss 0.004621351810539025\n",
      "Episode 18112; Testing Loss 0.005773492909306334; Training Loss 0.0046213433366317875\n",
      "Episode 18113; Testing Loss 0.005773402777289527; Training Loss 0.004621336179604735\n",
      "Episode 18114; Testing Loss 0.005773397737341484; Training Loss 0.004621328427479222\n",
      "Episode 18115; Testing Loss 0.005773441637109893; Training Loss 0.0046213198435234365\n",
      "Episode 18116; Testing Loss 0.005773441633049836; Training Loss 0.004621310399119794\n",
      "Episode 18117; Testing Loss 0.005773424086865101; Training Loss 0.004621303402366725\n",
      "Episode 18118; Testing Loss 0.005773401010578795; Training Loss 0.0046212964643782715\n",
      "Episode 18119; Testing Loss 0.005773475224600613; Training Loss 0.004621286165520131\n",
      "Episode 18120; Testing Loss 0.005773553311641211; Training Loss 0.004621278607618256\n",
      "Episode 18121; Testing Loss 0.005773429039674969; Training Loss 0.004621270349063454\n",
      "Episode 18122; Testing Loss 0.005773332492005697; Training Loss 0.004621262475960161\n",
      "Episode 18123; Testing Loss 0.005773397493318587; Training Loss 0.004621253786962876\n",
      "Episode 18124; Testing Loss 0.005773509624222509; Training Loss 0.004621246615763402\n",
      "Episode 18125; Testing Loss 0.00577341968028422; Training Loss 0.004621238174208422\n",
      "Episode 18126; Testing Loss 0.005773396229154298; Training Loss 0.004621230559572688\n",
      "Episode 18127; Testing Loss 0.005773507851755524; Training Loss 0.004621221725523572\n",
      "Episode 18128; Testing Loss 0.005773491347773536; Training Loss 0.004621214791623308\n",
      "Episode 18129; Testing Loss 0.005773323052996062; Training Loss 0.004621207055963959\n",
      "Episode 18130; Testing Loss 0.005773318157396199; Training Loss 0.004621199798529955\n",
      "Episode 18131; Testing Loss 0.005773541686040643; Training Loss 0.004621191686594448\n",
      "Episode 18132; Testing Loss 0.005773435338036509; Training Loss 0.0046211826502246325\n",
      "Episode 18133; Testing Loss 0.005773261614561723; Training Loss 0.0046211748254580765\n",
      "Episode 18134; Testing Loss 0.005773291904772004; Training Loss 0.004621167201343708\n",
      "Episode 18135; Testing Loss 0.0057733871062781465; Training Loss 0.004621158367034918\n",
      "Episode 18136; Testing Loss 0.00577337201202884; Training Loss 0.004621150359787596\n",
      "Episode 18137; Testing Loss 0.005773371042768865; Training Loss 0.004621142637514582\n",
      "Episode 18138; Testing Loss 0.005773258407363633; Training Loss 0.004621133696862169\n",
      "Episode 18139; Testing Loss 0.005773223529062355; Training Loss 0.004621126227930548\n",
      "Episode 18140; Testing Loss 0.0057732083313121185; Training Loss 0.004621118863559006\n",
      "Episode 18141; Testing Loss 0.005773291529886082; Training Loss 0.004621111075002054\n",
      "Episode 18142; Testing Loss 0.005773317952125732; Training Loss 0.0046211031788880785\n",
      "Episode 18143; Testing Loss 0.005773217414157811; Training Loss 0.004621094892082596\n",
      "Episode 18144; Testing Loss 0.005773169550960238; Training Loss 0.004621086035090524\n",
      "Episode 18145; Testing Loss 0.005773330694857204; Training Loss 0.004621077989700133\n",
      "Episode 18146; Testing Loss 0.005773339977750951; Training Loss 0.004621070149864882\n",
      "Episode 18147; Testing Loss 0.005773296431487879; Training Loss 0.004621061630795977\n",
      "Episode 18148; Testing Loss 0.005773241312485998; Training Loss 0.004621053526274842\n",
      "Episode 18149; Testing Loss 0.00577321906109742; Training Loss 0.004621045696721741\n",
      "Episode 18150; Testing Loss 0.005773284984375533; Training Loss 0.00462103766719313\n",
      "Episode 18151; Testing Loss 0.005773288270226819; Training Loss 0.004621029810394255\n",
      "Episode 18152; Testing Loss 0.005773189176046256; Training Loss 0.004621023736422338\n",
      "Episode 18153; Testing Loss 0.005773273628738673; Training Loss 0.004621014565049859\n",
      "Episode 18154; Testing Loss 0.005773280347521025; Training Loss 0.0046210065053974925\n",
      "Episode 18155; Testing Loss 0.005773152849211091; Training Loss 0.0046209984668515525\n",
      "Episode 18156; Testing Loss 0.005773158213617135; Training Loss 0.004620990699846218\n",
      "Episode 18157; Testing Loss 0.005773172978197019; Training Loss 0.0046209829586613994\n",
      "Episode 18158; Testing Loss 0.005773157485241257; Training Loss 0.004620974674568167\n",
      "Episode 18159; Testing Loss 0.005773185455250404; Training Loss 0.0046209663361834775\n",
      "Episode 18160; Testing Loss 0.005773237949715626; Training Loss 0.004620958700076253\n",
      "Episode 18161; Testing Loss 0.0057731814458495435; Training Loss 0.0046209508471974385\n",
      "Episode 18162; Testing Loss 0.0057731406148680406; Training Loss 0.0046209422632195706\n",
      "Episode 18163; Testing Loss 0.005773179725266669; Training Loss 0.0046209347008753935\n",
      "Episode 18164; Testing Loss 0.0057730941230884295; Training Loss 0.004620927889717133\n",
      "Episode 18165; Testing Loss 0.005772944199729529; Training Loss 0.004620919565766475\n",
      "Episode 18166; Testing Loss 0.005773001691670118; Training Loss 0.004620913180207578\n",
      "Episode 18167; Testing Loss 0.005773259764029245; Training Loss 0.004620903571733794\n",
      "Episode 18168; Testing Loss 0.005773206123218983; Training Loss 0.004620895697455102\n",
      "Episode 18169; Testing Loss 0.005772996639747657; Training Loss 0.004620889945648503\n",
      "Episode 18170; Testing Loss 0.005773079162624306; Training Loss 0.0046208802416940345\n",
      "Episode 18171; Testing Loss 0.005773365095580769; Training Loss 0.004620872363130674\n",
      "Episode 18172; Testing Loss 0.0057732788351700995; Training Loss 0.0046208636635897745\n",
      "Episode 18173; Testing Loss 0.005772878331509753; Training Loss 0.004620858026020051\n",
      "Episode 18174; Testing Loss 0.005772932144555587; Training Loss 0.004620848487955196\n",
      "Episode 18175; Testing Loss 0.005773210469111509; Training Loss 0.004620843022220139\n",
      "Episode 18176; Testing Loss 0.005773157713267009; Training Loss 0.0046208314268909685\n",
      "Episode 18177; Testing Loss 0.005772919382713356; Training Loss 0.00462082696493725\n",
      "Episode 18178; Testing Loss 0.005773056072548545; Training Loss 0.0046208154020940554\n",
      "Episode 18179; Testing Loss 0.005773291912662242; Training Loss 0.004620809858083954\n",
      "Episode 18180; Testing Loss 0.005773116300187739; Training Loss 0.004620801853523461\n",
      "Episode 18181; Testing Loss 0.005772892988018093; Training Loss 0.004620795037203206\n",
      "Episode 18182; Testing Loss 0.005772955440203258; Training Loss 0.004620782338378202\n",
      "Episode 18183; Testing Loss 0.005773160819174244; Training Loss 0.004620776473576134\n",
      "Episode 18184; Testing Loss 0.005773168372414337; Training Loss 0.004620769526414979\n",
      "Episode 18185; Testing Loss 0.005772997731420172; Training Loss 0.00462076195660112\n",
      "Episode 18186; Testing Loss 0.005772979181293085; Training Loss 0.004620752158772539\n",
      "Episode 18187; Testing Loss 0.005773022366610468; Training Loss 0.0046207431484167425\n",
      "Episode 18188; Testing Loss 0.005772938870093622; Training Loss 0.004620734842361144\n",
      "Episode 18189; Testing Loss 0.005772816179791487; Training Loss 0.004620726634898827\n",
      "Episode 18190; Testing Loss 0.005772898832790925; Training Loss 0.0046207181410163585\n",
      "Episode 18191; Testing Loss 0.005773047817356893; Training Loss 0.004620710643276092\n",
      "Episode 18192; Testing Loss 0.0057729750908184635; Training Loss 0.004620702774433645\n",
      "Episode 18193; Testing Loss 0.005772976569364461; Training Loss 0.00462069470664985\n",
      "Episode 18194; Testing Loss 0.005773046043425391; Training Loss 0.004620687335762152\n",
      "Episode 18195; Testing Loss 0.005772939110056164; Training Loss 0.0046206778851879396\n",
      "Episode 18196; Testing Loss 0.00577280953846829; Training Loss 0.004620670858302342\n",
      "Episode 18197; Testing Loss 0.005772891209650448; Training Loss 0.004620662179878731\n",
      "Episode 18198; Testing Loss 0.005773007426737157; Training Loss 0.004620655598586098\n",
      "Episode 18199; Testing Loss 0.005772884706240949; Training Loss 0.00462064574107291\n",
      "Episode 18200; Testing Loss 0.0057727784454786974; Training Loss 0.004620638364107729\n",
      "Episode 18201; Testing Loss 0.005772853050385454; Training Loss 0.004620630651489932\n",
      "Episode 18202; Testing Loss 0.0057729183656188374; Training Loss 0.0046206221821341095\n",
      "Episode 18203; Testing Loss 0.005772848329516312; Training Loss 0.004620613451776092\n",
      "Episode 18204; Testing Loss 0.00577283299841657; Training Loss 0.004620605852413389\n",
      "Episode 18205; Testing Loss 0.005772892148328655; Training Loss 0.004620597296569875\n",
      "Episode 18206; Testing Loss 0.005772916739827262; Training Loss 0.0046205884183427685\n",
      "Episode 18207; Testing Loss 0.005772888962686388; Training Loss 0.004620582651577627\n",
      "Episode 18208; Testing Loss 0.005772818396400536; Training Loss 0.004620574878563963\n",
      "Episode 18209; Testing Loss 0.00577281593292752; Training Loss 0.004620564465986043\n",
      "Episode 18210; Testing Loss 0.005772938261136826; Training Loss 0.00462055695666106\n",
      "Episode 18211; Testing Loss 0.005772862022086179; Training Loss 0.004620548856433549\n",
      "Episode 18212; Testing Loss 0.005772752889572775; Training Loss 0.004620541186754296\n",
      "Episode 18213; Testing Loss 0.0057727413569289045; Training Loss 0.004620532822279841\n",
      "Episode 18214; Testing Loss 0.0057728349173660105; Training Loss 0.0046205244890612784\n",
      "Episode 18215; Testing Loss 0.005772898882265359; Training Loss 0.00462051683767728\n",
      "Episode 18216; Testing Loss 0.005772844482933518; Training Loss 0.004620508459403084\n",
      "Episode 18217; Testing Loss 0.005772838193453048; Training Loss 0.004620501207571399\n",
      "Episode 18218; Testing Loss 0.005772753794239828; Training Loss 0.004620493336250423\n",
      "Episode 18219; Testing Loss 0.005772700635321363; Training Loss 0.004620484540486163\n",
      "Episode 18220; Testing Loss 0.005772652913558881; Training Loss 0.004620477007001844\n",
      "Episode 18221; Testing Loss 0.005772714548568582; Training Loss 0.004620469854078437\n",
      "Episode 18222; Testing Loss 0.005772751357843825; Training Loss 0.004620460165943875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18223; Testing Loss 0.0057726399323096995; Training Loss 0.004620452003105916\n",
      "Episode 18224; Testing Loss 0.0057726272298295285; Training Loss 0.004620443791656339\n",
      "Episode 18225; Testing Loss 0.005772767788118003; Training Loss 0.004620435751286611\n",
      "Episode 18226; Testing Loss 0.005772780026379118; Training Loss 0.004620428031078481\n",
      "Episode 18227; Testing Loss 0.005772566891497164; Training Loss 0.004620419856214314\n",
      "Episode 18228; Testing Loss 0.00577257545209395; Training Loss 0.004620411734376646\n",
      "Episode 18229; Testing Loss 0.005772717352407895; Training Loss 0.004620403829951806\n",
      "Episode 18230; Testing Loss 0.0057726861049061055; Training Loss 0.004620395649516672\n",
      "Episode 18231; Testing Loss 0.005772615889350378; Training Loss 0.004620387744437603\n",
      "Episode 18232; Testing Loss 0.005772621679331493; Training Loss 0.004620378543368469\n",
      "Episode 18233; Testing Loss 0.005772651267857329; Training Loss 0.004620371226297316\n",
      "Episode 18234; Testing Loss 0.005772685703746015; Training Loss 0.00462036331547185\n",
      "Episode 18235; Testing Loss 0.005772695488366974; Training Loss 0.004620354960566827\n",
      "Episode 18236; Testing Loss 0.005772611748373979; Training Loss 0.004620347312736592\n",
      "Episode 18237; Testing Loss 0.005772666551975879; Training Loss 0.0046203386420998705\n",
      "Episode 18238; Testing Loss 0.005772686298000516; Training Loss 0.004620331623658264\n",
      "Episode 18239; Testing Loss 0.005772601571486117; Training Loss 0.004620322455762832\n",
      "Episode 18240; Testing Loss 0.005772539224365143; Training Loss 0.004620318124022951\n",
      "Episode 18241; Testing Loss 0.005772651015889277; Training Loss 0.00462030798855846\n",
      "Episode 18242; Testing Loss 0.005772626780348143; Training Loss 0.004620299866608441\n",
      "Episode 18243; Testing Loss 0.005772467797046781; Training Loss 0.004620291320943829\n",
      "Episode 18244; Testing Loss 0.00577245905580925; Training Loss 0.004620283055333034\n",
      "Episode 18245; Testing Loss 0.0057727080022537324; Training Loss 0.004620275491188489\n",
      "Episode 18246; Testing Loss 0.005772683770247261; Training Loss 0.00462026721555743\n",
      "Episode 18247; Testing Loss 0.005772449027827773; Training Loss 0.004620260940255791\n",
      "Episode 18248; Testing Loss 0.005772491047106358; Training Loss 0.00462025213021256\n",
      "Episode 18249; Testing Loss 0.00577268557978344; Training Loss 0.004620245598537954\n",
      "Episode 18250; Testing Loss 0.005772552170845556; Training Loss 0.004620236570322108\n",
      "Episode 18251; Testing Loss 0.005772317219360272; Training Loss 0.0046202282984980775\n",
      "Episode 18252; Testing Loss 0.005772365659918343; Training Loss 0.004620220756981287\n",
      "Episode 18253; Testing Loss 0.005772649922642344; Training Loss 0.004620211461388523\n",
      "Episode 18254; Testing Loss 0.0057726780626725345; Training Loss 0.004620206784632257\n",
      "Episode 18255; Testing Loss 0.005772353131416259; Training Loss 0.004620197875131831\n",
      "Episode 18256; Testing Loss 0.005772295051826656; Training Loss 0.004620188783590921\n",
      "Episode 18257; Testing Loss 0.005772558526994856; Training Loss 0.004620180304532876\n",
      "Episode 18258; Testing Loss 0.005772532638706629; Training Loss 0.004620172794091456\n",
      "Episode 18259; Testing Loss 0.005772313368392728; Training Loss 0.004620166311555122\n",
      "Episode 18260; Testing Loss 0.005772333639539917; Training Loss 0.00462015601149747\n",
      "Episode 18261; Testing Loss 0.005772499966408103; Training Loss 0.004620149608548355\n",
      "Episode 18262; Testing Loss 0.005772448556198912; Training Loss 0.004620140938324979\n",
      "Episode 18263; Testing Loss 0.005772309121361291; Training Loss 0.004620131581241305\n",
      "Episode 18264; Testing Loss 0.005772302333505679; Training Loss 0.004620124142152372\n",
      "Episode 18265; Testing Loss 0.005772447506130962; Training Loss 0.0046201163856672825\n",
      "Episode 18266; Testing Loss 0.005772494671309478; Training Loss 0.004620107571899202\n",
      "Episode 18267; Testing Loss 0.005772349999703928; Training Loss 0.004620098781912678\n",
      "Episode 18268; Testing Loss 0.005772299454773991; Training Loss 0.004620092659546101\n",
      "Episode 18269; Testing Loss 0.005772422059773755; Training Loss 0.00462008284636253\n",
      "Episode 18270; Testing Loss 0.005772448480412969; Training Loss 0.00462007522986354\n",
      "Episode 18271; Testing Loss 0.005772364194986775; Training Loss 0.0046200679964898995\n",
      "Episode 18272; Testing Loss 0.005772313657824085; Training Loss 0.004620059680809054\n",
      "Episode 18273; Testing Loss 0.005772380337948688; Training Loss 0.004620050776137537\n",
      "Episode 18274; Testing Loss 0.005772435441431712; Training Loss 0.0046200427770519735\n",
      "Episode 18275; Testing Loss 0.005772255979978556; Training Loss 0.0046200340430543625\n",
      "Episode 18276; Testing Loss 0.005772194441993031; Training Loss 0.004620027724447443\n",
      "Episode 18277; Testing Loss 0.005772376527220347; Training Loss 0.0046200196044385\n",
      "Episode 18278; Testing Loss 0.00577238239098545; Training Loss 0.004620011270008766\n",
      "Episode 18279; Testing Loss 0.005772213462721987; Training Loss 0.004620002182841899\n",
      "Episode 18280; Testing Loss 0.005772256760704515; Training Loss 0.0046199956713529906\n",
      "Episode 18281; Testing Loss 0.005772385653183102; Training Loss 0.004619987750496271\n",
      "Episode 18282; Testing Loss 0.0057723429109142205; Training Loss 0.004619977796796025\n",
      "Episode 18283; Testing Loss 0.005772265031761369; Training Loss 0.004619970102157785\n",
      "Episode 18284; Testing Loss 0.005772239782557969; Training Loss 0.004619961506209223\n",
      "Episode 18285; Testing Loss 0.005772274901403113; Training Loss 0.004619954473649608\n",
      "Episode 18286; Testing Loss 0.005772299782659353; Training Loss 0.004619946443773015\n",
      "Episode 18287; Testing Loss 0.005772199156202244; Training Loss 0.004619938801921635\n",
      "Episode 18288; Testing Loss 0.005772233178043997; Training Loss 0.00461992991421233\n",
      "Episode 18289; Testing Loss 0.005772196169298219; Training Loss 0.004619921779401934\n",
      "Episode 18290; Testing Loss 0.005772202326008362; Training Loss 0.004619913654298081\n",
      "Episode 18291; Testing Loss 0.005772258051871907; Training Loss 0.0046199057131989804\n",
      "Episode 18292; Testing Loss 0.005772201901840553; Training Loss 0.004619897195847419\n",
      "Episode 18293; Testing Loss 0.005772229662911763; Training Loss 0.004619889801380765\n",
      "Episode 18294; Testing Loss 0.005772211688239562; Training Loss 0.0046198824761529336\n",
      "Episode 18295; Testing Loss 0.005772240356550629; Training Loss 0.004619874163451791\n",
      "Episode 18296; Testing Loss 0.005772193046202149; Training Loss 0.00461986614199968\n",
      "Episode 18297; Testing Loss 0.005772137257823291; Training Loss 0.004619857987762789\n",
      "Episode 18298; Testing Loss 0.005772210657207248; Training Loss 0.004619849306129302\n",
      "Episode 18299; Testing Loss 0.005772210473436824; Training Loss 0.004619841219949591\n",
      "Episode 18300; Testing Loss 0.0057721503386236744; Training Loss 0.0046198350659548155\n",
      "Episode 18301; Testing Loss 0.005772093311561434; Training Loss 0.004619826515509379\n",
      "Episode 18302; Testing Loss 0.005772122706149669; Training Loss 0.004619817730151424\n",
      "Episode 18303; Testing Loss 0.00577219749574698; Training Loss 0.004619811013088892\n",
      "Episode 18304; Testing Loss 0.005772067319426464; Training Loss 0.004619801760411366\n",
      "Episode 18305; Testing Loss 0.005771957292957617; Training Loss 0.004619796552202403\n",
      "Episode 18306; Testing Loss 0.00577217325076369; Training Loss 0.004619786283227361\n",
      "Episode 18307; Testing Loss 0.005772156208215872; Training Loss 0.004619778013835911\n",
      "Episode 18308; Testing Loss 0.0057719499360351385; Training Loss 0.004619772071924703\n",
      "Episode 18309; Testing Loss 0.005772015583127585; Training Loss 0.004619761136048759\n",
      "Episode 18310; Testing Loss 0.005772115506904353; Training Loss 0.004619754655555348\n",
      "Episode 18311; Testing Loss 0.005771996238068156; Training Loss 0.004619746813353757\n",
      "Episode 18312; Testing Loss 0.005771953930449329; Training Loss 0.0046197394591800086\n",
      "Episode 18313; Testing Loss 0.0057720168717314575; Training Loss 0.004619731112769312\n",
      "Episode 18314; Testing Loss 0.005772065105440002; Training Loss 0.004619722458951095\n",
      "Episode 18315; Testing Loss 0.005771978511751364; Training Loss 0.004619714802642625\n",
      "Episode 18316; Testing Loss 0.005771884130925491; Training Loss 0.0046197069668894464\n",
      "Episode 18317; Testing Loss 0.005771991310739976; Training Loss 0.004619698359522985\n",
      "Episode 18318; Testing Loss 0.005772064329009977; Training Loss 0.004619690959599579\n",
      "Episode 18319; Testing Loss 0.005771995327863253; Training Loss 0.004619681931352825\n",
      "Episode 18320; Testing Loss 0.005771988103817678; Training Loss 0.004619674132262381\n",
      "Episode 18321; Testing Loss 0.005772021385732918; Training Loss 0.00461966559896551\n",
      "Episode 18322; Testing Loss 0.005772045830133835; Training Loss 0.004619657846780767\n",
      "Episode 18323; Testing Loss 0.005771957433157301; Training Loss 0.004619650605215373\n",
      "Episode 18324; Testing Loss 0.005771989761521191; Training Loss 0.004619642440867956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18325; Testing Loss 0.0057720385398469356; Training Loss 0.004619633967977545\n",
      "Episode 18326; Testing Loss 0.005771960639248722; Training Loss 0.00461962604526795\n",
      "Episode 18327; Testing Loss 0.005771866145088586; Training Loss 0.004619617743270873\n",
      "Episode 18328; Testing Loss 0.005771927068014846; Training Loss 0.004619610396344631\n",
      "Episode 18329; Testing Loss 0.005772021251795923; Training Loss 0.004619603299496085\n",
      "Episode 18330; Testing Loss 0.005771982070226953; Training Loss 0.004619594286529304\n",
      "Episode 18331; Testing Loss 0.00577189980258605; Training Loss 0.004619585643965157\n",
      "Episode 18332; Testing Loss 0.005771917934602357; Training Loss 0.004619579344342755\n",
      "Episode 18333; Testing Loss 0.005771962136301006; Training Loss 0.004619570765278273\n",
      "Episode 18334; Testing Loss 0.005771933773468243; Training Loss 0.004619561542745722\n",
      "Episode 18335; Testing Loss 0.005771850925260681; Training Loss 0.0046195553853977875\n",
      "Episode 18336; Testing Loss 0.005771907259849944; Training Loss 0.004619545977861337\n",
      "Episode 18337; Testing Loss 0.005771872753278603; Training Loss 0.0046195379501582645\n",
      "Episode 18338; Testing Loss 0.0057718000109174316; Training Loss 0.004619530460907537\n",
      "Episode 18339; Testing Loss 0.005771925446376528; Training Loss 0.004619521541785269\n",
      "Episode 18340; Testing Loss 0.005772002496642776; Training Loss 0.0046195151588680405\n",
      "Episode 18341; Testing Loss 0.005771795594403543; Training Loss 0.004619505813617813\n",
      "Episode 18342; Testing Loss 0.005771751959096818; Training Loss 0.004619497754662552\n",
      "Episode 18343; Testing Loss 0.0057719235775916325; Training Loss 0.004619490716806355\n",
      "Episode 18344; Testing Loss 0.005771957899109731; Training Loss 0.004619482578117078\n",
      "Episode 18345; Testing Loss 0.005771796912586919; Training Loss 0.004619474686148386\n",
      "Episode 18346; Testing Loss 0.005771824023359354; Training Loss 0.004619464984702034\n",
      "Episode 18347; Testing Loss 0.005771840965030785; Training Loss 0.004619458648958604\n",
      "Episode 18348; Testing Loss 0.0057717489861314865; Training Loss 0.004619450686938731\n",
      "Episode 18349; Testing Loss 0.005771702554482775; Training Loss 0.004619442480704235\n",
      "Episode 18350; Testing Loss 0.005771883266976213; Training Loss 0.00461943369770865\n",
      "Episode 18351; Testing Loss 0.005771926169758229; Training Loss 0.004619425708646209\n",
      "Episode 18352; Testing Loss 0.005771803933411938; Training Loss 0.004619418279446973\n",
      "Episode 18353; Testing Loss 0.005771705144599989; Training Loss 0.004619410496591589\n",
      "Episode 18354; Testing Loss 0.005771834459505448; Training Loss 0.004619402363440666\n",
      "Episode 18355; Testing Loss 0.005771822789114033; Training Loss 0.0046193940043609836\n",
      "Episode 18356; Testing Loss 0.005771742416843109; Training Loss 0.004619386099410914\n",
      "Episode 18357; Testing Loss 0.00577171961116346; Training Loss 0.004619377886762282\n",
      "Episode 18358; Testing Loss 0.005771734220174916; Training Loss 0.004619370807298602\n",
      "Episode 18359; Testing Loss 0.005771715167392561; Training Loss 0.004619362500462505\n",
      "Episode 18360; Testing Loss 0.0057716583116280236; Training Loss 0.004619355226131989\n",
      "Episode 18361; Testing Loss 0.005771779817201516; Training Loss 0.004619345751573005\n",
      "Episode 18362; Testing Loss 0.005771718624480043; Training Loss 0.004619337561105519\n",
      "Episode 18363; Testing Loss 0.005771600758851706; Training Loss 0.004619331501551148\n",
      "Episode 18364; Testing Loss 0.005771727212504661; Training Loss 0.0046193227784343555\n",
      "Episode 18365; Testing Loss 0.005771689125922366; Training Loss 0.00461931391289006\n",
      "Episode 18366; Testing Loss 0.0057715663365616; Training Loss 0.0046193072446449835\n",
      "Episode 18367; Testing Loss 0.005771656560906924; Training Loss 0.00461929927988139\n",
      "Episode 18368; Testing Loss 0.0057717795459629705; Training Loss 0.004619291745249747\n",
      "Episode 18369; Testing Loss 0.005771608245779268; Training Loss 0.004619282756298438\n",
      "Episode 18370; Testing Loss 0.0057714855965267054; Training Loss 0.0046192749025639945\n",
      "Episode 18371; Testing Loss 0.005771594470596548; Training Loss 0.004619266827220217\n",
      "Episode 18372; Testing Loss 0.005771655483973199; Training Loss 0.004619259608607754\n",
      "Episode 18373; Testing Loss 0.005771481581788116; Training Loss 0.004619252291506838\n",
      "Episode 18374; Testing Loss 0.005771414539386394; Training Loss 0.004619244674022861\n",
      "Episode 18375; Testing Loss 0.005771579892685928; Training Loss 0.004619235105496852\n",
      "Episode 18376; Testing Loss 0.005771635628677249; Training Loss 0.004619227698796906\n",
      "Episode 18377; Testing Loss 0.005771490932916701; Training Loss 0.004619220312283865\n",
      "Episode 18378; Testing Loss 0.005771495106941075; Training Loss 0.004619211895932554\n",
      "Episode 18379; Testing Loss 0.005771719204527735; Training Loss 0.0046192036291745195\n",
      "Episode 18380; Testing Loss 0.0057717027071661165; Training Loss 0.004619195223199079\n",
      "Episode 18381; Testing Loss 0.005771413101587002; Training Loss 0.004619187937649605\n",
      "Episode 18382; Testing Loss 0.005771380205939989; Training Loss 0.004619180258867336\n",
      "Episode 18383; Testing Loss 0.00577163470887316; Training Loss 0.004619172426604541\n",
      "Episode 18384; Testing Loss 0.005771636354473116; Training Loss 0.004619163853779162\n",
      "Episode 18385; Testing Loss 0.005771412511043221; Training Loss 0.004619155308167085\n",
      "Episode 18386; Testing Loss 0.005771324229318682; Training Loss 0.004619147898968174\n",
      "Episode 18387; Testing Loss 0.005771536924378438; Training Loss 0.004619138582334118\n",
      "Episode 18388; Testing Loss 0.005771650223122228; Training Loss 0.004619132447140764\n",
      "Episode 18389; Testing Loss 0.0057714697536844; Training Loss 0.0046191226822696295\n",
      "Episode 18390; Testing Loss 0.005771349341232282; Training Loss 0.004619114928161772\n",
      "Episode 18391; Testing Loss 0.005771378988713144; Training Loss 0.004619105880447209\n",
      "Episode 18392; Testing Loss 0.005771479870905908; Training Loss 0.0046190988716106555\n",
      "Episode 18393; Testing Loss 0.005771553236879891; Training Loss 0.004619090906679448\n",
      "Episode 18394; Testing Loss 0.005771401871805515; Training Loss 0.004619081995934649\n",
      "Episode 18395; Testing Loss 0.005771236654234223; Training Loss 0.0046190753505704115\n",
      "Episode 18396; Testing Loss 0.005771378022450511; Training Loss 0.004619066768466806\n",
      "Episode 18397; Testing Loss 0.005771481720668064; Training Loss 0.0046190591018117075\n",
      "Episode 18398; Testing Loss 0.005771334902671111; Training Loss 0.004619052182752711\n",
      "Episode 18399; Testing Loss 0.0057713499034017434; Training Loss 0.004619042251810584\n",
      "Episode 18400; Testing Loss 0.005771416732270649; Training Loss 0.004619034826576741\n",
      "Episode 18401; Testing Loss 0.005771426356441581; Training Loss 0.004619026970476927\n",
      "Episode 18402; Testing Loss 0.005771320725455293; Training Loss 0.004619019484551383\n",
      "Episode 18403; Testing Loss 0.00577136278968438; Training Loss 0.00461901110427613\n",
      "Episode 18404; Testing Loss 0.005771365022629727; Training Loss 0.004619003104896951\n",
      "Episode 18405; Testing Loss 0.0057712901505637204; Training Loss 0.004618994900097295\n",
      "Episode 18406; Testing Loss 0.005771243384381812; Training Loss 0.004618988564936184\n",
      "Episode 18407; Testing Loss 0.005771266739792537; Training Loss 0.004618979624080682\n",
      "Episode 18408; Testing Loss 0.005771296717582611; Training Loss 0.004618970154735608\n",
      "Episode 18409; Testing Loss 0.0057713546445996525; Training Loss 0.0046189641115576465\n",
      "Episode 18410; Testing Loss 0.005771407487499849; Training Loss 0.004618956464161578\n",
      "Episode 18411; Testing Loss 0.005771357297080668; Training Loss 0.004618946478601447\n",
      "Episode 18412; Testing Loss 0.005771279407562878; Training Loss 0.004618940044544802\n",
      "Episode 18413; Testing Loss 0.005771195363298596; Training Loss 0.004618932906525069\n",
      "Episode 18414; Testing Loss 0.005771230477518571; Training Loss 0.004618923937175789\n",
      "Episode 18415; Testing Loss 0.005771359136908723; Training Loss 0.004618916262332419\n",
      "Episode 18416; Testing Loss 0.005771363978487847; Training Loss 0.004618907553187174\n",
      "Episode 18417; Testing Loss 0.005771230318722646; Training Loss 0.004618899550261016\n",
      "Episode 18418; Testing Loss 0.0057711743017160085; Training Loss 0.004618891188376721\n",
      "Episode 18419; Testing Loss 0.005771361647520045; Training Loss 0.004618883797737117\n",
      "Episode 18420; Testing Loss 0.005771307504553747; Training Loss 0.004618876025940506\n",
      "Episode 18421; Testing Loss 0.005771209254190838; Training Loss 0.004618866754305102\n",
      "Episode 18422; Testing Loss 0.0057711251238701244; Training Loss 0.0046188593870178036\n",
      "Episode 18423; Testing Loss 0.0057712063160488165; Training Loss 0.004618850659197272\n",
      "Episode 18424; Testing Loss 0.005771284142448389; Training Loss 0.004618843291103703\n",
      "Episode 18425; Testing Loss 0.005771173595270401; Training Loss 0.004618835115848962\n",
      "Episode 18426; Testing Loss 0.005771147434876178; Training Loss 0.00461882713119198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18427; Testing Loss 0.005771205826740615; Training Loss 0.004618818800204699\n",
      "Episode 18428; Testing Loss 0.005771252381846107; Training Loss 0.004618812597323489\n",
      "Episode 18429; Testing Loss 0.005771261430904902; Training Loss 0.00461880456271149\n",
      "Episode 18430; Testing Loss 0.00577106772417929; Training Loss 0.004618795657084828\n",
      "Episode 18431; Testing Loss 0.005771058395502687; Training Loss 0.004618787484546639\n",
      "Episode 18432; Testing Loss 0.005771214666271058; Training Loss 0.004618781628780482\n",
      "Episode 18433; Testing Loss 0.00577116084016364; Training Loss 0.004618772265480002\n",
      "Episode 18434; Testing Loss 0.00577098074760994; Training Loss 0.004618766609718499\n",
      "Episode 18435; Testing Loss 0.005771092476052247; Training Loss 0.004618755893277196\n",
      "Episode 18436; Testing Loss 0.005771206897704762; Training Loss 0.004618750882786858\n",
      "Episode 18437; Testing Loss 0.005771070039689046; Training Loss 0.004618742361362724\n",
      "Episode 18438; Testing Loss 0.005770935678172455; Training Loss 0.004618733830793043\n",
      "Episode 18439; Testing Loss 0.005771042794653552; Training Loss 0.004618726133697007\n",
      "Episode 18440; Testing Loss 0.005771179389114919; Training Loss 0.004618719536911544\n",
      "Episode 18441; Testing Loss 0.005771040448608735; Training Loss 0.004618708336949922\n",
      "Episode 18442; Testing Loss 0.005770855942945813; Training Loss 0.004618702099637151\n",
      "Episode 18443; Testing Loss 0.005770899848189451; Training Loss 0.004618695366084811\n",
      "Episode 18444; Testing Loss 0.005771093276251004; Training Loss 0.004618685805526549\n",
      "Episode 18445; Testing Loss 0.005771114275437428; Training Loss 0.004618678330741913\n",
      "Episode 18446; Testing Loss 0.005770987058703741; Training Loss 0.0046186707058144495\n",
      "Episode 18447; Testing Loss 0.005770894611224432; Training Loss 0.004618663161258798\n",
      "Episode 18448; Testing Loss 0.005770992088615925; Training Loss 0.004618654319866071\n",
      "Episode 18449; Testing Loss 0.005771041884150773; Training Loss 0.004618645897278182\n",
      "Episode 18450; Testing Loss 0.005770982949116455; Training Loss 0.0046186388980706304\n",
      "Episode 18451; Testing Loss 0.0057708281499421945; Training Loss 0.00461863048783422\n",
      "Episode 18452; Testing Loss 0.005770820406527333; Training Loss 0.004618622438175065\n",
      "Episode 18453; Testing Loss 0.005771021770909863; Training Loss 0.004618614842581332\n",
      "Episode 18454; Testing Loss 0.0057710880894832695; Training Loss 0.004618606590777314\n",
      "Episode 18455; Testing Loss 0.005770889840584494; Training Loss 0.004618599276223379\n",
      "Episode 18456; Testing Loss 0.005770895790433022; Training Loss 0.0046185904856395794\n",
      "Episode 18457; Testing Loss 0.005771068815480988; Training Loss 0.004618584241218382\n",
      "Episode 18458; Testing Loss 0.005771004141209961; Training Loss 0.004618575055133479\n",
      "Episode 18459; Testing Loss 0.0057707658814404586; Training Loss 0.004618567733431\n",
      "Episode 18460; Testing Loss 0.005770829444517515; Training Loss 0.004618561048025907\n",
      "Episode 18461; Testing Loss 0.005771089027273607; Training Loss 0.004618552916049286\n",
      "Episode 18462; Testing Loss 0.005770971513651848; Training Loss 0.004618543728580302\n",
      "Episode 18463; Testing Loss 0.005770661159996902; Training Loss 0.0046185365783309555\n",
      "Episode 18464; Testing Loss 0.005770709571771391; Training Loss 0.004618527218338912\n",
      "Episode 18465; Testing Loss 0.005770930708593521; Training Loss 0.004618520583868559\n",
      "Episode 18466; Testing Loss 0.005770979216822172; Training Loss 0.004618513739484504\n",
      "Episode 18467; Testing Loss 0.005770751145292724; Training Loss 0.004618504556241184\n",
      "Episode 18468; Testing Loss 0.005770682090877028; Training Loss 0.004618496630638856\n",
      "Episode 18469; Testing Loss 0.00577086196800418; Training Loss 0.0046184869670322275\n",
      "Episode 18470; Testing Loss 0.005770945052965334; Training Loss 0.004618479071190119\n",
      "Episode 18471; Testing Loss 0.00577081971494225; Training Loss 0.004618471975358697\n",
      "Episode 18472; Testing Loss 0.005770801475172613; Training Loss 0.0046184659957153925\n",
      "Episode 18473; Testing Loss 0.0057709227588745755; Training Loss 0.004618455731967316\n",
      "Episode 18474; Testing Loss 0.005770877823500342; Training Loss 0.004618449689031678\n",
      "Episode 18475; Testing Loss 0.0057706542237246075; Training Loss 0.004618442909383951\n",
      "Episode 18476; Testing Loss 0.005770669320654309; Training Loss 0.004618433968111867\n",
      "Episode 18477; Testing Loss 0.005770900607106423; Training Loss 0.0046184233376793575\n",
      "Episode 18478; Testing Loss 0.005770936549839711; Training Loss 0.0046184171634858365\n",
      "Episode 18479; Testing Loss 0.005770893564992362; Training Loss 0.004618409194094935\n",
      "Episode 18480; Testing Loss 0.005770880841693566; Training Loss 0.004618400999855187\n",
      "Episode 18481; Testing Loss 0.005770866291139329; Training Loss 0.004618392438537398\n",
      "Episode 18482; Testing Loss 0.0057707982566205344; Training Loss 0.004618383552291274\n",
      "Episode 18483; Testing Loss 0.005770716279409541; Training Loss 0.004618376605360314\n",
      "Episode 18484; Testing Loss 0.0057708206877126; Training Loss 0.004618368591235858\n",
      "Episode 18485; Testing Loss 0.005770892955925208; Training Loss 0.004618360513003379\n",
      "Episode 18486; Testing Loss 0.005770792059726004; Training Loss 0.004618351235571603\n",
      "Episode 18487; Testing Loss 0.005770717976452131; Training Loss 0.0046183458562422675\n",
      "Episode 18488; Testing Loss 0.0057707690154354604; Training Loss 0.004618337073919543\n",
      "Episode 18489; Testing Loss 0.005770799377822159; Training Loss 0.004618327786475022\n",
      "Episode 18490; Testing Loss 0.0057707650411358774; Training Loss 0.004618320883831414\n",
      "Episode 18491; Testing Loss 0.0057706866065404035; Training Loss 0.004618312419978209\n",
      "Episode 18492; Testing Loss 0.005770699706252522; Training Loss 0.004618304356534376\n",
      "Episode 18493; Testing Loss 0.005770712064940758; Training Loss 0.004618296144278554\n",
      "Episode 18494; Testing Loss 0.005770732247994308; Training Loss 0.004618288003930913\n",
      "Episode 18495; Testing Loss 0.005770784693384182; Training Loss 0.004618280507461547\n",
      "Episode 18496; Testing Loss 0.005770722345123411; Training Loss 0.004618272557788126\n",
      "Episode 18497; Testing Loss 0.005770671299244499; Training Loss 0.004618264293362504\n",
      "Episode 18498; Testing Loss 0.005770646423914059; Training Loss 0.004618256172990853\n",
      "Episode 18499; Testing Loss 0.005770635236790174; Training Loss 0.004618248316195534\n",
      "Episode 18500; Testing Loss 0.005770629933104855; Training Loss 0.004618241227697416\n",
      "Episode 18501; Testing Loss 0.0057705688849228635; Training Loss 0.004618232549550473\n",
      "Episode 18502; Testing Loss 0.005770599805048881; Training Loss 0.004618225924749833\n",
      "Episode 18503; Testing Loss 0.005770717486827341; Training Loss 0.004618218634112448\n",
      "Episode 18504; Testing Loss 0.005770733983475466; Training Loss 0.004618210518957846\n",
      "Episode 18505; Testing Loss 0.005770490937473275; Training Loss 0.004618202172235521\n",
      "Episode 18506; Testing Loss 0.005770397667457061; Training Loss 0.004618195076046654\n",
      "Episode 18507; Testing Loss 0.005770595723430933; Training Loss 0.004618186567864028\n",
      "Episode 18508; Testing Loss 0.005770604948802019; Training Loss 0.004618179064965984\n",
      "Episode 18509; Testing Loss 0.005770501280414883; Training Loss 0.004618170887262291\n",
      "Episode 18510; Testing Loss 0.00577052922567618; Training Loss 0.004618162124247808\n",
      "Episode 18511; Testing Loss 0.005770645685407587; Training Loss 0.0046181550836175474\n",
      "Episode 18512; Testing Loss 0.00577061715342821; Training Loss 0.004618146583498165\n",
      "Episode 18513; Testing Loss 0.005770505364585988; Training Loss 0.004618139714916618\n",
      "Episode 18514; Testing Loss 0.00577057846976834; Training Loss 0.004618130003523435\n",
      "Episode 18515; Testing Loss 0.005770595771821139; Training Loss 0.004618124171034771\n",
      "Episode 18516; Testing Loss 0.005770415675236933; Training Loss 0.004618116028006266\n",
      "Episode 18517; Testing Loss 0.0057703921433959835; Training Loss 0.004618107183435647\n",
      "Episode 18518; Testing Loss 0.00577056615245628; Training Loss 0.004618099548839186\n",
      "Episode 18519; Testing Loss 0.005770687527238948; Training Loss 0.004618093207483321\n",
      "Episode 18520; Testing Loss 0.005770576422650015; Training Loss 0.004618082946987347\n",
      "Episode 18521; Testing Loss 0.005770440083703314; Training Loss 0.004618077118015845\n",
      "Episode 18522; Testing Loss 0.005770427944098004; Training Loss 0.004618070562648138\n",
      "Episode 18523; Testing Loss 0.005770529751765355; Training Loss 0.004618060362857482\n",
      "Episode 18524; Testing Loss 0.0057706450234131755; Training Loss 0.0046180537506519065\n",
      "Episode 18525; Testing Loss 0.005770660578568097; Training Loss 0.004618047078875521\n",
      "Episode 18526; Testing Loss 0.005770583493265107; Training Loss 0.004618038359852957\n",
      "Episode 18527; Testing Loss 0.005770569481887591; Training Loss 0.004618027626256062\n",
      "Episode 18528; Testing Loss 0.005770545737553889; Training Loss 0.004618023615031545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18529; Testing Loss 0.00577049596781776; Training Loss 0.004618017348737641\n",
      "Episode 18530; Testing Loss 0.00577048106273806; Training Loss 0.004618008359178247\n",
      "Episode 18531; Testing Loss 0.005770464460116033; Training Loss 0.004617998420658209\n",
      "Episode 18532; Testing Loss 0.005770475845367887; Training Loss 0.0046179892280255\n",
      "Episode 18533; Testing Loss 0.005770576597224087; Training Loss 0.00461798292712562\n",
      "Episode 18534; Testing Loss 0.005770576571472346; Training Loss 0.004617975230440205\n",
      "Episode 18535; Testing Loss 0.005770517648044087; Training Loss 0.004617966593270195\n",
      "Episode 18536; Testing Loss 0.00577050556827883; Training Loss 0.004617957712232307\n",
      "Episode 18537; Testing Loss 0.00577049573003039; Training Loss 0.0046179497292881426\n",
      "Episode 18538; Testing Loss 0.005770455037878591; Training Loss 0.004617942181544384\n",
      "Episode 18539; Testing Loss 0.005770408164018044; Training Loss 0.004617933623035565\n",
      "Episode 18540; Testing Loss 0.005770423553522532; Training Loss 0.004617925563072012\n",
      "Episode 18541; Testing Loss 0.005770452295388196; Training Loss 0.004617917559926206\n",
      "Episode 18542; Testing Loss 0.005770489650375393; Training Loss 0.004617909012601817\n",
      "Episode 18543; Testing Loss 0.00577046900951191; Training Loss 0.0046179009701410685\n",
      "Episode 18544; Testing Loss 0.005770416688154626; Training Loss 0.004617894055218544\n",
      "Episode 18545; Testing Loss 0.00577037452327226; Training Loss 0.0046178865851443885\n",
      "Episode 18546; Testing Loss 0.0057704544984263076; Training Loss 0.004617877264983218\n",
      "Episode 18547; Testing Loss 0.005770491813876318; Training Loss 0.004617870797170737\n",
      "Episode 18548; Testing Loss 0.005770346295672259; Training Loss 0.004617862047186194\n",
      "Episode 18549; Testing Loss 0.005770249727026719; Training Loss 0.004617855869727035\n",
      "Episode 18550; Testing Loss 0.005770360468601298; Training Loss 0.004617847876843295\n",
      "Episode 18551; Testing Loss 0.005770464178743116; Training Loss 0.0046178403582224255\n",
      "Episode 18552; Testing Loss 0.005770334604880886; Training Loss 0.004617830681518621\n",
      "Episode 18553; Testing Loss 0.0057702288222331415; Training Loss 0.004617823692878505\n",
      "Episode 18554; Testing Loss 0.005770302718004311; Training Loss 0.004617816573895523\n",
      "Episode 18555; Testing Loss 0.005770362158880747; Training Loss 0.004617808538692701\n",
      "Episode 18556; Testing Loss 0.005770250476206938; Training Loss 0.004617798616798996\n",
      "Episode 18557; Testing Loss 0.005770171184428859; Training Loss 0.00461779334554035\n",
      "Episode 18558; Testing Loss 0.005770313636843518; Training Loss 0.004617783788465543\n",
      "Episode 18559; Testing Loss 0.005770411788791288; Training Loss 0.004617776535240368\n",
      "Episode 18560; Testing Loss 0.005770296970068899; Training Loss 0.0046177682503576204\n",
      "Episode 18561; Testing Loss 0.0057701711436026525; Training Loss 0.004617759091982283\n",
      "Episode 18562; Testing Loss 0.005770217152686444; Training Loss 0.004617752523401146\n",
      "Episode 18563; Testing Loss 0.005770211905888076; Training Loss 0.004617746104918495\n",
      "Episode 18564; Testing Loss 0.005770198075153835; Training Loss 0.0046177369267520404\n",
      "Episode 18565; Testing Loss 0.00577011318704885; Training Loss 0.0046177290469567615\n",
      "Episode 18566; Testing Loss 0.005770057509185116; Training Loss 0.004617721266111917\n",
      "Episode 18567; Testing Loss 0.00577012823642701; Training Loss 0.0046177118051252035\n",
      "Episode 18568; Testing Loss 0.0057702868469367745; Training Loss 0.004617703901934396\n",
      "Episode 18569; Testing Loss 0.0057702757468591276; Training Loss 0.004617695806613052\n",
      "Episode 18570; Testing Loss 0.005770159252413518; Training Loss 0.0046176886209638186\n",
      "Episode 18571; Testing Loss 0.005770099818174119; Training Loss 0.004617680955811728\n",
      "Episode 18572; Testing Loss 0.005770187181839852; Training Loss 0.004617672111346004\n",
      "Episode 18573; Testing Loss 0.005770335935654358; Training Loss 0.004617665047822429\n",
      "Episode 18574; Testing Loss 0.005770288186703474; Training Loss 0.0046176567458832704\n",
      "Episode 18575; Testing Loss 0.005770122596129806; Training Loss 0.00461764865237913\n",
      "Episode 18576; Testing Loss 0.005770094882315662; Training Loss 0.00461764215642135\n",
      "Episode 18577; Testing Loss 0.005770095591514895; Training Loss 0.004617633873063269\n",
      "Episode 18578; Testing Loss 0.005770158876502722; Training Loss 0.004617624629669187\n",
      "Episode 18579; Testing Loss 0.005770225058432761; Training Loss 0.004617616956637104\n",
      "Episode 18580; Testing Loss 0.005770188981357457; Training Loss 0.004617608809196153\n",
      "Episode 18581; Testing Loss 0.005770075476535665; Training Loss 0.004617601919075436\n",
      "Episode 18582; Testing Loss 0.005770121521043714; Training Loss 0.004617593675210979\n",
      "Episode 18583; Testing Loss 0.005770206222065811; Training Loss 0.004617585721427079\n",
      "Episode 18584; Testing Loss 0.005770079016693188; Training Loss 0.004617577644215727\n",
      "Episode 18585; Testing Loss 0.005770091930386599; Training Loss 0.00461757056929999\n",
      "Episode 18586; Testing Loss 0.005770205103802478; Training Loss 0.004617562950370976\n",
      "Episode 18587; Testing Loss 0.0057701692541246045; Training Loss 0.004617553405287567\n",
      "Episode 18588; Testing Loss 0.005770058948166846; Training Loss 0.004617545551904046\n",
      "Episode 18589; Testing Loss 0.005770064149972217; Training Loss 0.00461753827183164\n",
      "Episode 18590; Testing Loss 0.00577013488946263; Training Loss 0.0046175299455743805\n",
      "Episode 18591; Testing Loss 0.0057700908094377714; Training Loss 0.004617523685579152\n",
      "Episode 18592; Testing Loss 0.005770097615871815; Training Loss 0.004617514295978044\n",
      "Episode 18593; Testing Loss 0.005770075177426585; Training Loss 0.0046175079971717305\n",
      "Episode 18594; Testing Loss 0.005770022679494186; Training Loss 0.004617500346170295\n",
      "Episode 18595; Testing Loss 0.005770036192660867; Training Loss 0.004617490528729231\n",
      "Episode 18596; Testing Loss 0.005770105383523409; Training Loss 0.004617484227162136\n",
      "Episode 18597; Testing Loss 0.0057701528122484766; Training Loss 0.0046174759409177224\n",
      "Episode 18598; Testing Loss 0.005770011233210706; Training Loss 0.0046174670997657895\n",
      "Episode 18599; Testing Loss 0.005769879438909831; Training Loss 0.004617461199685178\n",
      "Episode 18600; Testing Loss 0.005770049456360283; Training Loss 0.004617451659216144\n",
      "Episode 18601; Testing Loss 0.005770118514944422; Training Loss 0.004617444700926653\n",
      "Episode 18602; Testing Loss 0.0057700046761448975; Training Loss 0.004617436190899722\n",
      "Episode 18603; Testing Loss 0.005769879090323089; Training Loss 0.004617429255158209\n",
      "Episode 18604; Testing Loss 0.005769907174417991; Training Loss 0.004617420339892861\n",
      "Episode 18605; Testing Loss 0.005769994944410086; Training Loss 0.004617412806555433\n",
      "Episode 18606; Testing Loss 0.005770088692580229; Training Loss 0.004617405632613649\n",
      "Episode 18607; Testing Loss 0.0057699738969128845; Training Loss 0.004617396671328254\n",
      "Episode 18608; Testing Loss 0.00576983610724291; Training Loss 0.004617389259316446\n",
      "Episode 18609; Testing Loss 0.005769922486578059; Training Loss 0.0046173820973501685\n",
      "Episode 18610; Testing Loss 0.005769912643006745; Training Loss 0.004617373260122726\n",
      "Episode 18611; Testing Loss 0.005769826187908664; Training Loss 0.004617366534358489\n",
      "Episode 18612; Testing Loss 0.005769936734531704; Training Loss 0.004617358573470284\n",
      "Episode 18613; Testing Loss 0.005770004951965767; Training Loss 0.0046173508970542565\n",
      "Episode 18614; Testing Loss 0.0057698575250414216; Training Loss 0.0046173414279931\n",
      "Episode 18615; Testing Loss 0.005769716490974077; Training Loss 0.004617334580900757\n",
      "Episode 18616; Testing Loss 0.0057698185526024875; Training Loss 0.004617326178340637\n",
      "Episode 18617; Testing Loss 0.005769976132114668; Training Loss 0.004617319198028642\n",
      "Episode 18618; Testing Loss 0.005769911569899628; Training Loss 0.00461731086760543\n",
      "Episode 18619; Testing Loss 0.005769761625746828; Training Loss 0.004617303355495867\n",
      "Episode 18620; Testing Loss 0.005769832183075649; Training Loss 0.004617293893027586\n",
      "Episode 18621; Testing Loss 0.0057699400461769116; Training Loss 0.004617286765173164\n",
      "Episode 18622; Testing Loss 0.0057698745644808695; Training Loss 0.0046172786569511695\n",
      "Episode 18623; Testing Loss 0.00576983161262161; Training Loss 0.004617271191656943\n",
      "Episode 18624; Testing Loss 0.005769858318932876; Training Loss 0.004617262258597381\n",
      "Episode 18625; Testing Loss 0.005769945098116308; Training Loss 0.004617256726445121\n",
      "Episode 18626; Testing Loss 0.005769803412835564; Training Loss 0.004617246916238837\n",
      "Episode 18627; Testing Loss 0.00576976034632167; Training Loss 0.004617241355468818\n",
      "Episode 18628; Testing Loss 0.005769993111675788; Training Loss 0.004617232630807181\n",
      "Episode 18629; Testing Loss 0.005769956318660567; Training Loss 0.004617224173613128\n",
      "Episode 18630; Testing Loss 0.005769815883897772; Training Loss 0.0046172164289686535\n",
      "Episode 18631; Testing Loss 0.005769806661965936; Training Loss 0.00461720896163691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18632; Testing Loss 0.005769972609438056; Training Loss 0.004617201701456247\n",
      "Episode 18633; Testing Loss 0.005769912782433688; Training Loss 0.004617193648511964\n",
      "Episode 18634; Testing Loss 0.005769787752547462; Training Loss 0.004617185951260757\n",
      "Episode 18635; Testing Loss 0.005769835239362372; Training Loss 0.004617177605052891\n",
      "Episode 18636; Testing Loss 0.005769961571183637; Training Loss 0.00461716998821224\n",
      "Episode 18637; Testing Loss 0.0057699009205543165; Training Loss 0.004617160850155193\n",
      "Episode 18638; Testing Loss 0.00576977822518081; Training Loss 0.004617152696629307\n",
      "Episode 18639; Testing Loss 0.005769740255212123; Training Loss 0.0046171459236259635\n",
      "Episode 18640; Testing Loss 0.005769787126040458; Training Loss 0.004617138042974629\n",
      "Episode 18641; Testing Loss 0.0057698400231519535; Training Loss 0.004617130052699782\n",
      "Episode 18642; Testing Loss 0.005769892488203936; Training Loss 0.00461712196037376\n",
      "Episode 18643; Testing Loss 0.005769789490697596; Training Loss 0.004617113638786586\n",
      "Episode 18644; Testing Loss 0.005769638116539806; Training Loss 0.004617107401459985\n",
      "Episode 18645; Testing Loss 0.005769726601086463; Training Loss 0.004617098834993693\n",
      "Episode 18646; Testing Loss 0.005769840408204359; Training Loss 0.004617091197210939\n",
      "Episode 18647; Testing Loss 0.005769699628565034; Training Loss 0.004617082604425885\n",
      "Episode 18648; Testing Loss 0.005769698837804147; Training Loss 0.004617074338662169\n",
      "Episode 18649; Testing Loss 0.005769758630900812; Training Loss 0.004617067635547977\n",
      "Episode 18650; Testing Loss 0.005769665710808537; Training Loss 0.004617058486554216\n",
      "Episode 18651; Testing Loss 0.005769523543561887; Training Loss 0.00461705488213247\n",
      "Episode 18652; Testing Loss 0.005769691949427854; Training Loss 0.004617043060505494\n",
      "Episode 18653; Testing Loss 0.005769798037704304; Training Loss 0.004617038385843265\n",
      "Episode 18654; Testing Loss 0.005769573653832684; Training Loss 0.004617030031255781\n",
      "Episode 18655; Testing Loss 0.0057695031509925115; Training Loss 0.004617022161497837\n",
      "Episode 18656; Testing Loss 0.005769721335930711; Training Loss 0.004617013614806575\n",
      "Episode 18657; Testing Loss 0.005769886842215909; Training Loss 0.004617008295604483\n",
      "Episode 18658; Testing Loss 0.005769587185715972; Training Loss 0.004616996701659881\n",
      "Episode 18659; Testing Loss 0.00576930952809525; Training Loss 0.004616993725669484\n",
      "Episode 18660; Testing Loss 0.005769591500255315; Training Loss 0.0046169813869532195\n",
      "Episode 18661; Testing Loss 0.0057697952612224615; Training Loss 0.004616975184718795\n",
      "Episode 18662; Testing Loss 0.005769595803189271; Training Loss 0.004616966193366942\n",
      "Episode 18663; Testing Loss 0.005769520295769684; Training Loss 0.00461696160679358\n",
      "Episode 18664; Testing Loss 0.005769710230007414; Training Loss 0.004616949833402382\n",
      "Episode 18665; Testing Loss 0.005769739369241643; Training Loss 0.004616944284859486\n",
      "Episode 18666; Testing Loss 0.005769502392080686; Training Loss 0.004616936522690699\n",
      "Episode 18667; Testing Loss 0.005769459883624543; Training Loss 0.0046169282784401106\n",
      "Episode 18668; Testing Loss 0.005769612844852575; Training Loss 0.004616918754729814\n",
      "Episode 18669; Testing Loss 0.005769721420248165; Training Loss 0.004616912851159677\n",
      "Episode 18670; Testing Loss 0.005769676388503928; Training Loss 0.004616903923251112\n",
      "Episode 18671; Testing Loss 0.00576958682920463; Training Loss 0.004616895862911776\n",
      "Episode 18672; Testing Loss 0.005769653970191956; Training Loss 0.004616886704823801\n",
      "Episode 18673; Testing Loss 0.005769699665241115; Training Loss 0.004616881559493006\n",
      "Episode 18674; Testing Loss 0.005769547528393829; Training Loss 0.0046168722074355845\n",
      "Episode 18675; Testing Loss 0.005769357373450672; Training Loss 0.004616864778190969\n",
      "Episode 18676; Testing Loss 0.005769499166617376; Training Loss 0.00461685593102859\n",
      "Episode 18677; Testing Loss 0.005769659411082019; Training Loss 0.00461684981479316\n",
      "Episode 18678; Testing Loss 0.005769521080929718; Training Loss 0.004616839367063167\n",
      "Episode 18679; Testing Loss 0.005769426229310824; Training Loss 0.0046168319664314115\n",
      "Episode 18680; Testing Loss 0.005769571084515139; Training Loss 0.00461682332850725\n",
      "Episode 18681; Testing Loss 0.005769670717201418; Training Loss 0.004616816848975014\n",
      "Episode 18682; Testing Loss 0.005769520133728605; Training Loss 0.004616808128943119\n",
      "Episode 18683; Testing Loss 0.00576936494683745; Training Loss 0.004616800540435726\n",
      "Episode 18684; Testing Loss 0.005769388774703136; Training Loss 0.004616792603521312\n",
      "Episode 18685; Testing Loss 0.0057696292195919836; Training Loss 0.004616785104153523\n",
      "Episode 18686; Testing Loss 0.005769619637542145; Training Loss 0.004616777725690115\n",
      "Episode 18687; Testing Loss 0.005769334719864718; Training Loss 0.004616770522182983\n",
      "Episode 18688; Testing Loss 0.0057693508793614885; Training Loss 0.004616761253717466\n",
      "Episode 18689; Testing Loss 0.005769535832277507; Training Loss 0.004616753430333801\n",
      "Episode 18690; Testing Loss 0.005769516751043544; Training Loss 0.004616746124171751\n",
      "Episode 18691; Testing Loss 0.005769289690235174; Training Loss 0.004616738275359649\n",
      "Episode 18692; Testing Loss 0.0057692730918688425; Training Loss 0.004616730312929339\n",
      "Episode 18693; Testing Loss 0.005769460800393139; Training Loss 0.0046167208984091435\n",
      "Episode 18694; Testing Loss 0.005769446022974507; Training Loss 0.004616713747495468\n",
      "Episode 18695; Testing Loss 0.0057693726872596485; Training Loss 0.004616706331689036\n",
      "Episode 18696; Testing Loss 0.005769359139979901; Training Loss 0.004616697097797347\n",
      "Episode 18697; Testing Loss 0.00576933215309875; Training Loss 0.0046166902154547215\n",
      "Episode 18698; Testing Loss 0.005769211259084709; Training Loss 0.0046166827620383\n",
      "Episode 18699; Testing Loss 0.005769203179685093; Training Loss 0.004616675137208497\n",
      "Episode 18700; Testing Loss 0.005769322963000429; Training Loss 0.00461666778364963\n",
      "Episode 18701; Testing Loss 0.005769335601672177; Training Loss 0.004616658875672529\n",
      "Episode 18702; Testing Loss 0.005769264864961007; Training Loss 0.004616649909282101\n",
      "Episode 18703; Testing Loss 0.005769280127176975; Training Loss 0.004616642939648243\n",
      "Episode 18704; Testing Loss 0.00576928557905598; Training Loss 0.004616634790831972\n",
      "Episode 18705; Testing Loss 0.005769441485844201; Training Loss 0.0046166271237948735\n",
      "Episode 18706; Testing Loss 0.005769343834149489; Training Loss 0.004616618177668888\n",
      "Episode 18707; Testing Loss 0.005769266427897515; Training Loss 0.004616611164093911\n",
      "Episode 18708; Testing Loss 0.00576930695116622; Training Loss 0.004616603214988024\n",
      "Episode 18709; Testing Loss 0.00576942247532572; Training Loss 0.004616596070244549\n",
      "Episode 18710; Testing Loss 0.005769360398377256; Training Loss 0.00461658803454522\n",
      "Episode 18711; Testing Loss 0.005769284271267004; Training Loss 0.004616581553027087\n",
      "Episode 18712; Testing Loss 0.005769379121791419; Training Loss 0.004616573316155003\n",
      "Episode 18713; Testing Loss 0.005769333999212392; Training Loss 0.004616565745969742\n",
      "Episode 18714; Testing Loss 0.005769206917368106; Training Loss 0.004616556904906575\n",
      "Episode 18715; Testing Loss 0.005769258689388263; Training Loss 0.0046165501390312514\n",
      "Episode 18716; Testing Loss 0.005769490074278946; Training Loss 0.004616542634037582\n",
      "Episode 18717; Testing Loss 0.005769445105583886; Training Loss 0.004616534766619823\n",
      "Episode 18718; Testing Loss 0.005769216722836343; Training Loss 0.004616525865811644\n",
      "Episode 18719; Testing Loss 0.005769146398441048; Training Loss 0.0046165182228719084\n",
      "Episode 18720; Testing Loss 0.00576924589420247; Training Loss 0.0046165102268540545\n",
      "Episode 18721; Testing Loss 0.0057693186077934925; Training Loss 0.004616501854961888\n",
      "Episode 18722; Testing Loss 0.005769286193963332; Training Loss 0.004616494078933977\n",
      "Episode 18723; Testing Loss 0.0057692186213000535; Training Loss 0.004616486216788777\n",
      "Episode 18724; Testing Loss 0.005769225483827513; Training Loss 0.004616479025181072\n",
      "Episode 18725; Testing Loss 0.005769180375703219; Training Loss 0.004616470674739754\n",
      "Episode 18726; Testing Loss 0.005769102280368293; Training Loss 0.004616463148116311\n",
      "Episode 18727; Testing Loss 0.005769183558775865; Training Loss 0.004616455074478218\n",
      "Episode 18728; Testing Loss 0.005769256220953047; Training Loss 0.00461644729288919\n",
      "Episode 18729; Testing Loss 0.005769165875426715; Training Loss 0.004616438599793645\n",
      "Episode 18730; Testing Loss 0.005769069635352733; Training Loss 0.004616431875865421\n",
      "Episode 18731; Testing Loss 0.005769209779837931; Training Loss 0.00461642318273273\n",
      "Episode 18732; Testing Loss 0.005769272879494881; Training Loss 0.004616415458148982\n",
      "Episode 18733; Testing Loss 0.005769183847743161; Training Loss 0.0046164070450094965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18734; Testing Loss 0.0057690572554494396; Training Loss 0.004616399043559179\n",
      "Episode 18735; Testing Loss 0.005769115283402268; Training Loss 0.004616394031593986\n",
      "Episode 18736; Testing Loss 0.005769104695267122; Training Loss 0.004616385639966759\n",
      "Episode 18737; Testing Loss 0.005769039501942987; Training Loss 0.004616375476445905\n",
      "Episode 18738; Testing Loss 0.0057691589647275165; Training Loss 0.004616368219393702\n",
      "Episode 18739; Testing Loss 0.005769227422841706; Training Loss 0.004616360598136116\n",
      "Episode 18740; Testing Loss 0.0057691261102703565; Training Loss 0.004616352280253476\n",
      "Episode 18741; Testing Loss 0.005768979194274172; Training Loss 0.004616344510541887\n",
      "Episode 18742; Testing Loss 0.005769038728832466; Training Loss 0.004616336517100291\n",
      "Episode 18743; Testing Loss 0.0057691158164408915; Training Loss 0.004616328190909767\n",
      "Episode 18744; Testing Loss 0.005769083191682379; Training Loss 0.004616324049596876\n",
      "Episode 18745; Testing Loss 0.005769192219068748; Training Loss 0.00461631401385994\n",
      "Episode 18746; Testing Loss 0.005769245565070172; Training Loss 0.004616308109436083\n",
      "Episode 18747; Testing Loss 0.0057689640056103495; Training Loss 0.004616300423314753\n",
      "Episode 18748; Testing Loss 0.005768850542454986; Training Loss 0.004616293553591966\n",
      "Episode 18749; Testing Loss 0.005769043304564982; Training Loss 0.004616283666875349\n",
      "Episode 18750; Testing Loss 0.005769125740389008; Training Loss 0.004616277496316127\n",
      "Episode 18751; Testing Loss 0.005768993311959842; Training Loss 0.004616268878736826\n",
      "Episode 18752; Testing Loss 0.0057689301329436875; Training Loss 0.004616259411812348\n",
      "Episode 18753; Testing Loss 0.005769028896389802; Training Loss 0.004616253432268606\n",
      "Episode 18754; Testing Loss 0.005769060396300439; Training Loss 0.004616246832684408\n",
      "Episode 18755; Testing Loss 0.00576902237044803; Training Loss 0.004616237168966819\n",
      "Episode 18756; Testing Loss 0.0057690114696209265; Training Loss 0.004616228824278722\n",
      "Episode 18757; Testing Loss 0.00576905497909385; Training Loss 0.0046162221179785025\n",
      "Episode 18758; Testing Loss 0.00576906948275491; Training Loss 0.004616215213861119\n",
      "Episode 18759; Testing Loss 0.0057690389855749105; Training Loss 0.004616206741898013\n",
      "Episode 18760; Testing Loss 0.0057690078482312696; Training Loss 0.004616197113682526\n",
      "Episode 18761; Testing Loss 0.005769020771914949; Training Loss 0.004616188002388216\n",
      "Episode 18762; Testing Loss 0.005768998407747302; Training Loss 0.004616181568934917\n",
      "Episode 18763; Testing Loss 0.005769019409481114; Training Loss 0.004616172923239369\n",
      "Episode 18764; Testing Loss 0.005769037329792136; Training Loss 0.004616164478488037\n",
      "Episode 18765; Testing Loss 0.0057690198000591; Training Loss 0.004616156758889605\n",
      "Episode 18766; Testing Loss 0.005768930595047363; Training Loss 0.004616150058324701\n",
      "Episode 18767; Testing Loss 0.005768786122515163; Training Loss 0.004616142687542254\n",
      "Episode 18768; Testing Loss 0.005768846940906832; Training Loss 0.004616134396155576\n",
      "Episode 18769; Testing Loss 0.005769014502897066; Training Loss 0.004616126535977119\n",
      "Episode 18770; Testing Loss 0.005769029150358608; Training Loss 0.004616118455500259\n",
      "Episode 18771; Testing Loss 0.005768867781310034; Training Loss 0.0046161105824374235\n",
      "Episode 18772; Testing Loss 0.005768744607072145; Training Loss 0.004616102674886612\n",
      "Episode 18773; Testing Loss 0.0057688684572079285; Training Loss 0.004616094266887232\n",
      "Episode 18774; Testing Loss 0.005768831968954587; Training Loss 0.004616087658638402\n",
      "Episode 18775; Testing Loss 0.005768752335573341; Training Loss 0.004616079802391291\n",
      "Episode 18776; Testing Loss 0.005768767056963789; Training Loss 0.004616070982782628\n",
      "Episode 18777; Testing Loss 0.005768839896758889; Training Loss 0.004616064767055909\n",
      "Episode 18778; Testing Loss 0.005768809193441211; Training Loss 0.004616056883844219\n",
      "Episode 18779; Testing Loss 0.005768786809238489; Training Loss 0.004616047083637786\n",
      "Episode 18780; Testing Loss 0.005768790075449782; Training Loss 0.004616040559624463\n",
      "Episode 18781; Testing Loss 0.005768811176372094; Training Loss 0.004616034707701826\n",
      "Episode 18782; Testing Loss 0.005768820056072571; Training Loss 0.004616025069673857\n",
      "Episode 18783; Testing Loss 0.005768779580830511; Training Loss 0.004616015578658785\n",
      "Episode 18784; Testing Loss 0.005768770685998989; Training Loss 0.004616011671564373\n",
      "Episode 18785; Testing Loss 0.005768816768342536; Training Loss 0.0046160047217495125\n",
      "Episode 18786; Testing Loss 0.005768807914484844; Training Loss 0.004615994927132424\n",
      "Episode 18787; Testing Loss 0.005768737071926435; Training Loss 0.004615984908459296\n",
      "Episode 18788; Testing Loss 0.005768683058429273; Training Loss 0.004615978798077084\n",
      "Episode 18789; Testing Loss 0.005768778123863608; Training Loss 0.004615972142633782\n",
      "Episode 18790; Testing Loss 0.005768810870744631; Training Loss 0.004615964190871409\n",
      "Episode 18791; Testing Loss 0.00576875199903819; Training Loss 0.004615955182260136\n",
      "Episode 18792; Testing Loss 0.005768786179369524; Training Loss 0.004615945506700242\n",
      "Episode 18793; Testing Loss 0.005768856574752129; Training Loss 0.004615938876219153\n",
      "Episode 18794; Testing Loss 0.005768870895147282; Training Loss 0.004615932150854248\n",
      "Episode 18795; Testing Loss 0.005768802825718226; Training Loss 0.004615922740846186\n",
      "Episode 18796; Testing Loss 0.0057687358665786895; Training Loss 0.004615914301219988\n",
      "Episode 18797; Testing Loss 0.005768830015594185; Training Loss 0.004615907151526426\n",
      "Episode 18798; Testing Loss 0.005768869299686147; Training Loss 0.004615899242296787\n",
      "Episode 18799; Testing Loss 0.005768723219461107; Training Loss 0.0046158905419376095\n",
      "Episode 18800; Testing Loss 0.005768651239573568; Training Loss 0.004615882814928081\n",
      "Episode 18801; Testing Loss 0.0057687140446082185; Training Loss 0.004615875186543452\n",
      "Episode 18802; Testing Loss 0.00576885337220478; Training Loss 0.00461586711643237\n",
      "Episode 18803; Testing Loss 0.005768830162989175; Training Loss 0.004615858856168908\n",
      "Episode 18804; Testing Loss 0.005768647540705587; Training Loss 0.004615851140037902\n",
      "Episode 18805; Testing Loss 0.005768578366582171; Training Loss 0.004615844303315367\n",
      "Episode 18806; Testing Loss 0.005768725426326437; Training Loss 0.004615834635457691\n",
      "Episode 18807; Testing Loss 0.0057687592117485345; Training Loss 0.004615828557542226\n",
      "Episode 18808; Testing Loss 0.005768626155353884; Training Loss 0.004615821542219049\n",
      "Episode 18809; Testing Loss 0.005768616582298518; Training Loss 0.0046158116766056795\n",
      "Episode 18810; Testing Loss 0.005768623078384553; Training Loss 0.004615806020925613\n",
      "Episode 18811; Testing Loss 0.00576852409691077; Training Loss 0.004615799001738021\n",
      "Episode 18812; Testing Loss 0.005768524904473622; Training Loss 0.00461578984085434\n",
      "Episode 18813; Testing Loss 0.00576858207767889; Training Loss 0.0046157809027972795\n",
      "Episode 18814; Testing Loss 0.005768641416574546; Training Loss 0.004615774688262196\n",
      "Episode 18815; Testing Loss 0.00576865136876368; Training Loss 0.00461576717168817\n",
      "Episode 18816; Testing Loss 0.005768598484513958; Training Loss 0.004615758208110563\n",
      "Episode 18817; Testing Loss 0.00576860397349186; Training Loss 0.004615749164392259\n",
      "Episode 18818; Testing Loss 0.005768532497718486; Training Loss 0.0046157414878636805\n",
      "Episode 18819; Testing Loss 0.005768553201221016; Training Loss 0.004615733256777467\n",
      "Episode 18820; Testing Loss 0.005768624619003345; Training Loss 0.004615725026800212\n",
      "Episode 18821; Testing Loss 0.005768674598636054; Training Loss 0.004615717919062895\n",
      "Episode 18822; Testing Loss 0.005768550089821771; Training Loss 0.004615710313050437\n",
      "Episode 18823; Testing Loss 0.0057685439411142535; Training Loss 0.00461570338430857\n",
      "Episode 18824; Testing Loss 0.005768585213415268; Training Loss 0.004615694687077279\n",
      "Episode 18825; Testing Loss 0.005768626248347557; Training Loss 0.0046156863949258\n",
      "Episode 18826; Testing Loss 0.0057686394662466375; Training Loss 0.004615678950158516\n",
      "Episode 18827; Testing Loss 0.005768596609895137; Training Loss 0.0046156714198405105\n",
      "Episode 18828; Testing Loss 0.00576848822502295; Training Loss 0.004615663579640418\n",
      "Episode 18829; Testing Loss 0.0057685343789016874; Training Loss 0.00461565533016106\n",
      "Episode 18830; Testing Loss 0.005768589808810386; Training Loss 0.004615647436075694\n",
      "Episode 18831; Testing Loss 0.005768630797672682; Training Loss 0.0046156399281164304\n",
      "Episode 18832; Testing Loss 0.0057686101527317304; Training Loss 0.00461563355635509\n",
      "Episode 18833; Testing Loss 0.005768493200469131; Training Loss 0.004615625108685403\n",
      "Episode 18834; Testing Loss 0.005768477628681692; Training Loss 0.004615616095986666\n",
      "Episode 18835; Testing Loss 0.005768533921469736; Training Loss 0.004615609938551307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18836; Testing Loss 0.005768610608518456; Training Loss 0.004615601573677687\n",
      "Episode 18837; Testing Loss 0.0057684590639614786; Training Loss 0.00461559253070361\n",
      "Episode 18838; Testing Loss 0.005768340928079488; Training Loss 0.0046155856981687044\n",
      "Episode 18839; Testing Loss 0.005768452411784064; Training Loss 0.004615576693006463\n",
      "Episode 18840; Testing Loss 0.005768535640534897; Training Loss 0.004615569352135837\n",
      "Episode 18841; Testing Loss 0.005768526786546296; Training Loss 0.00461556188699212\n",
      "Episode 18842; Testing Loss 0.005768486863830819; Training Loss 0.004615553336408456\n",
      "Episode 18843; Testing Loss 0.005768414044761215; Training Loss 0.004615544984142068\n",
      "Episode 18844; Testing Loss 0.005768369902251932; Training Loss 0.004615538733742323\n",
      "Episode 18845; Testing Loss 0.0057684448239533935; Training Loss 0.00461553073424242\n",
      "Episode 18846; Testing Loss 0.0057684653649086185; Training Loss 0.004615522326111125\n",
      "Episode 18847; Testing Loss 0.005768383195491813; Training Loss 0.004615515072988986\n",
      "Episode 18848; Testing Loss 0.005768332885452669; Training Loss 0.004615507480387521\n",
      "Episode 18849; Testing Loss 0.005768412429263319; Training Loss 0.0046154981364413\n",
      "Episode 18850; Testing Loss 0.005768468161015996; Training Loss 0.00461549177863987\n",
      "Episode 18851; Testing Loss 0.005768437971718356; Training Loss 0.004615485707910528\n",
      "Episode 18852; Testing Loss 0.005768429665298715; Training Loss 0.004615475062569486\n",
      "Episode 18853; Testing Loss 0.005768360686945007; Training Loss 0.00461546872275789\n",
      "Episode 18854; Testing Loss 0.005768361515342075; Training Loss 0.0046154601956771854\n",
      "Episode 18855; Testing Loss 0.005768397307894169; Training Loss 0.004615451269617965\n",
      "Episode 18856; Testing Loss 0.00576841928393594; Training Loss 0.004615444255404284\n",
      "Episode 18857; Testing Loss 0.005768378025750394; Training Loss 0.004615436777572299\n",
      "Episode 18858; Testing Loss 0.00576830635611706; Training Loss 0.004615427885361551\n",
      "Episode 18859; Testing Loss 0.005768409840302088; Training Loss 0.004615420874557177\n",
      "Episode 18860; Testing Loss 0.005768460961421884; Training Loss 0.004615412836997067\n",
      "Episode 18861; Testing Loss 0.0057683989606692404; Training Loss 0.004615405999841692\n",
      "Episode 18862; Testing Loss 0.005768391670607362; Training Loss 0.004615399005732151\n",
      "Episode 18863; Testing Loss 0.0057684507261398475; Training Loss 0.004615390153886766\n",
      "Episode 18864; Testing Loss 0.005768346664081024; Training Loss 0.004615382708571729\n",
      "Episode 18865; Testing Loss 0.0057682739628718525; Training Loss 0.004615374523737777\n",
      "Episode 18866; Testing Loss 0.005768315188091775; Training Loss 0.004615365546076516\n",
      "Episode 18867; Testing Loss 0.0057683746908192115; Training Loss 0.004615358950609188\n",
      "Episode 18868; Testing Loss 0.005768347185408035; Training Loss 0.004615350913101931\n",
      "Episode 18869; Testing Loss 0.005768284206530083; Training Loss 0.004615343366980453\n",
      "Episode 18870; Testing Loss 0.005768252155576919; Training Loss 0.004615335914130983\n",
      "Episode 18871; Testing Loss 0.005768303820747495; Training Loss 0.004615327435013685\n",
      "Episode 18872; Testing Loss 0.005768358144951351; Training Loss 0.004615319732389481\n",
      "Episode 18873; Testing Loss 0.00576837848762485; Training Loss 0.004615311986681583\n",
      "Episode 18874; Testing Loss 0.005768309972309238; Training Loss 0.004615304246350172\n",
      "Episode 18875; Testing Loss 0.0057681871164055026; Training Loss 0.004615295659275945\n",
      "Episode 18876; Testing Loss 0.005768076362327178; Training Loss 0.004615288682772918\n",
      "Episode 18877; Testing Loss 0.005768222605721654; Training Loss 0.0046152796054514805\n",
      "Episode 18878; Testing Loss 0.0057683566348593574; Training Loss 0.004615272232785002\n",
      "Episode 18879; Testing Loss 0.0057682541424102455; Training Loss 0.004615264344527636\n",
      "Episode 18880; Testing Loss 0.005768142452296431; Training Loss 0.00461525701643211\n",
      "Episode 18881; Testing Loss 0.00576821235178333; Training Loss 0.004615248722635108\n",
      "Episode 18882; Testing Loss 0.005768312829557119; Training Loss 0.004615242299214507\n",
      "Episode 18883; Testing Loss 0.005768233464622879; Training Loss 0.004615232823727843\n",
      "Episode 18884; Testing Loss 0.005768106017408555; Training Loss 0.004615225103536801\n",
      "Episode 18885; Testing Loss 0.005768170020102146; Training Loss 0.004615217698698102\n",
      "Episode 18886; Testing Loss 0.005768243239515918; Training Loss 0.004615209485994122\n",
      "Episode 18887; Testing Loss 0.005768214328162905; Training Loss 0.004615202631672926\n",
      "Episode 18888; Testing Loss 0.0057681709375825; Training Loss 0.004615193329548728\n",
      "Episode 18889; Testing Loss 0.0057681503629464425; Training Loss 0.004615186209076635\n",
      "Episode 18890; Testing Loss 0.005768248673870071; Training Loss 0.004615178985932581\n",
      "Episode 18891; Testing Loss 0.0057683148438208215; Training Loss 0.004615171061526756\n",
      "Episode 18892; Testing Loss 0.005768226180244588; Training Loss 0.004615164326465099\n",
      "Episode 18893; Testing Loss 0.005768246784850937; Training Loss 0.004615155630616661\n",
      "Episode 18894; Testing Loss 0.005768295582736992; Training Loss 0.00461514844617613\n",
      "Episode 18895; Testing Loss 0.005768198511490202; Training Loss 0.0046151400746620415\n",
      "Episode 18896; Testing Loss 0.005768117928271576; Training Loss 0.004615133785535573\n",
      "Episode 18897; Testing Loss 0.005768272042451283; Training Loss 0.004615125126407838\n",
      "Episode 18898; Testing Loss 0.005768301424530985; Training Loss 0.004615117200852686\n",
      "Episode 18899; Testing Loss 0.005768090881925479; Training Loss 0.004615108696904757\n",
      "Episode 18900; Testing Loss 0.005768069410010769; Training Loss 0.004615100440973982\n",
      "Episode 18901; Testing Loss 0.005768075885086322; Training Loss 0.004615092621271249\n",
      "Episode 18902; Testing Loss 0.005768114792985186; Training Loss 0.00461508497477141\n",
      "Episode 18903; Testing Loss 0.005768107849934658; Training Loss 0.004615077386580924\n",
      "Episode 18904; Testing Loss 0.005768109902347991; Training Loss 0.004615069427524647\n",
      "Episode 18905; Testing Loss 0.00576803670367638; Training Loss 0.0046150627512706625\n",
      "Episode 18906; Testing Loss 0.005768106945334452; Training Loss 0.00461505401832395\n",
      "Episode 18907; Testing Loss 0.005768202243417378; Training Loss 0.004615048144426529\n",
      "Episode 18908; Testing Loss 0.005768102236913498; Training Loss 0.004615039367556167\n",
      "Episode 18909; Testing Loss 0.005767984797742533; Training Loss 0.004615030456852062\n",
      "Episode 18910; Testing Loss 0.005768048447192993; Training Loss 0.004615025503615143\n",
      "Episode 18911; Testing Loss 0.005768159351777497; Training Loss 0.004615016699548407\n",
      "Episode 18912; Testing Loss 0.005768020799361524; Training Loss 0.004615008008932781\n",
      "Episode 18913; Testing Loss 0.005767864215773608; Training Loss 0.004615002034565286\n",
      "Episode 18914; Testing Loss 0.005767987657297672; Training Loss 0.004614992799444801\n",
      "Episode 18915; Testing Loss 0.005768226755377615; Training Loss 0.004614984744299512\n",
      "Episode 18916; Testing Loss 0.0057682502037087165; Training Loss 0.004614977220382528\n",
      "Episode 18917; Testing Loss 0.005768062720234916; Training Loss 0.0046149676482505324\n",
      "Episode 18918; Testing Loss 0.005767982383971051; Training Loss 0.004614961409801479\n",
      "Episode 18919; Testing Loss 0.005768053903512709; Training Loss 0.0046149535868746676\n",
      "Episode 18920; Testing Loss 0.005768098248872753; Training Loss 0.004614944746843269\n",
      "Episode 18921; Testing Loss 0.005768011050614025; Training Loss 0.004614940841376763\n",
      "Episode 18922; Testing Loss 0.0057680667591543345; Training Loss 0.004614929819353246\n",
      "Episode 18923; Testing Loss 0.00576810733614746; Training Loss 0.004614922194481993\n",
      "Episode 18924; Testing Loss 0.005768017434374506; Training Loss 0.004614914989682551\n",
      "Episode 18925; Testing Loss 0.005767915035346547; Training Loss 0.004614906657111421\n",
      "Episode 18926; Testing Loss 0.0057679840825291965; Training Loss 0.0046148992287482135\n",
      "Episode 18927; Testing Loss 0.00576806673189387; Training Loss 0.004614892706253374\n",
      "Episode 18928; Testing Loss 0.005768018130165489; Training Loss 0.004614883431792007\n",
      "Episode 18929; Testing Loss 0.0057679541102820375; Training Loss 0.004614875161628722\n",
      "Episode 18930; Testing Loss 0.005767924610340868; Training Loss 0.004614867835281673\n",
      "Episode 18931; Testing Loss 0.00576801262126752; Training Loss 0.0046148594044501745\n",
      "Episode 18932; Testing Loss 0.005768107018447122; Training Loss 0.004614851643382575\n",
      "Episode 18933; Testing Loss 0.005767990254343597; Training Loss 0.004614843490601248\n",
      "Episode 18934; Testing Loss 0.005767754319556845; Training Loss 0.004614837131045433\n",
      "Episode 18935; Testing Loss 0.005767805682202709; Training Loss 0.004614828953874148\n",
      "Episode 18936; Testing Loss 0.005767995359856736; Training Loss 0.004614820733450218\n",
      "Episode 18937; Testing Loss 0.005768021799601124; Training Loss 0.004614813308492721\n",
      "Episode 18938; Testing Loss 0.0057679370081977896; Training Loss 0.004614806076791262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18939; Testing Loss 0.005767923093531882; Training Loss 0.00461479756855132\n",
      "Episode 18940; Testing Loss 0.0057678495731377346; Training Loss 0.004614791105650933\n",
      "Episode 18941; Testing Loss 0.005767848350671937; Training Loss 0.004614782587289472\n",
      "Episode 18942; Testing Loss 0.005767919434437102; Training Loss 0.004614773699974334\n",
      "Episode 18943; Testing Loss 0.005767896843024449; Training Loss 0.004614766148219176\n",
      "Episode 18944; Testing Loss 0.005767859071585685; Training Loss 0.0046147596869669065\n",
      "Episode 18945; Testing Loss 0.005767950361820771; Training Loss 0.004614750197993741\n",
      "Episode 18946; Testing Loss 0.005767881281572411; Training Loss 0.004614742546557262\n",
      "Episode 18947; Testing Loss 0.005767761333612257; Training Loss 0.004614734430356836\n",
      "Episode 18948; Testing Loss 0.005767868940204129; Training Loss 0.004614726274899563\n",
      "Episode 18949; Testing Loss 0.005767909101383624; Training Loss 0.004614718397022331\n",
      "Episode 18950; Testing Loss 0.005767907997412781; Training Loss 0.00461471069524348\n",
      "Episode 18951; Testing Loss 0.005767868573561175; Training Loss 0.0046147035464766005\n",
      "Episode 18952; Testing Loss 0.005767802182190492; Training Loss 0.004614695530816116\n",
      "Episode 18953; Testing Loss 0.005767849778321645; Training Loss 0.004614687902844268\n",
      "Episode 18954; Testing Loss 0.005767930697806056; Training Loss 0.004614680116058853\n",
      "Episode 18955; Testing Loss 0.005767856916316266; Training Loss 0.004614671468114172\n",
      "Episode 18956; Testing Loss 0.0057677828629107515; Training Loss 0.004614663657616879\n",
      "Episode 18957; Testing Loss 0.005767855954752272; Training Loss 0.004614656237861953\n",
      "Episode 18958; Testing Loss 0.00576785157583009; Training Loss 0.0046146488376440555\n",
      "Episode 18959; Testing Loss 0.00576785579313697; Training Loss 0.004614640605215493\n",
      "Episode 18960; Testing Loss 0.0057677537626406985; Training Loss 0.004614633020708506\n",
      "Episode 18961; Testing Loss 0.005767762210382406; Training Loss 0.004614625143631692\n",
      "Episode 18962; Testing Loss 0.005767779632276945; Training Loss 0.00461461833942272\n",
      "Episode 18963; Testing Loss 0.005767740403260269; Training Loss 0.004614609572583397\n",
      "Episode 18964; Testing Loss 0.005767767256117603; Training Loss 0.004614602048598534\n",
      "Episode 18965; Testing Loss 0.005767865055967368; Training Loss 0.004614594716403587\n",
      "Episode 18966; Testing Loss 0.00576787606837602; Training Loss 0.004614586706263688\n",
      "Episode 18967; Testing Loss 0.005767656083742553; Training Loss 0.004614578515705334\n",
      "Episode 18968; Testing Loss 0.005767683626803305; Training Loss 0.004614570880319317\n",
      "Episode 18969; Testing Loss 0.005767749613639344; Training Loss 0.00461456309137672\n",
      "Episode 18970; Testing Loss 0.0057676828410170985; Training Loss 0.004614556308181792\n",
      "Episode 18971; Testing Loss 0.005767742441108821; Training Loss 0.00461454718019103\n",
      "Episode 18972; Testing Loss 0.005767661871431412; Training Loss 0.004614539222334749\n",
      "Episode 18973; Testing Loss 0.005767664240573785; Training Loss 0.0046145324089635915\n",
      "Episode 18974; Testing Loss 0.005767800677529195; Training Loss 0.004614524238735997\n",
      "Episode 18975; Testing Loss 0.005767775808021562; Training Loss 0.004614516118370061\n",
      "Episode 18976; Testing Loss 0.0057676180126914126; Training Loss 0.004614508119551362\n",
      "Episode 18977; Testing Loss 0.005767638762101711; Training Loss 0.004614500249434291\n",
      "Episode 18978; Testing Loss 0.005767718454868513; Training Loss 0.004614491881967877\n",
      "Episode 18979; Testing Loss 0.005767723334801671; Training Loss 0.004614484275718661\n",
      "Episode 18980; Testing Loss 0.005767624849252688; Training Loss 0.004614477888515243\n",
      "Episode 18981; Testing Loss 0.005767706033733538; Training Loss 0.004614468697763653\n",
      "Episode 18982; Testing Loss 0.00576781681890621; Training Loss 0.0046144635762001285\n",
      "Episode 18983; Testing Loss 0.005767633444917003; Training Loss 0.004614454015807824\n",
      "Episode 18984; Testing Loss 0.005767503395801707; Training Loss 0.004614447592893122\n",
      "Episode 18985; Testing Loss 0.0057677414105999966; Training Loss 0.0046144374910313335\n",
      "Episode 18986; Testing Loss 0.0057678250079651215; Training Loss 0.004614431139702775\n",
      "Episode 18987; Testing Loss 0.005767647922195051; Training Loss 0.004614424656016018\n",
      "Episode 18988; Testing Loss 0.005767638685617526; Training Loss 0.004614414957410409\n",
      "Episode 18989; Testing Loss 0.005767645673876228; Training Loss 0.004614408299315682\n",
      "Episode 18990; Testing Loss 0.005767663128537055; Training Loss 0.0046144004393031705\n",
      "Episode 18991; Testing Loss 0.005767696509543503; Training Loss 0.004614391791539661\n",
      "Episode 18992; Testing Loss 0.005767652273031509; Training Loss 0.004614384478272995\n",
      "Episode 18993; Testing Loss 0.005767704125254147; Training Loss 0.00461437726915077\n",
      "Episode 18994; Testing Loss 0.005767764949645383; Training Loss 0.004614369137237756\n",
      "Episode 18995; Testing Loss 0.005767549696393499; Training Loss 0.004614361136412571\n",
      "Episode 18996; Testing Loss 0.005767430993847487; Training Loss 0.004614353376859562\n",
      "Episode 18997; Testing Loss 0.005767543839040771; Training Loss 0.004614344692062287\n",
      "Episode 18998; Testing Loss 0.00576770486818873; Training Loss 0.004614338166479832\n",
      "Episode 18999; Testing Loss 0.005767655204114934; Training Loss 0.004614329609324803\n",
      "Episode 19000; Testing Loss 0.005767567049398003; Training Loss 0.004614320717041356\n",
      "Episode 19001; Testing Loss 0.005767516999529238; Training Loss 0.004614313154748938\n",
      "Episode 19002; Testing Loss 0.005767554123704445; Training Loss 0.004614304940381816\n",
      "Episode 19003; Testing Loss 0.0057675774391595935; Training Loss 0.0046142973563936505\n",
      "Episode 19004; Testing Loss 0.005767535820243969; Training Loss 0.004614289719884923\n",
      "Episode 19005; Testing Loss 0.005767496130634758; Training Loss 0.00461428150718145\n",
      "Episode 19006; Testing Loss 0.005767554432946349; Training Loss 0.0046142744525671875\n",
      "Episode 19007; Testing Loss 0.005767686125886495; Training Loss 0.0046142672861522165\n",
      "Episode 19008; Testing Loss 0.005767601776586164; Training Loss 0.0046142584091358576\n",
      "Episode 19009; Testing Loss 0.005767428262923895; Training Loss 0.004614251930412474\n",
      "Episode 19010; Testing Loss 0.005767480760643936; Training Loss 0.004614242189682543\n",
      "Episode 19011; Testing Loss 0.005767587756132256; Training Loss 0.004614236237398941\n",
      "Episode 19012; Testing Loss 0.005767534657225711; Training Loss 0.004614227753206894\n",
      "Episode 19013; Testing Loss 0.005767440446638924; Training Loss 0.004614220524451191\n",
      "Episode 19014; Testing Loss 0.0057675425825224926; Training Loss 0.004614211728888739\n",
      "Episode 19015; Testing Loss 0.0057676474118296935; Training Loss 0.004614204828904062\n",
      "Episode 19016; Testing Loss 0.005767559817916097; Training Loss 0.004614196018917268\n",
      "Episode 19017; Testing Loss 0.005767449500158111; Training Loss 0.004614189508918453\n",
      "Episode 19018; Testing Loss 0.005767579630291662; Training Loss 0.004614181675845458\n",
      "Episode 19019; Testing Loss 0.005767539361299437; Training Loss 0.004614173519698353\n",
      "Episode 19020; Testing Loss 0.0057674126220393414; Training Loss 0.00461416589363663\n",
      "Episode 19021; Testing Loss 0.005767518795688195; Training Loss 0.00461415785333899\n",
      "Episode 19022; Testing Loss 0.005767632896006801; Training Loss 0.00461415142836319\n",
      "Episode 19023; Testing Loss 0.005767532808259327; Training Loss 0.004614142357978606\n",
      "Episode 19024; Testing Loss 0.005767347477099752; Training Loss 0.0046141358882937815\n",
      "Episode 19025; Testing Loss 0.005767446489307106; Training Loss 0.00461412656753614\n",
      "Episode 19026; Testing Loss 0.005767588307909991; Training Loss 0.00461411934690344\n",
      "Episode 19027; Testing Loss 0.0057674738772031464; Training Loss 0.004614110520855954\n",
      "Episode 19028; Testing Loss 0.0057673750922375465; Training Loss 0.004614102588451793\n",
      "Episode 19029; Testing Loss 0.005767452800663835; Training Loss 0.004614096295476948\n",
      "Episode 19030; Testing Loss 0.005767442163524246; Training Loss 0.004614088199234806\n",
      "Episode 19031; Testing Loss 0.005767369980559274; Training Loss 0.0046140796257663624\n",
      "Episode 19032; Testing Loss 0.005767449296090669; Training Loss 0.004614071945233876\n",
      "Episode 19033; Testing Loss 0.005767494979537402; Training Loss 0.004614064326537985\n",
      "Episode 19034; Testing Loss 0.005767463570439651; Training Loss 0.004614055844241846\n",
      "Episode 19035; Testing Loss 0.005767409380773964; Training Loss 0.0046140479520689155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19036; Testing Loss 0.005767333150315465; Training Loss 0.004614041114100743\n",
      "Episode 19037; Testing Loss 0.005767381730330273; Training Loss 0.004614033685798734\n",
      "Episode 19038; Testing Loss 0.005767576349919601; Training Loss 0.004614027396287119\n",
      "Episode 19039; Testing Loss 0.005767426202612615; Training Loss 0.004614017542901916\n",
      "Episode 19040; Testing Loss 0.00576724850682922; Training Loss 0.004614011694367693\n",
      "Episode 19041; Testing Loss 0.005767422109196444; Training Loss 0.004614002221059791\n",
      "Episode 19042; Testing Loss 0.005767524859961001; Training Loss 0.004613996049328192\n",
      "Episode 19043; Testing Loss 0.0057673269832197215; Training Loss 0.004613986228192402\n",
      "Episode 19044; Testing Loss 0.005767270954270218; Training Loss 0.004613978509516183\n",
      "Episode 19045; Testing Loss 0.005767388480571072; Training Loss 0.004613970471137951\n",
      "Episode 19046; Testing Loss 0.005767538604846247; Training Loss 0.004613963827701217\n",
      "Episode 19047; Testing Loss 0.005767488055038308; Training Loss 0.0046139555230595335\n",
      "Episode 19048; Testing Loss 0.005767363986401045; Training Loss 0.004613947192686397\n",
      "Episode 19049; Testing Loss 0.0057673374153484145; Training Loss 0.0046139394945311045\n",
      "Episode 19050; Testing Loss 0.005767338533639901; Training Loss 0.004613932284351829\n",
      "Episode 19051; Testing Loss 0.005767319205044096; Training Loss 0.004613924725019833\n",
      "Episode 19052; Testing Loss 0.005767316936075006; Training Loss 0.004613916606449981\n",
      "Episode 19053; Testing Loss 0.005767378426164074; Training Loss 0.004613907933352273\n",
      "Episode 19054; Testing Loss 0.005767304809935171; Training Loss 0.004613900964313313\n",
      "Episode 19055; Testing Loss 0.005767367114653438; Training Loss 0.004613893521437019\n",
      "Episode 19056; Testing Loss 0.0057673749210993925; Training Loss 0.004613884886439192\n",
      "Episode 19057; Testing Loss 0.005767363785711905; Training Loss 0.004613877181972219\n",
      "Episode 19058; Testing Loss 0.005767209170246912; Training Loss 0.004613870247509669\n",
      "Episode 19059; Testing Loss 0.005767249721434867; Training Loss 0.004613863916292862\n",
      "Episode 19060; Testing Loss 0.005767232148909861; Training Loss 0.004613856289073679\n",
      "Episode 19061; Testing Loss 0.005767153287864364; Training Loss 0.004613847370102744\n",
      "Episode 19062; Testing Loss 0.0057672634870170915; Training Loss 0.0046138388910829855\n",
      "Episode 19063; Testing Loss 0.005767439412923852; Training Loss 0.004613832275204285\n",
      "Episode 19064; Testing Loss 0.0057673826673466135; Training Loss 0.00461382336936174\n",
      "Episode 19065; Testing Loss 0.005767168140928644; Training Loss 0.0046138162786603576\n",
      "Episode 19066; Testing Loss 0.005767199428584957; Training Loss 0.004613807645115435\n",
      "Episode 19067; Testing Loss 0.005767303635467343; Training Loss 0.004613800428597221\n",
      "Episode 19068; Testing Loss 0.005767287754068758; Training Loss 0.004613792252004854\n",
      "Episode 19069; Testing Loss 0.0057671856574521485; Training Loss 0.004613784757557316\n",
      "Episode 19070; Testing Loss 0.005767153729738585; Training Loss 0.004613777713424674\n",
      "Episode 19071; Testing Loss 0.0057672843892217825; Training Loss 0.004613769784694391\n",
      "Episode 19072; Testing Loss 0.005767419497273454; Training Loss 0.004613763221497059\n",
      "Episode 19073; Testing Loss 0.005767197131485917; Training Loss 0.004613753550633742\n",
      "Episode 19074; Testing Loss 0.00576699961056765; Training Loss 0.004613748346041823\n",
      "Episode 19075; Testing Loss 0.00576718808290113; Training Loss 0.0046137377474744085\n",
      "Episode 19076; Testing Loss 0.005767369465149748; Training Loss 0.00461373163446986\n",
      "Episode 19077; Testing Loss 0.005767255500950089; Training Loss 0.0046137223064905185\n",
      "Episode 19078; Testing Loss 0.005767143540308886; Training Loss 0.004613715414351202\n",
      "Episode 19079; Testing Loss 0.005767219052006414; Training Loss 0.004613706743622461\n",
      "Episode 19080; Testing Loss 0.005767322646518238; Training Loss 0.004613700044105712\n",
      "Episode 19081; Testing Loss 0.0057672715639849; Training Loss 0.004613690956382523\n",
      "Episode 19082; Testing Loss 0.00576707664942195; Training Loss 0.004613683868733145\n",
      "Episode 19083; Testing Loss 0.005767156936543211; Training Loss 0.004613676392743119\n",
      "Episode 19084; Testing Loss 0.0057671949236870685; Training Loss 0.004613668778703299\n",
      "Episode 19085; Testing Loss 0.0057671253022360015; Training Loss 0.0046136598264649555\n",
      "Episode 19086; Testing Loss 0.0057671467069204145; Training Loss 0.004613651765239624\n",
      "Episode 19087; Testing Loss 0.005767165224701344; Training Loss 0.004613644855711495\n",
      "Episode 19088; Testing Loss 0.005767206602517998; Training Loss 0.0046136369161829185\n",
      "Episode 19089; Testing Loss 0.005767152666438557; Training Loss 0.0046136295134982745\n",
      "Episode 19090; Testing Loss 0.005767037772699382; Training Loss 0.004613622191854939\n",
      "Episode 19091; Testing Loss 0.0057671287327530265; Training Loss 0.004613613545959618\n",
      "Episode 19092; Testing Loss 0.0057672619214554745; Training Loss 0.004613606923028673\n",
      "Episode 19093; Testing Loss 0.00576718231192101; Training Loss 0.0046135981368097\n",
      "Episode 19094; Testing Loss 0.005767058764155854; Training Loss 0.0046135892961228575\n",
      "Episode 19095; Testing Loss 0.005767117390329437; Training Loss 0.004613583076209239\n",
      "Episode 19096; Testing Loss 0.005767124692255134; Training Loss 0.004613576368734251\n",
      "Episode 19097; Testing Loss 0.005767039398092562; Training Loss 0.004613567405000582\n",
      "Episode 19098; Testing Loss 0.005767057560984445; Training Loss 0.004613557746525723\n",
      "Episode 19099; Testing Loss 0.005767181232382528; Training Loss 0.004613551507446605\n",
      "Episode 19100; Testing Loss 0.005767333903238905; Training Loss 0.004613543378070653\n",
      "Episode 19101; Testing Loss 0.0057672154805287505; Training Loss 0.004613535184431701\n",
      "Episode 19102; Testing Loss 0.005766978732309207; Training Loss 0.004613529333110704\n",
      "Episode 19103; Testing Loss 0.005767009131851777; Training Loss 0.004613519355646556\n",
      "Episode 19104; Testing Loss 0.005767159309789925; Training Loss 0.004613513239547055\n",
      "Episode 19105; Testing Loss 0.005767198808649299; Training Loss 0.0046135071958887134\n",
      "Episode 19106; Testing Loss 0.005767160028695148; Training Loss 0.004613495698469582\n",
      "Episode 19107; Testing Loss 0.005767054581488796; Training Loss 0.004613489157037673\n",
      "Episode 19108; Testing Loss 0.005767051823049989; Training Loss 0.00461348268420098\n",
      "Episode 19109; Testing Loss 0.005767164005276909; Training Loss 0.004613474455860239\n",
      "Episode 19110; Testing Loss 0.0057671390298176545; Training Loss 0.004613464782982577\n",
      "Episode 19111; Testing Loss 0.005767072995829988; Training Loss 0.004613456579143108\n",
      "Episode 19112; Testing Loss 0.005767118712158747; Training Loss 0.0046134489677754605\n",
      "Episode 19113; Testing Loss 0.005767243045724636; Training Loss 0.004613441298121997\n",
      "Episode 19114; Testing Loss 0.00576725214863525; Training Loss 0.004613433341781251\n",
      "Episode 19115; Testing Loss 0.005767084644076443; Training Loss 0.004613424820507073\n",
      "Episode 19116; Testing Loss 0.005767055334001023; Training Loss 0.004613416943746446\n",
      "Episode 19117; Testing Loss 0.005767108392803433; Training Loss 0.0046134089272497885\n",
      "Episode 19118; Testing Loss 0.005767074462310453; Training Loss 0.004613402734852703\n",
      "Episode 19119; Testing Loss 0.005767104805711023; Training Loss 0.004613394997550919\n",
      "Episode 19120; Testing Loss 0.0057671481898579615; Training Loss 0.004613385335645777\n",
      "Episode 19121; Testing Loss 0.005767177597463231; Training Loss 0.004613377417628733\n",
      "Episode 19122; Testing Loss 0.005767099209116135; Training Loss 0.004613370577499261\n",
      "Episode 19123; Testing Loss 0.0057670524146165355; Training Loss 0.004613361718735914\n",
      "Episode 19124; Testing Loss 0.005767094086782672; Training Loss 0.004613353030543005\n",
      "Episode 19125; Testing Loss 0.005767107701526607; Training Loss 0.004613346709371684\n",
      "Episode 19126; Testing Loss 0.005766980322483249; Training Loss 0.004613338101570174\n",
      "Episode 19127; Testing Loss 0.00576700155639805; Training Loss 0.004613331261015591\n",
      "Episode 19128; Testing Loss 0.005767178582952415; Training Loss 0.004613322912488955\n",
      "Episode 19129; Testing Loss 0.00576722928210522; Training Loss 0.004613314454726582\n",
      "Episode 19130; Testing Loss 0.005767130062719247; Training Loss 0.004613307006696201\n",
      "Episode 19131; Testing Loss 0.005767027031565015; Training Loss 0.004613299137501983\n",
      "Episode 19132; Testing Loss 0.005767049871241843; Training Loss 0.004613291101772503\n",
      "Episode 19133; Testing Loss 0.005767152469989195; Training Loss 0.004613284187160311\n",
      "Episode 19134; Testing Loss 0.005767107734540471; Training Loss 0.0046132758725707795\n",
      "Episode 19135; Testing Loss 0.005766931043005298; Training Loss 0.00461326836758417\n",
      "Episode 19136; Testing Loss 0.0057670106254776084; Training Loss 0.004613260396196202\n",
      "Episode 19137; Testing Loss 0.005767166131797076; Training Loss 0.0046132524499252125\n",
      "Episode 19138; Testing Loss 0.005767182382639508; Training Loss 0.0046132446693499625\n",
      "Episode 19139; Testing Loss 0.005767111658766458; Training Loss 0.004613236484292074\n",
      "Episode 19140; Testing Loss 0.005767076058092516; Training Loss 0.0046132283199532866\n",
      "Episode 19141; Testing Loss 0.005766968630570298; Training Loss 0.004613221550141863\n",
      "Episode 19142; Testing Loss 0.005766951731443203; Training Loss 0.004613212728692918\n",
      "Episode 19143; Testing Loss 0.005767055387262523; Training Loss 0.004613205743747267\n",
      "Episode 19144; Testing Loss 0.0057671434493770535; Training Loss 0.0046131981592289805\n",
      "Episode 19145; Testing Loss 0.005767100641659173; Training Loss 0.004613189010289004\n",
      "Episode 19146; Testing Loss 0.0057670090611681496; Training Loss 0.004613182797337348\n",
      "Episode 19147; Testing Loss 0.005767005281924025; Training Loss 0.0046131743161763146\n",
      "Episode 19148; Testing Loss 0.005767057280615001; Training Loss 0.004613165285130639\n",
      "Episode 19149; Testing Loss 0.005766977101588219; Training Loss 0.0046131599472480815\n",
      "Episode 19150; Testing Loss 0.005766950617364421; Training Loss 0.004613153634115645\n",
      "Episode 19151; Testing Loss 0.005767063638467761; Training Loss 0.004613141982062769\n",
      "Episode 19152; Testing Loss 0.005767032997275091; Training Loss 0.004613136551829725\n",
      "Episode 19153; Testing Loss 0.005766921125720684; Training Loss 0.004613131211682599\n",
      "Episode 19154; Testing Loss 0.005766954953563094; Training Loss 0.004613123179220112\n",
      "Episode 19155; Testing Loss 0.005767048902516628; Training Loss 0.004613112895048917\n",
      "Episode 19156; Testing Loss 0.0057670218324446494; Training Loss 0.004613103383639464\n",
      "Episode 19157; Testing Loss 0.005766965364054031; Training Loss 0.004613097174675443\n",
      "Episode 19158; Testing Loss 0.005766992953548893; Training Loss 0.004613089113779166\n",
      "Episode 19159; Testing Loss 0.005767071994165835; Training Loss 0.004613079855190859\n",
      "Episode 19160; Testing Loss 0.00576707440971708; Training Loss 0.0046130718702080045\n",
      "Episode 19161; Testing Loss 0.005767000617934057; Training Loss 0.004613065769566989\n",
      "Episode 19162; Testing Loss 0.005767029760067341; Training Loss 0.004613056475558602\n",
      "Episode 19163; Testing Loss 0.005767085302682567; Training Loss 0.004613049052349485\n",
      "Episode 19164; Testing Loss 0.005767068090482433; Training Loss 0.004613042751160637\n",
      "Episode 19165; Testing Loss 0.005766963078845329; Training Loss 0.004613034745321772\n",
      "Episode 19166; Testing Loss 0.005766916097942413; Training Loss 0.004613025894554963\n",
      "Episode 19167; Testing Loss 0.0057669657574466755; Training Loss 0.004613018875073377\n",
      "Episode 19168; Testing Loss 0.005766952868814176; Training Loss 0.004613011077336067\n",
      "Episode 19169; Testing Loss 0.005766976295974429; Training Loss 0.004613003093293068\n",
      "Episode 19170; Testing Loss 0.005767064233243147; Training Loss 0.004612995261173162\n",
      "Episode 19171; Testing Loss 0.005767139013519934; Training Loss 0.004612988030021575\n",
      "Episode 19172; Testing Loss 0.005767035005698667; Training Loss 0.004612979075908512\n",
      "Episode 19173; Testing Loss 0.005766848584272971; Training Loss 0.004612971071806685\n",
      "Episode 19174; Testing Loss 0.0057668696278797785; Training Loss 0.00461296428805186\n",
      "Episode 19175; Testing Loss 0.005766907564485517; Training Loss 0.004612957239291697\n",
      "Episode 19176; Testing Loss 0.005766849295541324; Training Loss 0.004612948219089284\n",
      "Episode 19177; Testing Loss 0.005766866717036896; Training Loss 0.0046129392554987705\n",
      "Episode 19178; Testing Loss 0.00576699740029159; Training Loss 0.004612933015107872\n",
      "Episode 19179; Testing Loss 0.005767132443947739; Training Loss 0.004612925143397568\n",
      "Episode 19180; Testing Loss 0.005766975287209992; Training Loss 0.004612914726424965\n",
      "Episode 19181; Testing Loss 0.0057667618059961115; Training Loss 0.004612909594874133\n",
      "Episode 19182; Testing Loss 0.005766777941919091; Training Loss 0.004612901471200168\n",
      "Episode 19183; Testing Loss 0.005766941724418074; Training Loss 0.0046128929112220305\n",
      "Episode 19184; Testing Loss 0.005766932350872597; Training Loss 0.004612883463503893\n",
      "Episode 19185; Testing Loss 0.005766882071283019; Training Loss 0.00461287590007375\n",
      "Episode 19186; Testing Loss 0.005766883485720466; Training Loss 0.004612867317727348\n",
      "Episode 19187; Testing Loss 0.005766851582772716; Training Loss 0.004612859952938906\n",
      "Episode 19188; Testing Loss 0.005766911667625452; Training Loss 0.004612851942786758\n",
      "Episode 19189; Testing Loss 0.005766924696523995; Training Loss 0.004612843806500706\n",
      "Episode 19190; Testing Loss 0.005767006305982924; Training Loss 0.0046128358688679065\n",
      "Episode 19191; Testing Loss 0.005766999883303957; Training Loss 0.0046128277340179704\n",
      "Episode 19192; Testing Loss 0.005766921332680293; Training Loss 0.00461281932200959\n",
      "Episode 19193; Testing Loss 0.00576694013254531; Training Loss 0.004612810469157458\n",
      "Episode 19194; Testing Loss 0.00576695989703202; Training Loss 0.004612803757154575\n",
      "Episode 19195; Testing Loss 0.005766983600459701; Training Loss 0.00461279555224476\n",
      "Episode 19196; Testing Loss 0.005766880004664786; Training Loss 0.004612787380695313\n",
      "Episode 19197; Testing Loss 0.005766867518192257; Training Loss 0.004612779399376377\n",
      "Episode 19198; Testing Loss 0.005766899307664015; Training Loss 0.004612771900900554\n",
      "Episode 19199; Testing Loss 0.005766940985372582; Training Loss 0.004612763604110187\n",
      "Episode 19200; Testing Loss 0.005766947033079187; Training Loss 0.004612755307581947\n",
      "Episode 19201; Testing Loss 0.005766878707404673; Training Loss 0.0046127471891387484\n",
      "Episode 19202; Testing Loss 0.005766911384726553; Training Loss 0.004612739294145398\n",
      "Episode 19203; Testing Loss 0.005766918019528834; Training Loss 0.004612730853086332\n",
      "Episode 19204; Testing Loss 0.005766880183081048; Training Loss 0.004612723508981889\n",
      "Episode 19205; Testing Loss 0.00576692777867383; Training Loss 0.004612715059792315\n",
      "Episode 19206; Testing Loss 0.005766938983833136; Training Loss 0.004612706407872296\n",
      "Episode 19207; Testing Loss 0.0057669019332275435; Training Loss 0.0046126998405842355\n",
      "Episode 19208; Testing Loss 0.00576675659853138; Training Loss 0.004612692189236965\n",
      "Episode 19209; Testing Loss 0.005766741661679153; Training Loss 0.004612682824197248\n",
      "Episode 19210; Testing Loss 0.005766867807617595; Training Loss 0.004612676511730712\n",
      "Episode 19211; Testing Loss 0.005767078481835246; Training Loss 0.0046126678901731\n",
      "Episode 19212; Testing Loss 0.0057669911392852526; Training Loss 0.004612659432056718\n",
      "Episode 19213; Testing Loss 0.0057667513287170845; Training Loss 0.0046126523140691865\n",
      "Episode 19214; Testing Loss 0.005766775992482846; Training Loss 0.004612642524752937\n",
      "Episode 19215; Testing Loss 0.005766939136501165; Training Loss 0.0046126353450462254\n",
      "Episode 19216; Testing Loss 0.0057669416413208266; Training Loss 0.0046126271918221955\n",
      "Episode 19217; Testing Loss 0.005766836836952308; Training Loss 0.0046126176971397065\n",
      "Episode 19218; Testing Loss 0.005766792443293591; Training Loss 0.004612609998212174\n",
      "Episode 19219; Testing Loss 0.005766879069186301; Training Loss 0.004612601836256478\n",
      "Episode 19220; Testing Loss 0.00576701666228559; Training Loss 0.004612593384800851\n",
      "Episode 19221; Testing Loss 0.005767005070626207; Training Loss 0.00461258522644571\n",
      "Episode 19222; Testing Loss 0.005766842145652188; Training Loss 0.0046125759851601425\n",
      "Episode 19223; Testing Loss 0.00576671585428736; Training Loss 0.004612569146872832\n",
      "Episode 19224; Testing Loss 0.00576677644777474; Training Loss 0.004612560785081125\n",
      "Episode 19225; Testing Loss 0.0057668944003475; Training Loss 0.004612552290224039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19226; Testing Loss 0.005766846448073747; Training Loss 0.004612543631164973\n",
      "Episode 19227; Testing Loss 0.005766848758285924; Training Loss 0.004612537932855711\n",
      "Episode 19228; Testing Loss 0.005767022320054006; Training Loss 0.004612527039566864\n",
      "Episode 19229; Testing Loss 0.005766965902519436; Training Loss 0.004612519170815992\n",
      "Episode 19230; Testing Loss 0.005766792770912796; Training Loss 0.00461250999537719\n",
      "Episode 19231; Testing Loss 0.0057667649912704925; Training Loss 0.004612502402786754\n",
      "Episode 19232; Testing Loss 0.005766985155666048; Training Loss 0.004612493873595964\n",
      "Episode 19233; Testing Loss 0.005767040329010139; Training Loss 0.004612485384176848\n",
      "Episode 19234; Testing Loss 0.005766846983862445; Training Loss 0.004612475808432471\n",
      "Episode 19235; Testing Loss 0.005766720508888879; Training Loss 0.004612468579795554\n",
      "Episode 19236; Testing Loss 0.005766860855025875; Training Loss 0.004612459004205298\n",
      "Episode 19237; Testing Loss 0.005766956130903922; Training Loss 0.004612452133723226\n",
      "Episode 19238; Testing Loss 0.0057667801392545905; Training Loss 0.004612443004147707\n",
      "Episode 19239; Testing Loss 0.00576671527072249; Training Loss 0.004612436180381683\n",
      "Episode 19240; Testing Loss 0.005766920504413833; Training Loss 0.004612426625953964\n",
      "Episode 19241; Testing Loss 0.0057669949271254295; Training Loss 0.004612419239579881\n",
      "Episode 19242; Testing Loss 0.005766810748488203; Training Loss 0.004612410648407374\n",
      "Episode 19243; Testing Loss 0.005766784706230048; Training Loss 0.004612403100069064\n",
      "Episode 19244; Testing Loss 0.005766959386684953; Training Loss 0.004612393389961663\n",
      "Episode 19245; Testing Loss 0.005767063000442935; Training Loss 0.004612386510429212\n",
      "Episode 19246; Testing Loss 0.005766867966662914; Training Loss 0.004612376195055509\n",
      "Episode 19247; Testing Loss 0.005766716150931595; Training Loss 0.0046123710004418\n",
      "Episode 19248; Testing Loss 0.005766860085353782; Training Loss 0.0046123616835029215\n",
      "Episode 19249; Testing Loss 0.005767012784423306; Training Loss 0.004612353354156535\n",
      "Episode 19250; Testing Loss 0.005766917369517529; Training Loss 0.004612345224438769\n",
      "Episode 19251; Testing Loss 0.0057667858395427435; Training Loss 0.004612338543767648\n",
      "Episode 19252; Testing Loss 0.005766858703748567; Training Loss 0.004612328202769021\n",
      "Episode 19253; Testing Loss 0.005766972620887676; Training Loss 0.004612321717749197\n",
      "Episode 19254; Testing Loss 0.005766877713753403; Training Loss 0.00461231426287372\n",
      "Episode 19255; Testing Loss 0.005766765098127257; Training Loss 0.004612304691527139\n",
      "Episode 19256; Testing Loss 0.005766748441107753; Training Loss 0.004612297839375663\n",
      "Episode 19257; Testing Loss 0.005766814084526358; Training Loss 0.004612289810086901\n",
      "Episode 19258; Testing Loss 0.005766873208432117; Training Loss 0.004612280872421369\n",
      "Episode 19259; Testing Loss 0.005766860504357982; Training Loss 0.004612271540359834\n",
      "Episode 19260; Testing Loss 0.005766842380892675; Training Loss 0.004612264357946791\n",
      "Episode 19261; Testing Loss 0.005766869698199293; Training Loss 0.004612257606737105\n",
      "Episode 19262; Testing Loss 0.005766898113622619; Training Loss 0.0046122477628298165\n",
      "Episode 19263; Testing Loss 0.005766828475512371; Training Loss 0.004612240470534472\n",
      "Episode 19264; Testing Loss 0.005766694449550943; Training Loss 0.004612234227228979\n",
      "Episode 19265; Testing Loss 0.005766790137531324; Training Loss 0.004612224268044633\n",
      "Episode 19266; Testing Loss 0.005766935636078255; Training Loss 0.004612215253858682\n",
      "Episode 19267; Testing Loss 0.0057669301971434595; Training Loss 0.0046122081066869445\n",
      "Episode 19268; Testing Loss 0.005766897370835727; Training Loss 0.004612200771299435\n",
      "Episode 19269; Testing Loss 0.00576687412698362; Training Loss 0.004612191184638255\n",
      "Episode 19270; Testing Loss 0.005766861456735667; Training Loss 0.004612181599476096\n",
      "Episode 19271; Testing Loss 0.005766807735727049; Training Loss 0.004612174626820587\n",
      "Episode 19272; Testing Loss 0.005766775508651972; Training Loss 0.004612167332976109\n",
      "Episode 19273; Testing Loss 0.00576681205019993; Training Loss 0.004612156991094142\n",
      "Episode 19274; Testing Loss 0.00576690430646079; Training Loss 0.0046121495469748945\n",
      "Episode 19275; Testing Loss 0.005767032908153217; Training Loss 0.0046121430385268215\n",
      "Episode 19276; Testing Loss 0.005766971527935645; Training Loss 0.004612133755675742\n",
      "Episode 19277; Testing Loss 0.005766837291352303; Training Loss 0.004612124187602997\n",
      "Episode 19278; Testing Loss 0.005766810305832091; Training Loss 0.004612115776299902\n",
      "Episode 19279; Testing Loss 0.0057667778500466215; Training Loss 0.004612108022580357\n",
      "Episode 19280; Testing Loss 0.005766773565907959; Training Loss 0.00461209906297988\n",
      "Episode 19281; Testing Loss 0.005766864562718795; Training Loss 0.004612091137246769\n",
      "Episode 19282; Testing Loss 0.005766927188891727; Training Loss 0.004612082882506973\n",
      "Episode 19283; Testing Loss 0.00576679145972043; Training Loss 0.0046120732995170565\n",
      "Episode 19284; Testing Loss 0.005766730094013709; Training Loss 0.004612064984887203\n",
      "Episode 19285; Testing Loss 0.005766794499324794; Training Loss 0.004612056059060165\n",
      "Episode 19286; Testing Loss 0.0057668155738095535; Training Loss 0.004612048668241581\n",
      "Episode 19287; Testing Loss 0.005766887300552172; Training Loss 0.00461204027420491\n",
      "Episode 19288; Testing Loss 0.005766871532502835; Training Loss 0.00461203144185286\n",
      "Episode 19289; Testing Loss 0.005766744544682084; Training Loss 0.004612023105003506\n",
      "Episode 19290; Testing Loss 0.005766692538717007; Training Loss 0.00461201448781681\n",
      "Episode 19291; Testing Loss 0.0057667884575829395; Training Loss 0.00461200543980442\n",
      "Episode 19292; Testing Loss 0.005766924172152386; Training Loss 0.0046119983013918975\n",
      "Episode 19293; Testing Loss 0.00576682241875041; Training Loss 0.004611988354999967\n",
      "Episode 19294; Testing Loss 0.005766631884484761; Training Loss 0.004611981738087199\n",
      "Episode 19295; Testing Loss 0.0057667510242667645; Training Loss 0.0046119719969938255\n",
      "Episode 19296; Testing Loss 0.005766820220774441; Training Loss 0.004611964415045997\n",
      "Episode 19297; Testing Loss 0.005766772062097978; Training Loss 0.004611955366857591\n",
      "Episode 19298; Testing Loss 0.005766729485380289; Training Loss 0.004611947661197722\n",
      "Episode 19299; Testing Loss 0.005766707323484589; Training Loss 0.0046119384851930805\n",
      "Episode 19300; Testing Loss 0.005766815740920808; Training Loss 0.004611931048349204\n",
      "Episode 19301; Testing Loss 0.005766983895421718; Training Loss 0.004611924011609979\n",
      "Episode 19302; Testing Loss 0.005766891767351867; Training Loss 0.004611914237369208\n",
      "Episode 19303; Testing Loss 0.00576671551595207; Training Loss 0.004611905837707247\n",
      "Episode 19304; Testing Loss 0.005766685135495351; Training Loss 0.004611898571392421\n",
      "Episode 19305; Testing Loss 0.00576680796003142; Training Loss 0.004611890236705077\n",
      "Episode 19306; Testing Loss 0.00576683840325593; Training Loss 0.004611882911323187\n",
      "Episode 19307; Testing Loss 0.0057666720250806255; Training Loss 0.004611874049300726\n",
      "Episode 19308; Testing Loss 0.005766619842677176; Training Loss 0.004611865991687448\n",
      "Episode 19309; Testing Loss 0.00576682493415746; Training Loss 0.004611858051152971\n",
      "Episode 19310; Testing Loss 0.005766870932933451; Training Loss 0.0046118487258631925\n",
      "Episode 19311; Testing Loss 0.005766782360696262; Training Loss 0.004611842287537243\n",
      "Episode 19312; Testing Loss 0.005766837168052676; Training Loss 0.004611835692658454\n",
      "Episode 19313; Testing Loss 0.005766980402882208; Training Loss 0.0046118263199060455\n",
      "Episode 19314; Testing Loss 0.00576684588952625; Training Loss 0.0046118181574563\n",
      "Episode 19315; Testing Loss 0.005766574397796786; Training Loss 0.004611811078381405\n",
      "Episode 19316; Testing Loss 0.005766591491339751; Training Loss 0.004611802752132901\n",
      "Episode 19317; Testing Loss 0.005766797709069708; Training Loss 0.004611793921613803\n",
      "Episode 19318; Testing Loss 0.005766841975702137; Training Loss 0.004611786458710312\n",
      "Episode 19319; Testing Loss 0.005766735034625778; Training Loss 0.004611779125635333\n",
      "Episode 19320; Testing Loss 0.005766785455079954; Training Loss 0.004611767564925644\n",
      "Episode 19321; Testing Loss 0.005766895550600525; Training Loss 0.004611763859837617\n",
      "Episode 19322; Testing Loss 0.005766827875941612; Training Loss 0.004611757293218253\n",
      "Episode 19323; Testing Loss 0.005766707740106456; Training Loss 0.0046117484341687736\n",
      "Episode 19324; Testing Loss 0.005766680813226914; Training Loss 0.0046117371143357365\n",
      "Episode 19325; Testing Loss 0.005766706341553007; Training Loss 0.004611729143469908\n",
      "Episode 19326; Testing Loss 0.005766765629800675; Training Loss 0.0046117230379479\n",
      "Episode 19327; Testing Loss 0.00576670559528726; Training Loss 0.0046117153945559606\n",
      "Episode 19328; Testing Loss 0.005766668308217663; Training Loss 0.004611705033621656\n",
      "Episode 19329; Testing Loss 0.005766648184723221; Training Loss 0.004611698642383608\n",
      "Episode 19330; Testing Loss 0.00576666375233159; Training Loss 0.004611691871572921\n",
      "Episode 19331; Testing Loss 0.0057667573845055645; Training Loss 0.004611684303009621\n",
      "Episode 19332; Testing Loss 0.005766845982207122; Training Loss 0.004611675038719493\n",
      "Episode 19333; Testing Loss 0.005766810867371989; Training Loss 0.0046116653051144875\n",
      "Episode 19334; Testing Loss 0.005766796117740946; Training Loss 0.004611658194853747\n",
      "Episode 19335; Testing Loss 0.005766802006111795; Training Loss 0.00461165008187329\n",
      "Episode 19336; Testing Loss 0.005766773150400216; Training Loss 0.004611641033812823\n",
      "Episode 19337; Testing Loss 0.00576676207528668; Training Loss 0.004611632359719634\n",
      "Episode 19338; Testing Loss 0.005766710877646286; Training Loss 0.004611625273850671\n",
      "Episode 19339; Testing Loss 0.005766694585792991; Training Loss 0.004611616779931304\n",
      "Episode 19340; Testing Loss 0.0057667359908813685; Training Loss 0.004611608504065084\n",
      "Episode 19341; Testing Loss 0.005766698851998145; Training Loss 0.00461160008086785\n",
      "Episode 19342; Testing Loss 0.005766659819678448; Training Loss 0.004611593003685031\n",
      "Episode 19343; Testing Loss 0.005766618963865807; Training Loss 0.004611585408496462\n",
      "Episode 19344; Testing Loss 0.005766615740157056; Training Loss 0.00461157634570717\n",
      "Episode 19345; Testing Loss 0.005766682582492049; Training Loss 0.004611569561739473\n",
      "Episode 19346; Testing Loss 0.005766797900979424; Training Loss 0.004611562195247817\n",
      "Episode 19347; Testing Loss 0.00576672746338194; Training Loss 0.004611553639842465\n",
      "Episode 19348; Testing Loss 0.005766557470518268; Training Loss 0.0046115459364353744\n",
      "Episode 19349; Testing Loss 0.0057665411554247295; Training Loss 0.004611538849460733\n",
      "Episode 19350; Testing Loss 0.0057665933439961774; Training Loss 0.004611529767730882\n",
      "Episode 19351; Testing Loss 0.00576662758050151; Training Loss 0.004611521157229403\n",
      "Episode 19352; Testing Loss 0.005766641458624555; Training Loss 0.004611514066746554\n",
      "Episode 19353; Testing Loss 0.005766635926985168; Training Loss 0.004611505417778118\n",
      "Episode 19354; Testing Loss 0.005766559607789853; Training Loss 0.004611497136962519\n",
      "Episode 19355; Testing Loss 0.005766463146355031; Training Loss 0.004611490051791375\n",
      "Episode 19356; Testing Loss 0.005766496044252316; Training Loss 0.0046114816581051795\n",
      "Episode 19357; Testing Loss 0.005766678092423408; Training Loss 0.004611473701793337\n",
      "Episode 19358; Testing Loss 0.005766694139437011; Training Loss 0.004611466317382451\n",
      "Episode 19359; Testing Loss 0.005766587310344667; Training Loss 0.004611458999418495\n",
      "Episode 19360; Testing Loss 0.0057666139538519266; Training Loss 0.004611451426171076\n",
      "Episode 19361; Testing Loss 0.005766738243334488; Training Loss 0.004611442906743874\n",
      "Episode 19362; Testing Loss 0.005766674096242068; Training Loss 0.0046114361482040315\n",
      "Episode 19363; Testing Loss 0.0057665120731131975; Training Loss 0.0046114273938433065\n",
      "Episode 19364; Testing Loss 0.005766455231047706; Training Loss 0.0046114189710692735\n",
      "Episode 19365; Testing Loss 0.005766531649275382; Training Loss 0.004611411669960609\n",
      "Episode 19366; Testing Loss 0.005766586727449174; Training Loss 0.004611401553602875\n",
      "Episode 19367; Testing Loss 0.005766622540656889; Training Loss 0.004611395874412687\n",
      "Episode 19368; Testing Loss 0.0057667367963140145; Training Loss 0.004611389108392374\n",
      "Episode 19369; Testing Loss 0.00576676548085774; Training Loss 0.004611379815402286\n",
      "Episode 19370; Testing Loss 0.005766583248195021; Training Loss 0.00461137196190723\n",
      "Episode 19371; Testing Loss 0.005766337378212755; Training Loss 0.004611365919859922\n",
      "Episode 19372; Testing Loss 0.005766365417610007; Training Loss 0.004611357055205743\n",
      "Episode 19373; Testing Loss 0.005766549287793042; Training Loss 0.004611348057093242\n",
      "Episode 19374; Testing Loss 0.005766569907676679; Training Loss 0.004611341844165287\n",
      "Episode 19375; Testing Loss 0.00576648248816575; Training Loss 0.004611336219995424\n",
      "Episode 19376; Testing Loss 0.005766492161113698; Training Loss 0.004611324427317761\n",
      "Episode 19377; Testing Loss 0.005766560801081885; Training Loss 0.004611317028878164\n",
      "Episode 19378; Testing Loss 0.005766435913681142; Training Loss 0.004611310089757676\n",
      "Episode 19379; Testing Loss 0.005766309702386179; Training Loss 0.004611303125481496\n",
      "Episode 19380; Testing Loss 0.005766419487998792; Training Loss 0.0046112926254362795\n",
      "Episode 19381; Testing Loss 0.005766624856995329; Training Loss 0.00461128529549542\n",
      "Episode 19382; Testing Loss 0.005766592845216485; Training Loss 0.004611276600159745\n",
      "Episode 19383; Testing Loss 0.005766421730159706; Training Loss 0.004611269410017131\n",
      "Episode 19384; Testing Loss 0.005766449938118154; Training Loss 0.004611261397024756\n",
      "Episode 19385; Testing Loss 0.005766529541121283; Training Loss 0.004611254916569818\n",
      "Episode 19386; Testing Loss 0.005766410293812529; Training Loss 0.004611245833805087\n",
      "Episode 19387; Testing Loss 0.005766345687920472; Training Loss 0.004611239493840559\n",
      "Episode 19388; Testing Loss 0.005766502465336458; Training Loss 0.004611230924910645\n",
      "Episode 19389; Testing Loss 0.005766623691671436; Training Loss 0.004611222521249076\n",
      "Episode 19390; Testing Loss 0.005766549264225217; Training Loss 0.004611214106410598\n",
      "Episode 19391; Testing Loss 0.005766484834377179; Training Loss 0.004611207814185252\n",
      "Episode 19392; Testing Loss 0.005766483704277015; Training Loss 0.004611199119903872\n",
      "Episode 19393; Testing Loss 0.005766479508326108; Training Loss 0.004611189632140025\n",
      "Episode 19394; Testing Loss 0.005766421026657211; Training Loss 0.004611182167167984\n",
      "Episode 19395; Testing Loss 0.005766351083908972; Training Loss 0.004611174155616517\n",
      "Episode 19396; Testing Loss 0.005766329435830003; Training Loss 0.0046111668625762395\n",
      "Episode 19397; Testing Loss 0.005766409074767384; Training Loss 0.004611158680244556\n",
      "Episode 19398; Testing Loss 0.005766466388866191; Training Loss 0.004611150891070738\n",
      "Episode 19399; Testing Loss 0.005766441857616666; Training Loss 0.004611143230573491\n",
      "Episode 19400; Testing Loss 0.005766403981327739; Training Loss 0.004611135138046491\n",
      "Episode 19401; Testing Loss 0.00576637043281638; Training Loss 0.004611127178596998\n",
      "Episode 19402; Testing Loss 0.005766396680169698; Training Loss 0.004611119774969157\n",
      "Episode 19403; Testing Loss 0.005766362183909651; Training Loss 0.004611112187248696\n",
      "Episode 19404; Testing Loss 0.005766363470422473; Training Loss 0.004611104326106836\n",
      "Episode 19405; Testing Loss 0.005766486639024491; Training Loss 0.004611096180574756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19406; Testing Loss 0.005766470711908943; Training Loss 0.004611090134178228\n",
      "Episode 19407; Testing Loss 0.005766255436432045; Training Loss 0.004611081585042648\n",
      "Episode 19408; Testing Loss 0.0057662202936934245; Training Loss 0.004611073361570458\n",
      "Episode 19409; Testing Loss 0.0057663583463027895; Training Loss 0.004611066326486065\n",
      "Episode 19410; Testing Loss 0.005766365388211386; Training Loss 0.004611059477722281\n",
      "Episode 19411; Testing Loss 0.005766156472478067; Training Loss 0.004611050861151678\n",
      "Episode 19412; Testing Loss 0.005766196959156771; Training Loss 0.004611042309553032\n",
      "Episode 19413; Testing Loss 0.005766382611001838; Training Loss 0.004611034338991266\n",
      "Episode 19414; Testing Loss 0.00576649648819939; Training Loss 0.004611027846038441\n",
      "Episode 19415; Testing Loss 0.005766342903232655; Training Loss 0.004611018202908013\n",
      "Episode 19416; Testing Loss 0.005766108467900625; Training Loss 0.004611012290438905\n",
      "Episode 19417; Testing Loss 0.005766205987334519; Training Loss 0.004611003247173639\n",
      "Episode 19418; Testing Loss 0.00576633549505216; Training Loss 0.004610997984455331\n",
      "Episode 19419; Testing Loss 0.00576621147208852; Training Loss 0.0046109875267978636\n",
      "Episode 19420; Testing Loss 0.005766118282237284; Training Loss 0.004610982417359076\n",
      "Episode 19421; Testing Loss 0.00576633977896984; Training Loss 0.004610972723206683\n",
      "Episode 19422; Testing Loss 0.0057663857643379494; Training Loss 0.004610965854837954\n",
      "Episode 19423; Testing Loss 0.00576620659595212; Training Loss 0.004610957593598659\n",
      "Episode 19424; Testing Loss 0.005766207571298598; Training Loss 0.0046109490522419725\n",
      "Episode 19425; Testing Loss 0.00576633435906421; Training Loss 0.0046109444334832\n",
      "Episode 19426; Testing Loss 0.0057661709882324335; Training Loss 0.0046109357374359205\n",
      "Episode 19427; Testing Loss 0.005766017336962792; Training Loss 0.004610928076619691\n",
      "Episode 19428; Testing Loss 0.00576617141776516; Training Loss 0.004610917874888709\n",
      "Episode 19429; Testing Loss 0.005766347124065392; Training Loss 0.0046109122274283175\n",
      "Episode 19430; Testing Loss 0.005766208842378758; Training Loss 0.004610901920102887\n",
      "Episode 19431; Testing Loss 0.005765982158877566; Training Loss 0.004610896350530533\n",
      "Episode 19432; Testing Loss 0.005766101910111977; Training Loss 0.0046108870491896395\n",
      "Episode 19433; Testing Loss 0.0057662148478270225; Training Loss 0.004610881617961497\n",
      "Episode 19434; Testing Loss 0.005766088101915812; Training Loss 0.004610871525743247\n",
      "Episode 19435; Testing Loss 0.005765988289338263; Training Loss 0.004610865914226641\n",
      "Episode 19436; Testing Loss 0.005766193175597669; Training Loss 0.00461085695422817\n",
      "Episode 19437; Testing Loss 0.005766409146724327; Training Loss 0.004610851933061841\n",
      "Episode 19438; Testing Loss 0.005766147937951742; Training Loss 0.004610840429106292\n",
      "Episode 19439; Testing Loss 0.005765829922574636; Training Loss 0.004610837083625147\n",
      "Episode 19440; Testing Loss 0.005766041915547646; Training Loss 0.004610824942080875\n",
      "Episode 19441; Testing Loss 0.0057662466900410695; Training Loss 0.004610819519869521\n",
      "Episode 19442; Testing Loss 0.005766077414883328; Training Loss 0.004610809347093873\n",
      "Episode 19443; Testing Loss 0.0057659102879585354; Training Loss 0.004610802435457974\n",
      "Episode 19444; Testing Loss 0.005766033213125645; Training Loss 0.0046107942506238234\n",
      "Episode 19445; Testing Loss 0.005766224864949299; Training Loss 0.004610787915323396\n",
      "Episode 19446; Testing Loss 0.0057661170221814245; Training Loss 0.004610778365409398\n",
      "Episode 19447; Testing Loss 0.005765955288675087; Training Loss 0.0046107707900898826\n",
      "Episode 19448; Testing Loss 0.005765954407084226; Training Loss 0.004610762597486626\n",
      "Episode 19449; Testing Loss 0.005766046862171604; Training Loss 0.004610756675521688\n",
      "Episode 19450; Testing Loss 0.005766011319810901; Training Loss 0.004610748657410637\n",
      "Episode 19451; Testing Loss 0.005765934000973584; Training Loss 0.004610740202236913\n",
      "Episode 19452; Testing Loss 0.005766032731480962; Training Loss 0.004610732365455509\n",
      "Episode 19453; Testing Loss 0.005766147627726575; Training Loss 0.004610726368564842\n",
      "Episode 19454; Testing Loss 0.00576602595888581; Training Loss 0.004610716942688473\n",
      "Episode 19455; Testing Loss 0.0057658902989177726; Training Loss 0.004610709826067838\n",
      "Episode 19456; Testing Loss 0.005765958946181666; Training Loss 0.004610701879183538\n",
      "Episode 19457; Testing Loss 0.005766094877498275; Training Loss 0.00461069455758358\n",
      "Episode 19458; Testing Loss 0.005766024643928061; Training Loss 0.004610685781493643\n",
      "Episode 19459; Testing Loss 0.005765853947554209; Training Loss 0.00461067948284272\n",
      "Episode 19460; Testing Loss 0.005765910306391702; Training Loss 0.004610670360200655\n",
      "Episode 19461; Testing Loss 0.005765988073975549; Training Loss 0.004610662316215604\n",
      "Episode 19462; Testing Loss 0.005765937579904034; Training Loss 0.0046106547576381775\n",
      "Episode 19463; Testing Loss 0.005765802084079497; Training Loss 0.004610646509301462\n",
      "Episode 19464; Testing Loss 0.005765799440775106; Training Loss 0.004610638916686786\n",
      "Episode 19465; Testing Loss 0.0057659122004717015; Training Loss 0.004610631602178569\n",
      "Episode 19466; Testing Loss 0.005765924881595368; Training Loss 0.004610623381151167\n",
      "Episode 19467; Testing Loss 0.005765841128443871; Training Loss 0.004610619005801939\n",
      "Episode 19468; Testing Loss 0.005765969550187865; Training Loss 0.0046106087222969735\n",
      "Episode 19469; Testing Loss 0.0057659589777057955; Training Loss 0.004610600184540173\n",
      "Episode 19470; Testing Loss 0.005765771822818245; Training Loss 0.004610594011259415\n",
      "Episode 19471; Testing Loss 0.0057657953157639035; Training Loss 0.004610585186421814\n",
      "Episode 19472; Testing Loss 0.005765768079971749; Training Loss 0.004610578029302467\n",
      "Episode 19473; Testing Loss 0.0057657383237092906; Training Loss 0.004610570136791913\n",
      "Episode 19474; Testing Loss 0.0057658496107423; Training Loss 0.004610561694448235\n",
      "Episode 19475; Testing Loss 0.0057659210869283025; Training Loss 0.004610553573433679\n",
      "Episode 19476; Testing Loss 0.00576585555628785; Training Loss 0.004610547235892925\n",
      "Episode 19477; Testing Loss 0.005765726398431921; Training Loss 0.00461053946909669\n",
      "Episode 19478; Testing Loss 0.0057657120899349025; Training Loss 0.004610531275850018\n",
      "Episode 19479; Testing Loss 0.005765780688220986; Training Loss 0.004610524273596588\n",
      "Episode 19480; Testing Loss 0.005765686257638521; Training Loss 0.004610515359821523\n",
      "Episode 19481; Testing Loss 0.005765591657942851; Training Loss 0.0046105088722453785\n",
      "Episode 19482; Testing Loss 0.005765563546675038; Training Loss 0.0046105013564312625\n",
      "Episode 19483; Testing Loss 0.00576567458437952; Training Loss 0.004610491363125332\n",
      "Episode 19484; Testing Loss 0.005765789089554238; Training Loss 0.004610485347885625\n",
      "Episode 19485; Testing Loss 0.005765853527833988; Training Loss 0.004610476814394822\n",
      "Episode 19486; Testing Loss 0.005765762246564012; Training Loss 0.004610467617239373\n",
      "Episode 19487; Testing Loss 0.005765625425075078; Training Loss 0.004610462613870696\n",
      "Episode 19488; Testing Loss 0.005765568414797167; Training Loss 0.004610455922112608\n",
      "Episode 19489; Testing Loss 0.0057656719013124405; Training Loss 0.0046104462319513356\n",
      "Episode 19490; Testing Loss 0.005765728028598072; Training Loss 0.004610436276127285\n",
      "Episode 19491; Testing Loss 0.00576562392799471; Training Loss 0.00461043125409312\n",
      "Episode 19492; Testing Loss 0.005765683322260898; Training Loss 0.004610426545748797\n",
      "Episode 19493; Testing Loss 0.005765792431743043; Training Loss 0.00461041583773568\n",
      "Episode 19494; Testing Loss 0.005765682378350866; Training Loss 0.0046104064709115745\n",
      "Episode 19495; Testing Loss 0.005765410431606709; Training Loss 0.004610399475751465\n",
      "Episode 19496; Testing Loss 0.005765427445497284; Training Loss 0.004610390709584193\n",
      "Episode 19497; Testing Loss 0.005765706661041571; Training Loss 0.004610382680799051\n",
      "Episode 19498; Testing Loss 0.005765733153441905; Training Loss 0.004610375260944158\n",
      "Episode 19499; Testing Loss 0.005765610012176859; Training Loss 0.004610368659340106\n",
      "Episode 19500; Testing Loss 0.00576562819794056; Training Loss 0.004610360006641171\n",
      "Episode 19501; Testing Loss 0.005765703144954006; Training Loss 0.00461035237014354\n",
      "Episode 19502; Testing Loss 0.005765624302070421; Training Loss 0.004610343597898725\n",
      "Episode 19503; Testing Loss 0.005765428745396005; Training Loss 0.004610335764877695\n",
      "Episode 19504; Testing Loss 0.005765495059555655; Training Loss 0.00461032769444905\n",
      "Episode 19505; Testing Loss 0.005765599506857271; Training Loss 0.004610320008924886\n",
      "Episode 19506; Testing Loss 0.00576548495201521; Training Loss 0.0046103117857471355\n",
      "Episode 19507; Testing Loss 0.005765455814031742; Training Loss 0.004610303805891969\n",
      "Episode 19508; Testing Loss 0.005765541281840429; Training Loss 0.004610296858993655\n",
      "Episode 19509; Testing Loss 0.005765464327396841; Training Loss 0.004610288838631519\n",
      "Episode 19510; Testing Loss 0.005765397306739779; Training Loss 0.004610280904632754\n",
      "Episode 19511; Testing Loss 0.005765493774240747; Training Loss 0.004610272708194675\n",
      "Episode 19512; Testing Loss 0.0057655911776381685; Training Loss 0.0046102648415635465\n",
      "Episode 19513; Testing Loss 0.005765518663891375; Training Loss 0.00461025728025655\n",
      "Episode 19514; Testing Loss 0.005765412290682936; Training Loss 0.004610249520034923\n",
      "Episode 19515; Testing Loss 0.00576545495316904; Training Loss 0.004610241778627519\n",
      "Episode 19516; Testing Loss 0.005765578565560483; Training Loss 0.004610233909236115\n",
      "Episode 19517; Testing Loss 0.005765547277210997; Training Loss 0.004610225941038649\n",
      "Episode 19518; Testing Loss 0.005765367457351212; Training Loss 0.004610219225952398\n",
      "Episode 19519; Testing Loss 0.005765399817989447; Training Loss 0.004610209672093331\n",
      "Episode 19520; Testing Loss 0.005765457127474511; Training Loss 0.004610203941572061\n",
      "Episode 19521; Testing Loss 0.005765304766408954; Training Loss 0.004610194575528871\n",
      "Episode 19522; Testing Loss 0.005765283409613169; Training Loss 0.004610187324837902\n",
      "Episode 19523; Testing Loss 0.005765309714050394; Training Loss 0.004610179595744519\n",
      "Episode 19524; Testing Loss 0.005765436402849401; Training Loss 0.00461017149718296\n",
      "Episode 19525; Testing Loss 0.005765409146161539; Training Loss 0.0046101637555494394\n",
      "Episode 19526; Testing Loss 0.005765379947041021; Training Loss 0.004610155953017169\n",
      "Episode 19527; Testing Loss 0.005765427550143832; Training Loss 0.004610148101143841\n",
      "Episode 19528; Testing Loss 0.005765321520620636; Training Loss 0.004610139470945146\n",
      "Episode 19529; Testing Loss 0.005765247534576707; Training Loss 0.004610131987352886\n",
      "Episode 19530; Testing Loss 0.00576528743958576; Training Loss 0.0046101243484855964\n",
      "Episode 19531; Testing Loss 0.005765466182267615; Training Loss 0.004610120535374325\n",
      "Episode 19532; Testing Loss 0.005765302322595117; Training Loss 0.004610110325858239\n",
      "Episode 19533; Testing Loss 0.005765131276215262; Training Loss 0.004610102892892353\n",
      "Episode 19534; Testing Loss 0.005765268607302461; Training Loss 0.004610096194118871\n",
      "Episode 19535; Testing Loss 0.0057654803222199225; Training Loss 0.0046100888860174305\n",
      "Episode 19536; Testing Loss 0.00576533167223401; Training Loss 0.004610078631324042\n",
      "Episode 19537; Testing Loss 0.005765152825752772; Training Loss 0.004610071730744041\n",
      "Episode 19538; Testing Loss 0.0057652109756948745; Training Loss 0.004610063850405928\n",
      "Episode 19539; Testing Loss 0.005765353620104415; Training Loss 0.004610055390886635\n",
      "Episode 19540; Testing Loss 0.005765322517869839; Training Loss 0.004610048360352267\n",
      "Episode 19541; Testing Loss 0.005765125563217731; Training Loss 0.004610040266143093\n",
      "Episode 19542; Testing Loss 0.005765056784623296; Training Loss 0.00461003224537698\n",
      "Episode 19543; Testing Loss 0.005765187177235199; Training Loss 0.0046100245860302164\n",
      "Episode 19544; Testing Loss 0.005765252655493285; Training Loss 0.004610017568109307\n",
      "Episode 19545; Testing Loss 0.005765125327460862; Training Loss 0.004610008032434121\n",
      "Episode 19546; Testing Loss 0.005765086638095412; Training Loss 0.0046100034602806405\n",
      "Episode 19547; Testing Loss 0.0057652939004143285; Training Loss 0.004609993799941621\n",
      "Episode 19548; Testing Loss 0.0057653287134360924; Training Loss 0.004609986484343761\n",
      "Episode 19549; Testing Loss 0.005765081942673053; Training Loss 0.004609978722404793\n",
      "Episode 19550; Testing Loss 0.005765028300050263; Training Loss 0.004609971260561905\n",
      "Episode 19551; Testing Loss 0.005765269855194772; Training Loss 0.00460996225443496\n",
      "Episode 19552; Testing Loss 0.005765254309642708; Training Loss 0.004609955554079044\n",
      "Episode 19553; Testing Loss 0.005764997132968679; Training Loss 0.004609948394954291\n",
      "Episode 19554; Testing Loss 0.005764999315670155; Training Loss 0.004609940004306734\n",
      "Episode 19555; Testing Loss 0.005765220713524335; Training Loss 0.00460993268082588\n",
      "Episode 19556; Testing Loss 0.005765201710776313; Training Loss 0.004609924764192657\n",
      "Episode 19557; Testing Loss 0.005765026698523517; Training Loss 0.004609915491515823\n",
      "Episode 19558; Testing Loss 0.005765075986567405; Training Loss 0.004609907230936893\n",
      "Episode 19559; Testing Loss 0.0057651797237494385; Training Loss 0.004609900087631496\n",
      "Episode 19560; Testing Loss 0.005765046230271562; Training Loss 0.004609891601053299\n",
      "Episode 19561; Testing Loss 0.005764941396430143; Training Loss 0.00460988518394973\n",
      "Episode 19562; Testing Loss 0.00576506974812168; Training Loss 0.004609875826322277\n",
      "Episode 19563; Testing Loss 0.005765110082760939; Training Loss 0.004609868073439937\n",
      "Episode 19564; Testing Loss 0.005765059249212722; Training Loss 0.004609860838307413\n",
      "Episode 19565; Testing Loss 0.005765022480838159; Training Loss 0.004609852616631708\n",
      "Episode 19566; Testing Loss 0.0057650405791480445; Training Loss 0.004609845511028265\n",
      "Episode 19567; Testing Loss 0.00576505429947892; Training Loss 0.0046098386687691735\n",
      "Episode 19568; Testing Loss 0.005764975336900424; Training Loss 0.004609830613799332\n",
      "Episode 19569; Testing Loss 0.005764940118745351; Training Loss 0.004609822136885719\n",
      "Episode 19570; Testing Loss 0.005764998733719859; Training Loss 0.004609815352954198\n",
      "Episode 19571; Testing Loss 0.0057649809583412304; Training Loss 0.004609807779401442\n",
      "Episode 19572; Testing Loss 0.005764914129102; Training Loss 0.004609798746543588\n",
      "Episode 19573; Testing Loss 0.005764879723860752; Training Loss 0.004609791848566962\n",
      "Episode 19574; Testing Loss 0.005765025579243664; Training Loss 0.004609783088923256\n",
      "Episode 19575; Testing Loss 0.005764997606813682; Training Loss 0.00460977743217495\n",
      "Episode 19576; Testing Loss 0.005764807840139648; Training Loss 0.004609769832203498\n",
      "Episode 19577; Testing Loss 0.005764828125677706; Training Loss 0.004609760142333807\n",
      "Episode 19578; Testing Loss 0.005764956752379832; Training Loss 0.004609754149841646\n",
      "Episode 19579; Testing Loss 0.005764899408437375; Training Loss 0.004609746523255796\n",
      "Episode 19580; Testing Loss 0.005764788932420448; Training Loss 0.004609736652746178\n",
      "Episode 19581; Testing Loss 0.00576474011894194; Training Loss 0.004609730245072784\n",
      "Episode 19582; Testing Loss 0.00576482264985012; Training Loss 0.004609721226316223\n",
      "Episode 19583; Testing Loss 0.005764973054878987; Training Loss 0.004609713471626196\n",
      "Episode 19584; Testing Loss 0.005764973109900543; Training Loss 0.0046097065880930455\n",
      "Episode 19585; Testing Loss 0.005764820539673368; Training Loss 0.004609698431777678\n",
      "Episode 19586; Testing Loss 0.00576483477588458; Training Loss 0.0046096901075453525\n",
      "Episode 19587; Testing Loss 0.005764927567159191; Training Loss 0.004609684674438528\n",
      "Episode 19588; Testing Loss 0.005764793677160122; Training Loss 0.004609675636570228\n",
      "Episode 19589; Testing Loss 0.005764644062658999; Training Loss 0.0046096687838183764\n",
      "Episode 19590; Testing Loss 0.005764784193374665; Training Loss 0.004609659144111727\n",
      "Episode 19591; Testing Loss 0.005764933775333383; Training Loss 0.004609652987372259\n",
      "Episode 19592; Testing Loss 0.005764811151872232; Training Loss 0.0046096442090200265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19593; Testing Loss 0.005764699105643117; Training Loss 0.004609636664254301\n",
      "Episode 19594; Testing Loss 0.005764723682997529; Training Loss 0.004609628755211869\n",
      "Episode 19595; Testing Loss 0.005764807504287798; Training Loss 0.004609621374407779\n",
      "Episode 19596; Testing Loss 0.005764783793138425; Training Loss 0.0046096131891954165\n",
      "Episode 19597; Testing Loss 0.005764641335230632; Training Loss 0.004609605959494897\n",
      "Episode 19598; Testing Loss 0.005764701656016822; Training Loss 0.0046095964922115585\n",
      "Episode 19599; Testing Loss 0.005764852352040265; Training Loss 0.004609589659274263\n",
      "Episode 19600; Testing Loss 0.005764895040773814; Training Loss 0.004609582869013385\n",
      "Episode 19601; Testing Loss 0.0057646501225690865; Training Loss 0.004609575445926612\n",
      "Episode 19602; Testing Loss 0.005764648748405577; Training Loss 0.004609566274535088\n",
      "Episode 19603; Testing Loss 0.005764790819821421; Training Loss 0.0046095590514136685\n",
      "Episode 19604; Testing Loss 0.00576470152216576; Training Loss 0.004609550300445124\n",
      "Episode 19605; Testing Loss 0.005764590924166164; Training Loss 0.004609542499122328\n",
      "Episode 19606; Testing Loss 0.005764600252495643; Training Loss 0.004609534785922379\n",
      "Episode 19607; Testing Loss 0.0057647119503583255; Training Loss 0.004609526919609987\n",
      "Episode 19608; Testing Loss 0.005764703850398328; Training Loss 0.004609518811060724\n",
      "Episode 19609; Testing Loss 0.0057646000409516005; Training Loss 0.0046095113591524475\n",
      "Episode 19610; Testing Loss 0.00576465429164408; Training Loss 0.004609503934775686\n",
      "Episode 19611; Testing Loss 0.005764645920171757; Training Loss 0.004609496982596445\n",
      "Episode 19612; Testing Loss 0.005764623340799923; Training Loss 0.004609488512274036\n",
      "Episode 19613; Testing Loss 0.005764526868217729; Training Loss 0.004609480967317074\n",
      "Episode 19614; Testing Loss 0.0057646089009748885; Training Loss 0.004609472722499916\n",
      "Episode 19615; Testing Loss 0.005764714125503414; Training Loss 0.004609465688607299\n",
      "Episode 19616; Testing Loss 0.005764626927184052; Training Loss 0.004609456918234609\n",
      "Episode 19617; Testing Loss 0.005764524213981738; Training Loss 0.004609449480194323\n",
      "Episode 19618; Testing Loss 0.005764537105048923; Training Loss 0.004609443086164812\n",
      "Episode 19619; Testing Loss 0.005764504135747387; Training Loss 0.004609434928977086\n",
      "Episode 19620; Testing Loss 0.005764461920305335; Training Loss 0.004609426683163699\n",
      "Episode 19621; Testing Loss 0.005764545635613835; Training Loss 0.00460941858080402\n",
      "Episode 19622; Testing Loss 0.0057645373393194355; Training Loss 0.004609411063215607\n",
      "Episode 19623; Testing Loss 0.005764424179981834; Training Loss 0.004609403762086561\n",
      "Episode 19624; Testing Loss 0.0057645313215846155; Training Loss 0.004609397291546225\n",
      "Episode 19625; Testing Loss 0.005764477089277623; Training Loss 0.004609389765540886\n",
      "Episode 19626; Testing Loss 0.005764336686459716; Training Loss 0.00460938233179736\n",
      "Episode 19627; Testing Loss 0.005764435797032957; Training Loss 0.004609372205103495\n",
      "Episode 19628; Testing Loss 0.00576462076143093; Training Loss 0.004609366535034385\n",
      "Episode 19629; Testing Loss 0.005764575243500605; Training Loss 0.004609357521089261\n",
      "Episode 19630; Testing Loss 0.00576434753014917; Training Loss 0.0046093503757987705\n",
      "Episode 19631; Testing Loss 0.0057643753625468445; Training Loss 0.0046093422416500895\n",
      "Episode 19632; Testing Loss 0.005764590511116721; Training Loss 0.004609334145550344\n",
      "Episode 19633; Testing Loss 0.005764655243158419; Training Loss 0.004609327297644868\n",
      "Episode 19634; Testing Loss 0.005764439489188959; Training Loss 0.004609317175696001\n",
      "Episode 19635; Testing Loss 0.005764270881742156; Training Loss 0.0046093118593057295\n",
      "Episode 19636; Testing Loss 0.005764317423947415; Training Loss 0.00460930374044099\n",
      "Episode 19637; Testing Loss 0.005764426383762158; Training Loss 0.004609295266173244\n",
      "Episode 19638; Testing Loss 0.005764473647413342; Training Loss 0.004609288207208606\n",
      "Episode 19639; Testing Loss 0.005764411137216437; Training Loss 0.0046092799044997355\n",
      "Episode 19640; Testing Loss 0.005764351138087795; Training Loss 0.004609271986987089\n",
      "Episode 19641; Testing Loss 0.005764474384627246; Training Loss 0.004609264449756146\n",
      "Episode 19642; Testing Loss 0.005764456342717905; Training Loss 0.004609255755985948\n",
      "Episode 19643; Testing Loss 0.005764304546873156; Training Loss 0.00460925207577596\n",
      "Episode 19644; Testing Loss 0.005764369271644098; Training Loss 0.004609241576394785\n",
      "Episode 19645; Testing Loss 0.005764404267258332; Training Loss 0.004609236690564296\n",
      "Episode 19646; Testing Loss 0.0057642672115412585; Training Loss 0.004609230157331399\n",
      "Episode 19647; Testing Loss 0.005764179836159817; Training Loss 0.004609223980936109\n",
      "Episode 19648; Testing Loss 0.00576428605897617; Training Loss 0.004609212858905668\n",
      "Episode 19649; Testing Loss 0.005764385935326362; Training Loss 0.004609203423704573\n",
      "Episode 19650; Testing Loss 0.005764249917468639; Training Loss 0.004609199487841825\n",
      "Episode 19651; Testing Loss 0.005764193073539041; Training Loss 0.004609191267119477\n",
      "Episode 19652; Testing Loss 0.005764294550614071; Training Loss 0.004609180009634704\n",
      "Episode 19653; Testing Loss 0.005764322325855574; Training Loss 0.004609174080451137\n",
      "Episode 19654; Testing Loss 0.005764208876144848; Training Loss 0.004609167275549858\n",
      "Episode 19655; Testing Loss 0.005764189583659653; Training Loss 0.004609157524598029\n",
      "Episode 19656; Testing Loss 0.005764238631369409; Training Loss 0.00460915015076851\n",
      "Episode 19657; Testing Loss 0.005764286825920671; Training Loss 0.0046091439502723925\n",
      "Episode 19658; Testing Loss 0.005764222682077379; Training Loss 0.0046091349603052814\n",
      "Episode 19659; Testing Loss 0.005764139811333162; Training Loss 0.004609125990216104\n",
      "Episode 19660; Testing Loss 0.005764164480351208; Training Loss 0.004609118995583389\n",
      "Episode 19661; Testing Loss 0.00576420942130436; Training Loss 0.00460911162428511\n",
      "Episode 19662; Testing Loss 0.005764293049118283; Training Loss 0.004609102262381931\n",
      "Episode 19663; Testing Loss 0.005764318094777887; Training Loss 0.004609094944016912\n",
      "Episode 19664; Testing Loss 0.005764215772449247; Training Loss 0.004609086494528243\n",
      "Episode 19665; Testing Loss 0.005764099913438702; Training Loss 0.0046090801416698406\n",
      "Episode 19666; Testing Loss 0.005764102508674544; Training Loss 0.004609071995156675\n",
      "Episode 19667; Testing Loss 0.0057642435943511755; Training Loss 0.004609063048966817\n",
      "Episode 19668; Testing Loss 0.005764299011746735; Training Loss 0.00460905654557853\n",
      "Episode 19669; Testing Loss 0.005764179080345768; Training Loss 0.00460904948674757\n",
      "Episode 19670; Testing Loss 0.0057640240744753135; Training Loss 0.0046090405683433654\n",
      "Episode 19671; Testing Loss 0.005764116323058689; Training Loss 0.004609033367420566\n",
      "Episode 19672; Testing Loss 0.00576411645667888; Training Loss 0.004609025994757329\n",
      "Episode 19673; Testing Loss 0.005764031801161356; Training Loss 0.0046090181321026\n",
      "Episode 19674; Testing Loss 0.005764001304181689; Training Loss 0.0046090105462003265\n",
      "Episode 19675; Testing Loss 0.00576398073369585; Training Loss 0.004609002148741937\n",
      "Episode 19676; Testing Loss 0.0057639858861337386; Training Loss 0.004608995194747895\n",
      "Episode 19677; Testing Loss 0.005764083638658512; Training Loss 0.004608986771977735\n",
      "Episode 19678; Testing Loss 0.00576411623809994; Training Loss 0.004608979388017953\n",
      "Episode 19679; Testing Loss 0.005764044907270428; Training Loss 0.004608970319691492\n",
      "Episode 19680; Testing Loss 0.005763922819005899; Training Loss 0.004608963438323549\n",
      "Episode 19681; Testing Loss 0.005763946117435102; Training Loss 0.0046089559999245005\n",
      "Episode 19682; Testing Loss 0.005763950938767724; Training Loss 0.0046089479876292834\n",
      "Episode 19683; Testing Loss 0.005763930613063558; Training Loss 0.004608940754247162\n",
      "Episode 19684; Testing Loss 0.005764025489348878; Training Loss 0.004608932768987469\n",
      "Episode 19685; Testing Loss 0.005764045123114282; Training Loss 0.004608925057927808\n",
      "Episode 19686; Testing Loss 0.005764024172242711; Training Loss 0.004608917695476398\n",
      "Episode 19687; Testing Loss 0.0057639339948450195; Training Loss 0.004608909718155207\n",
      "Episode 19688; Testing Loss 0.005763892868414072; Training Loss 0.004608902647135554\n",
      "Episode 19689; Testing Loss 0.005763936721577343; Training Loss 0.004608895036113033\n",
      "Episode 19690; Testing Loss 0.005763942669410029; Training Loss 0.004608886836570483\n",
      "Episode 19691; Testing Loss 0.0057639002956865444; Training Loss 0.004608877674491604\n",
      "Episode 19692; Testing Loss 0.0057639389926094345; Training Loss 0.004608872120501987\n",
      "Episode 19693; Testing Loss 0.005763988248915203; Training Loss 0.004608865287836042\n",
      "Episode 19694; Testing Loss 0.0057639372855198; Training Loss 0.004608855625029907\n",
      "Episode 19695; Testing Loss 0.005763830056222586; Training Loss 0.004608848939014229\n",
      "Episode 19696; Testing Loss 0.005763832706438761; Training Loss 0.004608841709620817\n",
      "Episode 19697; Testing Loss 0.005763881573008947; Training Loss 0.004608833013349077\n",
      "Episode 19698; Testing Loss 0.00576388520973636; Training Loss 0.004608826886240052\n",
      "Episode 19699; Testing Loss 0.00576379579549004; Training Loss 0.004608818775576772\n",
      "Episode 19700; Testing Loss 0.005763752202236218; Training Loss 0.00460880983423386\n",
      "Episode 19701; Testing Loss 0.00576392780479924; Training Loss 0.004608802438445297\n",
      "Episode 19702; Testing Loss 0.005764014600864; Training Loss 0.0046087957513492985\n",
      "Episode 19703; Testing Loss 0.005763784666790275; Training Loss 0.004608786012489102\n",
      "Episode 19704; Testing Loss 0.005763562226203429; Training Loss 0.004608780224102693\n",
      "Episode 19705; Testing Loss 0.005763671306327306; Training Loss 0.004608771649205363\n",
      "Episode 19706; Testing Loss 0.005763919441373032; Training Loss 0.00460876514340036\n",
      "Episode 19707; Testing Loss 0.005763851838935898; Training Loss 0.004608756967197987\n",
      "Episode 19708; Testing Loss 0.005763702402387898; Training Loss 0.0046087494973809425\n",
      "Episode 19709; Testing Loss 0.005763718639486747; Training Loss 0.004608740370046576\n",
      "Episode 19710; Testing Loss 0.005763758339224857; Training Loss 0.004608732777475051\n",
      "Episode 19711; Testing Loss 0.005763724533669624; Training Loss 0.00460872620736755\n",
      "Episode 19712; Testing Loss 0.005763727717687225; Training Loss 0.004608718751816488\n",
      "Episode 19713; Testing Loss 0.005763794042145248; Training Loss 0.004608709910635444\n",
      "Episode 19714; Testing Loss 0.0057637283585309645; Training Loss 0.0046087030210065615\n",
      "Episode 19715; Testing Loss 0.005763690098943869; Training Loss 0.004608694545636955\n",
      "Episode 19716; Testing Loss 0.005763794665043468; Training Loss 0.0046086864887656485\n",
      "Episode 19717; Testing Loss 0.005763727635050036; Training Loss 0.004608679108345299\n",
      "Episode 19718; Testing Loss 0.005763661612907619; Training Loss 0.004608671741974778\n",
      "Episode 19719; Testing Loss 0.005763702412354376; Training Loss 0.004608663787304624\n",
      "Episode 19720; Testing Loss 0.005763702465470582; Training Loss 0.004608656778766646\n",
      "Episode 19721; Testing Loss 0.005763675439367983; Training Loss 0.004608647453122145\n",
      "Episode 19722; Testing Loss 0.005763652695779093; Training Loss 0.0046086402304677376\n",
      "Episode 19723; Testing Loss 0.005763580000980194; Training Loss 0.004608632629353091\n",
      "Episode 19724; Testing Loss 0.0057635430388464975; Training Loss 0.0046086259916381465\n",
      "Episode 19725; Testing Loss 0.00576351204834163; Training Loss 0.00460861822430584\n",
      "Episode 19726; Testing Loss 0.00576357788250716; Training Loss 0.004608608625599476\n",
      "Episode 19727; Testing Loss 0.005763705717099742; Training Loss 0.004608602063780199\n",
      "Episode 19728; Testing Loss 0.005763648507061984; Training Loss 0.004608594871298405\n",
      "Episode 19729; Testing Loss 0.005763597434272639; Training Loss 0.004608586390653916\n",
      "Episode 19730; Testing Loss 0.0057635549446429495; Training Loss 0.004608578390941545\n",
      "Episode 19731; Testing Loss 0.005763525770215146; Training Loss 0.004608571461676891\n",
      "Episode 19732; Testing Loss 0.005763505094611224; Training Loss 0.0046085636170919856\n",
      "Episode 19733; Testing Loss 0.005763551714862145; Training Loss 0.004608555152145568\n",
      "Episode 19734; Testing Loss 0.005763577411324627; Training Loss 0.004608547917560496\n",
      "Episode 19735; Testing Loss 0.005763608024076126; Training Loss 0.004608539940682086\n",
      "Episode 19736; Testing Loss 0.005763509653003787; Training Loss 0.004608532155109329\n",
      "Episode 19737; Testing Loss 0.0057635209081623485; Training Loss 0.004608524505767847\n",
      "Episode 19738; Testing Loss 0.005763453134077762; Training Loss 0.00460851747068537\n",
      "Episode 19739; Testing Loss 0.005763476726169808; Training Loss 0.004608508926205498\n",
      "Episode 19740; Testing Loss 0.0057635082877631125; Training Loss 0.004608501888736808\n",
      "Episode 19741; Testing Loss 0.00576349181176445; Training Loss 0.004608494143554944\n",
      "Episode 19742; Testing Loss 0.005763475067102492; Training Loss 0.0046084864989260396\n",
      "Episode 19743; Testing Loss 0.005763522096409053; Training Loss 0.004608478818008423\n",
      "Episode 19744; Testing Loss 0.005763540510236763; Training Loss 0.00460847154920839\n",
      "Episode 19745; Testing Loss 0.005763447152108339; Training Loss 0.004608464337584283\n",
      "Episode 19746; Testing Loss 0.005763391457044991; Training Loss 0.0046084559975039475\n",
      "Episode 19747; Testing Loss 0.005763426178662262; Training Loss 0.004608447970663154\n",
      "Episode 19748; Testing Loss 0.005763578770342015; Training Loss 0.004608440817685661\n",
      "Episode 19749; Testing Loss 0.00576351837971662; Training Loss 0.004608432555085821\n",
      "Episode 19750; Testing Loss 0.005763322996986934; Training Loss 0.004608425346595116\n",
      "Episode 19751; Testing Loss 0.005763402387112017; Training Loss 0.004608417393880818\n",
      "Episode 19752; Testing Loss 0.005763431936513756; Training Loss 0.004608409813298583\n",
      "Episode 19753; Testing Loss 0.005763418772026353; Training Loss 0.004608401297225443\n",
      "Episode 19754; Testing Loss 0.0057634290450364715; Training Loss 0.0046083945476085\n",
      "Episode 19755; Testing Loss 0.005763364738228372; Training Loss 0.004608387798269152\n",
      "Episode 19756; Testing Loss 0.005763365324710385; Training Loss 0.004608379454246062\n",
      "Episode 19757; Testing Loss 0.005763460460702729; Training Loss 0.004608371542217246\n",
      "Episode 19758; Testing Loss 0.005763392607700732; Training Loss 0.004608363404051545\n",
      "Episode 19759; Testing Loss 0.0057632795675399305; Training Loss 0.004608357240024911\n",
      "Episode 19760; Testing Loss 0.005763190442249222; Training Loss 0.004608349943959726\n",
      "Episode 19761; Testing Loss 0.0057633134005247244; Training Loss 0.004608340084666707\n",
      "Episode 19762; Testing Loss 0.00576348332672289; Training Loss 0.00460833472008667\n",
      "Episode 19763; Testing Loss 0.0057634364420345655; Training Loss 0.004608324822649089\n",
      "Episode 19764; Testing Loss 0.005763256777315454; Training Loss 0.0046083181119887726\n",
      "Episode 19765; Testing Loss 0.00576317343941424; Training Loss 0.004608310871871204\n",
      "Episode 19766; Testing Loss 0.0057632838730467964; Training Loss 0.004608300972450447\n",
      "Episode 19767; Testing Loss 0.005763435132250377; Training Loss 0.004608296508553187\n",
      "Episode 19768; Testing Loss 0.005763489064662998; Training Loss 0.00460828777532915\n",
      "Episode 19769; Testing Loss 0.005763251342736659; Training Loss 0.004608279614788191\n",
      "Episode 19770; Testing Loss 0.005763087804885388; Training Loss 0.00460827501795481\n",
      "Episode 19771; Testing Loss 0.005763297038683273; Training Loss 0.00460826424989837\n",
      "Episode 19772; Testing Loss 0.005763389711880112; Training Loss 0.004608256937811396\n",
      "Episode 19773; Testing Loss 0.005763236822902445; Training Loss 0.0046082490135231965\n",
      "Episode 19774; Testing Loss 0.0057631596511367525; Training Loss 0.004608242554857394\n",
      "Episode 19775; Testing Loss 0.0057633138565600815; Training Loss 0.0046082324636407875\n",
      "Episode 19776; Testing Loss 0.005763296860654799; Training Loss 0.00460822552391412\n",
      "Episode 19777; Testing Loss 0.005763042018162011; Training Loss 0.004608219578958207\n",
      "Episode 19778; Testing Loss 0.005763068051218763; Training Loss 0.004608211220892893\n",
      "Episode 19779; Testing Loss 0.00576327270982878; Training Loss 0.0046082034971905335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19780; Testing Loss 0.0057632208480812; Training Loss 0.004608195618623248\n",
      "Episode 19781; Testing Loss 0.005762994900264088; Training Loss 0.004608190038784515\n",
      "Episode 19782; Testing Loss 0.005763061266326924; Training Loss 0.004608180433667397\n",
      "Episode 19783; Testing Loss 0.005763326950846667; Training Loss 0.004608173584537792\n",
      "Episode 19784; Testing Loss 0.005763219693366988; Training Loss 0.004608165418792322\n",
      "Episode 19785; Testing Loss 0.005762977769193237; Training Loss 0.004608159158190637\n",
      "Episode 19786; Testing Loss 0.005763031667599243; Training Loss 0.004608149338701135\n",
      "Episode 19787; Testing Loss 0.005763279310378282; Training Loss 0.004608143116791921\n",
      "Episode 19788; Testing Loss 0.005763178838449022; Training Loss 0.004608134658549557\n",
      "Episode 19789; Testing Loss 0.005762979106570467; Training Loss 0.004608128922996469\n",
      "Episode 19790; Testing Loss 0.005763111651184823; Training Loss 0.0046081178801034494\n",
      "Episode 19791; Testing Loss 0.0057632654779644545; Training Loss 0.004608113310941666\n",
      "Episode 19792; Testing Loss 0.005763036612484806; Training Loss 0.004608103556389852\n",
      "Episode 19793; Testing Loss 0.00576284098234747; Training Loss 0.004608098174539969\n",
      "Episode 19794; Testing Loss 0.005763024756469456; Training Loss 0.004608090065687812\n",
      "Episode 19795; Testing Loss 0.005763180052777201; Training Loss 0.0046080834218875525\n",
      "Episode 19796; Testing Loss 0.005763031272336482; Training Loss 0.004608071709798171\n",
      "Episode 19797; Testing Loss 0.005762923694858885; Training Loss 0.004608067282080974\n",
      "Episode 19798; Testing Loss 0.0057631854069611224; Training Loss 0.004608058833527132\n",
      "Episode 19799; Testing Loss 0.005763302912210989; Training Loss 0.004608053614505585\n",
      "Episode 19800; Testing Loss 0.005763015336393836; Training Loss 0.004608042167178778\n",
      "Episode 19801; Testing Loss 0.005762791250135302; Training Loss 0.004608036292150718\n",
      "Episode 19802; Testing Loss 0.005762976863739042; Training Loss 0.0046080265494477315\n",
      "Episode 19803; Testing Loss 0.00576313009343403; Training Loss 0.004608022658436144\n",
      "Episode 19804; Testing Loss 0.0057628628545885775; Training Loss 0.004608011639266971\n",
      "Episode 19805; Testing Loss 0.005762704776302355; Training Loss 0.00460800653309072\n",
      "Episode 19806; Testing Loss 0.00576294446710036; Training Loss 0.004607995135644523\n",
      "Episode 19807; Testing Loss 0.0057631698962781796; Training Loss 0.004607988921906025\n",
      "Episode 19808; Testing Loss 0.005763042172802111; Training Loss 0.004607980931563901\n",
      "Episode 19809; Testing Loss 0.005762770859884013; Training Loss 0.004607973438071413\n",
      "Episode 19810; Testing Loss 0.00576281125813775; Training Loss 0.004607965163054405\n",
      "Episode 19811; Testing Loss 0.005763027305372164; Training Loss 0.00460795822987378\n",
      "Episode 19812; Testing Loss 0.005763020705425207; Training Loss 0.004607949847267557\n",
      "Episode 19813; Testing Loss 0.00576285068824898; Training Loss 0.004607940995651937\n",
      "Episode 19814; Testing Loss 0.005762818720839603; Training Loss 0.004607935497349644\n",
      "Episode 19815; Testing Loss 0.005763000229492557; Training Loss 0.004607927285780647\n",
      "Episode 19816; Testing Loss 0.005763046652462567; Training Loss 0.004607919294331878\n",
      "Episode 19817; Testing Loss 0.005762882178465833; Training Loss 0.004607911451577224\n",
      "Episode 19818; Testing Loss 0.005762864735643097; Training Loss 0.004607903863198937\n",
      "Episode 19819; Testing Loss 0.005762967952675951; Training Loss 0.004607895269318549\n",
      "Episode 19820; Testing Loss 0.005762971609293489; Training Loss 0.004607889258961297\n",
      "Episode 19821; Testing Loss 0.005762746427202457; Training Loss 0.004607881260723526\n",
      "Episode 19822; Testing Loss 0.005762650270107183; Training Loss 0.004607873700974494\n",
      "Episode 19823; Testing Loss 0.005762792377166045; Training Loss 0.004607864988717923\n",
      "Episode 19824; Testing Loss 0.005762906566412631; Training Loss 0.004607859292667126\n",
      "Episode 19825; Testing Loss 0.005762868061463912; Training Loss 0.004607850210752278\n",
      "Episode 19826; Testing Loss 0.005762753678940955; Training Loss 0.004607841889351749\n",
      "Episode 19827; Testing Loss 0.005762773354967992; Training Loss 0.004607833911279077\n",
      "Episode 19828; Testing Loss 0.0057628344837470005; Training Loss 0.004607828054488694\n",
      "Episode 19829; Testing Loss 0.005762728112664745; Training Loss 0.004607818411027947\n",
      "Episode 19830; Testing Loss 0.005762625790935809; Training Loss 0.004607813529572804\n",
      "Episode 19831; Testing Loss 0.005762780465689348; Training Loss 0.0046078057944800525\n",
      "Episode 19832; Testing Loss 0.00576298200119365; Training Loss 0.0046077977666328755\n",
      "Episode 19833; Testing Loss 0.005762795565671684; Training Loss 0.004607788455946604\n",
      "Episode 19834; Testing Loss 0.005762627962225946; Training Loss 0.004607781912920554\n",
      "Episode 19835; Testing Loss 0.005762746572881424; Training Loss 0.004607772061144646\n",
      "Episode 19836; Testing Loss 0.005762834108177021; Training Loss 0.00460776642171716\n",
      "Episode 19837; Testing Loss 0.005762657696822966; Training Loss 0.004607757526903005\n",
      "Episode 19838; Testing Loss 0.005762552074345639; Training Loss 0.004607749178011172\n",
      "Episode 19839; Testing Loss 0.005762639194721725; Training Loss 0.004607743456498248\n",
      "Episode 19840; Testing Loss 0.005762737744531644; Training Loss 0.0046077357364700715\n",
      "Episode 19841; Testing Loss 0.005762683480684657; Training Loss 0.004607726297016804\n",
      "Episode 19842; Testing Loss 0.005762594874899918; Training Loss 0.004607719435591723\n",
      "Episode 19843; Testing Loss 0.005762644589066012; Training Loss 0.0046077121626435305\n",
      "Episode 19844; Testing Loss 0.005762752200916912; Training Loss 0.004607703679113746\n",
      "Episode 19845; Testing Loss 0.005762733029885658; Training Loss 0.004607695740479222\n",
      "Episode 19846; Testing Loss 0.005762648245821561; Training Loss 0.004607689302175223\n",
      "Episode 19847; Testing Loss 0.005762660787170271; Training Loss 0.004607680556615895\n",
      "Episode 19848; Testing Loss 0.005762761011630826; Training Loss 0.00460767301597001\n",
      "Episode 19849; Testing Loss 0.005762659391121497; Training Loss 0.004607664337530708\n",
      "Episode 19850; Testing Loss 0.005762524092890073; Training Loss 0.0046076569688812494\n",
      "Episode 19851; Testing Loss 0.005762565192194383; Training Loss 0.0046076487340134635\n",
      "Episode 19852; Testing Loss 0.005762664451658837; Training Loss 0.0046076404847444\n",
      "Episode 19853; Testing Loss 0.005762750076848979; Training Loss 0.004607634040722317\n",
      "Episode 19854; Testing Loss 0.005762586836016489; Training Loss 0.0046076257464159566\n",
      "Episode 19855; Testing Loss 0.005762571906767083; Training Loss 0.004607618018753381\n",
      "Episode 19856; Testing Loss 0.00576263079019506; Training Loss 0.004607610607089\n",
      "Episode 19857; Testing Loss 0.005762568913840487; Training Loss 0.004607602697222088\n",
      "Episode 19858; Testing Loss 0.005762478823789951; Training Loss 0.004607596876306494\n",
      "Episode 19859; Testing Loss 0.005762599808202087; Training Loss 0.00460758796133896\n",
      "Episode 19860; Testing Loss 0.005762673035598694; Training Loss 0.00460758183111184\n",
      "Episode 19861; Testing Loss 0.0057624971263199815; Training Loss 0.004607572713567861\n",
      "Episode 19862; Testing Loss 0.005762387044138583; Training Loss 0.004607566199148172\n",
      "Episode 19863; Testing Loss 0.005762439024896004; Training Loss 0.004607558764584666\n",
      "Episode 19864; Testing Loss 0.005762491655026541; Training Loss 0.004607549226150007\n",
      "Episode 19865; Testing Loss 0.005762534844156703; Training Loss 0.004607541387422616\n",
      "Episode 19866; Testing Loss 0.005762659288061032; Training Loss 0.004607534419710992\n",
      "Episode 19867; Testing Loss 0.0057626000013813045; Training Loss 0.004607526387957073\n",
      "Episode 19868; Testing Loss 0.005762394968579899; Training Loss 0.004607518720389624\n",
      "Episode 19869; Testing Loss 0.005762337120129723; Training Loss 0.004607511065209652\n",
      "Episode 19870; Testing Loss 0.005762444908401666; Training Loss 0.004607502253357799\n",
      "Episode 19871; Testing Loss 0.00576254547450448; Training Loss 0.0046074955688148785\n",
      "Episode 19872; Testing Loss 0.005762502934202651; Training Loss 0.004607487344804513\n",
      "Episode 19873; Testing Loss 0.005762399995176646; Training Loss 0.004607480447635981\n",
      "Episode 19874; Testing Loss 0.005762284024529208; Training Loss 0.004607473772642326\n",
      "Episode 19875; Testing Loss 0.005762384218091338; Training Loss 0.004607464832429404\n",
      "Episode 19876; Testing Loss 0.005762518526747951; Training Loss 0.004607457091490253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19877; Testing Loss 0.005762536343549219; Training Loss 0.004607449607969344\n",
      "Episode 19878; Testing Loss 0.005762481317487106; Training Loss 0.004607442736144157\n",
      "Episode 19879; Testing Loss 0.005762495462091784; Training Loss 0.004607434335786533\n",
      "Episode 19880; Testing Loss 0.005762448451212312; Training Loss 0.004607425682202903\n",
      "Episode 19881; Testing Loss 0.005762345121667514; Training Loss 0.004607418319617837\n",
      "Episode 19882; Testing Loss 0.00576224261013981; Training Loss 0.004607411921359367\n",
      "Episode 19883; Testing Loss 0.005762355358486821; Training Loss 0.004607402862800095\n",
      "Episode 19884; Testing Loss 0.005762449866620987; Training Loss 0.004607396473506419\n",
      "Episode 19885; Testing Loss 0.005762317760196584; Training Loss 0.004607389126610485\n",
      "Episode 19886; Testing Loss 0.0057623053282940045; Training Loss 0.00460738043833802\n",
      "Episode 19887; Testing Loss 0.005762442943426698; Training Loss 0.0046073729790421306\n",
      "Episode 19888; Testing Loss 0.0057623466273297504; Training Loss 0.004607365114041621\n",
      "Episode 19889; Testing Loss 0.0057622025957580385; Training Loss 0.004607358158816418\n",
      "Episode 19890; Testing Loss 0.005762268102638863; Training Loss 0.004607349104512731\n",
      "Episode 19891; Testing Loss 0.0057624245017502945; Training Loss 0.004607342236733202\n",
      "Episode 19892; Testing Loss 0.005762355791696604; Training Loss 0.004607335138684178\n",
      "Episode 19893; Testing Loss 0.00576224921310504; Training Loss 0.004607328053110324\n",
      "Episode 19894; Testing Loss 0.0057623474505644695; Training Loss 0.004607318319408284\n",
      "Episode 19895; Testing Loss 0.005762397135589784; Training Loss 0.004607312427448218\n",
      "Episode 19896; Testing Loss 0.005762182479117393; Training Loss 0.004607303469709454\n",
      "Episode 19897; Testing Loss 0.0057621431077802335; Training Loss 0.004607298987533319\n",
      "Episode 19898; Testing Loss 0.005762460517523464; Training Loss 0.004607290671184128\n",
      "Episode 19899; Testing Loss 0.005762416563928775; Training Loss 0.0046072823335868655\n",
      "Episode 19900; Testing Loss 0.005762105713584159; Training Loss 0.004607275513705363\n",
      "Episode 19901; Testing Loss 0.005762146225717058; Training Loss 0.004607266402987712\n",
      "Episode 19902; Testing Loss 0.005762385537477144; Training Loss 0.0046072596304716175\n",
      "Episode 19903; Testing Loss 0.005762336321920489; Training Loss 0.004607250308538102\n",
      "Episode 19904; Testing Loss 0.005762099513916699; Training Loss 0.004607245388659646\n",
      "Episode 19905; Testing Loss 0.005762207035593672; Training Loss 0.004607234011548888\n",
      "Episode 19906; Testing Loss 0.005762380104353438; Training Loss 0.00460723063826872\n",
      "Episode 19907; Testing Loss 0.005762197856280504; Training Loss 0.004607221824962771\n",
      "Episode 19908; Testing Loss 0.005762016285266706; Training Loss 0.004607213913980889\n",
      "Episode 19909; Testing Loss 0.005762117383450711; Training Loss 0.004607203776894006\n",
      "Episode 19910; Testing Loss 0.005762300126357138; Training Loss 0.004607198455816979\n",
      "Episode 19911; Testing Loss 0.0057622733251947; Training Loss 0.004607191019985185\n",
      "Episode 19912; Testing Loss 0.00576221198730944; Training Loss 0.0046071825799241534\n",
      "Episode 19913; Testing Loss 0.005762191558013858; Training Loss 0.004607174712420296\n",
      "Episode 19914; Testing Loss 0.005762135470817789; Training Loss 0.0046071682167690585\n",
      "Episode 19915; Testing Loss 0.005762015137215038; Training Loss 0.0046071606080620425\n",
      "Episode 19916; Testing Loss 0.005761965300939017; Training Loss 0.00460715184187838\n",
      "Episode 19917; Testing Loss 0.005762049609242622; Training Loss 0.004607144365452588\n",
      "Episode 19918; Testing Loss 0.0057621559182998395; Training Loss 0.004607136033840264\n",
      "Episode 19919; Testing Loss 0.00576212788041507; Training Loss 0.004607128707843902\n",
      "Episode 19920; Testing Loss 0.005762035846294119; Training Loss 0.004607122564993715\n",
      "Episode 19921; Testing Loss 0.005762161098362862; Training Loss 0.004607112797230717\n",
      "Episode 19922; Testing Loss 0.00576228427178467; Training Loss 0.0046071069611857566\n",
      "Episode 19923; Testing Loss 0.0057621585749246815; Training Loss 0.004607098802317204\n",
      "Episode 19924; Testing Loss 0.005762018985960392; Training Loss 0.004607091198370542\n",
      "Episode 19925; Testing Loss 0.005762038776399808; Training Loss 0.004607082269110448\n",
      "Episode 19926; Testing Loss 0.005762138175032838; Training Loss 0.0046070766107656974\n",
      "Episode 19927; Testing Loss 0.005762059675433578; Training Loss 0.00460706796359131\n",
      "Episode 19928; Testing Loss 0.005761892024468543; Training Loss 0.004607059633759699\n",
      "Episode 19929; Testing Loss 0.005761947072547602; Training Loss 0.004607051353859929\n",
      "Episode 19930; Testing Loss 0.005762068055335446; Training Loss 0.004607043984014702\n",
      "Episode 19931; Testing Loss 0.005762038350857778; Training Loss 0.004607035943362107\n",
      "Episode 19932; Testing Loss 0.005761969328755863; Training Loss 0.0046070278632930565\n",
      "Episode 19933; Testing Loss 0.005761939130373705; Training Loss 0.00460702126282463\n",
      "Episode 19934; Testing Loss 0.005761943638274168; Training Loss 0.004607014411863514\n",
      "Episode 19935; Testing Loss 0.005761895380413774; Training Loss 0.004607006719862226\n",
      "Episode 19936; Testing Loss 0.005761911963213311; Training Loss 0.004606998627628336\n",
      "Episode 19937; Testing Loss 0.0057618911803335145; Training Loss 0.004606990849398241\n",
      "Episode 19938; Testing Loss 0.005761953641434197; Training Loss 0.0046069824257640385\n",
      "Episode 19939; Testing Loss 0.005762026688386713; Training Loss 0.004606976121366554\n",
      "Episode 19940; Testing Loss 0.0057620529302493855; Training Loss 0.0046069690363264755\n",
      "Episode 19941; Testing Loss 0.005762030016790708; Training Loss 0.004606960108487686\n",
      "Episode 19942; Testing Loss 0.0057618627870463035; Training Loss 0.004606952578330634\n",
      "Episode 19943; Testing Loss 0.005761770115970453; Training Loss 0.004606945930350077\n",
      "Episode 19944; Testing Loss 0.005761829359167471; Training Loss 0.004606939265216744\n",
      "Episode 19945; Testing Loss 0.005761816079445848; Training Loss 0.004606931702990791\n",
      "Episode 19946; Testing Loss 0.005761706815824224; Training Loss 0.004606923665666027\n",
      "Episode 19947; Testing Loss 0.005761779143080954; Training Loss 0.00460691518907896\n",
      "Episode 19948; Testing Loss 0.005761964789621593; Training Loss 0.004606906869803654\n",
      "Episode 19949; Testing Loss 0.005761906735054914; Training Loss 0.004606899597523325\n",
      "Episode 19950; Testing Loss 0.005761837867913364; Training Loss 0.00460689265948177\n",
      "Episode 19951; Testing Loss 0.005761892752597866; Training Loss 0.0046068845631977735\n",
      "Episode 19952; Testing Loss 0.005761839847411255; Training Loss 0.004606876979818224\n",
      "Episode 19953; Testing Loss 0.005761667087449969; Training Loss 0.004606869885069996\n",
      "Episode 19954; Testing Loss 0.005761706530676624; Training Loss 0.004606862369615486\n",
      "Episode 19955; Testing Loss 0.005761812851105807; Training Loss 0.004606854202830747\n",
      "Episode 19956; Testing Loss 0.00576172854350012; Training Loss 0.004606846023162853\n",
      "Episode 19957; Testing Loss 0.005761715160370244; Training Loss 0.004606838296041606\n",
      "Episode 19958; Testing Loss 0.005761765380005142; Training Loss 0.004606829690560647\n",
      "Episode 19959; Testing Loss 0.005761884335528645; Training Loss 0.004606823830206809\n",
      "Episode 19960; Testing Loss 0.005761879541304566; Training Loss 0.004606816245494577\n",
      "Episode 19961; Testing Loss 0.005761805516927125; Training Loss 0.004606807633035879\n",
      "Episode 19962; Testing Loss 0.005761769465935153; Training Loss 0.004606799764105095\n",
      "Episode 19963; Testing Loss 0.005761652440688458; Training Loss 0.004606792913177364\n",
      "Episode 19964; Testing Loss 0.005761679805542027; Training Loss 0.00460678551902132\n",
      "Episode 19965; Testing Loss 0.005761705580563294; Training Loss 0.004606777672977758\n",
      "Episode 19966; Testing Loss 0.005761632829895899; Training Loss 0.0046067703212765475\n",
      "Episode 19967; Testing Loss 0.005761696218964733; Training Loss 0.004606761770197871\n",
      "Episode 19968; Testing Loss 0.005761787856476032; Training Loss 0.004606753764902612\n",
      "Episode 19969; Testing Loss 0.005761740910576595; Training Loss 0.004606748100244124\n",
      "Episode 19970; Testing Loss 0.005761630618927258; Training Loss 0.004606740181026298\n",
      "Episode 19971; Testing Loss 0.005761592528111513; Training Loss 0.004606731678598356\n",
      "Episode 19972; Testing Loss 0.00576168997902457; Training Loss 0.00460672367863373\n",
      "Episode 19973; Testing Loss 0.005761719018180017; Training Loss 0.0046067158039709965\n",
      "Episode 19974; Testing Loss 0.005761686064081678; Training Loss 0.004606708765039949\n",
      "Episode 19975; Testing Loss 0.0057615740407317255; Training Loss 0.004606701271242546\n",
      "Episode 19976; Testing Loss 0.005761631256159049; Training Loss 0.00460669298729058\n",
      "Episode 19977; Testing Loss 0.005761703342441635; Training Loss 0.0046066863137454025\n",
      "Episode 19978; Testing Loss 0.005761689523230131; Training Loss 0.004606679219929134\n",
      "Episode 19979; Testing Loss 0.005761634196207429; Training Loss 0.004606670746595114\n",
      "Episode 19980; Testing Loss 0.005761644712077168; Training Loss 0.00460666285818944\n",
      "Episode 19981; Testing Loss 0.0057616196639613745; Training Loss 0.004606654469128166\n",
      "Episode 19982; Testing Loss 0.0057615792249680045; Training Loss 0.00460664758695002\n",
      "Episode 19983; Testing Loss 0.005761449200758258; Training Loss 0.0046066411243140416\n",
      "Episode 19984; Testing Loss 0.005761463006430491; Training Loss 0.0046066330695820204\n",
      "Episode 19985; Testing Loss 0.005761513591088891; Training Loss 0.0046066250138427035\n",
      "Episode 19986; Testing Loss 0.005761552004058274; Training Loss 0.004606616389055117\n",
      "Episode 19987; Testing Loss 0.005761501908784795; Training Loss 0.0046066105002141025\n",
      "Episode 19988; Testing Loss 0.005761571789250997; Training Loss 0.004606602315168381\n",
      "Episode 19989; Testing Loss 0.005761632973240599; Training Loss 0.004606594215773018\n",
      "Episode 19990; Testing Loss 0.005761517407404828; Training Loss 0.004606586106502068\n",
      "Episode 19991; Testing Loss 0.005761491838743925; Training Loss 0.004606579033656292\n",
      "Episode 19992; Testing Loss 0.005761545562660188; Training Loss 0.00460657081628231\n",
      "Episode 19993; Testing Loss 0.0057616054223807; Training Loss 0.004606563796512042\n",
      "Episode 19994; Testing Loss 0.00576163170676839; Training Loss 0.004606555790496309\n",
      "Episode 19995; Testing Loss 0.0057615138367613705; Training Loss 0.004606548030129157\n",
      "Episode 19996; Testing Loss 0.005761392744474705; Training Loss 0.004606541772387535\n",
      "Episode 19997; Testing Loss 0.005761336606320439; Training Loss 0.004606534250966401\n",
      "Episode 19998; Testing Loss 0.005761426281074739; Training Loss 0.004606525431992444\n",
      "Episode 19999; Testing Loss 0.005761515528853588; Training Loss 0.004606518710777438\n"
     ]
    }
   ],
   "source": [
    "# train on GPU\n",
    "nn.train_gpu(db.i[:5000,:], db.o[:5000,:], epo=20000, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(nn.ltst, \"k--\")\n",
    "ax.plot(nn.ltrn, \"k-\")\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate controller\n",
    "cont = mlp_controller(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(traj[:,0], traj[:,-1], \"k--\")\n",
    "ax.plot(traj[:,0], cont.predict(traj[:,1:5]), \"k-\")\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simulate with neural network control\n",
    "x0 = [0,0,np.pi,0]\n",
    "dyn  = dynamics(x0,[0,0,0,0], 0)\n",
    "t, x, u = dyn.propagate_controlled(9.8, cont)\n",
    "ax = plot_traj(x, arm=True, n=500)\n",
    "ax = plot_controls(t, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nn, open(\"nn.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = pickle.load(open(\"nn.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6146bb6f5635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mltst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"k--\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mltrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"k-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nnn' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(nnn.ltst, \"k--\")\n",
    "ax.plot(nnn.ltrn, \"k-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
